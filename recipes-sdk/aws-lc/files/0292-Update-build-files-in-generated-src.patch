From d0e4ea8b04a74ca5e955d34ffe85363ddd57f7b5 Mon Sep 17 00:00:00 2001
From: Nevine Ebeid <nebeid@amazon.com>
Date: Tue, 11 Apr 2023 12:20:13 -0400
Subject: [PATCH] Update build files in generated-src.

---
 .../crypto/fipsmodule/p256-armv8-asm.S        | 72 ++++++++++++-------
 .../crypto/fipsmodule/p256-armv8-asm.S        | 72 ++++++++++++-------
 .../crypto/fipsmodule/p256-armv8-asm.S        | 72 ++++++++++++-------
 3 files changed, 144 insertions(+), 72 deletions(-)

diff --git a/generated-src/ios-aarch64/crypto/fipsmodule/p256-armv8-asm.S b/generated-src/ios-aarch64/crypto/fipsmodule/p256-armv8-asm.S
index cd24809cd..b18602f9c 100644
--- a/generated-src/ios-aarch64/crypto/fipsmodule/p256-armv8-asm.S
+++ b/generated-src/ios-aarch64/crypto/fipsmodule/p256-armv8-asm.S
@@ -14,7 +14,7 @@
 #endif
 #include "openssl/arm_arch.h"
 
-.text
+.section	__TEXT,__const
 .align	5
 Lpoly:
 .quad	0xffffffffffffffff,0x00000000ffffffff,0x0000000000000000,0xffffffff00000001
@@ -30,6 +30,7 @@ LordK:
 .quad	0xccd1c8aaee00bc4f
 .byte	69,67,80,95,78,73,83,84,90,50,53,54,32,102,111,114,32,65,82,77,118,56,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
 .align	2
+.text
 
 // void	ecp_nistz256_mul_mont(BN_ULONG x0[4],const BN_ULONG x1[4],
 //					     const BN_ULONG x2[4]);
@@ -46,8 +47,10 @@ _ecp_nistz256_mul_mont:
 	ldr	x3,[x2]		// bp[0]
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly@PAGE
+	add	x13,x13,Lpoly@PAGEOFF
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_mul_mont
 
@@ -70,8 +73,10 @@ _ecp_nistz256_sqr_mont:
 
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly@PAGE
+	add	x13,x13,Lpoly@PAGEOFF
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_sqr_mont
 
@@ -93,8 +98,10 @@ _ecp_nistz256_div_by_2:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly@PAGE
+	add	x13,x13,Lpoly@PAGEOFF
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_div_by_2
 
@@ -115,8 +122,10 @@ _ecp_nistz256_mul_by_2:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly@PAGE
+	add	x13,x13,Lpoly@PAGEOFF
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 	mov	x8,x14
 	mov	x9,x15
 	mov	x10,x16
@@ -141,8 +150,10 @@ _ecp_nistz256_mul_by_3:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly@PAGE
+	add	x13,x13,Lpoly@PAGEOFF
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 	mov	x8,x14
 	mov	x9,x15
 	mov	x10,x16
@@ -179,8 +190,10 @@ _ecp_nistz256_sub:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly@PAGE
+	add	x13,x13,Lpoly@PAGEOFF
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_sub_from
 
@@ -204,8 +217,10 @@ _ecp_nistz256_neg:
 	mov	x15,xzr
 	mov	x16,xzr
 	mov	x17,xzr
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly@PAGE
+	add	x13,x13,Lpoly@PAGEOFF
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_sub_from
 
@@ -601,9 +616,11 @@ Ldouble_shortcut:
 	mov	x21,x0
 	ldp	x16,x17,[x1,#48]
 	mov	x22,x1
-	ldr	x12,Lpoly+8
+	adrp	x13,Lpoly@PAGE
+	add	x13,x13,Lpoly@PAGEOFF
+	ldr	x12,[x13,#8]
 	mov	x8,x14
-	ldr	x13,Lpoly+24
+	ldr	x13,[x13,#24]
 	mov	x9,x15
 	ldp	x4,x5,[x22,#64]	// forward load for p256_sqr_mont
 	mov	x10,x16
@@ -747,8 +764,10 @@ _ecp_nistz256_point_add:
 	mov	x21,x0
 	mov	x22,x1
 	mov	x23,x2
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly@PAGE
+	add	x13,x13,Lpoly@PAGEOFF
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 	orr	x8,x4,x5
 	orr	x10,x6,x7
 	orr	x25,x8,x10
@@ -999,8 +1018,10 @@ _ecp_nistz256_point_add_affine:
 	mov	x21,x0
 	mov	x22,x1
 	mov	x23,x2
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly@PAGE
+	add	x13,x13,Lpoly@PAGEOFF
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	ldp	x4,x5,[x1,#64]	// in1_z
 	ldp	x6,x7,[x1,#64+16]
@@ -1146,7 +1167,8 @@ _ecp_nistz256_point_add_affine:
 	ldp	x10,x11,[x23,#0+48]
 	stp	x14,x15,[x21,#0]
 	stp	x16,x17,[x21,#0+16]
-	adr	x23,Lone_mont-64
+	adrp	x23,Lone_mont@PAGE-64
+	add	x23,x23,Lone_mont@PAGEOFF-64
 	ldp	x14,x15,[x22,#32]	// in1
 	cmp	x24,#0			// ~, remember?
 	ldp	x16,x17,[x22,#32+16]
@@ -1205,7 +1227,8 @@ _ecp_nistz256_ord_mul_mont:
 	stp	x21,x22,[sp,#32]
 	stp	x23,x24,[sp,#48]
 
-	adr	x23,Lord
+	adrp	x23,Lord@PAGE
+	add	x23,x23,Lord@PAGEOFF
 	ldr	x3,[x2]		// bp[0]
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
@@ -1416,7 +1439,8 @@ _ecp_nistz256_ord_sqr_mont:
 	stp	x21,x22,[sp,#32]
 	stp	x23,x24,[sp,#48]
 
-	adr	x23,Lord
+	adrp	x23,Lord@PAGE
+	add	x23,x23,Lord@PAGEOFF
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
 
diff --git a/generated-src/linux-aarch64/crypto/fipsmodule/p256-armv8-asm.S b/generated-src/linux-aarch64/crypto/fipsmodule/p256-armv8-asm.S
index 780bfe578..accc37552 100644
--- a/generated-src/linux-aarch64/crypto/fipsmodule/p256-armv8-asm.S
+++ b/generated-src/linux-aarch64/crypto/fipsmodule/p256-armv8-asm.S
@@ -14,7 +14,7 @@
 #endif
 #include "openssl/arm_arch.h"
 
-.text
+.section	.rodata
 .align	5
 .Lpoly:
 .quad	0xffffffffffffffff,0x00000000ffffffff,0x0000000000000000,0xffffffff00000001
@@ -30,6 +30,7 @@
 .quad	0xccd1c8aaee00bc4f
 .byte	69,67,80,95,78,73,83,84,90,50,53,54,32,102,111,114,32,65,82,77,118,56,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
 .align	2
+.text
 
 // void	ecp_nistz256_mul_mont(BN_ULONG x0[4],const BN_ULONG x1[4],
 //					     const BN_ULONG x2[4]);
@@ -46,8 +47,10 @@ ecp_nistz256_mul_mont:
 	ldr	x3,[x2]		// bp[0]
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
-	ldr	x12,.Lpoly+8
-	ldr	x13,.Lpoly+24
+	adrp	x13,.Lpoly
+	add	x13,x13,:lo12:.Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_mul_mont
 
@@ -70,8 +73,10 @@ ecp_nistz256_sqr_mont:
 
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
-	ldr	x12,.Lpoly+8
-	ldr	x13,.Lpoly+24
+	adrp	x13,.Lpoly
+	add	x13,x13,:lo12:.Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_sqr_mont
 
@@ -93,8 +98,10 @@ ecp_nistz256_div_by_2:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,.Lpoly+8
-	ldr	x13,.Lpoly+24
+	adrp	x13,.Lpoly
+	add	x13,x13,:lo12:.Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_div_by_2
 
@@ -115,8 +122,10 @@ ecp_nistz256_mul_by_2:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,.Lpoly+8
-	ldr	x13,.Lpoly+24
+	adrp	x13,.Lpoly
+	add	x13,x13,:lo12:.Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 	mov	x8,x14
 	mov	x9,x15
 	mov	x10,x16
@@ -141,8 +150,10 @@ ecp_nistz256_mul_by_3:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,.Lpoly+8
-	ldr	x13,.Lpoly+24
+	adrp	x13,.Lpoly
+	add	x13,x13,:lo12:.Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 	mov	x8,x14
 	mov	x9,x15
 	mov	x10,x16
@@ -179,8 +190,10 @@ ecp_nistz256_sub:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,.Lpoly+8
-	ldr	x13,.Lpoly+24
+	adrp	x13,.Lpoly
+	add	x13,x13,:lo12:.Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_sub_from
 
@@ -204,8 +217,10 @@ ecp_nistz256_neg:
 	mov	x15,xzr
 	mov	x16,xzr
 	mov	x17,xzr
-	ldr	x12,.Lpoly+8
-	ldr	x13,.Lpoly+24
+	adrp	x13,.Lpoly
+	add	x13,x13,:lo12:.Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_sub_from
 
@@ -601,9 +616,11 @@ ecp_nistz256_point_double:
 	mov	x21,x0
 	ldp	x16,x17,[x1,#48]
 	mov	x22,x1
-	ldr	x12,.Lpoly+8
+	adrp	x13,.Lpoly
+	add	x13,x13,:lo12:.Lpoly
+	ldr	x12,[x13,#8]
 	mov	x8,x14
-	ldr	x13,.Lpoly+24
+	ldr	x13,[x13,#24]
 	mov	x9,x15
 	ldp	x4,x5,[x22,#64]	// forward load for p256_sqr_mont
 	mov	x10,x16
@@ -747,8 +764,10 @@ ecp_nistz256_point_add:
 	mov	x21,x0
 	mov	x22,x1
 	mov	x23,x2
-	ldr	x12,.Lpoly+8
-	ldr	x13,.Lpoly+24
+	adrp	x13,.Lpoly
+	add	x13,x13,:lo12:.Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 	orr	x8,x4,x5
 	orr	x10,x6,x7
 	orr	x25,x8,x10
@@ -999,8 +1018,10 @@ ecp_nistz256_point_add_affine:
 	mov	x21,x0
 	mov	x22,x1
 	mov	x23,x2
-	ldr	x12,.Lpoly+8
-	ldr	x13,.Lpoly+24
+	adrp	x13,.Lpoly
+	add	x13,x13,:lo12:.Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	ldp	x4,x5,[x1,#64]	// in1_z
 	ldp	x6,x7,[x1,#64+16]
@@ -1146,7 +1167,8 @@ ecp_nistz256_point_add_affine:
 	ldp	x10,x11,[x23,#0+48]
 	stp	x14,x15,[x21,#0]
 	stp	x16,x17,[x21,#0+16]
-	adr	x23,.Lone_mont-64
+	adrp	x23,.Lone_mont-64
+	add	x23,x23,:lo12:.Lone_mont-64
 	ldp	x14,x15,[x22,#32]	// in1
 	cmp	x24,#0			// ~, remember?
 	ldp	x16,x17,[x22,#32+16]
@@ -1205,7 +1227,8 @@ ecp_nistz256_ord_mul_mont:
 	stp	x21,x22,[sp,#32]
 	stp	x23,x24,[sp,#48]
 
-	adr	x23,.Lord
+	adrp	x23,.Lord
+	add	x23,x23,:lo12:.Lord
 	ldr	x3,[x2]		// bp[0]
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
@@ -1416,7 +1439,8 @@ ecp_nistz256_ord_sqr_mont:
 	stp	x21,x22,[sp,#32]
 	stp	x23,x24,[sp,#48]
 
-	adr	x23,.Lord
+	adrp	x23,.Lord
+	add	x23,x23,:lo12:.Lord
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
 
diff --git a/generated-src/win-aarch64/crypto/fipsmodule/p256-armv8-asm.S b/generated-src/win-aarch64/crypto/fipsmodule/p256-armv8-asm.S
index a85f64056..4d9e97029 100644
--- a/generated-src/win-aarch64/crypto/fipsmodule/p256-armv8-asm.S
+++ b/generated-src/win-aarch64/crypto/fipsmodule/p256-armv8-asm.S
@@ -14,7 +14,7 @@
 #endif
 #include "openssl/arm_arch.h"
 
-.text
+.section	.rodata
 .align	5
 Lpoly:
 .quad	0xffffffffffffffff,0x00000000ffffffff,0x0000000000000000,0xffffffff00000001
@@ -30,6 +30,7 @@ LordK:
 .quad	0xccd1c8aaee00bc4f
 .byte	69,67,80,95,78,73,83,84,90,50,53,54,32,102,111,114,32,65,82,77,118,56,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
 .align	2
+.text
 
 // void	ecp_nistz256_mul_mont(BN_ULONG x0[4],const BN_ULONG x1[4],
 //					     const BN_ULONG x2[4]);
@@ -48,8 +49,10 @@ ecp_nistz256_mul_mont:
 	ldr	x3,[x2]		// bp[0]
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly
+	add	x13,x13,:lo12:Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_mul_mont
 
@@ -74,8 +77,10 @@ ecp_nistz256_sqr_mont:
 
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly
+	add	x13,x13,:lo12:Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_sqr_mont
 
@@ -99,8 +104,10 @@ ecp_nistz256_div_by_2:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly
+	add	x13,x13,:lo12:Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_div_by_2
 
@@ -123,8 +130,10 @@ ecp_nistz256_mul_by_2:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly
+	add	x13,x13,:lo12:Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 	mov	x8,x14
 	mov	x9,x15
 	mov	x10,x16
@@ -151,8 +160,10 @@ ecp_nistz256_mul_by_3:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly
+	add	x13,x13,:lo12:Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 	mov	x8,x14
 	mov	x9,x15
 	mov	x10,x16
@@ -191,8 +202,10 @@ ecp_nistz256_sub:
 
 	ldp	x14,x15,[x1]
 	ldp	x16,x17,[x1,#16]
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly
+	add	x13,x13,:lo12:Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_sub_from
 
@@ -218,8 +231,10 @@ ecp_nistz256_neg:
 	mov	x15,xzr
 	mov	x16,xzr
 	mov	x17,xzr
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly
+	add	x13,x13,:lo12:Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	bl	__ecp_nistz256_sub_from
 
@@ -629,9 +644,11 @@ Ldouble_shortcut:
 	mov	x21,x0
 	ldp	x16,x17,[x1,#48]
 	mov	x22,x1
-	ldr	x12,Lpoly+8
+	adrp	x13,Lpoly
+	add	x13,x13,:lo12:Lpoly
+	ldr	x12,[x13,#8]
 	mov	x8,x14
-	ldr	x13,Lpoly+24
+	ldr	x13,[x13,#24]
 	mov	x9,x15
 	ldp	x4,x5,[x22,#64]	// forward load for p256_sqr_mont
 	mov	x10,x16
@@ -777,8 +794,10 @@ ecp_nistz256_point_add:
 	mov	x21,x0
 	mov	x22,x1
 	mov	x23,x2
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly
+	add	x13,x13,:lo12:Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 	orr	x8,x4,x5
 	orr	x10,x6,x7
 	orr	x25,x8,x10
@@ -1031,8 +1050,10 @@ ecp_nistz256_point_add_affine:
 	mov	x21,x0
 	mov	x22,x1
 	mov	x23,x2
-	ldr	x12,Lpoly+8
-	ldr	x13,Lpoly+24
+	adrp	x13,Lpoly
+	add	x13,x13,:lo12:Lpoly
+	ldr	x12,[x13,#8]
+	ldr	x13,[x13,#24]
 
 	ldp	x4,x5,[x1,#64]	// in1_z
 	ldp	x6,x7,[x1,#64+16]
@@ -1178,7 +1199,8 @@ ecp_nistz256_point_add_affine:
 	ldp	x10,x11,[x23,#0+48]
 	stp	x14,x15,[x21,#0]
 	stp	x16,x17,[x21,#0+16]
-	adr	x23,Lone_mont-64
+	adrp	x23,Lone_mont-64
+	add	x23,x23,:lo12:Lone_mont-64
 	ldp	x14,x15,[x22,#32]	// in1
 	cmp	x24,#0			// ~, remember?
 	ldp	x16,x17,[x22,#32+16]
@@ -1239,7 +1261,8 @@ ecp_nistz256_ord_mul_mont:
 	stp	x21,x22,[sp,#32]
 	stp	x23,x24,[sp,#48]
 
-	adr	x23,Lord
+	adrp	x23,Lord
+	add	x23,x23,:lo12:Lord
 	ldr	x3,[x2]		// bp[0]
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
@@ -1452,7 +1475,8 @@ ecp_nistz256_ord_sqr_mont:
 	stp	x21,x22,[sp,#32]
 	stp	x23,x24,[sp,#48]
 
-	adr	x23,Lord
+	adrp	x23,Lord
+	add	x23,x23,:lo12:Lord
 	ldp	x4,x5,[x1]
 	ldp	x6,x7,[x1,#16]
 
