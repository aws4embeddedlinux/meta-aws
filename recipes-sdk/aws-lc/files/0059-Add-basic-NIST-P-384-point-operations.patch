From 3b3b826835926297301798e4c5ca190ffd959e6f Mon Sep 17 00:00:00 2001
From: John Harrison <jargh@amazon.com>
Date: Fri, 1 Jul 2022 20:50:08 -0500
Subject: [PATCH] Add basic NIST P-384 point operations

A point doubling function, point addition function, and point mixed
addition function for the P-384 curve, all using Jacobian coordinates
in a Montgomery representation, with input nondegeneracy assumed.
Once again, the addition and mixed addition functions offer only
marginal efficiency gains over just calling a sequence of basic field
operations, but the doubling has some beneficial mathematically
equivalent short-cutting of the intermediate modular reductions.

s2n-bignum original commit: https://github.com/awslabs/s2n-bignum/commit/df8e913c542e5392a9f9cb6cd42fc90c5a02f72e
---
 arm/p384/Makefile               |    5 +-
 arm/p384/p384_montjadd.S        |  893 +++++++++++++++++++++++++++
 arm/p384/p384_montjdouble.S     |  963 +++++++++++++++++++++++++++++
 arm/p384/p384_montjmixadd.S     |  884 +++++++++++++++++++++++++++
 x86_att/p384/p384_montjadd.S    |  955 +++++++++++++++++++++++++++++
 x86_att/p384/p384_montjdouble.S | 1014 +++++++++++++++++++++++++++++++
 x86_att/p384/p384_montjmixadd.S |  941 ++++++++++++++++++++++++++++
 7 files changed, 5654 insertions(+), 1 deletion(-)
 create mode 100644 arm/p384/p384_montjadd.S
 create mode 100644 arm/p384/p384_montjdouble.S
 create mode 100644 arm/p384/p384_montjmixadd.S
 create mode 100644 x86_att/p384/p384_montjadd.S
 create mode 100644 x86_att/p384/p384_montjdouble.S
 create mode 100644 x86_att/p384/p384_montjmixadd.S

diff --git a/arm/p384/Makefile b/arm/p384/Makefile
index 469a20ff1..11a560550 100644
--- a/arm/p384/Makefile
+++ b/arm/p384/Makefile
@@ -53,7 +53,10 @@ OBJ = bignum_add_p384.o \
       bignum_optneg_p384.o \
       bignum_sub_p384.o \
       bignum_tomont_p384.o \
-      bignum_triple_p384.o
+      bignum_triple_p384.o \
+      p384_montjadd.o \
+      p384_montjdouble.o \
+      p384_montjmixadd.o
 
 %.o : %.S ; $(CC) -E -I../../include $< | $(GAS) -o $@ -
 
diff --git a/arm/p384/p384_montjadd.S b/arm/p384/p384_montjadd.S
new file mode 100644
index 000000000..138afa9dc
--- /dev/null
+++ b/arm/p384/p384_montjadd.S
@@ -0,0 +1,893 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Point addition on NIST curve P-384 in Montgomery-Jacobian coordinates
+//
+//    extern void p384_montjadd
+//      (uint64_t p3[static 18],uint64_t p1[static 18],uint64_t p2[static 18]);
+//
+// Does p3 := p1 + p2 where all points are regarded as Jacobian triples with
+// each coordinate in the Montgomery domain, i.e. x' = (2^384 * x) mod p_384.
+// A Jacobian triple (x',y',z') represents affine point (x/z^2,y/z^3).
+//
+// Standard ARM ABI: X0 = p3, X1 = p1, X2 = p2
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(p384_montjadd)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(p384_montjadd)
+        .text
+        .balign 4
+
+// Size of individual field elements
+
+#define NUMSIZE 48
+
+// Stable homes for input arguments during main code sequence
+
+#define input_z x24
+#define input_x x25
+#define input_y x26
+
+// Pointer-offset pairs for inputs and outputs
+
+#define x_1 input_x, #0
+#define y_1 input_x, #NUMSIZE
+#define z_1 input_x, #(2*NUMSIZE)
+
+#define x_2 input_y, #0
+#define y_2 input_y, #NUMSIZE
+#define z_2 input_y, #(2*NUMSIZE)
+
+#define x_3 input_z, #0
+#define y_3 input_z, #NUMSIZE
+#define z_3 input_z, #(2*NUMSIZE)
+
+// Pointer-offset pairs for temporaries, with some aliasing
+// NSPACE is the total stack needed for these temporaries
+
+#define z1sq sp, #(NUMSIZE*0)
+#define ww sp, #(NUMSIZE*0)
+
+#define yd sp, #(NUMSIZE*1)
+#define y2a sp, #(NUMSIZE*1)
+
+#define x2a sp, #(NUMSIZE*2)
+#define zzx2 sp, #(NUMSIZE*2)
+
+#define zz sp, #(NUMSIZE*3)
+#define t1 sp, #(NUMSIZE*3)
+
+#define t2 sp, #(NUMSIZE*4)
+#define x1a sp, #(NUMSIZE*4)
+#define zzx1 sp, #(NUMSIZE*4)
+
+#define xd sp, #(NUMSIZE*5)
+#define z2sq sp, #(NUMSIZE*5)
+
+#define y1a sp, #(NUMSIZE*6)
+
+#define NSPACE (NUMSIZE*7)
+
+// Corresponds exactly to bignum_montmul_p384_alt
+
+#define montmul_p384(P0,P1,P2)                  \
+        ldp     x3, x4, [P1];                   \
+        ldp     x5, x6, [P2];                   \
+        mul     x12, x3, x5;                    \
+        umulh   x13, x3, x5;                    \
+        mul     x11, x3, x6;                    \
+        umulh   x14, x3, x6;                    \
+        adds    x13, x13, x11;                  \
+        ldp     x7, x8, [P2+16];                \
+        mul     x11, x3, x7;                    \
+        umulh   x15, x3, x7;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x3, x8;                    \
+        umulh   x16, x3, x8;                    \
+        adcs    x15, x15, x11;                  \
+        ldp     x9, x10, [P2+32];               \
+        mul     x11, x3, x9;                    \
+        umulh   x17, x3, x9;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x3, x10;                   \
+        umulh   x19, x3, x10;                   \
+        adcs    x17, x17, x11;                  \
+        adc     x19, x19, xzr;                  \
+        mul     x11, x4, x5;                    \
+        adds    x13, x13, x11;                  \
+        mul     x11, x4, x6;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x4, x7;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x19, x19, x11;                  \
+        cset    x20, cs;                        \
+        umulh   x11, x4, x5;                    \
+        adds    x14, x14, x11;                  \
+        umulh   x11, x4, x6;                    \
+        adcs    x15, x15, x11;                  \
+        umulh   x11, x4, x7;                    \
+        adcs    x16, x16, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x17, x17, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x4, x10;                   \
+        adc     x20, x20, x11;                  \
+        ldp     x3, x4, [P1+16];                \
+        mul     x11, x3, x5;                    \
+        adds    x14, x14, x11;                  \
+        mul     x11, x3, x6;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x3, x7;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x3, x8;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x3, x9;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x3, x10;                   \
+        adcs    x20, x20, x11;                  \
+        cset    x21, cs;                        \
+        umulh   x11, x3, x5;                    \
+        adds    x15, x15, x11;                  \
+        umulh   x11, x3, x6;                    \
+        adcs    x16, x16, x11;                  \
+        umulh   x11, x3, x7;                    \
+        adcs    x17, x17, x11;                  \
+        umulh   x11, x3, x8;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x3, x9;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x3, x10;                   \
+        adc     x21, x21, x11;                  \
+        mul     x11, x4, x5;                    \
+        adds    x15, x15, x11;                  \
+        mul     x11, x4, x6;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x4, x7;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x20, x20, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x21, x21, x11;                  \
+        cset    x22, cs;                        \
+        umulh   x11, x4, x5;                    \
+        adds    x16, x16, x11;                  \
+        umulh   x11, x4, x6;                    \
+        adcs    x17, x17, x11;                  \
+        umulh   x11, x4, x7;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x21, x21, x11;                  \
+        umulh   x11, x4, x10;                   \
+        adc     x22, x22, x11;                  \
+        ldp     x3, x4, [P1+32];                \
+        mul     x11, x3, x5;                    \
+        adds    x16, x16, x11;                  \
+        mul     x11, x3, x6;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x3, x7;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x3, x8;                    \
+        adcs    x20, x20, x11;                  \
+        mul     x11, x3, x9;                    \
+        adcs    x21, x21, x11;                  \
+        mul     x11, x3, x10;                   \
+        adcs    x22, x22, x11;                  \
+        cset    x2, cs;                         \
+        umulh   x11, x3, x5;                    \
+        adds    x17, x17, x11;                  \
+        umulh   x11, x3, x6;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x3, x7;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x3, x8;                    \
+        adcs    x21, x21, x11;                  \
+        umulh   x11, x3, x9;                    \
+        adcs    x22, x22, x11;                  \
+        umulh   x11, x3, x10;                   \
+        adc     x2, x2, x11;                    \
+        mul     x11, x4, x5;                    \
+        adds    x17, x17, x11;                  \
+        mul     x11, x4, x6;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x4, x7;                    \
+        adcs    x20, x20, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x21, x21, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x22, x22, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x2, x2, x11;                    \
+        cset    x1, cs;                         \
+        umulh   x11, x4, x5;                    \
+        adds    x19, x19, x11;                  \
+        umulh   x11, x4, x6;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x4, x7;                    \
+        adcs    x21, x21, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x22, x22, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x2, x2, x11;                    \
+        umulh   x11, x4, x10;                   \
+        adc     x1, x1, x11;                    \
+        lsl     x7, x12, #32;                   \
+        add     x12, x7, x12;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x12;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x12;                    \
+        umulh   x6, x6, x12;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x12;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x13, x13, x7;                   \
+        sbcs    x14, x14, x6;                   \
+        sbcs    x15, x15, x5;                   \
+        sbcs    x16, x16, xzr;                  \
+        sbcs    x17, x17, xzr;                  \
+        sbc     x12, x12, xzr;                  \
+        lsl     x7, x13, #32;                   \
+        add     x13, x7, x13;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x13;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x13;                    \
+        umulh   x6, x6, x13;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x13;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x14, x14, x7;                   \
+        sbcs    x15, x15, x6;                   \
+        sbcs    x16, x16, x5;                   \
+        sbcs    x17, x17, xzr;                  \
+        sbcs    x12, x12, xzr;                  \
+        sbc     x13, x13, xzr;                  \
+        lsl     x7, x14, #32;                   \
+        add     x14, x7, x14;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x14;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x14;                    \
+        umulh   x6, x6, x14;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x14;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x15, x15, x7;                   \
+        sbcs    x16, x16, x6;                   \
+        sbcs    x17, x17, x5;                   \
+        sbcs    x12, x12, xzr;                  \
+        sbcs    x13, x13, xzr;                  \
+        sbc     x14, x14, xzr;                  \
+        lsl     x7, x15, #32;                   \
+        add     x15, x7, x15;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x15;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x15;                    \
+        umulh   x6, x6, x15;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x15;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x16, x16, x7;                   \
+        sbcs    x17, x17, x6;                   \
+        sbcs    x12, x12, x5;                   \
+        sbcs    x13, x13, xzr;                  \
+        sbcs    x14, x14, xzr;                  \
+        sbc     x15, x15, xzr;                  \
+        lsl     x7, x16, #32;                   \
+        add     x16, x7, x16;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x16;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x16;                    \
+        umulh   x6, x6, x16;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x16;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x17, x17, x7;                   \
+        sbcs    x12, x12, x6;                   \
+        sbcs    x13, x13, x5;                   \
+        sbcs    x14, x14, xzr;                  \
+        sbcs    x15, x15, xzr;                  \
+        sbc     x16, x16, xzr;                  \
+        lsl     x7, x17, #32;                   \
+        add     x17, x7, x17;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x17;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x17;                    \
+        umulh   x6, x6, x17;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x17;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x12, x12, x7;                   \
+        sbcs    x13, x13, x6;                   \
+        sbcs    x14, x14, x5;                   \
+        sbcs    x15, x15, xzr;                  \
+        sbcs    x16, x16, xzr;                  \
+        sbc     x17, x17, xzr;                  \
+        adds    x12, x12, x19;                  \
+        adcs    x13, x13, x20;                  \
+        adcs    x14, x14, x21;                  \
+        adcs    x15, x15, x22;                  \
+        adcs    x16, x16, x2;                   \
+        adcs    x17, x17, x1;                   \
+        adc     x10, xzr, xzr;                  \
+        mov     x11, #0xffffffff00000001;       \
+        adds    x19, x12, x11;                  \
+        mov     x11, #0xffffffff;               \
+        adcs    x20, x13, x11;                  \
+        mov     x11, #0x1;                      \
+        adcs    x21, x14, x11;                  \
+        adcs    x22, x15, xzr;                  \
+        adcs    x2, x16, xzr;                   \
+        adcs    x1, x17, xzr;                   \
+        adcs    x10, x10, xzr;                  \
+        csel    x12, x12, x19, eq;              \
+        csel    x13, x13, x20, eq;              \
+        csel    x14, x14, x21, eq;              \
+        csel    x15, x15, x22, eq;              \
+        csel    x16, x16, x2, eq;               \
+        csel    x17, x17, x1, eq;               \
+        stp     x12, x13, [P0];                 \
+        stp     x14, x15, [P0+16];              \
+        stp     x16, x17, [P0+32]
+
+// Corresponds exactly to bignum_montsqr_p384_alt
+
+#define montsqr_p384(P0,P1)                     \
+        ldp     x2, x3, [P1];                   \
+        mul     x9, x2, x3;                     \
+        umulh   x10, x2, x3;                    \
+        ldp     x4, x5, [P1+16];                \
+        mul     x8, x2, x4;                     \
+        adds    x10, x10, x8;                   \
+        mul     x11, x2, x5;                    \
+        mul     x8, x3, x4;                     \
+        adcs    x11, x11, x8;                   \
+        umulh   x12, x2, x5;                    \
+        mul     x8, x3, x5;                     \
+        adcs    x12, x12, x8;                   \
+        ldp     x6, x7, [P1+32];                \
+        mul     x13, x2, x7;                    \
+        mul     x8, x3, x6;                     \
+        adcs    x13, x13, x8;                   \
+        umulh   x14, x2, x7;                    \
+        mul     x8, x3, x7;                     \
+        adcs    x14, x14, x8;                   \
+        mul     x15, x5, x6;                    \
+        adcs    x15, x15, xzr;                  \
+        umulh   x16, x5, x6;                    \
+        adc     x16, x16, xzr;                  \
+        umulh   x8, x2, x4;                     \
+        adds    x11, x11, x8;                   \
+        umulh   x8, x3, x4;                     \
+        adcs    x12, x12, x8;                   \
+        umulh   x8, x3, x5;                     \
+        adcs    x13, x13, x8;                   \
+        umulh   x8, x3, x6;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x3, x7;                     \
+        adcs    x15, x15, x8;                   \
+        adc     x16, x16, xzr;                  \
+        mul     x8, x2, x6;                     \
+        adds    x12, x12, x8;                   \
+        mul     x8, x4, x5;                     \
+        adcs    x13, x13, x8;                   \
+        mul     x8, x4, x6;                     \
+        adcs    x14, x14, x8;                   \
+        mul     x8, x4, x7;                     \
+        adcs    x15, x15, x8;                   \
+        mul     x8, x5, x7;                     \
+        adcs    x16, x16, x8;                   \
+        mul     x17, x6, x7;                    \
+        adcs    x17, x17, xzr;                  \
+        umulh   x19, x6, x7;                    \
+        adc     x19, x19, xzr;                  \
+        umulh   x8, x2, x6;                     \
+        adds    x13, x13, x8;                   \
+        umulh   x8, x4, x5;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x4, x6;                     \
+        adcs    x15, x15, x8;                   \
+        umulh   x8, x4, x7;                     \
+        adcs    x16, x16, x8;                   \
+        umulh   x8, x5, x7;                     \
+        adcs    x17, x17, x8;                   \
+        adc     x19, x19, xzr;                  \
+        adds    x9, x9, x9;                     \
+        adcs    x10, x10, x10;                  \
+        adcs    x11, x11, x11;                  \
+        adcs    x12, x12, x12;                  \
+        adcs    x13, x13, x13;                  \
+        adcs    x14, x14, x14;                  \
+        adcs    x15, x15, x15;                  \
+        adcs    x16, x16, x16;                  \
+        adcs    x17, x17, x17;                  \
+        adcs    x19, x19, x19;                  \
+        cset    x20, hs;                        \
+        umulh   x8, x2, x2;                     \
+        mul     x2, x2, x2;                     \
+        adds    x9, x9, x8;                     \
+        mul     x8, x3, x3;                     \
+        adcs    x10, x10, x8;                   \
+        umulh   x8, x3, x3;                     \
+        adcs    x11, x11, x8;                   \
+        mul     x8, x4, x4;                     \
+        adcs    x12, x12, x8;                   \
+        umulh   x8, x4, x4;                     \
+        adcs    x13, x13, x8;                   \
+        mul     x8, x5, x5;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x5, x5;                     \
+        adcs    x15, x15, x8;                   \
+        mul     x8, x6, x6;                     \
+        adcs    x16, x16, x8;                   \
+        umulh   x8, x6, x6;                     \
+        adcs    x17, x17, x8;                   \
+        mul     x8, x7, x7;                     \
+        adcs    x19, x19, x8;                   \
+        umulh   x8, x7, x7;                     \
+        adc     x20, x20, x8;                   \
+        lsl     x5, x2, #32;                    \
+        add     x2, x5, x2;                     \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x2;                     \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x2;                     \
+        umulh   x4, x4, x2;                     \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x2;                     \
+        adc     x3, xzr, xzr;                   \
+        subs    x9, x9, x5;                     \
+        sbcs    x10, x10, x4;                   \
+        sbcs    x11, x11, x3;                   \
+        sbcs    x12, x12, xzr;                  \
+        sbcs    x13, x13, xzr;                  \
+        sbc     x2, x2, xzr;                    \
+        lsl     x5, x9, #32;                    \
+        add     x9, x5, x9;                     \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x9;                     \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x9;                     \
+        umulh   x4, x4, x9;                     \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x9;                     \
+        adc     x3, xzr, xzr;                   \
+        subs    x10, x10, x5;                   \
+        sbcs    x11, x11, x4;                   \
+        sbcs    x12, x12, x3;                   \
+        sbcs    x13, x13, xzr;                  \
+        sbcs    x2, x2, xzr;                    \
+        sbc     x9, x9, xzr;                    \
+        lsl     x5, x10, #32;                   \
+        add     x10, x5, x10;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x10;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x10;                    \
+        umulh   x4, x4, x10;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x10;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x11, x11, x5;                   \
+        sbcs    x12, x12, x4;                   \
+        sbcs    x13, x13, x3;                   \
+        sbcs    x2, x2, xzr;                    \
+        sbcs    x9, x9, xzr;                    \
+        sbc     x10, x10, xzr;                  \
+        lsl     x5, x11, #32;                   \
+        add     x11, x5, x11;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x11;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x11;                    \
+        umulh   x4, x4, x11;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x11;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x12, x12, x5;                   \
+        sbcs    x13, x13, x4;                   \
+        sbcs    x2, x2, x3;                     \
+        sbcs    x9, x9, xzr;                    \
+        sbcs    x10, x10, xzr;                  \
+        sbc     x11, x11, xzr;                  \
+        lsl     x5, x12, #32;                   \
+        add     x12, x5, x12;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x12;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x12;                    \
+        umulh   x4, x4, x12;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x12;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x13, x13, x5;                   \
+        sbcs    x2, x2, x4;                     \
+        sbcs    x9, x9, x3;                     \
+        sbcs    x10, x10, xzr;                  \
+        sbcs    x11, x11, xzr;                  \
+        sbc     x12, x12, xzr;                  \
+        lsl     x5, x13, #32;                   \
+        add     x13, x5, x13;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x13;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x13;                    \
+        umulh   x4, x4, x13;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x13;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x2, x2, x5;                     \
+        sbcs    x9, x9, x4;                     \
+        sbcs    x10, x10, x3;                   \
+        sbcs    x11, x11, xzr;                  \
+        sbcs    x12, x12, xzr;                  \
+        sbc     x13, x13, xzr;                  \
+        adds    x2, x2, x14;                    \
+        adcs    x9, x9, x15;                    \
+        adcs    x10, x10, x16;                  \
+        adcs    x11, x11, x17;                  \
+        adcs    x12, x12, x19;                  \
+        adcs    x13, x13, x20;                  \
+        adc     x6, xzr, xzr;                   \
+        mov     x8, #-4294967295;               \
+        adds    x14, x2, x8;                    \
+        mov     x8, #4294967295;                \
+        adcs    x15, x9, x8;                    \
+        mov     x8, #1;                         \
+        adcs    x16, x10, x8;                   \
+        adcs    x17, x11, xzr;                  \
+        adcs    x19, x12, xzr;                  \
+        adcs    x20, x13, xzr;                  \
+        adcs    x6, x6, xzr;                    \
+        csel    x2, x2, x14, eq;                \
+        csel    x9, x9, x15, eq;                \
+        csel    x10, x10, x16, eq;              \
+        csel    x11, x11, x17, eq;              \
+        csel    x12, x12, x19, eq;              \
+        csel    x13, x13, x20, eq;              \
+        stp     x2, x9, [P0];                   \
+        stp     x10, x11, [P0+16];              \
+        stp     x12, x13, [P0+32]
+
+// Almost-Montgomery variant which we use when an input to other muls
+// with the other argument fully reduced (which is always safe). In
+// fact, with the Karatsuba-based Montgomery mul here, we don't even
+// *need* the restriction that the other argument is reduced.
+
+#define amontsqr_p384(P0,P1)                    \
+        ldp     x2, x3, [P1];                   \
+        mul     x9, x2, x3;                     \
+        umulh   x10, x2, x3;                    \
+        ldp     x4, x5, [P1+16];                \
+        mul     x8, x2, x4;                     \
+        adds    x10, x10, x8;                   \
+        mul     x11, x2, x5;                    \
+        mul     x8, x3, x4;                     \
+        adcs    x11, x11, x8;                   \
+        umulh   x12, x2, x5;                    \
+        mul     x8, x3, x5;                     \
+        adcs    x12, x12, x8;                   \
+        ldp     x6, x7, [P1+32];                \
+        mul     x13, x2, x7;                    \
+        mul     x8, x3, x6;                     \
+        adcs    x13, x13, x8;                   \
+        umulh   x14, x2, x7;                    \
+        mul     x8, x3, x7;                     \
+        adcs    x14, x14, x8;                   \
+        mul     x15, x5, x6;                    \
+        adcs    x15, x15, xzr;                  \
+        umulh   x16, x5, x6;                    \
+        adc     x16, x16, xzr;                  \
+        umulh   x8, x2, x4;                     \
+        adds    x11, x11, x8;                   \
+        umulh   x8, x3, x4;                     \
+        adcs    x12, x12, x8;                   \
+        umulh   x8, x3, x5;                     \
+        adcs    x13, x13, x8;                   \
+        umulh   x8, x3, x6;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x3, x7;                     \
+        adcs    x15, x15, x8;                   \
+        adc     x16, x16, xzr;                  \
+        mul     x8, x2, x6;                     \
+        adds    x12, x12, x8;                   \
+        mul     x8, x4, x5;                     \
+        adcs    x13, x13, x8;                   \
+        mul     x8, x4, x6;                     \
+        adcs    x14, x14, x8;                   \
+        mul     x8, x4, x7;                     \
+        adcs    x15, x15, x8;                   \
+        mul     x8, x5, x7;                     \
+        adcs    x16, x16, x8;                   \
+        mul     x17, x6, x7;                    \
+        adcs    x17, x17, xzr;                  \
+        umulh   x19, x6, x7;                    \
+        adc     x19, x19, xzr;                  \
+        umulh   x8, x2, x6;                     \
+        adds    x13, x13, x8;                   \
+        umulh   x8, x4, x5;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x4, x6;                     \
+        adcs    x15, x15, x8;                   \
+        umulh   x8, x4, x7;                     \
+        adcs    x16, x16, x8;                   \
+        umulh   x8, x5, x7;                     \
+        adcs    x17, x17, x8;                   \
+        adc     x19, x19, xzr;                  \
+        adds    x9, x9, x9;                     \
+        adcs    x10, x10, x10;                  \
+        adcs    x11, x11, x11;                  \
+        adcs    x12, x12, x12;                  \
+        adcs    x13, x13, x13;                  \
+        adcs    x14, x14, x14;                  \
+        adcs    x15, x15, x15;                  \
+        adcs    x16, x16, x16;                  \
+        adcs    x17, x17, x17;                  \
+        adcs    x19, x19, x19;                  \
+        cset    x20, hs;                        \
+        umulh   x8, x2, x2;                     \
+        mul     x2, x2, x2;                     \
+        adds    x9, x9, x8;                     \
+        mul     x8, x3, x3;                     \
+        adcs    x10, x10, x8;                   \
+        umulh   x8, x3, x3;                     \
+        adcs    x11, x11, x8;                   \
+        mul     x8, x4, x4;                     \
+        adcs    x12, x12, x8;                   \
+        umulh   x8, x4, x4;                     \
+        adcs    x13, x13, x8;                   \
+        mul     x8, x5, x5;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x5, x5;                     \
+        adcs    x15, x15, x8;                   \
+        mul     x8, x6, x6;                     \
+        adcs    x16, x16, x8;                   \
+        umulh   x8, x6, x6;                     \
+        adcs    x17, x17, x8;                   \
+        mul     x8, x7, x7;                     \
+        adcs    x19, x19, x8;                   \
+        umulh   x8, x7, x7;                     \
+        adc     x20, x20, x8;                   \
+        lsl     x5, x2, #32;                    \
+        add     x2, x5, x2;                     \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x2;                     \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x2;                     \
+        umulh   x4, x4, x2;                     \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x2;                     \
+        adc     x3, xzr, xzr;                   \
+        subs    x9, x9, x5;                     \
+        sbcs    x10, x10, x4;                   \
+        sbcs    x11, x11, x3;                   \
+        sbcs    x12, x12, xzr;                  \
+        sbcs    x13, x13, xzr;                  \
+        sbc     x2, x2, xzr;                    \
+        lsl     x5, x9, #32;                    \
+        add     x9, x5, x9;                     \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x9;                     \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x9;                     \
+        umulh   x4, x4, x9;                     \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x9;                     \
+        adc     x3, xzr, xzr;                   \
+        subs    x10, x10, x5;                   \
+        sbcs    x11, x11, x4;                   \
+        sbcs    x12, x12, x3;                   \
+        sbcs    x13, x13, xzr;                  \
+        sbcs    x2, x2, xzr;                    \
+        sbc     x9, x9, xzr;                    \
+        lsl     x5, x10, #32;                   \
+        add     x10, x5, x10;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x10;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x10;                    \
+        umulh   x4, x4, x10;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x10;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x11, x11, x5;                   \
+        sbcs    x12, x12, x4;                   \
+        sbcs    x13, x13, x3;                   \
+        sbcs    x2, x2, xzr;                    \
+        sbcs    x9, x9, xzr;                    \
+        sbc     x10, x10, xzr;                  \
+        lsl     x5, x11, #32;                   \
+        add     x11, x5, x11;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x11;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x11;                    \
+        umulh   x4, x4, x11;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x11;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x12, x12, x5;                   \
+        sbcs    x13, x13, x4;                   \
+        sbcs    x2, x2, x3;                     \
+        sbcs    x9, x9, xzr;                    \
+        sbcs    x10, x10, xzr;                  \
+        sbc     x11, x11, xzr;                  \
+        lsl     x5, x12, #32;                   \
+        add     x12, x5, x12;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x12;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x12;                    \
+        umulh   x4, x4, x12;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x12;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x13, x13, x5;                   \
+        sbcs    x2, x2, x4;                     \
+        sbcs    x9, x9, x3;                     \
+        sbcs    x10, x10, xzr;                  \
+        sbcs    x11, x11, xzr;                  \
+        sbc     x12, x12, xzr;                  \
+        lsl     x5, x13, #32;                   \
+        add     x13, x5, x13;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x13;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x13;                    \
+        umulh   x4, x4, x13;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x13;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x2, x2, x5;                     \
+        sbcs    x9, x9, x4;                     \
+        sbcs    x10, x10, x3;                   \
+        sbcs    x11, x11, xzr;                  \
+        sbcs    x12, x12, xzr;                  \
+        sbc     x13, x13, xzr;                  \
+        adds    x2, x2, x14;                    \
+        adcs    x9, x9, x15;                    \
+        adcs    x10, x10, x16;                  \
+        adcs    x11, x11, x17;                  \
+        adcs    x12, x12, x19;                  \
+        adcs    x13, x13, x20;                  \
+        mov     x14, #-4294967295;              \
+        mov     x15, #4294967295;               \
+        csel    x14, x14, xzr, cs;              \
+        csel    x15, x15, xzr, cs;              \
+        cset    x16, cs;                        \
+        adds    x2, x2, x14;                    \
+        adcs    x9, x9, x15;                    \
+        adcs    x10, x10, x16;                  \
+        adcs    x11, x11, xzr;                  \
+        adcs    x12, x12, xzr;                  \
+        adc     x13, x13, xzr;                  \
+        stp     x2, x9, [P0];                   \
+        stp     x10, x11, [P0+16];              \
+        stp     x12, x13, [P0+32]
+
+// Corresponds exactly to bignum_sub_p384
+
+#define sub_p384(P0,P1,P2)                      \
+        ldp     x5, x6, [P1];                   \
+        ldp     x4, x3, [P2];                   \
+        subs    x5, x5, x4;                     \
+        sbcs    x6, x6, x3;                     \
+        ldp     x7, x8, [P1+16];                \
+        ldp     x4, x3, [P2+16];                \
+        sbcs    x7, x7, x4;                     \
+        sbcs    x8, x8, x3;                     \
+        ldp     x9, x10, [P1+32];               \
+        ldp     x4, x3, [P2+32];                \
+        sbcs    x9, x9, x4;                     \
+        sbcs    x10, x10, x3;                   \
+        csetm   x3, lo;                         \
+        mov     x4, #4294967295;                \
+        and     x4, x4, x3;                     \
+        adds    x5, x5, x4;                     \
+        eor     x4, x4, x3;                     \
+        adcs    x6, x6, x4;                     \
+        mov     x4, #-2;                        \
+        and     x4, x4, x3;                     \
+        adcs    x7, x7, x4;                     \
+        adcs    x8, x8, x3;                     \
+        adcs    x9, x9, x3;                     \
+        adc     x10, x10, x3;                   \
+        stp     x5, x6, [P0];                   \
+        stp     x7, x8, [P0+16];                \
+        stp     x9, x10, [P0+32]
+
+S2N_BN_SYMBOL(p384_montjadd):
+
+// Save regs and make room on stack for temporary variables
+
+        stp     x19, x20, [sp, #-16]!
+        stp     x21, x22, [sp, #-16]!
+        stp     x23, x24, [sp, #-16]!
+        stp     x25, x26, [sp, #-16]!
+        sub     sp, sp, NSPACE
+
+// Move the input arguments to stable places
+
+        mov     input_z, x0
+        mov     input_x, x1
+        mov     input_y, x2
+
+// Main code, just a sequence of basic field operations
+// 8 * multiply + 3 * square + 7 * subtract
+
+        amontsqr_p384(z1sq,z_1)
+        amontsqr_p384(z2sq,z_2)
+
+        montmul_p384(y1a,z_2,y_1)
+        montmul_p384(y2a,z_1,y_2)
+
+        montmul_p384(x2a,z1sq,x_2)
+        montmul_p384(x1a,z2sq,x_1)
+        montmul_p384(y2a,z1sq,y2a)
+        montmul_p384(y1a,z2sq,y1a)
+
+        sub_p384(xd,x2a,x1a)
+        sub_p384(yd,y2a,y1a)
+
+        amontsqr_p384(zz,xd)
+        montsqr_p384(ww,yd)
+
+        montmul_p384(zzx1,zz,x1a)
+        montmul_p384(zzx2,zz,x2a)
+
+        sub_p384(x_3,ww,zzx1)
+        sub_p384(t1,zzx2,zzx1)
+
+        montmul_p384(xd,xd,z_1)
+
+        sub_p384(x_3,x_3,zzx2)
+
+        sub_p384(t2,zzx1,x_3)
+
+        montmul_p384(t1,t1,y1a)
+        montmul_p384(z_3,xd,z_2)
+        montmul_p384(t2,yd,t2)
+
+        sub_p384(y_3,t2,t1)
+
+// Restore stack and registers
+
+        add     sp, sp, NSPACE
+
+        ldp     x25, x26, [sp], 16
+        ldp     x23, x24, [sp], 16
+        ldp     x21, x22, [sp], 16
+        ldp     x19, x20, [sp], 16
+
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack, "", %progbits
+#endif
diff --git a/arm/p384/p384_montjdouble.S b/arm/p384/p384_montjdouble.S
new file mode 100644
index 000000000..8fa2ad323
--- /dev/null
+++ b/arm/p384/p384_montjdouble.S
@@ -0,0 +1,963 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Point doubling on NIST curve P-384 in Montgomery-Jacobian coordinates
+//
+//    extern void p384_montjdouble
+//      (uint64_t p3[static 18],uint64_t p1[static 18]);
+//
+// Does p3 := 2 * p1 where all points are regarded as Jacobian triples with
+// each coordinate in the Montgomery domain, i.e. x' = (2^384 * x) mod p_384.
+// A Jacobian triple (x',y',z') represents affine point (x/z^2,y/z^3).
+//
+// Standard ARM ABI: X0 = p3, X1 = p1
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(p384_montjdouble)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(p384_montjdouble)
+        .text
+        .balign 4
+
+// Size of individual field elements
+
+#define NUMSIZE 48
+
+// Stable homes for input arguments during main code sequence
+
+#define input_z x23
+#define input_x x24
+
+// Pointer-offset pairs for inputs and outputs
+
+#define x_1 input_x, #0
+#define y_1 input_x, #NUMSIZE
+#define z_1 input_x, #(2*NUMSIZE)
+
+#define x_3 input_z, #0
+#define y_3 input_z, #NUMSIZE
+#define z_3 input_z, #(2*NUMSIZE)
+
+// Pointer-offset pairs for temporaries, with some aliasing
+// NSPACE is the total stack needed for these temporaries
+
+#define z2 sp, #(NUMSIZE*0)
+#define y2 sp, #(NUMSIZE*1)
+#define x2p sp, #(NUMSIZE*2)
+#define xy2 sp, #(NUMSIZE*3)
+
+#define y4 sp, #(NUMSIZE*4)
+#define t2 sp, #(NUMSIZE*4)
+
+#define dx2 sp, #(NUMSIZE*5)
+#define t1 sp, #(NUMSIZE*5)
+
+#define d sp, #(NUMSIZE*6)
+#define x4p sp, #(NUMSIZE*6)
+
+#define NSPACE (NUMSIZE*7)
+
+// Corresponds exactly to bignum_montmul_p384_alt
+
+#define montmul_p384(P0,P1,P2)                  \
+        ldp     x3, x4, [P1];                   \
+        ldp     x5, x6, [P2];                   \
+        mul     x12, x3, x5;                    \
+        umulh   x13, x3, x5;                    \
+        mul     x11, x3, x6;                    \
+        umulh   x14, x3, x6;                    \
+        adds    x13, x13, x11;                  \
+        ldp     x7, x8, [P2+16];                \
+        mul     x11, x3, x7;                    \
+        umulh   x15, x3, x7;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x3, x8;                    \
+        umulh   x16, x3, x8;                    \
+        adcs    x15, x15, x11;                  \
+        ldp     x9, x10, [P2+32];               \
+        mul     x11, x3, x9;                    \
+        umulh   x17, x3, x9;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x3, x10;                   \
+        umulh   x19, x3, x10;                   \
+        adcs    x17, x17, x11;                  \
+        adc     x19, x19, xzr;                  \
+        mul     x11, x4, x5;                    \
+        adds    x13, x13, x11;                  \
+        mul     x11, x4, x6;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x4, x7;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x19, x19, x11;                  \
+        cset    x20, cs;                        \
+        umulh   x11, x4, x5;                    \
+        adds    x14, x14, x11;                  \
+        umulh   x11, x4, x6;                    \
+        adcs    x15, x15, x11;                  \
+        umulh   x11, x4, x7;                    \
+        adcs    x16, x16, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x17, x17, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x4, x10;                   \
+        adc     x20, x20, x11;                  \
+        ldp     x3, x4, [P1+16];                \
+        mul     x11, x3, x5;                    \
+        adds    x14, x14, x11;                  \
+        mul     x11, x3, x6;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x3, x7;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x3, x8;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x3, x9;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x3, x10;                   \
+        adcs    x20, x20, x11;                  \
+        cset    x21, cs;                        \
+        umulh   x11, x3, x5;                    \
+        adds    x15, x15, x11;                  \
+        umulh   x11, x3, x6;                    \
+        adcs    x16, x16, x11;                  \
+        umulh   x11, x3, x7;                    \
+        adcs    x17, x17, x11;                  \
+        umulh   x11, x3, x8;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x3, x9;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x3, x10;                   \
+        adc     x21, x21, x11;                  \
+        mul     x11, x4, x5;                    \
+        adds    x15, x15, x11;                  \
+        mul     x11, x4, x6;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x4, x7;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x20, x20, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x21, x21, x11;                  \
+        cset    x22, cs;                        \
+        umulh   x11, x4, x5;                    \
+        adds    x16, x16, x11;                  \
+        umulh   x11, x4, x6;                    \
+        adcs    x17, x17, x11;                  \
+        umulh   x11, x4, x7;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x21, x21, x11;                  \
+        umulh   x11, x4, x10;                   \
+        adc     x22, x22, x11;                  \
+        ldp     x3, x4, [P1+32];                \
+        mul     x11, x3, x5;                    \
+        adds    x16, x16, x11;                  \
+        mul     x11, x3, x6;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x3, x7;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x3, x8;                    \
+        adcs    x20, x20, x11;                  \
+        mul     x11, x3, x9;                    \
+        adcs    x21, x21, x11;                  \
+        mul     x11, x3, x10;                   \
+        adcs    x22, x22, x11;                  \
+        cset    x2, cs;                         \
+        umulh   x11, x3, x5;                    \
+        adds    x17, x17, x11;                  \
+        umulh   x11, x3, x6;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x3, x7;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x3, x8;                    \
+        adcs    x21, x21, x11;                  \
+        umulh   x11, x3, x9;                    \
+        adcs    x22, x22, x11;                  \
+        umulh   x11, x3, x10;                   \
+        adc     x2, x2, x11;                    \
+        mul     x11, x4, x5;                    \
+        adds    x17, x17, x11;                  \
+        mul     x11, x4, x6;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x4, x7;                    \
+        adcs    x20, x20, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x21, x21, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x22, x22, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x2, x2, x11;                    \
+        cset    x1, cs;                         \
+        umulh   x11, x4, x5;                    \
+        adds    x19, x19, x11;                  \
+        umulh   x11, x4, x6;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x4, x7;                    \
+        adcs    x21, x21, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x22, x22, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x2, x2, x11;                    \
+        umulh   x11, x4, x10;                   \
+        adc     x1, x1, x11;                    \
+        lsl     x7, x12, #32;                   \
+        add     x12, x7, x12;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x12;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x12;                    \
+        umulh   x6, x6, x12;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x12;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x13, x13, x7;                   \
+        sbcs    x14, x14, x6;                   \
+        sbcs    x15, x15, x5;                   \
+        sbcs    x16, x16, xzr;                  \
+        sbcs    x17, x17, xzr;                  \
+        sbc     x12, x12, xzr;                  \
+        lsl     x7, x13, #32;                   \
+        add     x13, x7, x13;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x13;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x13;                    \
+        umulh   x6, x6, x13;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x13;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x14, x14, x7;                   \
+        sbcs    x15, x15, x6;                   \
+        sbcs    x16, x16, x5;                   \
+        sbcs    x17, x17, xzr;                  \
+        sbcs    x12, x12, xzr;                  \
+        sbc     x13, x13, xzr;                  \
+        lsl     x7, x14, #32;                   \
+        add     x14, x7, x14;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x14;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x14;                    \
+        umulh   x6, x6, x14;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x14;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x15, x15, x7;                   \
+        sbcs    x16, x16, x6;                   \
+        sbcs    x17, x17, x5;                   \
+        sbcs    x12, x12, xzr;                  \
+        sbcs    x13, x13, xzr;                  \
+        sbc     x14, x14, xzr;                  \
+        lsl     x7, x15, #32;                   \
+        add     x15, x7, x15;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x15;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x15;                    \
+        umulh   x6, x6, x15;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x15;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x16, x16, x7;                   \
+        sbcs    x17, x17, x6;                   \
+        sbcs    x12, x12, x5;                   \
+        sbcs    x13, x13, xzr;                  \
+        sbcs    x14, x14, xzr;                  \
+        sbc     x15, x15, xzr;                  \
+        lsl     x7, x16, #32;                   \
+        add     x16, x7, x16;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x16;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x16;                    \
+        umulh   x6, x6, x16;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x16;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x17, x17, x7;                   \
+        sbcs    x12, x12, x6;                   \
+        sbcs    x13, x13, x5;                   \
+        sbcs    x14, x14, xzr;                  \
+        sbcs    x15, x15, xzr;                  \
+        sbc     x16, x16, xzr;                  \
+        lsl     x7, x17, #32;                   \
+        add     x17, x7, x17;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x17;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x17;                    \
+        umulh   x6, x6, x17;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x17;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x12, x12, x7;                   \
+        sbcs    x13, x13, x6;                   \
+        sbcs    x14, x14, x5;                   \
+        sbcs    x15, x15, xzr;                  \
+        sbcs    x16, x16, xzr;                  \
+        sbc     x17, x17, xzr;                  \
+        adds    x12, x12, x19;                  \
+        adcs    x13, x13, x20;                  \
+        adcs    x14, x14, x21;                  \
+        adcs    x15, x15, x22;                  \
+        adcs    x16, x16, x2;                   \
+        adcs    x17, x17, x1;                   \
+        adc     x10, xzr, xzr;                  \
+        mov     x11, #0xffffffff00000001;       \
+        adds    x19, x12, x11;                  \
+        mov     x11, #0xffffffff;               \
+        adcs    x20, x13, x11;                  \
+        mov     x11, #0x1;                      \
+        adcs    x21, x14, x11;                  \
+        adcs    x22, x15, xzr;                  \
+        adcs    x2, x16, xzr;                   \
+        adcs    x1, x17, xzr;                   \
+        adcs    x10, x10, xzr;                  \
+        csel    x12, x12, x19, eq;              \
+        csel    x13, x13, x20, eq;              \
+        csel    x14, x14, x21, eq;              \
+        csel    x15, x15, x22, eq;              \
+        csel    x16, x16, x2, eq;               \
+        csel    x17, x17, x1, eq;               \
+        stp     x12, x13, [P0];                 \
+        stp     x14, x15, [P0+16];              \
+        stp     x16, x17, [P0+32]
+
+// Corresponds exactly to bignum_montsqr_p384_alt
+
+#define montsqr_p384(P0,P1)                     \
+        ldp     x2, x3, [P1];                   \
+        mul     x9, x2, x3;                     \
+        umulh   x10, x2, x3;                    \
+        ldp     x4, x5, [P1+16];                \
+        mul     x8, x2, x4;                     \
+        adds    x10, x10, x8;                   \
+        mul     x11, x2, x5;                    \
+        mul     x8, x3, x4;                     \
+        adcs    x11, x11, x8;                   \
+        umulh   x12, x2, x5;                    \
+        mul     x8, x3, x5;                     \
+        adcs    x12, x12, x8;                   \
+        ldp     x6, x7, [P1+32];                \
+        mul     x13, x2, x7;                    \
+        mul     x8, x3, x6;                     \
+        adcs    x13, x13, x8;                   \
+        umulh   x14, x2, x7;                    \
+        mul     x8, x3, x7;                     \
+        adcs    x14, x14, x8;                   \
+        mul     x15, x5, x6;                    \
+        adcs    x15, x15, xzr;                  \
+        umulh   x16, x5, x6;                    \
+        adc     x16, x16, xzr;                  \
+        umulh   x8, x2, x4;                     \
+        adds    x11, x11, x8;                   \
+        umulh   x8, x3, x4;                     \
+        adcs    x12, x12, x8;                   \
+        umulh   x8, x3, x5;                     \
+        adcs    x13, x13, x8;                   \
+        umulh   x8, x3, x6;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x3, x7;                     \
+        adcs    x15, x15, x8;                   \
+        adc     x16, x16, xzr;                  \
+        mul     x8, x2, x6;                     \
+        adds    x12, x12, x8;                   \
+        mul     x8, x4, x5;                     \
+        adcs    x13, x13, x8;                   \
+        mul     x8, x4, x6;                     \
+        adcs    x14, x14, x8;                   \
+        mul     x8, x4, x7;                     \
+        adcs    x15, x15, x8;                   \
+        mul     x8, x5, x7;                     \
+        adcs    x16, x16, x8;                   \
+        mul     x17, x6, x7;                    \
+        adcs    x17, x17, xzr;                  \
+        umulh   x19, x6, x7;                    \
+        adc     x19, x19, xzr;                  \
+        umulh   x8, x2, x6;                     \
+        adds    x13, x13, x8;                   \
+        umulh   x8, x4, x5;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x4, x6;                     \
+        adcs    x15, x15, x8;                   \
+        umulh   x8, x4, x7;                     \
+        adcs    x16, x16, x8;                   \
+        umulh   x8, x5, x7;                     \
+        adcs    x17, x17, x8;                   \
+        adc     x19, x19, xzr;                  \
+        adds    x9, x9, x9;                     \
+        adcs    x10, x10, x10;                  \
+        adcs    x11, x11, x11;                  \
+        adcs    x12, x12, x12;                  \
+        adcs    x13, x13, x13;                  \
+        adcs    x14, x14, x14;                  \
+        adcs    x15, x15, x15;                  \
+        adcs    x16, x16, x16;                  \
+        adcs    x17, x17, x17;                  \
+        adcs    x19, x19, x19;                  \
+        cset    x20, hs;                        \
+        umulh   x8, x2, x2;                     \
+        mul     x2, x2, x2;                     \
+        adds    x9, x9, x8;                     \
+        mul     x8, x3, x3;                     \
+        adcs    x10, x10, x8;                   \
+        umulh   x8, x3, x3;                     \
+        adcs    x11, x11, x8;                   \
+        mul     x8, x4, x4;                     \
+        adcs    x12, x12, x8;                   \
+        umulh   x8, x4, x4;                     \
+        adcs    x13, x13, x8;                   \
+        mul     x8, x5, x5;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x5, x5;                     \
+        adcs    x15, x15, x8;                   \
+        mul     x8, x6, x6;                     \
+        adcs    x16, x16, x8;                   \
+        umulh   x8, x6, x6;                     \
+        adcs    x17, x17, x8;                   \
+        mul     x8, x7, x7;                     \
+        adcs    x19, x19, x8;                   \
+        umulh   x8, x7, x7;                     \
+        adc     x20, x20, x8;                   \
+        lsl     x5, x2, #32;                    \
+        add     x2, x5, x2;                     \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x2;                     \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x2;                     \
+        umulh   x4, x4, x2;                     \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x2;                     \
+        adc     x3, xzr, xzr;                   \
+        subs    x9, x9, x5;                     \
+        sbcs    x10, x10, x4;                   \
+        sbcs    x11, x11, x3;                   \
+        sbcs    x12, x12, xzr;                  \
+        sbcs    x13, x13, xzr;                  \
+        sbc     x2, x2, xzr;                    \
+        lsl     x5, x9, #32;                    \
+        add     x9, x5, x9;                     \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x9;                     \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x9;                     \
+        umulh   x4, x4, x9;                     \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x9;                     \
+        adc     x3, xzr, xzr;                   \
+        subs    x10, x10, x5;                   \
+        sbcs    x11, x11, x4;                   \
+        sbcs    x12, x12, x3;                   \
+        sbcs    x13, x13, xzr;                  \
+        sbcs    x2, x2, xzr;                    \
+        sbc     x9, x9, xzr;                    \
+        lsl     x5, x10, #32;                   \
+        add     x10, x5, x10;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x10;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x10;                    \
+        umulh   x4, x4, x10;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x10;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x11, x11, x5;                   \
+        sbcs    x12, x12, x4;                   \
+        sbcs    x13, x13, x3;                   \
+        sbcs    x2, x2, xzr;                    \
+        sbcs    x9, x9, xzr;                    \
+        sbc     x10, x10, xzr;                  \
+        lsl     x5, x11, #32;                   \
+        add     x11, x5, x11;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x11;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x11;                    \
+        umulh   x4, x4, x11;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x11;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x12, x12, x5;                   \
+        sbcs    x13, x13, x4;                   \
+        sbcs    x2, x2, x3;                     \
+        sbcs    x9, x9, xzr;                    \
+        sbcs    x10, x10, xzr;                  \
+        sbc     x11, x11, xzr;                  \
+        lsl     x5, x12, #32;                   \
+        add     x12, x5, x12;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x12;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x12;                    \
+        umulh   x4, x4, x12;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x12;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x13, x13, x5;                   \
+        sbcs    x2, x2, x4;                     \
+        sbcs    x9, x9, x3;                     \
+        sbcs    x10, x10, xzr;                  \
+        sbcs    x11, x11, xzr;                  \
+        sbc     x12, x12, xzr;                  \
+        lsl     x5, x13, #32;                   \
+        add     x13, x5, x13;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x13;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x13;                    \
+        umulh   x4, x4, x13;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x13;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x2, x2, x5;                     \
+        sbcs    x9, x9, x4;                     \
+        sbcs    x10, x10, x3;                   \
+        sbcs    x11, x11, xzr;                  \
+        sbcs    x12, x12, xzr;                  \
+        sbc     x13, x13, xzr;                  \
+        adds    x2, x2, x14;                    \
+        adcs    x9, x9, x15;                    \
+        adcs    x10, x10, x16;                  \
+        adcs    x11, x11, x17;                  \
+        adcs    x12, x12, x19;                  \
+        adcs    x13, x13, x20;                  \
+        adc     x6, xzr, xzr;                   \
+        mov     x8, #-4294967295;               \
+        adds    x14, x2, x8;                    \
+        mov     x8, #4294967295;                \
+        adcs    x15, x9, x8;                    \
+        mov     x8, #1;                         \
+        adcs    x16, x10, x8;                   \
+        adcs    x17, x11, xzr;                  \
+        adcs    x19, x12, xzr;                  \
+        adcs    x20, x13, xzr;                  \
+        adcs    x6, x6, xzr;                    \
+        csel    x2, x2, x14, eq;                \
+        csel    x9, x9, x15, eq;                \
+        csel    x10, x10, x16, eq;              \
+        csel    x11, x11, x17, eq;              \
+        csel    x12, x12, x19, eq;              \
+        csel    x13, x13, x20, eq;              \
+        stp     x2, x9, [P0];                   \
+        stp     x10, x11, [P0+16];              \
+        stp     x12, x13, [P0+32]
+
+// Corresponds exactly to bignum_sub_p384
+
+#define sub_p384(P0,P1,P2)                      \
+        ldp     x5, x6, [P1];                   \
+        ldp     x4, x3, [P2];                   \
+        subs    x5, x5, x4;                     \
+        sbcs    x6, x6, x3;                     \
+        ldp     x7, x8, [P1+16];                \
+        ldp     x4, x3, [P2+16];                \
+        sbcs    x7, x7, x4;                     \
+        sbcs    x8, x8, x3;                     \
+        ldp     x9, x10, [P1+32];               \
+        ldp     x4, x3, [P2+32];                \
+        sbcs    x9, x9, x4;                     \
+        sbcs    x10, x10, x3;                   \
+        csetm   x3, lo;                         \
+        mov     x4, #4294967295;                \
+        and     x4, x4, x3;                     \
+        adds    x5, x5, x4;                     \
+        eor     x4, x4, x3;                     \
+        adcs    x6, x6, x4;                     \
+        mov     x4, #-2;                        \
+        and     x4, x4, x3;                     \
+        adcs    x7, x7, x4;                     \
+        adcs    x8, x8, x3;                     \
+        adcs    x9, x9, x3;                     \
+        adc     x10, x10, x3;                   \
+        stp     x5, x6, [P0];                   \
+        stp     x7, x8, [P0+16];                \
+        stp     x9, x10, [P0+32]
+
+// Corresponds exactly to bignum_add_p384
+
+#define add_p384(P0,P1,P2)                      \
+        ldp     x5, x6, [P1];                   \
+        ldp     x4, x3, [P2];                   \
+        adds    x5, x5, x4;                     \
+        adcs    x6, x6, x3;                     \
+        ldp     x7, x8, [P1+16];                \
+        ldp     x4, x3, [P2+16];                \
+        adcs    x7, x7, x4;                     \
+        adcs    x8, x8, x3;                     \
+        ldp     x9, x10, [P1+32];               \
+        ldp     x4, x3, [P2+32];                \
+        adcs    x9, x9, x4;                     \
+        adcs    x10, x10, x3;                   \
+        adc     x3, xzr, xzr;                   \
+        mov     x4, #0xffffffff;                \
+        cmp     x5, x4;                         \
+        mov     x4, #0xffffffff00000000;        \
+        sbcs    xzr, x6, x4;                    \
+        mov     x4, #0xfffffffffffffffe;        \
+        sbcs    xzr, x7, x4;                    \
+        adcs    xzr, x8, xzr;                   \
+        adcs    xzr, x9, xzr;                   \
+        adcs    xzr, x10, xzr;                  \
+        adcs    x3, x3, xzr;                    \
+        csetm   x3, ne;                         \
+        mov     x4, #0xffffffff;                \
+        and     x4, x4, x3;                     \
+        subs    x5, x5, x4;                     \
+        eor     x4, x4, x3;                     \
+        sbcs    x6, x6, x4;                     \
+        mov     x4, #0xfffffffffffffffe;        \
+        and     x4, x4, x3;                     \
+        sbcs    x7, x7, x4;                     \
+        sbcs    x8, x8, x3;                     \
+        sbcs    x9, x9, x3;                     \
+        sbc     x10, x10, x3;                   \
+        stp     x5, x6, [P0];                   \
+        stp     x7, x8, [P0+16];                \
+        stp     x9, x10, [P0+32]
+
+// P0 = 4 * P1 - P2
+
+#define cmsub41_p384(P0,P1,P2)                  \
+        ldp     x1, x2, [P1];                   \
+        ldp     x3, x4, [P1+16];                \
+        ldp     x5, x6, [P1+32];                \
+        lsl     x0, x1, #2;                     \
+        ldp     x7, x8, [P2];                   \
+        subs    x0, x0, x7;                     \
+        extr    x1, x2, x1, #62;                \
+        sbcs    x1, x1, x8;                     \
+        ldp     x7, x8, [P2+16];                \
+        extr    x2, x3, x2, #62;                \
+        sbcs    x2, x2, x7;                     \
+        extr    x3, x4, x3, #62;                \
+        sbcs    x3, x3, x8;                     \
+        extr    x4, x5, x4, #62;                \
+        ldp     x7, x8, [P2+32];                \
+        sbcs    x4, x4, x7;                     \
+        extr    x5, x6, x5, #62;                \
+        sbcs    x5, x5, x8;                     \
+        lsr     x6, x6, #62;                    \
+        adc     x6, x6, xzr;                    \
+        lsl     x7, x6, #32;                    \
+        subs    x8, x6, x7;                     \
+        sbc     x7, x7, xzr;                    \
+        adds    x0, x0, x8;                     \
+        adcs    x1, x1, x7;                     \
+        adcs    x2, x2, x6;                     \
+        adcs    x3, x3, xzr;                    \
+        adcs    x4, x4, xzr;                    \
+        adcs    x5, x5, xzr;                    \
+        csetm   x8, cc;                         \
+        mov     x9, #0xffffffff;                \
+        and     x9, x9, x8;                     \
+        adds    x0, x0, x9;                     \
+        eor     x9, x9, x8;                     \
+        adcs    x1, x1, x9;                     \
+        mov     x9, #0xfffffffffffffffe;        \
+        and     x9, x9, x8;                     \
+        adcs    x2, x2, x9;                     \
+        adcs    x3, x3, x8;                     \
+        adcs    x4, x4, x8;                     \
+        adc     x5, x5, x8;                     \
+        stp     x0, x1, [P0];                   \
+        stp     x2, x3, [P0+16];                \
+        stp     x4, x5, [P0+32]
+
+// P0 = C * P1 - D * P2
+
+#define cmsub_p384(P0,C,P1,D,P2)                \
+        ldp     x0, x1, [P2];                   \
+        mov     x6, #0x00000000ffffffff;        \
+        subs    x6, x6, x0;                     \
+        mov     x7, #0xffffffff00000000;        \
+        sbcs    x7, x7, x1;                     \
+        ldp     x0, x1, [P2+16];                \
+        mov     x8, #0xfffffffffffffffe;        \
+        sbcs    x8, x8, x0;                     \
+        mov     x13, #0xffffffffffffffff;       \
+        sbcs    x9, x13, x1;                    \
+        ldp     x0, x1, [P2+32];                \
+        sbcs    x10, x13, x0;                   \
+        sbc     x11, x13, x1;                   \
+        mov     x12, D;                         \
+        mul     x0, x12, x6;                    \
+        mul     x1, x12, x7;                    \
+        mul     x2, x12, x8;                    \
+        mul     x3, x12, x9;                    \
+        mul     x4, x12, x10;                   \
+        mul     x5, x12, x11;                   \
+        umulh   x6, x12, x6;                    \
+        umulh   x7, x12, x7;                    \
+        umulh   x8, x12, x8;                    \
+        umulh   x9, x12, x9;                    \
+        umulh   x10, x12, x10;                  \
+        umulh   x12, x12, x11;                  \
+        adds    x1, x1, x6;                     \
+        adcs    x2, x2, x7;                     \
+        adcs    x3, x3, x8;                     \
+        adcs    x4, x4, x9;                     \
+        adcs    x5, x5, x10;                    \
+        mov     x6, #1;                         \
+        adc     x6, x12, x6;                    \
+        ldp     x8, x9, [P1];                   \
+        ldp     x10, x11, [P1+16];              \
+        ldp     x12, x13, [P1+32];              \
+        mov     x14, C;                         \
+        mul     x15, x14, x8;                   \
+        umulh   x8, x14, x8;                    \
+        adds    x0, x0, x15;                    \
+        mul     x15, x14, x9;                   \
+        umulh   x9, x14, x9;                    \
+        adcs    x1, x1, x15;                    \
+        mul     x15, x14, x10;                  \
+        umulh   x10, x14, x10;                  \
+        adcs    x2, x2, x15;                    \
+        mul     x15, x14, x11;                  \
+        umulh   x11, x14, x11;                  \
+        adcs    x3, x3, x15;                    \
+        mul     x15, x14, x12;                  \
+        umulh   x12, x14, x12;                  \
+        adcs    x4, x4, x15;                    \
+        mul     x15, x14, x13;                  \
+        umulh   x13, x14, x13;                  \
+        adcs    x5, x5, x15;                    \
+        adc     x6, x6, xzr;                    \
+        adds    x1, x1, x8;                     \
+        adcs    x2, x2, x9;                     \
+        adcs    x3, x3, x10;                    \
+        adcs    x4, x4, x11;                    \
+        adcs    x5, x5, x12;                    \
+        adcs    x6, x6, x13;                    \
+        lsl     x7, x6, #32;                    \
+        subs    x8, x6, x7;                     \
+        sbc     x7, x7, xzr;                    \
+        adds    x0, x0, x8;                     \
+        adcs    x1, x1, x7;                     \
+        adcs    x2, x2, x6;                     \
+        adcs    x3, x3, xzr;                    \
+        adcs    x4, x4, xzr;                    \
+        adcs    x5, x5, xzr;                    \
+        csetm   x6, cc;                         \
+        mov     x7, #0xffffffff;                \
+        and     x7, x7, x6;                     \
+        adds    x0, x0, x7;                     \
+        eor     x7, x7, x6;                     \
+        adcs    x1, x1, x7;                     \
+        mov     x7, #0xfffffffffffffffe;        \
+        and     x7, x7, x6;                     \
+        adcs    x2, x2, x7;                     \
+        adcs    x3, x3, x6;                     \
+        adcs    x4, x4, x6;                     \
+        adc     x5, x5, x6;                     \
+        stp     x0, x1, [P0];                   \
+        stp     x2, x3, [P0+16];                \
+        stp     x4, x5, [P0+32]
+
+// A weak version of add that only guarantees sum in 6 digits
+
+#define weakadd_p384(P0,P1,P2)                  \
+        ldp     x5, x6, [P1];                   \
+        ldp     x4, x3, [P2];                   \
+        adds    x5, x5, x4;                     \
+        adcs    x6, x6, x3;                     \
+        ldp     x7, x8, [P1+16];                \
+        ldp     x4, x3, [P2+16];                \
+        adcs    x7, x7, x4;                     \
+        adcs    x8, x8, x3;                     \
+        ldp     x9, x10, [P1+32];               \
+        ldp     x4, x3, [P2+32];                \
+        adcs    x9, x9, x4;                     \
+        adcs    x10, x10, x3;                   \
+        csetm   x3, cs;                         \
+        mov     x4, #0xffffffff;                \
+        and     x4, x4, x3;                     \
+        subs    x5, x5, x4;                     \
+        eor     x4, x4, x3;                     \
+        sbcs    x6, x6, x4;                     \
+        mov     x4, #0xfffffffffffffffe;        \
+        and     x4, x4, x3;                     \
+        sbcs    x7, x7, x4;                     \
+        sbcs    x8, x8, x3;                     \
+        sbcs    x9, x9, x3;                     \
+        sbc     x10, x10, x3;                   \
+        stp     x5, x6, [P0];                   \
+        stp     x7, x8, [P0+16];                \
+        stp     x9, x10, [P0+32]
+
+// P0 = 3 * P1 - 8 * P2
+
+#define cmsub38_p384(P0,P1,P2)                  \
+        ldp     x0, x1, [P2];                   \
+        mov     x6, #0x00000000ffffffff;        \
+        subs    x6, x6, x0;                     \
+        mov     x7, #0xffffffff00000000;        \
+        sbcs    x7, x7, x1;                     \
+        ldp     x0, x1, [P2+16];                \
+        mov     x8, #0xfffffffffffffffe;        \
+        sbcs    x8, x8, x0;                     \
+        mov     x13, #0xffffffffffffffff;       \
+        sbcs    x9, x13, x1;                    \
+        ldp     x0, x1, [P2+32];                \
+        sbcs    x10, x13, x0;                   \
+        sbc     x11, x13, x1;                   \
+        lsl     x0, x6, #3;                     \
+        extr    x1, x7, x6, #61;                \
+        extr    x2, x8, x7, #61;                \
+        extr    x3, x9, x8, #61;                \
+        extr    x4, x10, x9, #61;               \
+        extr    x5, x11, x10, #61;              \
+        lsr     x6, x11, #61;                   \
+        add     x6, x6, #1;                     \
+        ldp     x8, x9, [P1];                   \
+        ldp     x10, x11, [P1+16];              \
+        ldp     x12, x13, [P1+32];              \
+        mov     x14, 3;                         \
+        mul     x15, x14, x8;                   \
+        umulh   x8, x14, x8;                    \
+        adds    x0, x0, x15;                    \
+        mul     x15, x14, x9;                   \
+        umulh   x9, x14, x9;                    \
+        adcs    x1, x1, x15;                    \
+        mul     x15, x14, x10;                  \
+        umulh   x10, x14, x10;                  \
+        adcs    x2, x2, x15;                    \
+        mul     x15, x14, x11;                  \
+        umulh   x11, x14, x11;                  \
+        adcs    x3, x3, x15;                    \
+        mul     x15, x14, x12;                  \
+        umulh   x12, x14, x12;                  \
+        adcs    x4, x4, x15;                    \
+        mul     x15, x14, x13;                  \
+        umulh   x13, x14, x13;                  \
+        adcs    x5, x5, x15;                    \
+        adc     x6, x6, xzr;                    \
+        adds    x1, x1, x8;                     \
+        adcs    x2, x2, x9;                     \
+        adcs    x3, x3, x10;                    \
+        adcs    x4, x4, x11;                    \
+        adcs    x5, x5, x12;                    \
+        adcs    x6, x6, x13;                    \
+        lsl     x7, x6, #32;                    \
+        subs    x8, x6, x7;                     \
+        sbc     x7, x7, xzr;                    \
+        adds    x0, x0, x8;                     \
+        adcs    x1, x1, x7;                     \
+        adcs    x2, x2, x6;                     \
+        adcs    x3, x3, xzr;                    \
+        adcs    x4, x4, xzr;                    \
+        adcs    x5, x5, xzr;                    \
+        csetm   x6, cc;                         \
+        mov     x7, #0xffffffff;                \
+        and     x7, x7, x6;                     \
+        adds    x0, x0, x7;                     \
+        eor     x7, x7, x6;                     \
+        adcs    x1, x1, x7;                     \
+        mov     x7, #0xfffffffffffffffe;        \
+        and     x7, x7, x6;                     \
+        adcs    x2, x2, x7;                     \
+        adcs    x3, x3, x6;                     \
+        adcs    x4, x4, x6;                     \
+        adc     x5, x5, x6;                     \
+        stp     x0, x1, [P0];                   \
+        stp     x2, x3, [P0+16];                \
+        stp     x4, x5, [P0+32]
+
+S2N_BN_SYMBOL(p384_montjdouble):
+
+// Save regs and make room on stack for temporary variables
+
+        stp     x19, x20, [sp, #-16]!
+        stp     x21, x22, [sp, #-16]!
+        stp     x23, x24, [sp, #-16]!
+        sub     sp, sp, NSPACE
+
+// Move the input arguments to stable places
+
+        mov     input_z, x0
+        mov     input_x, x1
+
+// Main code, just a sequence of basic field operations
+
+// z2 = z^2
+// y2 = y^2
+
+        montsqr_p384(z2,z_1)
+        montsqr_p384(y2,y_1)
+
+// x2p = x^2 - z^4 = (x + z^2) * (x - z^2)
+
+        weakadd_p384(t1,x_1,z2)
+        sub_p384(t2,x_1,z2)
+        montmul_p384(x2p,t1,t2)
+
+// t1 = y + z
+// x4p = x2p^2
+// xy2 = x * y^2
+
+        add_p384(t1,y_1,z_1)
+        montsqr_p384(x4p,x2p)
+        montmul_p384(xy2,x_1,y2)
+
+// t2 = (y + z)^2
+
+        montsqr_p384(t2,t1)
+
+// d = 12 * xy2 - 9 * x4p
+// t1 = y^2 + 2 * y * z
+
+        cmsub_p384(d,12,xy2,9,x4p)
+        sub_p384(t1,t2,z2)
+
+// y4 = y^4
+
+        montsqr_p384(y4,y2)
+
+// z_3' = 2 * y * z
+// dx2 = d * x2p
+
+        sub_p384(z_3,t1,y2)
+        montmul_p384(dx2,d,x2p)
+
+// x' = 4 * xy2 - d
+
+        cmsub41_p384(x_3,xy2,d)
+
+// y' = 3 * dx2 - 8 * y4
+
+        cmsub38_p384(y_3,dx2,y4)
+
+// Restore stack and registers
+
+        add     sp, sp, NSPACE
+
+        ldp     x23, x24, [sp], 16
+        ldp     x21, x22, [sp], 16
+        ldp     x19, x20, [sp], 16
+
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack, "", %progbits
+#endif
diff --git a/arm/p384/p384_montjmixadd.S b/arm/p384/p384_montjmixadd.S
new file mode 100644
index 000000000..f7467be28
--- /dev/null
+++ b/arm/p384/p384_montjmixadd.S
@@ -0,0 +1,884 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Point mixed addition on NIST curve P-384 in Montgomery-Jacobian coordinates
+//
+//    extern void p384_montjmixadd
+//      (uint64_t p3[static 18],uint64_t p1[static 18],uint64_t p2[static 12]);
+//
+// Does p3 := p1 + p2 where all points are regarded as Jacobian triples with
+// each coordinate in the Montgomery domain, i.e. x' = (2^384 * x) mod p_384.
+// A Jacobian triple (x',y',z') represents affine point (x/z^2,y/z^3).
+// The "mixed" part means that p2 only has x and y coordinates, with the
+// implicit z coordinate assumed to be the identity.
+//
+// Standard ARM ABI: X0 = p3, X1 = p1, X2 = p2
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(p384_montjmixadd)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(p384_montjmixadd)
+        .text
+        .balign 4
+
+// Size of individual field elements
+
+#define NUMSIZE 48
+
+// Stable homes for input arguments during main code sequence
+
+#define input_z x24
+#define input_x x25
+#define input_y x26
+
+// Pointer-offset pairs for inputs and outputs
+
+#define x_1 input_x, #0
+#define y_1 input_x, #NUMSIZE
+#define z_1 input_x, #(2*NUMSIZE)
+
+#define x_2 input_y, #0
+#define y_2 input_y, #NUMSIZE
+
+#define x_3 input_z, #0
+#define y_3 input_z, #NUMSIZE
+#define z_3 input_z, #(2*NUMSIZE)
+
+// Pointer-offset pairs for temporaries, with some aliasing
+// NSPACE is the total stack needed for these temporaries
+
+#define zp2 sp, #(NUMSIZE*0)
+#define ww sp, #(NUMSIZE*0)
+
+#define yd sp, #(NUMSIZE*1)
+#define y2a sp, #(NUMSIZE*1)
+
+#define x2a sp, #(NUMSIZE*2)
+#define zzx2 sp, #(NUMSIZE*2)
+
+#define zz sp, #(NUMSIZE*3)
+#define t1 sp, #(NUMSIZE*3)
+
+#define t2 sp, #(NUMSIZE*4)
+#define zzx1 sp, #(NUMSIZE*4)
+
+#define xd sp, #(NUMSIZE*5)
+
+#define NSPACE (NUMSIZE*6)
+
+// Corresponds exactly to bignum_montmul_p384_alt
+
+#define montmul_p384(P0,P1,P2)                  \
+        ldp     x3, x4, [P1];                   \
+        ldp     x5, x6, [P2];                   \
+        mul     x12, x3, x5;                    \
+        umulh   x13, x3, x5;                    \
+        mul     x11, x3, x6;                    \
+        umulh   x14, x3, x6;                    \
+        adds    x13, x13, x11;                  \
+        ldp     x7, x8, [P2+16];                \
+        mul     x11, x3, x7;                    \
+        umulh   x15, x3, x7;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x3, x8;                    \
+        umulh   x16, x3, x8;                    \
+        adcs    x15, x15, x11;                  \
+        ldp     x9, x10, [P2+32];               \
+        mul     x11, x3, x9;                    \
+        umulh   x17, x3, x9;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x3, x10;                   \
+        umulh   x19, x3, x10;                   \
+        adcs    x17, x17, x11;                  \
+        adc     x19, x19, xzr;                  \
+        mul     x11, x4, x5;                    \
+        adds    x13, x13, x11;                  \
+        mul     x11, x4, x6;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x4, x7;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x19, x19, x11;                  \
+        cset    x20, cs;                        \
+        umulh   x11, x4, x5;                    \
+        adds    x14, x14, x11;                  \
+        umulh   x11, x4, x6;                    \
+        adcs    x15, x15, x11;                  \
+        umulh   x11, x4, x7;                    \
+        adcs    x16, x16, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x17, x17, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x4, x10;                   \
+        adc     x20, x20, x11;                  \
+        ldp     x3, x4, [P1+16];                \
+        mul     x11, x3, x5;                    \
+        adds    x14, x14, x11;                  \
+        mul     x11, x3, x6;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x3, x7;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x3, x8;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x3, x9;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x3, x10;                   \
+        adcs    x20, x20, x11;                  \
+        cset    x21, cs;                        \
+        umulh   x11, x3, x5;                    \
+        adds    x15, x15, x11;                  \
+        umulh   x11, x3, x6;                    \
+        adcs    x16, x16, x11;                  \
+        umulh   x11, x3, x7;                    \
+        adcs    x17, x17, x11;                  \
+        umulh   x11, x3, x8;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x3, x9;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x3, x10;                   \
+        adc     x21, x21, x11;                  \
+        mul     x11, x4, x5;                    \
+        adds    x15, x15, x11;                  \
+        mul     x11, x4, x6;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x4, x7;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x20, x20, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x21, x21, x11;                  \
+        cset    x22, cs;                        \
+        umulh   x11, x4, x5;                    \
+        adds    x16, x16, x11;                  \
+        umulh   x11, x4, x6;                    \
+        adcs    x17, x17, x11;                  \
+        umulh   x11, x4, x7;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x21, x21, x11;                  \
+        umulh   x11, x4, x10;                   \
+        adc     x22, x22, x11;                  \
+        ldp     x3, x4, [P1+32];                \
+        mul     x11, x3, x5;                    \
+        adds    x16, x16, x11;                  \
+        mul     x11, x3, x6;                    \
+        adcs    x17, x17, x11;                  \
+        mul     x11, x3, x7;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x3, x8;                    \
+        adcs    x20, x20, x11;                  \
+        mul     x11, x3, x9;                    \
+        adcs    x21, x21, x11;                  \
+        mul     x11, x3, x10;                   \
+        adcs    x22, x22, x11;                  \
+        cset    x2, cs;                         \
+        umulh   x11, x3, x5;                    \
+        adds    x17, x17, x11;                  \
+        umulh   x11, x3, x6;                    \
+        adcs    x19, x19, x11;                  \
+        umulh   x11, x3, x7;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x3, x8;                    \
+        adcs    x21, x21, x11;                  \
+        umulh   x11, x3, x9;                    \
+        adcs    x22, x22, x11;                  \
+        umulh   x11, x3, x10;                   \
+        adc     x2, x2, x11;                    \
+        mul     x11, x4, x5;                    \
+        adds    x17, x17, x11;                  \
+        mul     x11, x4, x6;                    \
+        adcs    x19, x19, x11;                  \
+        mul     x11, x4, x7;                    \
+        adcs    x20, x20, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x21, x21, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x22, x22, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x2, x2, x11;                    \
+        cset    x1, cs;                         \
+        umulh   x11, x4, x5;                    \
+        adds    x19, x19, x11;                  \
+        umulh   x11, x4, x6;                    \
+        adcs    x20, x20, x11;                  \
+        umulh   x11, x4, x7;                    \
+        adcs    x21, x21, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x22, x22, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x2, x2, x11;                    \
+        umulh   x11, x4, x10;                   \
+        adc     x1, x1, x11;                    \
+        lsl     x7, x12, #32;                   \
+        add     x12, x7, x12;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x12;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x12;                    \
+        umulh   x6, x6, x12;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x12;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x13, x13, x7;                   \
+        sbcs    x14, x14, x6;                   \
+        sbcs    x15, x15, x5;                   \
+        sbcs    x16, x16, xzr;                  \
+        sbcs    x17, x17, xzr;                  \
+        sbc     x12, x12, xzr;                  \
+        lsl     x7, x13, #32;                   \
+        add     x13, x7, x13;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x13;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x13;                    \
+        umulh   x6, x6, x13;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x13;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x14, x14, x7;                   \
+        sbcs    x15, x15, x6;                   \
+        sbcs    x16, x16, x5;                   \
+        sbcs    x17, x17, xzr;                  \
+        sbcs    x12, x12, xzr;                  \
+        sbc     x13, x13, xzr;                  \
+        lsl     x7, x14, #32;                   \
+        add     x14, x7, x14;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x14;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x14;                    \
+        umulh   x6, x6, x14;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x14;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x15, x15, x7;                   \
+        sbcs    x16, x16, x6;                   \
+        sbcs    x17, x17, x5;                   \
+        sbcs    x12, x12, xzr;                  \
+        sbcs    x13, x13, xzr;                  \
+        sbc     x14, x14, xzr;                  \
+        lsl     x7, x15, #32;                   \
+        add     x15, x7, x15;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x15;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x15;                    \
+        umulh   x6, x6, x15;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x15;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x16, x16, x7;                   \
+        sbcs    x17, x17, x6;                   \
+        sbcs    x12, x12, x5;                   \
+        sbcs    x13, x13, xzr;                  \
+        sbcs    x14, x14, xzr;                  \
+        sbc     x15, x15, xzr;                  \
+        lsl     x7, x16, #32;                   \
+        add     x16, x7, x16;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x16;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x16;                    \
+        umulh   x6, x6, x16;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x16;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x17, x17, x7;                   \
+        sbcs    x12, x12, x6;                   \
+        sbcs    x13, x13, x5;                   \
+        sbcs    x14, x14, xzr;                  \
+        sbcs    x15, x15, xzr;                  \
+        sbc     x16, x16, xzr;                  \
+        lsl     x7, x17, #32;                   \
+        add     x17, x7, x17;                   \
+        mov     x7, #0xffffffff00000001;        \
+        umulh   x7, x7, x17;                    \
+        mov     x6, #0xffffffff;                \
+        mul     x5, x6, x17;                    \
+        umulh   x6, x6, x17;                    \
+        adds    x7, x7, x5;                     \
+        adcs    x6, x6, x17;                    \
+        adc     x5, xzr, xzr;                   \
+        subs    x12, x12, x7;                   \
+        sbcs    x13, x13, x6;                   \
+        sbcs    x14, x14, x5;                   \
+        sbcs    x15, x15, xzr;                  \
+        sbcs    x16, x16, xzr;                  \
+        sbc     x17, x17, xzr;                  \
+        adds    x12, x12, x19;                  \
+        adcs    x13, x13, x20;                  \
+        adcs    x14, x14, x21;                  \
+        adcs    x15, x15, x22;                  \
+        adcs    x16, x16, x2;                   \
+        adcs    x17, x17, x1;                   \
+        adc     x10, xzr, xzr;                  \
+        mov     x11, #0xffffffff00000001;       \
+        adds    x19, x12, x11;                  \
+        mov     x11, #0xffffffff;               \
+        adcs    x20, x13, x11;                  \
+        mov     x11, #0x1;                      \
+        adcs    x21, x14, x11;                  \
+        adcs    x22, x15, xzr;                  \
+        adcs    x2, x16, xzr;                   \
+        adcs    x1, x17, xzr;                   \
+        adcs    x10, x10, xzr;                  \
+        csel    x12, x12, x19, eq;              \
+        csel    x13, x13, x20, eq;              \
+        csel    x14, x14, x21, eq;              \
+        csel    x15, x15, x22, eq;              \
+        csel    x16, x16, x2, eq;               \
+        csel    x17, x17, x1, eq;               \
+        stp     x12, x13, [P0];                 \
+        stp     x14, x15, [P0+16];              \
+        stp     x16, x17, [P0+32]
+
+// Corresponds exactly to bignum_montsqr_p384_alt
+
+#define montsqr_p384(P0,P1)                     \
+        ldp     x2, x3, [P1];                   \
+        mul     x9, x2, x3;                     \
+        umulh   x10, x2, x3;                    \
+        ldp     x4, x5, [P1+16];                \
+        mul     x8, x2, x4;                     \
+        adds    x10, x10, x8;                   \
+        mul     x11, x2, x5;                    \
+        mul     x8, x3, x4;                     \
+        adcs    x11, x11, x8;                   \
+        umulh   x12, x2, x5;                    \
+        mul     x8, x3, x5;                     \
+        adcs    x12, x12, x8;                   \
+        ldp     x6, x7, [P1+32];                \
+        mul     x13, x2, x7;                    \
+        mul     x8, x3, x6;                     \
+        adcs    x13, x13, x8;                   \
+        umulh   x14, x2, x7;                    \
+        mul     x8, x3, x7;                     \
+        adcs    x14, x14, x8;                   \
+        mul     x15, x5, x6;                    \
+        adcs    x15, x15, xzr;                  \
+        umulh   x16, x5, x6;                    \
+        adc     x16, x16, xzr;                  \
+        umulh   x8, x2, x4;                     \
+        adds    x11, x11, x8;                   \
+        umulh   x8, x3, x4;                     \
+        adcs    x12, x12, x8;                   \
+        umulh   x8, x3, x5;                     \
+        adcs    x13, x13, x8;                   \
+        umulh   x8, x3, x6;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x3, x7;                     \
+        adcs    x15, x15, x8;                   \
+        adc     x16, x16, xzr;                  \
+        mul     x8, x2, x6;                     \
+        adds    x12, x12, x8;                   \
+        mul     x8, x4, x5;                     \
+        adcs    x13, x13, x8;                   \
+        mul     x8, x4, x6;                     \
+        adcs    x14, x14, x8;                   \
+        mul     x8, x4, x7;                     \
+        adcs    x15, x15, x8;                   \
+        mul     x8, x5, x7;                     \
+        adcs    x16, x16, x8;                   \
+        mul     x17, x6, x7;                    \
+        adcs    x17, x17, xzr;                  \
+        umulh   x19, x6, x7;                    \
+        adc     x19, x19, xzr;                  \
+        umulh   x8, x2, x6;                     \
+        adds    x13, x13, x8;                   \
+        umulh   x8, x4, x5;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x4, x6;                     \
+        adcs    x15, x15, x8;                   \
+        umulh   x8, x4, x7;                     \
+        adcs    x16, x16, x8;                   \
+        umulh   x8, x5, x7;                     \
+        adcs    x17, x17, x8;                   \
+        adc     x19, x19, xzr;                  \
+        adds    x9, x9, x9;                     \
+        adcs    x10, x10, x10;                  \
+        adcs    x11, x11, x11;                  \
+        adcs    x12, x12, x12;                  \
+        adcs    x13, x13, x13;                  \
+        adcs    x14, x14, x14;                  \
+        adcs    x15, x15, x15;                  \
+        adcs    x16, x16, x16;                  \
+        adcs    x17, x17, x17;                  \
+        adcs    x19, x19, x19;                  \
+        cset    x20, hs;                        \
+        umulh   x8, x2, x2;                     \
+        mul     x2, x2, x2;                     \
+        adds    x9, x9, x8;                     \
+        mul     x8, x3, x3;                     \
+        adcs    x10, x10, x8;                   \
+        umulh   x8, x3, x3;                     \
+        adcs    x11, x11, x8;                   \
+        mul     x8, x4, x4;                     \
+        adcs    x12, x12, x8;                   \
+        umulh   x8, x4, x4;                     \
+        adcs    x13, x13, x8;                   \
+        mul     x8, x5, x5;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x5, x5;                     \
+        adcs    x15, x15, x8;                   \
+        mul     x8, x6, x6;                     \
+        adcs    x16, x16, x8;                   \
+        umulh   x8, x6, x6;                     \
+        adcs    x17, x17, x8;                   \
+        mul     x8, x7, x7;                     \
+        adcs    x19, x19, x8;                   \
+        umulh   x8, x7, x7;                     \
+        adc     x20, x20, x8;                   \
+        lsl     x5, x2, #32;                    \
+        add     x2, x5, x2;                     \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x2;                     \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x2;                     \
+        umulh   x4, x4, x2;                     \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x2;                     \
+        adc     x3, xzr, xzr;                   \
+        subs    x9, x9, x5;                     \
+        sbcs    x10, x10, x4;                   \
+        sbcs    x11, x11, x3;                   \
+        sbcs    x12, x12, xzr;                  \
+        sbcs    x13, x13, xzr;                  \
+        sbc     x2, x2, xzr;                    \
+        lsl     x5, x9, #32;                    \
+        add     x9, x5, x9;                     \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x9;                     \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x9;                     \
+        umulh   x4, x4, x9;                     \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x9;                     \
+        adc     x3, xzr, xzr;                   \
+        subs    x10, x10, x5;                   \
+        sbcs    x11, x11, x4;                   \
+        sbcs    x12, x12, x3;                   \
+        sbcs    x13, x13, xzr;                  \
+        sbcs    x2, x2, xzr;                    \
+        sbc     x9, x9, xzr;                    \
+        lsl     x5, x10, #32;                   \
+        add     x10, x5, x10;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x10;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x10;                    \
+        umulh   x4, x4, x10;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x10;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x11, x11, x5;                   \
+        sbcs    x12, x12, x4;                   \
+        sbcs    x13, x13, x3;                   \
+        sbcs    x2, x2, xzr;                    \
+        sbcs    x9, x9, xzr;                    \
+        sbc     x10, x10, xzr;                  \
+        lsl     x5, x11, #32;                   \
+        add     x11, x5, x11;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x11;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x11;                    \
+        umulh   x4, x4, x11;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x11;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x12, x12, x5;                   \
+        sbcs    x13, x13, x4;                   \
+        sbcs    x2, x2, x3;                     \
+        sbcs    x9, x9, xzr;                    \
+        sbcs    x10, x10, xzr;                  \
+        sbc     x11, x11, xzr;                  \
+        lsl     x5, x12, #32;                   \
+        add     x12, x5, x12;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x12;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x12;                    \
+        umulh   x4, x4, x12;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x12;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x13, x13, x5;                   \
+        sbcs    x2, x2, x4;                     \
+        sbcs    x9, x9, x3;                     \
+        sbcs    x10, x10, xzr;                  \
+        sbcs    x11, x11, xzr;                  \
+        sbc     x12, x12, xzr;                  \
+        lsl     x5, x13, #32;                   \
+        add     x13, x5, x13;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x13;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x13;                    \
+        umulh   x4, x4, x13;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x13;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x2, x2, x5;                     \
+        sbcs    x9, x9, x4;                     \
+        sbcs    x10, x10, x3;                   \
+        sbcs    x11, x11, xzr;                  \
+        sbcs    x12, x12, xzr;                  \
+        sbc     x13, x13, xzr;                  \
+        adds    x2, x2, x14;                    \
+        adcs    x9, x9, x15;                    \
+        adcs    x10, x10, x16;                  \
+        adcs    x11, x11, x17;                  \
+        adcs    x12, x12, x19;                  \
+        adcs    x13, x13, x20;                  \
+        adc     x6, xzr, xzr;                   \
+        mov     x8, #-4294967295;               \
+        adds    x14, x2, x8;                    \
+        mov     x8, #4294967295;                \
+        adcs    x15, x9, x8;                    \
+        mov     x8, #1;                         \
+        adcs    x16, x10, x8;                   \
+        adcs    x17, x11, xzr;                  \
+        adcs    x19, x12, xzr;                  \
+        adcs    x20, x13, xzr;                  \
+        adcs    x6, x6, xzr;                    \
+        csel    x2, x2, x14, eq;                \
+        csel    x9, x9, x15, eq;                \
+        csel    x10, x10, x16, eq;              \
+        csel    x11, x11, x17, eq;              \
+        csel    x12, x12, x19, eq;              \
+        csel    x13, x13, x20, eq;              \
+        stp     x2, x9, [P0];                   \
+        stp     x10, x11, [P0+16];              \
+        stp     x12, x13, [P0+32]
+
+// Almost-Montgomery variant which we use when an input to other muls
+// with the other argument fully reduced (which is always safe). In
+// fact, with the Karatsuba-based Montgomery mul here, we don't even
+// *need* the restriction that the other argument is reduced.
+
+#define amontsqr_p384(P0,P1)                    \
+        ldp     x2, x3, [P1];                   \
+        mul     x9, x2, x3;                     \
+        umulh   x10, x2, x3;                    \
+        ldp     x4, x5, [P1+16];                \
+        mul     x8, x2, x4;                     \
+        adds    x10, x10, x8;                   \
+        mul     x11, x2, x5;                    \
+        mul     x8, x3, x4;                     \
+        adcs    x11, x11, x8;                   \
+        umulh   x12, x2, x5;                    \
+        mul     x8, x3, x5;                     \
+        adcs    x12, x12, x8;                   \
+        ldp     x6, x7, [P1+32];                \
+        mul     x13, x2, x7;                    \
+        mul     x8, x3, x6;                     \
+        adcs    x13, x13, x8;                   \
+        umulh   x14, x2, x7;                    \
+        mul     x8, x3, x7;                     \
+        adcs    x14, x14, x8;                   \
+        mul     x15, x5, x6;                    \
+        adcs    x15, x15, xzr;                  \
+        umulh   x16, x5, x6;                    \
+        adc     x16, x16, xzr;                  \
+        umulh   x8, x2, x4;                     \
+        adds    x11, x11, x8;                   \
+        umulh   x8, x3, x4;                     \
+        adcs    x12, x12, x8;                   \
+        umulh   x8, x3, x5;                     \
+        adcs    x13, x13, x8;                   \
+        umulh   x8, x3, x6;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x3, x7;                     \
+        adcs    x15, x15, x8;                   \
+        adc     x16, x16, xzr;                  \
+        mul     x8, x2, x6;                     \
+        adds    x12, x12, x8;                   \
+        mul     x8, x4, x5;                     \
+        adcs    x13, x13, x8;                   \
+        mul     x8, x4, x6;                     \
+        adcs    x14, x14, x8;                   \
+        mul     x8, x4, x7;                     \
+        adcs    x15, x15, x8;                   \
+        mul     x8, x5, x7;                     \
+        adcs    x16, x16, x8;                   \
+        mul     x17, x6, x7;                    \
+        adcs    x17, x17, xzr;                  \
+        umulh   x19, x6, x7;                    \
+        adc     x19, x19, xzr;                  \
+        umulh   x8, x2, x6;                     \
+        adds    x13, x13, x8;                   \
+        umulh   x8, x4, x5;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x4, x6;                     \
+        adcs    x15, x15, x8;                   \
+        umulh   x8, x4, x7;                     \
+        adcs    x16, x16, x8;                   \
+        umulh   x8, x5, x7;                     \
+        adcs    x17, x17, x8;                   \
+        adc     x19, x19, xzr;                  \
+        adds    x9, x9, x9;                     \
+        adcs    x10, x10, x10;                  \
+        adcs    x11, x11, x11;                  \
+        adcs    x12, x12, x12;                  \
+        adcs    x13, x13, x13;                  \
+        adcs    x14, x14, x14;                  \
+        adcs    x15, x15, x15;                  \
+        adcs    x16, x16, x16;                  \
+        adcs    x17, x17, x17;                  \
+        adcs    x19, x19, x19;                  \
+        cset    x20, hs;                        \
+        umulh   x8, x2, x2;                     \
+        mul     x2, x2, x2;                     \
+        adds    x9, x9, x8;                     \
+        mul     x8, x3, x3;                     \
+        adcs    x10, x10, x8;                   \
+        umulh   x8, x3, x3;                     \
+        adcs    x11, x11, x8;                   \
+        mul     x8, x4, x4;                     \
+        adcs    x12, x12, x8;                   \
+        umulh   x8, x4, x4;                     \
+        adcs    x13, x13, x8;                   \
+        mul     x8, x5, x5;                     \
+        adcs    x14, x14, x8;                   \
+        umulh   x8, x5, x5;                     \
+        adcs    x15, x15, x8;                   \
+        mul     x8, x6, x6;                     \
+        adcs    x16, x16, x8;                   \
+        umulh   x8, x6, x6;                     \
+        adcs    x17, x17, x8;                   \
+        mul     x8, x7, x7;                     \
+        adcs    x19, x19, x8;                   \
+        umulh   x8, x7, x7;                     \
+        adc     x20, x20, x8;                   \
+        lsl     x5, x2, #32;                    \
+        add     x2, x5, x2;                     \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x2;                     \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x2;                     \
+        umulh   x4, x4, x2;                     \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x2;                     \
+        adc     x3, xzr, xzr;                   \
+        subs    x9, x9, x5;                     \
+        sbcs    x10, x10, x4;                   \
+        sbcs    x11, x11, x3;                   \
+        sbcs    x12, x12, xzr;                  \
+        sbcs    x13, x13, xzr;                  \
+        sbc     x2, x2, xzr;                    \
+        lsl     x5, x9, #32;                    \
+        add     x9, x5, x9;                     \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x9;                     \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x9;                     \
+        umulh   x4, x4, x9;                     \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x9;                     \
+        adc     x3, xzr, xzr;                   \
+        subs    x10, x10, x5;                   \
+        sbcs    x11, x11, x4;                   \
+        sbcs    x12, x12, x3;                   \
+        sbcs    x13, x13, xzr;                  \
+        sbcs    x2, x2, xzr;                    \
+        sbc     x9, x9, xzr;                    \
+        lsl     x5, x10, #32;                   \
+        add     x10, x5, x10;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x10;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x10;                    \
+        umulh   x4, x4, x10;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x10;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x11, x11, x5;                   \
+        sbcs    x12, x12, x4;                   \
+        sbcs    x13, x13, x3;                   \
+        sbcs    x2, x2, xzr;                    \
+        sbcs    x9, x9, xzr;                    \
+        sbc     x10, x10, xzr;                  \
+        lsl     x5, x11, #32;                   \
+        add     x11, x5, x11;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x11;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x11;                    \
+        umulh   x4, x4, x11;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x11;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x12, x12, x5;                   \
+        sbcs    x13, x13, x4;                   \
+        sbcs    x2, x2, x3;                     \
+        sbcs    x9, x9, xzr;                    \
+        sbcs    x10, x10, xzr;                  \
+        sbc     x11, x11, xzr;                  \
+        lsl     x5, x12, #32;                   \
+        add     x12, x5, x12;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x12;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x12;                    \
+        umulh   x4, x4, x12;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x12;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x13, x13, x5;                   \
+        sbcs    x2, x2, x4;                     \
+        sbcs    x9, x9, x3;                     \
+        sbcs    x10, x10, xzr;                  \
+        sbcs    x11, x11, xzr;                  \
+        sbc     x12, x12, xzr;                  \
+        lsl     x5, x13, #32;                   \
+        add     x13, x5, x13;                   \
+        mov     x5, #-4294967295;               \
+        umulh   x5, x5, x13;                    \
+        mov     x4, #4294967295;                \
+        mul     x3, x4, x13;                    \
+        umulh   x4, x4, x13;                    \
+        adds    x5, x5, x3;                     \
+        adcs    x4, x4, x13;                    \
+        adc     x3, xzr, xzr;                   \
+        subs    x2, x2, x5;                     \
+        sbcs    x9, x9, x4;                     \
+        sbcs    x10, x10, x3;                   \
+        sbcs    x11, x11, xzr;                  \
+        sbcs    x12, x12, xzr;                  \
+        sbc     x13, x13, xzr;                  \
+        adds    x2, x2, x14;                    \
+        adcs    x9, x9, x15;                    \
+        adcs    x10, x10, x16;                  \
+        adcs    x11, x11, x17;                  \
+        adcs    x12, x12, x19;                  \
+        adcs    x13, x13, x20;                  \
+        mov     x14, #-4294967295;              \
+        mov     x15, #4294967295;               \
+        csel    x14, x14, xzr, cs;              \
+        csel    x15, x15, xzr, cs;              \
+        cset    x16, cs;                        \
+        adds    x2, x2, x14;                    \
+        adcs    x9, x9, x15;                    \
+        adcs    x10, x10, x16;                  \
+        adcs    x11, x11, xzr;                  \
+        adcs    x12, x12, xzr;                  \
+        adc     x13, x13, xzr;                  \
+        stp     x2, x9, [P0];                   \
+        stp     x10, x11, [P0+16];              \
+        stp     x12, x13, [P0+32]
+
+// Corresponds exactly to bignum_sub_p384
+
+#define sub_p384(P0,P1,P2)                      \
+        ldp     x5, x6, [P1];                   \
+        ldp     x4, x3, [P2];                   \
+        subs    x5, x5, x4;                     \
+        sbcs    x6, x6, x3;                     \
+        ldp     x7, x8, [P1+16];                \
+        ldp     x4, x3, [P2+16];                \
+        sbcs    x7, x7, x4;                     \
+        sbcs    x8, x8, x3;                     \
+        ldp     x9, x10, [P1+32];               \
+        ldp     x4, x3, [P2+32];                \
+        sbcs    x9, x9, x4;                     \
+        sbcs    x10, x10, x3;                   \
+        csetm   x3, lo;                         \
+        mov     x4, #4294967295;                \
+        and     x4, x4, x3;                     \
+        adds    x5, x5, x4;                     \
+        eor     x4, x4, x3;                     \
+        adcs    x6, x6, x4;                     \
+        mov     x4, #-2;                        \
+        and     x4, x4, x3;                     \
+        adcs    x7, x7, x4;                     \
+        adcs    x8, x8, x3;                     \
+        adcs    x9, x9, x3;                     \
+        adc     x10, x10, x3;                   \
+        stp     x5, x6, [P0];                   \
+        stp     x7, x8, [P0+16];                \
+        stp     x9, x10, [P0+32]
+
+S2N_BN_SYMBOL(p384_montjmixadd):
+
+// Save regs and make room on stack for temporary variables
+
+        stp     x19, x20, [sp, #-16]!
+        stp     x21, x22, [sp, #-16]!
+        stp     x23, x24, [sp, #-16]!
+        stp     x25, x26, [sp, #-16]!
+        sub     sp, sp, NSPACE
+
+// Move the input arguments to stable places
+
+        mov     input_z, x0
+        mov     input_x, x1
+        mov     input_y, x2
+
+// Main code, just a sequence of basic field operations
+// 8 * multiply + 3 * square + 7 * subtract
+
+        amontsqr_p384(zp2,z_1)
+        montmul_p384(y2a,z_1,y_2)
+
+        montmul_p384(x2a,zp2,x_2)
+        montmul_p384(y2a,zp2,y2a)
+
+        sub_p384(xd,x2a,x_1)
+        sub_p384(yd,y2a,y_1)
+
+        amontsqr_p384(zz,xd)
+        montsqr_p384(ww,yd)
+
+        montmul_p384(zzx1,zz,x_1)
+        montmul_p384(zzx2,zz,x2a)
+
+        sub_p384(x_3,ww,zzx1)
+        sub_p384(t1,zzx2,zzx1)
+
+        montmul_p384(z_3,xd,z_1)
+
+        sub_p384(x_3,x_3,zzx2)
+
+        sub_p384(t2,zzx1,x_3)
+
+        montmul_p384(t1,t1,y_1)
+        montmul_p384(t2,yd,t2)
+
+        sub_p384(y_3,t2,t1)
+
+// Restore stack and registers
+
+        add     sp, sp, NSPACE
+
+        ldp     x25, x26, [sp], 16
+        ldp     x23, x24, [sp], 16
+        ldp     x21, x22, [sp], 16
+        ldp     x19, x20, [sp], 16
+
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack, "", %progbits
+#endif
diff --git a/x86_att/p384/p384_montjadd.S b/x86_att/p384/p384_montjadd.S
new file mode 100644
index 000000000..e550f3860
--- /dev/null
+++ b/x86_att/p384/p384_montjadd.S
@@ -0,0 +1,955 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Point addition on NIST curve P-384 in Montgomery-Jacobian coordinates
+//
+//    extern void p384_montjadd
+//      (uint64_t p3[static 18],uint64_t p1[static 18],uint64_t p2[static 18]);
+//
+// Does p3 := p1 + p2 where all points are regarded as Jacobian triples with
+// each coordinate in the Montgomery domain, i.e. x' = (2^384 * x) mod p_384.
+// A Jacobian triple (x',y',z') represents affine point (x/z^2,y/z^3).
+//
+// Standard x86-64 ABI: RDI = p3, RSI = p1, RDX = p2
+// Microsoft x64 ABI:   RCX = p3, RDX = p1, R8 = p2
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(p384_montjadd)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(p384_montjadd)
+        .text
+        .balign 4
+
+// Size of individual field elements
+
+#define NUMSIZE 48
+
+// Pointer-offset pairs for inputs and outputs
+// These assume %rdi = p3, %rsi = p1 and %rcx = p2,
+// which needs to be set up explicitly before use
+
+#define x_1 0(%rsi)
+#define y_1 NUMSIZE(%rsi)
+#define z_1 (2*NUMSIZE)(%rsi)
+
+#define x_2 0(%rcx)
+#define y_2 NUMSIZE(%rcx)
+#define z_2 (2*NUMSIZE)(%rcx)
+
+#define x_3 0(%rdi)
+#define y_3 NUMSIZE(%rdi)
+#define z_3 (2*NUMSIZE)(%rdi)
+
+// In one place it's convenient to use another register
+// since the squaring function overwrites %rcx
+
+#define z_2_alt (2*NUMSIZE)(%rsi)
+
+// Pointer-offset pairs for temporaries, with some aliasing
+// NSPACE is the total stack needed for these temporaries
+
+#define z1sq (NUMSIZE*0)(%rsp)
+#define ww (NUMSIZE*0)(%rsp)
+
+#define yd (NUMSIZE*1)(%rsp)
+#define y2a (NUMSIZE*1)(%rsp)
+
+#define x2a (NUMSIZE*2)(%rsp)
+#define zzx2 (NUMSIZE*2)(%rsp)
+
+#define zz (NUMSIZE*3)(%rsp)
+#define t1 (NUMSIZE*3)(%rsp)
+
+#define t2 (NUMSIZE*4)(%rsp)
+#define x1a (NUMSIZE*4)(%rsp)
+#define zzx1 (NUMSIZE*4)(%rsp)
+
+#define xd (NUMSIZE*5)(%rsp)
+#define z2sq (NUMSIZE*5)(%rsp)
+
+#define y1a (NUMSIZE*6)(%rsp)
+
+// Temporaries for the actual input pointers
+
+#define input_x (NUMSIZE*7)(%rsp)
+#define input_y (NUMSIZE*7+8)(%rsp)
+#define input_z (NUMSIZE*7+16)(%rsp)
+
+#define NSPACE (NUMSIZE*7+24)
+
+// Corresponds exactly to bignum_montmul_p384
+
+#define montmul_p384(P0,P1,P2)                  \
+        movq   P2, %rdx ;                        \
+        xorl   %r15d, %r15d ;                       \
+        mulxq  P1, %r8, %r9 ;                      \
+        mulxq  0x8+P1, %rbx, %r10 ;                \
+        addq   %rbx, %r9 ;                          \
+        mulxq  0x10+P1, %rbx, %r11 ;               \
+        adcq   %rbx, %r10 ;                         \
+        mulxq  0x18+P1, %rbx, %r12 ;               \
+        adcq   %rbx, %r11 ;                         \
+        mulxq  0x20+P1, %rbx, %r13 ;               \
+        adcq   %rbx, %r12 ;                         \
+        mulxq  0x28+P1, %rbx, %r14 ;               \
+        adcq   %rbx, %r13 ;                         \
+        adcq   %r15, %r14 ;                         \
+        movq   %r8, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r8, %rdx ;                          \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r8, %rbx ;                      \
+        adcq   %r8, %rax ;                          \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r9 ;                          \
+        sbbq   %rbx, %r10 ;                         \
+        sbbq   %rbp, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r14 ;                         \
+        adcq   $0x0, %r15 ;                         \
+        movq   0x8+P2, %rdx ;                    \
+        xorl   %r8d, %r8d ;                         \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r9 ;                          \
+        adoxq  %rbx, %r10 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r10 ;                         \
+        adoxq  %rbx, %r11 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        adoxq  %r8, %r15 ;                          \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r14 ;                         \
+        adcq   %rbx, %r15 ;                         \
+        adcq   %r8, %r8 ;                           \
+        movq   %r9, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r9, %rdx ;                          \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r9, %rbx ;                      \
+        adcq   %r9, %rax ;                          \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r10 ;                         \
+        sbbq   %rbx, %r11 ;                         \
+        sbbq   %rbp, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        sbbq   $0x0, %r14 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r15 ;                         \
+        adcq   $0x0, %r8 ;                          \
+        movq   0x10+P2, %rdx ;                   \
+        xorl   %r9d, %r9d ;                         \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r10 ;                         \
+        adoxq  %rbx, %r11 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        adoxq  %r9, %r8 ;                           \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r15 ;                         \
+        adcq   %rbx, %r8 ;                          \
+        adcq   %r9, %r9 ;                           \
+        movq   %r10, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r10, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r10, %rbx ;                     \
+        adcq   %r10, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r11 ;                         \
+        sbbq   %rbx, %r12 ;                         \
+        sbbq   %rbp, %r13 ;                         \
+        sbbq   $0x0, %r14 ;                         \
+        sbbq   $0x0, %r15 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r8 ;                          \
+        adcq   $0x0, %r9 ;                          \
+        movq   0x18+P2, %rdx ;                   \
+        xorl   %r10d, %r10d ;                       \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rbx, %r8 ;                          \
+        adoxq  %r10, %r9 ;                          \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r8 ;                          \
+        adcq   %rbx, %r9 ;                          \
+        adcq   %r10, %r10 ;                         \
+        movq   %r11, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r11, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r11, %rbx ;                     \
+        adcq   %r11, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r12 ;                         \
+        sbbq   %rbx, %r13 ;                         \
+        sbbq   %rbp, %r14 ;                         \
+        sbbq   $0x0, %r15 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r9 ;                          \
+        adcq   $0x0, %r10 ;                         \
+        movq   0x20+P2, %rdx ;                   \
+        xorl   %r11d, %r11d ;                       \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rbx, %r8 ;                          \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r8 ;                          \
+        adoxq  %rbx, %r9 ;                          \
+        adoxq  %r11, %r10 ;                         \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r9 ;                          \
+        adcq   %rbx, %r10 ;                         \
+        adcq   %r11, %r11 ;                         \
+        movq   %r12, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r12, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r12, %rbx ;                     \
+        adcq   %r12, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r13 ;                         \
+        sbbq   %rbx, %r14 ;                         \
+        sbbq   %rbp, %r15 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r10 ;                         \
+        adcq   $0x0, %r11 ;                         \
+        movq   0x28+P2, %rdx ;                   \
+        xorl   %r12d, %r12d ;                       \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rbx, %r8 ;                          \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r8 ;                          \
+        adoxq  %rbx, %r9 ;                          \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r9 ;                          \
+        adoxq  %rbx, %r10 ;                         \
+        adoxq  %r12, %r11 ;                         \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r10 ;                         \
+        adcq   %rbx, %r11 ;                         \
+        adcq   %r12, %r12 ;                         \
+        movq   %r13, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r13, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r13, %rbx ;                     \
+        adcq   %r13, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r14 ;                         \
+        sbbq   %rbx, %r15 ;                         \
+        sbbq   %rbp, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r11 ;                         \
+        adcq   $0x0, %r12 ;                         \
+        xorl   %edx, %edx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        xorl   %r13d, %r13d ;                       \
+        movq   $0xffffffff00000001, %rax ;          \
+        addq   %r14, %rax ;                         \
+        movl   $0xffffffff, %ebx ;                  \
+        adcq   %r15, %rbx ;                         \
+        movl   $0x1, %ecx ;                         \
+        adcq   %r8, %rcx ;                          \
+        adcq   %r9, %rdx ;                          \
+        adcq   %r10, %rbp ;                         \
+        adcq   %r11, %r13 ;                         \
+        adcq   $0x0, %r12 ;                         \
+        cmovne %rax, %r14 ;                         \
+        cmovne %rbx, %r15 ;                         \
+        cmovne %rcx, %r8 ;                          \
+        cmovne %rdx, %r9 ;                          \
+        cmovne %rbp, %r10 ;                         \
+        cmovne %r13, %r11 ;                         \
+        movq   %r14, P0 ;                        \
+        movq   %r15, 0x8+P0 ;                    \
+        movq   %r8, 0x10+P0 ;                    \
+        movq   %r9, 0x18+P0 ;                    \
+        movq   %r10, 0x20+P0 ;                   \
+        movq   %r11, 0x28+P0
+
+// Corresponds exactly to bignum_montsqr_p384
+
+#define montsqr_p384(P0,P1)                     \
+        movq   P1, %rdx ;                        \
+        mulxq  0x8+P1, %r9, %r10 ;                 \
+        mulxq  0x18+P1, %r11, %r12 ;               \
+        mulxq  0x28+P1, %r13, %r14 ;               \
+        movq   0x18+P1, %rdx ;                   \
+        mulxq  0x20+P1, %r15, %rcx ;               \
+        xorl   %ebp, %ebp ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r10 ;                         \
+        adoxq  %rbx, %r11 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        movq   0x8+P1, %rdx ;                    \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        adcxq  %rbp, %r15 ;                         \
+        adoxq  %rbp, %rcx ;                         \
+        adcq   %rbp, %rcx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   0x20+P1, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x28+P1, %rax, %rdx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rdx, %rcx ;                         \
+        movq   0x28+P1, %rdx ;                   \
+        mulxq  0x20+P1, %rbx, %rbp ;               \
+        mulxq  0x18+P1, %rax, %rdx ;               \
+        adcxq  %rax, %rcx ;                         \
+        adoxq  %rdx, %rbx ;                         \
+        movl   $0x0, %eax ;                         \
+        adcxq  %rax, %rbx ;                         \
+        adoxq  %rax, %rbp ;                         \
+        adcq   %rax, %rbp ;                         \
+        xorq   %rax, %rax ;                         \
+        movq   P1, %rdx ;                        \
+        mulxq  P1, %r8, %rax ;                     \
+        adcxq  %r9, %r9 ;                           \
+        adoxq  %rax, %r9 ;                          \
+        movq   0x8+P1, %rdx ;                    \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r10, %r10 ;                         \
+        adoxq  %rax, %r10 ;                         \
+        adcxq  %r11, %r11 ;                         \
+        adoxq  %rdx, %r11 ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r12, %r12 ;                         \
+        adoxq  %rax, %r12 ;                         \
+        adcxq  %r13, %r13 ;                         \
+        adoxq  %rdx, %r13 ;                         \
+        movq   0x18+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r14, %r14 ;                         \
+        adoxq  %rax, %r14 ;                         \
+        adcxq  %r15, %r15 ;                         \
+        adoxq  %rdx, %r15 ;                         \
+        movq   0x20+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %rcx, %rcx ;                         \
+        adoxq  %rax, %rcx ;                         \
+        adcxq  %rbx, %rbx ;                         \
+        adoxq  %rdx, %rbx ;                         \
+        movq   0x28+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rsi ;                     \
+        adcxq  %rbp, %rbp ;                         \
+        adoxq  %rax, %rbp ;                         \
+        movl   $0x0, %eax ;                         \
+        adcxq  %rax, %rsi ;                         \
+        adoxq  %rax, %rsi ;                         \
+        movq   %rbx, P0 ;                        \
+        movq   %r8, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r8, %rdx ;                          \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r8, %rax ;                      \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r8 ;                      \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r8 ;                          \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r9 ;                          \
+        sbbq   %r8, %r10 ;                          \
+        sbbq   %rbx, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        movq   %rdx, %r8 ;                          \
+        sbbq   $0x0, %r8 ;                          \
+        movq   %r9, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r9, %rdx ;                          \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r9, %rax ;                      \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r9 ;                      \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r9 ;                          \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r10 ;                         \
+        sbbq   %r9, %r11 ;                          \
+        sbbq   %rbx, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        movq   %rdx, %r9 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %r10, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r10, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r10, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r10 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r10 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r11 ;                         \
+        sbbq   %r10, %r12 ;                         \
+        sbbq   %rbx, %r13 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %rdx, %r10 ;                         \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %r11, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r11, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r11, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r11 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r11 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r12 ;                         \
+        sbbq   %r11, %r13 ;                         \
+        sbbq   %rbx, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %rdx, %r11 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r12, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r12, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r12, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r12 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r12 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r13 ;                         \
+        sbbq   %r12, %r8 ;                          \
+        sbbq   %rbx, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %rdx, %r12 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %r13, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r13, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r13, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r13 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r13 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r8 ;                          \
+        sbbq   %r13, %r9 ;                          \
+        sbbq   %rbx, %r10 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %rdx, %r13 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        movq   P0, %rbx ;                        \
+        addq   %r8, %r14 ;                          \
+        adcq   %r9, %r15 ;                          \
+        adcq   %r10, %rcx ;                         \
+        adcq   %r11, %rbx ;                         \
+        adcq   %r12, %rbp ;                         \
+        adcq   %r13, %rsi ;                         \
+        movl   $0x0, %r8d ;                         \
+        adcq   %r8, %r8 ;                           \
+        xorq   %r11, %r11 ;                         \
+        xorq   %r12, %r12 ;                         \
+        xorq   %r13, %r13 ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        addq   %r14, %rax ;                         \
+        movl   $0xffffffff, %r9d ;                  \
+        adcq   %r15, %r9 ;                          \
+        movl   $0x1, %r10d ;                        \
+        adcq   %rcx, %r10 ;                         \
+        adcq   %rbx, %r11 ;                         \
+        adcq   %rbp, %r12 ;                         \
+        adcq   %rsi, %r13 ;                         \
+        adcq   $0x0, %r8 ;                          \
+        cmovne %rax, %r14 ;                         \
+        cmovne %r9, %r15 ;                          \
+        cmovne %r10, %rcx ;                         \
+        cmovne %r11, %rbx ;                         \
+        cmovne %r12, %rbp ;                         \
+        cmovne %r13, %rsi ;                         \
+        movq   %r14, P0 ;                        \
+        movq   %r15, 0x8+P0 ;                    \
+        movq   %rcx, 0x10+P0 ;                   \
+        movq   %rbx, 0x18+P0 ;                   \
+        movq   %rbp, 0x20+P0 ;                   \
+        movq   %rsi, 0x28+P0
+
+// Almost-Montgomery variant which we use when an input to other muls
+// with the other argument fully reduced (which is always safe).
+
+#define amontsqr_p384(P0,P1)                    \
+        movq   P1, %rdx ;                        \
+        mulxq  0x8+P1, %r9, %r10 ;                 \
+        mulxq  0x18+P1, %r11, %r12 ;               \
+        mulxq  0x28+P1, %r13, %r14 ;               \
+        movq   0x18+P1, %rdx ;                   \
+        mulxq  0x20+P1, %r15, %rcx ;               \
+        xorl   %ebp, %ebp ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r10 ;                         \
+        adoxq  %rbx, %r11 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        movq   0x8+P1, %rdx ;                    \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        adcxq  %rbp, %r15 ;                         \
+        adoxq  %rbp, %rcx ;                         \
+        adcq   %rbp, %rcx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   0x20+P1, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x28+P1, %rax, %rdx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rdx, %rcx ;                         \
+        movq   0x28+P1, %rdx ;                   \
+        mulxq  0x20+P1, %rbx, %rbp ;               \
+        mulxq  0x18+P1, %rax, %rdx ;               \
+        adcxq  %rax, %rcx ;                         \
+        adoxq  %rdx, %rbx ;                         \
+        movl   $0x0, %eax ;                         \
+        adcxq  %rax, %rbx ;                         \
+        adoxq  %rax, %rbp ;                         \
+        adcq   %rax, %rbp ;                         \
+        xorq   %rax, %rax ;                         \
+        movq   P1, %rdx ;                        \
+        mulxq  P1, %r8, %rax ;                     \
+        adcxq  %r9, %r9 ;                           \
+        adoxq  %rax, %r9 ;                          \
+        movq   0x8+P1, %rdx ;                    \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r10, %r10 ;                         \
+        adoxq  %rax, %r10 ;                         \
+        adcxq  %r11, %r11 ;                         \
+        adoxq  %rdx, %r11 ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r12, %r12 ;                         \
+        adoxq  %rax, %r12 ;                         \
+        adcxq  %r13, %r13 ;                         \
+        adoxq  %rdx, %r13 ;                         \
+        movq   0x18+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r14, %r14 ;                         \
+        adoxq  %rax, %r14 ;                         \
+        adcxq  %r15, %r15 ;                         \
+        adoxq  %rdx, %r15 ;                         \
+        movq   0x20+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %rcx, %rcx ;                         \
+        adoxq  %rax, %rcx ;                         \
+        adcxq  %rbx, %rbx ;                         \
+        adoxq  %rdx, %rbx ;                         \
+        movq   0x28+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rsi ;                     \
+        adcxq  %rbp, %rbp ;                         \
+        adoxq  %rax, %rbp ;                         \
+        movl   $0x0, %eax ;                         \
+        adcxq  %rax, %rsi ;                         \
+        adoxq  %rax, %rsi ;                         \
+        movq   %rbx, P0 ;                        \
+        movq   %r8, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r8, %rdx ;                          \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r8, %rax ;                      \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r8 ;                      \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r8 ;                          \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r9 ;                          \
+        sbbq   %r8, %r10 ;                          \
+        sbbq   %rbx, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        movq   %rdx, %r8 ;                          \
+        sbbq   $0x0, %r8 ;                          \
+        movq   %r9, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r9, %rdx ;                          \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r9, %rax ;                      \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r9 ;                      \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r9 ;                          \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r10 ;                         \
+        sbbq   %r9, %r11 ;                          \
+        sbbq   %rbx, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        movq   %rdx, %r9 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %r10, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r10, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r10, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r10 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r10 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r11 ;                         \
+        sbbq   %r10, %r12 ;                         \
+        sbbq   %rbx, %r13 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %rdx, %r10 ;                         \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %r11, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r11, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r11, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r11 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r11 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r12 ;                         \
+        sbbq   %r11, %r13 ;                         \
+        sbbq   %rbx, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %rdx, %r11 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r12, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r12, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r12, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r12 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r12 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r13 ;                         \
+        sbbq   %r12, %r8 ;                          \
+        sbbq   %rbx, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %rdx, %r12 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %r13, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r13, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r13, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r13 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r13 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r8 ;                          \
+        sbbq   %r13, %r9 ;                          \
+        sbbq   %rbx, %r10 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %rdx, %r13 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        movq   P0, %rbx ;                        \
+        addq   %r8, %r14 ;                          \
+        adcq   %r9, %r15 ;                          \
+        adcq   %r10, %rcx ;                         \
+        adcq   %r11, %rbx ;                         \
+        adcq   %r12, %rbp ;                         \
+        adcq   %r13, %rsi ;                         \
+        movl   $0x0, %r8d ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        movl   $0xffffffff, %r9d ;                  \
+        movl   $0x1, %r10d ;                        \
+        cmovnc %r8, %rax ;                         \
+        cmovnc %r8, %r9 ;                          \
+        cmovnc %r8, %r10 ;                         \
+        addq   %rax, %r14 ;                        \
+        adcq   %r9, %r15 ;                         \
+        adcq   %r10, %rcx ;                        \
+        adcq   %r8, %rbx ;                         \
+        adcq   %r8, %rbp ;                         \
+        adcq   %r8, %rsi ;                         \
+        movq   %r14, P0 ;                        \
+        movq   %r15, 0x8+P0 ;                    \
+        movq   %rcx, 0x10+P0 ;                   \
+        movq   %rbx, 0x18+P0 ;                   \
+        movq   %rbp, 0x20+P0 ;                   \
+        movq   %rsi, 0x28+P0
+
+// Corresponds exactly to bignum_sub_p384
+
+#define sub_p384(P0,P1,P2)                      \
+        movq   P1, %rax ;                        \
+        subq   P2, %rax ;                        \
+        movq   0x8+P1, %rdx ;                    \
+        sbbq   0x8+P2, %rdx ;                    \
+        movq   0x10+P1, %r8 ;                    \
+        sbbq   0x10+P2, %r8 ;                    \
+        movq   0x18+P1, %r9 ;                    \
+        sbbq   0x18+P2, %r9 ;                    \
+        movq   0x20+P1, %r10 ;                   \
+        sbbq   0x20+P2, %r10 ;                   \
+        movq   0x28+P1, %r11 ;                   \
+        sbbq   0x28+P2, %r11 ;                   \
+        sbbq   %rcx, %rcx ;                         \
+        movl   $0xffffffff, %esi ;                  \
+        andq   %rsi, %rcx ;                         \
+        xorq   %rsi, %rsi ;                         \
+        subq   %rcx, %rsi ;                         \
+        subq   %rsi, %rax ;                         \
+        movq   %rax, P0 ;                        \
+        sbbq   %rcx, %rdx ;                         \
+        movq   %rdx, 0x8+P0 ;                    \
+        sbbq   %rax, %rax ;                         \
+        andq   %rsi, %rcx ;                         \
+        negq   %rax;                             \
+        sbbq   %rcx, %r8 ;                          \
+        movq   %r8, 0x10+P0 ;                    \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %r9, 0x18+P0 ;                    \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %r10, 0x20+P0 ;                   \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r11, 0x28+P0
+
+S2N_BN_SYMBOL(p384_montjadd):
+
+#if WINDOWS_ABI
+        pushq   %rdi
+        pushq   %rsi
+        movq    %rcx, %rdi
+        movq    %rdx, %rsi
+        movq    %r8, %rdx
+#endif
+
+// Save registers and make room on stack for temporary variables
+// Put the input arguments in non-volatile places on the stack
+
+        pushq  %rbx
+        pushq  %rbp
+        pushq  %r12
+        pushq  %r13
+        pushq  %r14
+        pushq  %r15
+
+        subq    $NSPACE, %rsp
+
+        movq    %rdi, input_z
+        movq    %rsi, input_x
+        movq    %rdx, input_y
+
+// Main code, just a sequence of basic field operations
+// 8 * multiply + 3 * square + 7 * subtract
+
+        amontsqr_p384(z1sq,z_1)
+        movq    input_y, %rsi
+        amontsqr_p384(z2sq,z_2_alt)
+
+        movq    input_x, %rsi
+        movq    input_y, %rcx
+        montmul_p384(y1a,z_2,y_1)
+        movq    input_x, %rsi
+        movq    input_y, %rcx
+        montmul_p384(y2a,z_1,y_2)
+
+        movq    input_y, %rcx
+        montmul_p384(x2a,z1sq,x_2)
+        movq    input_x, %rsi
+        montmul_p384(x1a,z2sq,x_1)
+        montmul_p384(y2a,z1sq,y2a)
+        montmul_p384(y1a,z2sq,y1a)
+
+        sub_p384(xd,x2a,x1a)
+        sub_p384(yd,y2a,y1a)
+
+        amontsqr_p384(zz,xd)
+        montsqr_p384(ww,yd)
+
+        montmul_p384(zzx1,zz,x1a)
+        montmul_p384(zzx2,zz,x2a)
+
+        movq    input_z, %rdi
+        sub_p384(x_3,ww,zzx1)
+        sub_p384(t1,zzx2,zzx1)
+
+        movq    input_x, %rsi
+        montmul_p384(xd,xd,z_1)
+
+        movq    input_z, %rdi
+        sub_p384(x_3,x_3,zzx2)
+
+        movq    input_z, %rdi
+        sub_p384(t2,zzx1,x_3)
+
+        montmul_p384(t1,t1,y1a)
+
+        movq    input_z, %rdi
+        movq    input_y, %rcx
+        montmul_p384(z_3,xd,z_2)
+        montmul_p384(t2,yd,t2)
+
+        movq    input_z, %rdi
+        sub_p384(y_3,t2,t1)
+
+// Restore stack and registers
+
+        addq    $NSPACE, %rsp
+        popq    %r15
+        popq    %r14
+        popq    %r13
+        popq    %r12
+        popq    %rbp
+        popq    %rbx
+
+#if WINDOWS_ABI
+        popq   %rsi
+        popq   %rdi
+#endif
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack, "", %progbits
+#endif
diff --git a/x86_att/p384/p384_montjdouble.S b/x86_att/p384/p384_montjdouble.S
new file mode 100644
index 000000000..d7de78579
--- /dev/null
+++ b/x86_att/p384/p384_montjdouble.S
@@ -0,0 +1,1014 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Point doubling on NIST curve P-384 in Montgomery-Jacobian coordinates
+//
+//    extern void p384_montjdouble
+//      (uint64_t p3[static 18],uint64_t p1[static 18]);
+//
+// Does p3 := 2 * p1 where all points are regarded as Jacobian triples with
+// each coordinate in the Montgomery domain, i.e. x' = (2^384 * x) mod p_384.
+// A Jacobian triple (x',y',z') represents affine point (x/z^2,y/z^3).
+//
+// Standard x86-64 ABI: RDI = p3, RSI = p1
+// Microsoft x64 ABI:   RCX = p3, RDX = p1
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(p384_montjdouble)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(p384_montjdouble)
+        .text
+        .balign 4
+
+// Size of individual field elements
+
+#define NUMSIZE 48
+
+// Pointer-offset pairs for inputs and outputs
+// These assume %rdi = p3, %rsi = p1. The latter stays true
+// but montsqr below modifies %rdi as well. Thus, we need
+// to save %rdi and restore it before the writes to outputs.
+
+#define x_1 0(%rsi)
+#define y_1 NUMSIZE(%rsi)
+#define z_1 (2*NUMSIZE)(%rsi)
+
+#define x_3 0(%rdi)
+#define y_3 NUMSIZE(%rdi)
+#define z_3 (2*NUMSIZE)(%rdi)
+
+// Pointer-offset pairs for temporaries, with some aliasing
+// NSPACE is the total stack needed for these temporaries
+
+#define z2 (NUMSIZE*0)(%rsp)
+#define y2 (NUMSIZE*1)(%rsp)
+#define x2p (NUMSIZE*2)(%rsp)
+#define xy2 (NUMSIZE*3)(%rsp)
+
+#define y4 (NUMSIZE*4)(%rsp)
+#define t2 (NUMSIZE*4)(%rsp)
+
+#define dx2 (NUMSIZE*5)(%rsp)
+#define t1 (NUMSIZE*5)(%rsp)
+
+#define d (NUMSIZE*6)(%rsp)
+#define x4p (NUMSIZE*6)(%rsp)
+
+// Safe place for pointer to the output
+
+#define input_z (NUMSIZE*7)(%rsp)
+
+#define NSPACE (NUMSIZE*7+8)
+
+// Corresponds exactly to bignum_montmul_p384
+
+#define montmul_p384(P0,P1,P2)                  \
+        movq   P2, %rdx ;                        \
+        xorl   %r15d, %r15d ;                       \
+        mulxq  P1, %r8, %r9 ;                      \
+        mulxq  0x8+P1, %rbx, %r10 ;                \
+        addq   %rbx, %r9 ;                          \
+        mulxq  0x10+P1, %rbx, %r11 ;               \
+        adcq   %rbx, %r10 ;                         \
+        mulxq  0x18+P1, %rbx, %r12 ;               \
+        adcq   %rbx, %r11 ;                         \
+        mulxq  0x20+P1, %rbx, %r13 ;               \
+        adcq   %rbx, %r12 ;                         \
+        mulxq  0x28+P1, %rbx, %r14 ;               \
+        adcq   %rbx, %r13 ;                         \
+        adcq   %r15, %r14 ;                         \
+        movq   %r8, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r8, %rdx ;                          \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r8, %rbx ;                      \
+        adcq   %r8, %rax ;                          \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r9 ;                          \
+        sbbq   %rbx, %r10 ;                         \
+        sbbq   %rbp, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r14 ;                         \
+        adcq   $0x0, %r15 ;                         \
+        movq   0x8+P2, %rdx ;                    \
+        xorl   %r8d, %r8d ;                         \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r9 ;                          \
+        adoxq  %rbx, %r10 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r10 ;                         \
+        adoxq  %rbx, %r11 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        adoxq  %r8, %r15 ;                          \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r14 ;                         \
+        adcq   %rbx, %r15 ;                         \
+        adcq   %r8, %r8 ;                           \
+        movq   %r9, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r9, %rdx ;                          \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r9, %rbx ;                      \
+        adcq   %r9, %rax ;                          \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r10 ;                         \
+        sbbq   %rbx, %r11 ;                         \
+        sbbq   %rbp, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        sbbq   $0x0, %r14 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r15 ;                         \
+        adcq   $0x0, %r8 ;                          \
+        movq   0x10+P2, %rdx ;                   \
+        xorl   %r9d, %r9d ;                         \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r10 ;                         \
+        adoxq  %rbx, %r11 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        adoxq  %r9, %r8 ;                           \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r15 ;                         \
+        adcq   %rbx, %r8 ;                          \
+        adcq   %r9, %r9 ;                           \
+        movq   %r10, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r10, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r10, %rbx ;                     \
+        adcq   %r10, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r11 ;                         \
+        sbbq   %rbx, %r12 ;                         \
+        sbbq   %rbp, %r13 ;                         \
+        sbbq   $0x0, %r14 ;                         \
+        sbbq   $0x0, %r15 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r8 ;                          \
+        adcq   $0x0, %r9 ;                          \
+        movq   0x18+P2, %rdx ;                   \
+        xorl   %r10d, %r10d ;                       \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rbx, %r8 ;                          \
+        adoxq  %r10, %r9 ;                          \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r8 ;                          \
+        adcq   %rbx, %r9 ;                          \
+        adcq   %r10, %r10 ;                         \
+        movq   %r11, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r11, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r11, %rbx ;                     \
+        adcq   %r11, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r12 ;                         \
+        sbbq   %rbx, %r13 ;                         \
+        sbbq   %rbp, %r14 ;                         \
+        sbbq   $0x0, %r15 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r9 ;                          \
+        adcq   $0x0, %r10 ;                         \
+        movq   0x20+P2, %rdx ;                   \
+        xorl   %r11d, %r11d ;                       \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rbx, %r8 ;                          \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r8 ;                          \
+        adoxq  %rbx, %r9 ;                          \
+        adoxq  %r11, %r10 ;                         \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r9 ;                          \
+        adcq   %rbx, %r10 ;                         \
+        adcq   %r11, %r11 ;                         \
+        movq   %r12, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r12, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r12, %rbx ;                     \
+        adcq   %r12, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r13 ;                         \
+        sbbq   %rbx, %r14 ;                         \
+        sbbq   %rbp, %r15 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r10 ;                         \
+        adcq   $0x0, %r11 ;                         \
+        movq   0x28+P2, %rdx ;                   \
+        xorl   %r12d, %r12d ;                       \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rbx, %r8 ;                          \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r8 ;                          \
+        adoxq  %rbx, %r9 ;                          \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r9 ;                          \
+        adoxq  %rbx, %r10 ;                         \
+        adoxq  %r12, %r11 ;                         \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r10 ;                         \
+        adcq   %rbx, %r11 ;                         \
+        adcq   %r12, %r12 ;                         \
+        movq   %r13, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r13, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r13, %rbx ;                     \
+        adcq   %r13, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r14 ;                         \
+        sbbq   %rbx, %r15 ;                         \
+        sbbq   %rbp, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r11 ;                         \
+        adcq   $0x0, %r12 ;                         \
+        xorl   %edx, %edx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        xorl   %r13d, %r13d ;                       \
+        movq   $0xffffffff00000001, %rax ;          \
+        addq   %r14, %rax ;                         \
+        movl   $0xffffffff, %ebx ;                  \
+        adcq   %r15, %rbx ;                         \
+        movl   $0x1, %ecx ;                         \
+        adcq   %r8, %rcx ;                          \
+        adcq   %r9, %rdx ;                          \
+        adcq   %r10, %rbp ;                         \
+        adcq   %r11, %r13 ;                         \
+        adcq   $0x0, %r12 ;                         \
+        cmovne %rax, %r14 ;                         \
+        cmovne %rbx, %r15 ;                         \
+        cmovne %rcx, %r8 ;                          \
+        cmovne %rdx, %r9 ;                          \
+        cmovne %rbp, %r10 ;                         \
+        cmovne %r13, %r11 ;                         \
+        movq   %r14, P0 ;                        \
+        movq   %r15, 0x8+P0 ;                    \
+        movq   %r8, 0x10+P0 ;                    \
+        movq   %r9, 0x18+P0 ;                    \
+        movq   %r10, 0x20+P0 ;                   \
+        movq   %r11, 0x28+P0
+
+// Corresponds exactly to bignum_montsqr_p384
+
+#define montsqr_p384(P0,P1)                     \
+        movq   P1, %rdx ;                        \
+        mulxq  0x8+P1, %r9, %r10 ;                 \
+        mulxq  0x18+P1, %r11, %r12 ;               \
+        mulxq  0x28+P1, %r13, %r14 ;               \
+        movq   0x18+P1, %rdx ;                   \
+        mulxq  0x20+P1, %r15, %rcx ;               \
+        xorl   %ebp, %ebp ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r10 ;                         \
+        adoxq  %rbx, %r11 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        movq   0x8+P1, %rdx ;                    \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        adcxq  %rbp, %r15 ;                         \
+        adoxq  %rbp, %rcx ;                         \
+        adcq   %rbp, %rcx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   0x20+P1, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x28+P1, %rax, %rdx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rdx, %rcx ;                         \
+        movq   0x28+P1, %rdx ;                   \
+        mulxq  0x20+P1, %rbx, %rbp ;               \
+        mulxq  0x18+P1, %rax, %rdx ;               \
+        adcxq  %rax, %rcx ;                         \
+        adoxq  %rdx, %rbx ;                         \
+        movl   $0x0, %eax ;                         \
+        adcxq  %rax, %rbx ;                         \
+        adoxq  %rax, %rbp ;                         \
+        adcq   %rax, %rbp ;                         \
+        xorq   %rax, %rax ;                         \
+        movq   P1, %rdx ;                        \
+        mulxq  P1, %r8, %rax ;                     \
+        adcxq  %r9, %r9 ;                           \
+        adoxq  %rax, %r9 ;                          \
+        movq   0x8+P1, %rdx ;                    \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r10, %r10 ;                         \
+        adoxq  %rax, %r10 ;                         \
+        adcxq  %r11, %r11 ;                         \
+        adoxq  %rdx, %r11 ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r12, %r12 ;                         \
+        adoxq  %rax, %r12 ;                         \
+        adcxq  %r13, %r13 ;                         \
+        adoxq  %rdx, %r13 ;                         \
+        movq   0x18+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r14, %r14 ;                         \
+        adoxq  %rax, %r14 ;                         \
+        adcxq  %r15, %r15 ;                         \
+        adoxq  %rdx, %r15 ;                         \
+        movq   0x20+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %rcx, %rcx ;                         \
+        adoxq  %rax, %rcx ;                         \
+        adcxq  %rbx, %rbx ;                         \
+        adoxq  %rdx, %rbx ;                         \
+        movq   0x28+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdi ;                     \
+        adcxq  %rbp, %rbp ;                         \
+        adoxq  %rax, %rbp ;                         \
+        movl   $0x0, %eax ;                         \
+        adcxq  %rax, %rdi ;                         \
+        adoxq  %rax, %rdi ;                         \
+        movq   %rbx, P0 ;                        \
+        movq   %r8, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r8, %rdx ;                          \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r8, %rax ;                      \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r8 ;                      \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r8 ;                          \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r9 ;                          \
+        sbbq   %r8, %r10 ;                          \
+        sbbq   %rbx, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        movq   %rdx, %r8 ;                          \
+        sbbq   $0x0, %r8 ;                          \
+        movq   %r9, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r9, %rdx ;                          \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r9, %rax ;                      \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r9 ;                      \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r9 ;                          \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r10 ;                         \
+        sbbq   %r9, %r11 ;                          \
+        sbbq   %rbx, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        movq   %rdx, %r9 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %r10, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r10, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r10, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r10 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r10 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r11 ;                         \
+        sbbq   %r10, %r12 ;                         \
+        sbbq   %rbx, %r13 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %rdx, %r10 ;                         \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %r11, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r11, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r11, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r11 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r11 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r12 ;                         \
+        sbbq   %r11, %r13 ;                         \
+        sbbq   %rbx, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %rdx, %r11 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r12, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r12, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r12, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r12 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r12 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r13 ;                         \
+        sbbq   %r12, %r8 ;                          \
+        sbbq   %rbx, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %rdx, %r12 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %r13, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r13, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r13, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r13 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r13 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r8 ;                          \
+        sbbq   %r13, %r9 ;                          \
+        sbbq   %rbx, %r10 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %rdx, %r13 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        movq   P0, %rbx ;                        \
+        addq   %r8, %r14 ;                          \
+        adcq   %r9, %r15 ;                          \
+        adcq   %r10, %rcx ;                         \
+        adcq   %r11, %rbx ;                         \
+        adcq   %r12, %rbp ;                         \
+        adcq   %r13, %rdi ;                         \
+        movl   $0x0, %r8d ;                         \
+        adcq   %r8, %r8 ;                           \
+        xorq   %r11, %r11 ;                         \
+        xorq   %r12, %r12 ;                         \
+        xorq   %r13, %r13 ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        addq   %r14, %rax ;                         \
+        movl   $0xffffffff, %r9d ;                  \
+        adcq   %r15, %r9 ;                          \
+        movl   $0x1, %r10d ;                        \
+        adcq   %rcx, %r10 ;                         \
+        adcq   %rbx, %r11 ;                         \
+        adcq   %rbp, %r12 ;                         \
+        adcq   %rdi, %r13 ;                         \
+        adcq   $0x0, %r8 ;                          \
+        cmovne %rax, %r14 ;                         \
+        cmovne %r9, %r15 ;                          \
+        cmovne %r10, %rcx ;                         \
+        cmovne %r11, %rbx ;                         \
+        cmovne %r12, %rbp ;                         \
+        cmovne %r13, %rdi ;                         \
+        movq   %r14, P0 ;                        \
+        movq   %r15, 0x8+P0 ;                    \
+        movq   %rcx, 0x10+P0 ;                   \
+        movq   %rbx, 0x18+P0 ;                   \
+        movq   %rbp, 0x20+P0 ;                   \
+        movq   %rdi, 0x28+P0
+
+#define sub_p384(P0,P1,P2)                      \
+        movq   P1, %rax ;                        \
+        subq   P2, %rax ;                        \
+        movq   0x8+P1, %rdx ;                    \
+        sbbq   0x8+P2, %rdx ;                    \
+        movq   0x10+P1, %r8 ;                    \
+        sbbq   0x10+P2, %r8 ;                    \
+        movq   0x18+P1, %r9 ;                    \
+        sbbq   0x18+P2, %r9 ;                    \
+        movq   0x20+P1, %r10 ;                   \
+        sbbq   0x20+P2, %r10 ;                   \
+        movq   0x28+P1, %r11 ;                   \
+        sbbq   0x28+P2, %r11 ;                   \
+        sbbq   %rcx, %rcx ;                         \
+        movl   $0xffffffff, %ebx ;                  \
+        andq   %rbx, %rcx ;                         \
+        xorq   %rbx, %rbx ;                         \
+        subq   %rcx, %rbx ;                         \
+        subq   %rbx, %rax ;                         \
+        movq   %rax, P0 ;                        \
+        sbbq   %rcx, %rdx ;                         \
+        movq   %rdx, 0x8+P0 ;                    \
+        sbbq   %rax, %rax ;                         \
+        andq   %rbx, %rcx ;                         \
+        negq   %rax;                             \
+        sbbq   %rcx, %r8 ;                          \
+        movq   %r8, 0x10+P0 ;                    \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %r9, 0x18+P0 ;                    \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %r10, 0x20+P0 ;                   \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r11, 0x28+P0
+
+// Simplified bignum_add_p384, without carry chain suspension
+
+#define add_p384(P0,P1,P2)                      \
+        movq   P1, %rax ;                        \
+        addq   P2, %rax ;                        \
+        movq   0x8+P1, %rcx ;                    \
+        adcq   0x8+P2, %rcx ;                    \
+        movq   0x10+P1, %r8 ;                    \
+        adcq   0x10+P2, %r8 ;                    \
+        movq   0x18+P1, %r9 ;                    \
+        adcq   0x18+P2, %r9 ;                    \
+        movq   0x20+P1, %r10 ;                   \
+        adcq   0x20+P2, %r10 ;                   \
+        movq   0x28+P1, %r11 ;                   \
+        adcq   0x28+P2, %r11 ;                   \
+        movl   $0x0, %edx ;                         \
+        adcq   %rdx, %rdx ;                         \
+        movq   $0xffffffff00000001, %rbp ;          \
+        addq   %rbp, %rax ;                         \
+        movl   $0xffffffff, %ebp ;                  \
+        adcq   %rbp, %rcx ;                         \
+        adcq   $0x1, %r8 ;                          \
+        adcq   $0x0, %r9 ;                          \
+        adcq   $0x0, %r10 ;                         \
+        adcq   $0x0, %r11 ;                         \
+        adcq   $0xffffffffffffffff, %rdx ;          \
+        movl   $1, %ebx ;                           \
+        andq   %rdx, %rbx ;                         \
+        andq   %rbp, %rdx ;                         \
+        xorq   %rbp, %rbp ;                         \
+        subq   %rdx, %rbp ;                         \
+        subq   %rbp, %rax ;                         \
+        movq   %rax, P0 ;                        \
+        sbbq   %rdx, %rcx ;                         \
+        movq   %rcx, 0x8+P0 ;                    \
+        sbbq   %rbx, %r8 ;                          \
+        movq   %r8, 0x10+P0 ;                    \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %r9, 0x18+P0 ;                    \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %r10, 0x20+P0 ;                   \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r11, 0x28+P0
+
+// P0 = 4 * P1 - P2
+
+#define cmsub41_p384(P0,P1,P2)                  \
+        movq    40+P1, %rdx ;                   \
+        movq    %rdx, %r13 ;                       \
+        shrq    $62, %rdx ;                        \
+        movq    32+P1, %r12 ;                   \
+        shldq   $2, %r12, %r13 ;                    \
+        movq    24+P1, %r11 ;                   \
+        shldq   $2, %r11, %r12 ;                    \
+        movq    16+P1, %r10 ;                   \
+        shldq   $2, %r10, %r11 ;                    \
+        movq    8+P1, %r9 ;                     \
+        shldq   $2, %r9, %r10 ;                     \
+        movq    P1, %r8 ;                       \
+        shldq   $2, %r8, %r9 ;                      \
+        shlq    $2, %r8 ;                          \
+        addq    $1, %rdx ;                         \
+        subq   P2, %r8 ;                        \
+        sbbq   0x8+P2, %r9 ;                    \
+        sbbq   0x10+P2, %r10 ;                  \
+        sbbq   0x18+P2, %r11 ;                  \
+        sbbq   0x20+P2, %r12 ;                  \
+        sbbq   0x28+P2, %r13 ;                  \
+        sbbq   $0, %rdx ;                           \
+        xorq   %rcx, %rcx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rax, %rcx ;                     \
+        adcxq  %rax, %r8 ;                          \
+        adoxq  %rcx, %r9 ;                          \
+        movl   $0xffffffff, %eax ;                  \
+        mulxq  %rax, %rax, %rcx ;                     \
+        adcxq  %rax, %r9 ;                          \
+        adoxq  %rcx, %r10 ;                         \
+        adcxq  %rdx, %r10 ;                         \
+        movl   $0x0, %eax ;                         \
+        movl   $0x0, %ecx ;                         \
+        adoxq  %rax, %rax ;                         \
+        adcq   %rax, %r11 ;                         \
+        adcq   %rcx, %r12 ;                         \
+        adcq   %rcx, %r13 ;                         \
+        adcq   %rcx, %rcx ;                         \
+        subq   $0x1, %rcx ;                         \
+        movl   $0xffffffff, %edx ;                  \
+        xorq   %rax, %rax ;                         \
+        andq   %rcx, %rdx ;                         \
+        subq   %rdx, %rax ;                         \
+        andq   $0x1, %rcx ;                         \
+        subq   %rax, %r8 ;                          \
+        movq   %r8, P0 ;                         \
+        sbbq   %rdx, %r9 ;                          \
+        movq   %r9, 0x8+P0 ;                     \
+        sbbq   %rcx, %r10 ;                         \
+        movq   %r10, 0x10+P0 ;                   \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r11, 0x18+P0 ;                   \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %r12, 0x20+P0 ;                   \
+        sbbq   $0x0, %r13 ;                         \
+        movq   %r13, 0x28+P0
+
+// P0 = C * P1 - D * P2
+
+#define cmsub_p384(P0,C,P1,D,P2)                \
+        movq    $0x00000000ffffffff, %r8 ;         \
+        subq    P2, %r8 ;                       \
+        movq    $0xffffffff00000000, %r9 ;         \
+        sbbq    8+P2, %r9 ;                     \
+        movq    $0xfffffffffffffffe, %r10 ;        \
+        sbbq    16+P2, %r10 ;                   \
+        movq    $0xffffffffffffffff, %r11 ;        \
+        sbbq    24+P2, %r11 ;                   \
+        movq    $0xffffffffffffffff, %r12 ;        \
+        sbbq    32+P2, %r12 ;                   \
+        movq    $0xffffffffffffffff, %r13 ;        \
+        sbbq    40+P2, %r13 ;                   \
+        movq    $D, %rdx ;                         \
+        mulxq   %r8, %r8, %rax ;                     \
+        mulxq   %r9, %r9, %rcx ;                     \
+        addq    %rax, %r9 ;                        \
+        mulxq   %r10, %r10, %rax ;                   \
+        adcq    %rcx, %r10 ;                       \
+        mulxq   %r11, %r11, %rcx ;                   \
+        adcq    %rax, %r11 ;                       \
+        mulxq   %r12, %r12, %rax ;                   \
+        adcq    %rcx, %r12 ;                       \
+        mulxq   %r13, %r13, %r14 ;                   \
+        adcq    %rax, %r13 ;                       \
+        adcq    $1, %r14 ;                         \
+        xorl    %ecx, %ecx ;                       \
+        movq    $C, %rdx ;                         \
+        mulxq   P1, %rax, %rbx ;                 \
+        adcxq   %rax, %r8 ;                        \
+        adoxq   %rbx, %r9 ;                        \
+        mulxq   8+P1, %rax, %rbx ;               \
+        adcxq   %rax, %r9 ;                        \
+        adoxq   %rbx, %r10 ;                       \
+        mulxq   16+P1, %rax, %rbx ;              \
+        adcxq   %rax, %r10 ;                       \
+        adoxq   %rbx, %r11 ;                       \
+        mulxq   24+P1, %rax, %rbx ;              \
+        adcxq   %rax, %r11 ;                       \
+        adoxq   %rbx, %r12 ;                       \
+        mulxq   32+P1, %rax, %rbx ;              \
+        adcxq   %rax, %r12 ;                       \
+        adoxq   %rbx, %r13 ;                       \
+        mulxq   40+P1, %rax, %rdx ;              \
+        adcxq   %rax, %r13 ;                       \
+        adoxq   %r14, %rdx ;                       \
+        adcxq   %rcx, %rdx ;                       \
+        xorq   %rcx, %rcx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rax, %rcx ;                     \
+        adcxq  %rax, %r8 ;                          \
+        adoxq  %rcx, %r9 ;                          \
+        movl   $0xffffffff, %eax ;                  \
+        mulxq  %rax, %rax, %rcx ;                     \
+        adcxq  %rax, %r9 ;                          \
+        adoxq  %rcx, %r10 ;                         \
+        adcxq  %rdx, %r10 ;                         \
+        movl   $0x0, %eax ;                         \
+        movl   $0x0, %ecx ;                         \
+        adoxq  %rax, %rax ;                         \
+        adcq   %rax, %r11 ;                         \
+        adcq   %rcx, %r12 ;                         \
+        adcq   %rcx, %r13 ;                         \
+        adcq   %rcx, %rcx ;                         \
+        subq   $0x1, %rcx ;                         \
+        movl   $0xffffffff, %edx ;                  \
+        xorq   %rax, %rax ;                         \
+        andq   %rcx, %rdx ;                         \
+        subq   %rdx, %rax ;                         \
+        andq   $0x1, %rcx ;                         \
+        subq   %rax, %r8 ;                          \
+        movq   %r8, P0 ;                         \
+        sbbq   %rdx, %r9 ;                          \
+        movq   %r9, 0x8+P0 ;                     \
+        sbbq   %rcx, %r10 ;                         \
+        movq   %r10, 0x10+P0 ;                   \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r11, 0x18+P0 ;                   \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %r12, 0x20+P0 ;                   \
+        sbbq   $0x0, %r13 ;                         \
+        movq   %r13, 0x28+P0
+
+// A weak version of add that only guarantees sum in 6 digits
+
+#define weakadd_p384(P0,P1,P2)                  \
+        movq   P1, %rax ;                        \
+        addq   P2, %rax ;                        \
+        movq   0x8+P1, %rcx ;                    \
+        adcq   0x8+P2, %rcx ;                    \
+        movq   0x10+P1, %r8 ;                    \
+        adcq   0x10+P2, %r8 ;                    \
+        movq   0x18+P1, %r9 ;                    \
+        adcq   0x18+P2, %r9 ;                    \
+        movq   0x20+P1, %r10 ;                   \
+        adcq   0x20+P2, %r10 ;                   \
+        movq   0x28+P1, %r11 ;                   \
+        adcq   0x28+P2, %r11 ;                   \
+        sbbq   %rdx, %rdx ;                         \
+        movl   $1, %ebx ;                           \
+        andq   %rdx, %rbx ;                         \
+        movl   $0xffffffff, %ebp ;                  \
+        andq   %rbp, %rdx ;                         \
+        xorq   %rbp, %rbp ;                         \
+        subq   %rdx, %rbp ;                         \
+        addq   %rbp, %rax ;                         \
+        movq   %rax, P0 ;                        \
+        adcq   %rdx, %rcx ;                         \
+        movq   %rcx, 0x8+P0 ;                    \
+        adcq   %rbx, %r8 ;                          \
+        movq   %r8, 0x10+P0 ;                    \
+        adcq   $0x0, %r9 ;                          \
+        movq   %r9, 0x18+P0 ;                    \
+        adcq   $0x0, %r10 ;                         \
+        movq   %r10, 0x20+P0 ;                   \
+        adcq   $0x0, %r11 ;                         \
+        movq   %r11, 0x28+P0
+
+// P0 = 3 * P1 - 8 * P2
+
+#define cmsub38_p384(P0,P1,P2)                  \
+        movq    $0x00000000ffffffff, %r8 ;         \
+        subq    P2, %r8 ;                       \
+        movq    $0xffffffff00000000, %r9 ;         \
+        sbbq    8+P2, %r9 ;                     \
+        movq    $0xfffffffffffffffe, %r10 ;        \
+        sbbq    16+P2, %r10 ;                   \
+        movq    $0xffffffffffffffff, %r11 ;        \
+        sbbq    24+P2, %r11 ;                   \
+        movq    $0xffffffffffffffff, %r12 ;        \
+        sbbq    32+P2, %r12 ;                   \
+        movq    $0xffffffffffffffff, %r13 ;        \
+        sbbq    40+P2, %r13 ;                   \
+        movq    %r13, %r14 ;                       \
+        shrq    $61, %r14 ;                        \
+        shldq   $3, %r12, %r13 ;                    \
+        shldq   $3, %r11, %r12 ;                    \
+        shldq   $3, %r10, %r11 ;                    \
+        shldq   $3, %r9, %r10 ;                     \
+        shldq   $3, %r8, %r9 ;                      \
+        shlq    $3, %r8 ;                          \
+        addq    $1, %r14 ;                         \
+        xorl    %ecx, %ecx ;                       \
+        movq    $3, %rdx ;                         \
+        mulxq   P1, %rax, %rbx ;                 \
+        adcxq   %rax, %r8 ;                        \
+        adoxq   %rbx, %r9 ;                        \
+        mulxq   8+P1, %rax, %rbx ;               \
+        adcxq   %rax, %r9 ;                        \
+        adoxq   %rbx, %r10 ;                       \
+        mulxq   16+P1, %rax, %rbx ;              \
+        adcxq   %rax, %r10 ;                       \
+        adoxq   %rbx, %r11 ;                       \
+        mulxq   24+P1, %rax, %rbx ;              \
+        adcxq   %rax, %r11 ;                       \
+        adoxq   %rbx, %r12 ;                       \
+        mulxq   32+P1, %rax, %rbx ;              \
+        adcxq   %rax, %r12 ;                       \
+        adoxq   %rbx, %r13 ;                       \
+        mulxq   40+P1, %rax, %rdx ;              \
+        adcxq   %rax, %r13 ;                       \
+        adoxq   %r14, %rdx ;                       \
+        adcxq   %rcx, %rdx ;                       \
+        xorq   %rcx, %rcx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rax, %rcx ;                     \
+        adcxq  %rax, %r8 ;                          \
+        adoxq  %rcx, %r9 ;                          \
+        movl   $0xffffffff, %eax ;                  \
+        mulxq  %rax, %rax, %rcx ;                     \
+        adcxq  %rax, %r9 ;                          \
+        adoxq  %rcx, %r10 ;                         \
+        adcxq  %rdx, %r10 ;                         \
+        movl   $0x0, %eax ;                         \
+        movl   $0x0, %ecx ;                         \
+        adoxq  %rax, %rax ;                         \
+        adcq   %rax, %r11 ;                         \
+        adcq   %rcx, %r12 ;                         \
+        adcq   %rcx, %r13 ;                         \
+        adcq   %rcx, %rcx ;                         \
+        subq   $0x1, %rcx ;                         \
+        movl   $0xffffffff, %edx ;                  \
+        xorq   %rax, %rax ;                         \
+        andq   %rcx, %rdx ;                         \
+        subq   %rdx, %rax ;                         \
+        andq   $0x1, %rcx ;                         \
+        subq   %rax, %r8 ;                          \
+        movq   %r8, P0 ;                         \
+        sbbq   %rdx, %r9 ;                          \
+        movq   %r9, 0x8+P0 ;                     \
+        sbbq   %rcx, %r10 ;                         \
+        movq   %r10, 0x10+P0 ;                   \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r11, 0x18+P0 ;                   \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %r12, 0x20+P0 ;                   \
+        sbbq   $0x0, %r13 ;                         \
+        movq   %r13, 0x28+P0
+
+S2N_BN_SYMBOL(p384_montjdouble):
+
+#if WINDOWS_ABI
+        pushq   %rdi
+        pushq   %rsi
+        movq    %rcx, %rdi
+        movq    %rdx, %rsi
+#endif
+
+// Save registers and make room on stack for temporary variables
+// Save the outpuy pointer %rdi which gets overwritten in earlier
+// operations before it is used.
+
+        pushq  %rbx
+        pushq  %rbp
+        pushq  %r12
+        pushq  %r13
+        pushq  %r14
+        pushq  %r15
+
+        subq    $NSPACE, %rsp
+
+        movq    %rdi, input_z
+
+// Main code, just a sequence of basic field operations
+
+// z2 = z^2
+// y2 = y^2
+
+        montsqr_p384(z2,z_1)
+        montsqr_p384(y2,y_1)
+
+// x2p = x^2 - z^4 = (x + z^2) * (x - z^2)
+
+        weakadd_p384(t1,x_1,z2)
+        sub_p384(t2,x_1,z2)
+        montmul_p384(x2p,t1,t2)
+
+// t1 = y + z
+// x4p = x2p^2
+// xy2 = x * y^2
+
+        add_p384(t1,y_1,z_1)
+        montsqr_p384(x4p,x2p)
+        montmul_p384(xy2,x_1,y2)
+
+// t2 = (y + z)^2
+
+        montsqr_p384(t2,t1)
+
+// d = 12 * xy2 - 9 * x4p
+// t1 = y^2 + 2 * y * z
+
+        cmsub_p384(d,12,xy2,9,x4p)
+        sub_p384(t1,t2,z2)
+
+// y4 = y^4
+
+        montsqr_p384(y4,y2)
+
+// Restore the output pointer to write to x_3, y_3 and z_3.
+
+        movq    input_z, %rdi
+
+// z_3' = 2 * y * z
+// dx2 = d * x2p
+
+        sub_p384(z_3,t1,y2)
+        montmul_p384(dx2,d,x2p)
+
+// x' = 4 * xy2 - d
+
+        cmsub41_p384(x_3,xy2,d)
+
+// y' = 3 * dx2 - 8 * y4
+
+        cmsub38_p384(y_3,dx2,y4)
+
+// Restore stack and registers
+
+        addq    $NSPACE, %rsp
+        popq    %r15
+        popq    %r14
+        popq    %r13
+        popq    %r12
+        popq    %rbp
+        popq    %rbx
+
+#if WINDOWS_ABI
+        popq   %rsi
+        popq   %rdi
+#endif
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack, "", %progbits
+#endif
diff --git a/x86_att/p384/p384_montjmixadd.S b/x86_att/p384/p384_montjmixadd.S
new file mode 100644
index 000000000..6749209eb
--- /dev/null
+++ b/x86_att/p384/p384_montjmixadd.S
@@ -0,0 +1,941 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Point mixed addition on NIST curve P-384 in Montgomery-Jacobian coordinates
+//
+//    extern void p384_montjmixadd
+//      (uint64_t p3[static 18],uint64_t p1[static 18],uint64_t p2[static 12]);
+//
+// Does p3 := p1 + p2 where all points are regarded as Jacobian triples with
+// each coordinate in the Montgomery domain, i.e. x' = (2^384 * x) mod p_384.
+// A Jacobian triple (x',y',z') represents affine point (x/z^2,y/z^3).
+// The "mixed" part means that p2 only has x and y coordinates, with the
+// implicit z coordinate assumed to be the identity.
+//
+// Standard x86-64 ABI: RDI = p3, RSI = p1, RDX = p2
+// Microsoft x64 ABI:   RCX = p3, RDX = p1, R8 = p2
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(p384_montjmixadd)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(p384_montjmixadd)
+        .text
+        .balign 4
+
+// Size of individual field elements
+
+#define NUMSIZE 48
+
+// Pointer-offset pairs for inputs and outputs
+// These assume %rdi = p3, %rsi = p1 and %rcx = p2,
+// which needs to be set up explicitly before use
+
+#define x_1 0(%rsi)
+#define y_1 NUMSIZE(%rsi)
+#define z_1 (2*NUMSIZE)(%rsi)
+
+#define x_2 0(%rcx)
+#define y_2 NUMSIZE(%rcx)
+
+#define x_3 0(%rdi)
+#define y_3 NUMSIZE(%rdi)
+#define z_3 (2*NUMSIZE)(%rdi)
+
+// Pointer-offset pairs for temporaries, with some aliasing
+// NSPACE is the total stack needed for these temporaries
+
+#define zp2 (NUMSIZE*0)(%rsp)
+#define ww (NUMSIZE*0)(%rsp)
+
+#define yd (NUMSIZE*1)(%rsp)
+#define y2a (NUMSIZE*1)(%rsp)
+
+#define x2a (NUMSIZE*2)(%rsp)
+#define zzx2 (NUMSIZE*2)(%rsp)
+
+#define zz (NUMSIZE*3)(%rsp)
+#define t1 (NUMSIZE*3)(%rsp)
+
+#define t2 (NUMSIZE*4)(%rsp)
+#define zzx1 (NUMSIZE*4)(%rsp)
+
+#define xd (NUMSIZE*5)(%rsp)
+
+// Temporaries for the actual input pointers
+
+#define input_x (NUMSIZE*6)(%rsp)
+#define input_y (NUMSIZE*6+8)(%rsp)
+#define input_z (NUMSIZE*6+16)(%rsp)
+
+#define NSPACE (NUMSIZE*6+24)
+
+// Corresponds exactly to bignum_montmul_p384
+
+#define montmul_p384(P0,P1,P2)                  \
+        movq   P2, %rdx ;                        \
+        xorl   %r15d, %r15d ;                       \
+        mulxq  P1, %r8, %r9 ;                      \
+        mulxq  0x8+P1, %rbx, %r10 ;                \
+        addq   %rbx, %r9 ;                          \
+        mulxq  0x10+P1, %rbx, %r11 ;               \
+        adcq   %rbx, %r10 ;                         \
+        mulxq  0x18+P1, %rbx, %r12 ;               \
+        adcq   %rbx, %r11 ;                         \
+        mulxq  0x20+P1, %rbx, %r13 ;               \
+        adcq   %rbx, %r12 ;                         \
+        mulxq  0x28+P1, %rbx, %r14 ;               \
+        adcq   %rbx, %r13 ;                         \
+        adcq   %r15, %r14 ;                         \
+        movq   %r8, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r8, %rdx ;                          \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r8, %rbx ;                      \
+        adcq   %r8, %rax ;                          \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r9 ;                          \
+        sbbq   %rbx, %r10 ;                         \
+        sbbq   %rbp, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r14 ;                         \
+        adcq   $0x0, %r15 ;                         \
+        movq   0x8+P2, %rdx ;                    \
+        xorl   %r8d, %r8d ;                         \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r9 ;                          \
+        adoxq  %rbx, %r10 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r10 ;                         \
+        adoxq  %rbx, %r11 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        adoxq  %r8, %r15 ;                          \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r14 ;                         \
+        adcq   %rbx, %r15 ;                         \
+        adcq   %r8, %r8 ;                           \
+        movq   %r9, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r9, %rdx ;                          \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r9, %rbx ;                      \
+        adcq   %r9, %rax ;                          \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r10 ;                         \
+        sbbq   %rbx, %r11 ;                         \
+        sbbq   %rbp, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        sbbq   $0x0, %r14 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r15 ;                         \
+        adcq   $0x0, %r8 ;                          \
+        movq   0x10+P2, %rdx ;                   \
+        xorl   %r9d, %r9d ;                         \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r10 ;                         \
+        adoxq  %rbx, %r11 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        adoxq  %r9, %r8 ;                           \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r15 ;                         \
+        adcq   %rbx, %r8 ;                          \
+        adcq   %r9, %r9 ;                           \
+        movq   %r10, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r10, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r10, %rbx ;                     \
+        adcq   %r10, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r11 ;                         \
+        sbbq   %rbx, %r12 ;                         \
+        sbbq   %rbp, %r13 ;                         \
+        sbbq   $0x0, %r14 ;                         \
+        sbbq   $0x0, %r15 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r8 ;                          \
+        adcq   $0x0, %r9 ;                          \
+        movq   0x18+P2, %rdx ;                   \
+        xorl   %r10d, %r10d ;                       \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rbx, %r8 ;                          \
+        adoxq  %r10, %r9 ;                          \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r8 ;                          \
+        adcq   %rbx, %r9 ;                          \
+        adcq   %r10, %r10 ;                         \
+        movq   %r11, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r11, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r11, %rbx ;                     \
+        adcq   %r11, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r12 ;                         \
+        sbbq   %rbx, %r13 ;                         \
+        sbbq   %rbp, %r14 ;                         \
+        sbbq   $0x0, %r15 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r9 ;                          \
+        adcq   $0x0, %r10 ;                         \
+        movq   0x20+P2, %rdx ;                   \
+        xorl   %r11d, %r11d ;                       \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rbx, %r8 ;                          \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r8 ;                          \
+        adoxq  %rbx, %r9 ;                          \
+        adoxq  %r11, %r10 ;                         \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r9 ;                          \
+        adcq   %rbx, %r10 ;                         \
+        adcq   %r11, %r11 ;                         \
+        movq   %r12, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r12, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r12, %rbx ;                     \
+        adcq   %r12, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r13 ;                         \
+        sbbq   %rbx, %r14 ;                         \
+        sbbq   %rbp, %r15 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r10 ;                         \
+        adcq   $0x0, %r11 ;                         \
+        movq   0x28+P2, %rdx ;                   \
+        xorl   %r12d, %r12d ;                       \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x10+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rbx, %r8 ;                          \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r8 ;                          \
+        adoxq  %rbx, %r9 ;                          \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r9 ;                          \
+        adoxq  %rbx, %r10 ;                         \
+        adoxq  %r12, %r11 ;                         \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcq   %rax, %r10 ;                         \
+        adcq   %rbx, %r11 ;                         \
+        adcq   %r12, %r12 ;                         \
+        movq   %r13, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r13, %rdx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %rbx, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %r13, %rbx ;                     \
+        adcq   %r13, %rax ;                         \
+        adcq   %rdx, %rbx ;                         \
+        adcl   %ebp, %ebp ;                         \
+        subq   %rax, %r14 ;                         \
+        sbbq   %rbx, %r15 ;                         \
+        sbbq   %rbp, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        sbbq   $0x0, %rdx ;                         \
+        addq   %rdx, %r11 ;                         \
+        adcq   $0x0, %r12 ;                         \
+        xorl   %edx, %edx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        xorl   %r13d, %r13d ;                       \
+        movq   $0xffffffff00000001, %rax ;          \
+        addq   %r14, %rax ;                         \
+        movl   $0xffffffff, %ebx ;                  \
+        adcq   %r15, %rbx ;                         \
+        movl   $0x1, %ecx ;                         \
+        adcq   %r8, %rcx ;                          \
+        adcq   %r9, %rdx ;                          \
+        adcq   %r10, %rbp ;                         \
+        adcq   %r11, %r13 ;                         \
+        adcq   $0x0, %r12 ;                         \
+        cmovne %rax, %r14 ;                         \
+        cmovne %rbx, %r15 ;                         \
+        cmovne %rcx, %r8 ;                          \
+        cmovne %rdx, %r9 ;                          \
+        cmovne %rbp, %r10 ;                         \
+        cmovne %r13, %r11 ;                         \
+        movq   %r14, P0 ;                        \
+        movq   %r15, 0x8+P0 ;                    \
+        movq   %r8, 0x10+P0 ;                    \
+        movq   %r9, 0x18+P0 ;                    \
+        movq   %r10, 0x20+P0 ;                   \
+        movq   %r11, 0x28+P0
+
+// Corresponds exactly to bignum_montsqr_p384
+
+#define montsqr_p384(P0,P1)                     \
+        movq   P1, %rdx ;                        \
+        mulxq  0x8+P1, %r9, %r10 ;                 \
+        mulxq  0x18+P1, %r11, %r12 ;               \
+        mulxq  0x28+P1, %r13, %r14 ;               \
+        movq   0x18+P1, %rdx ;                   \
+        mulxq  0x20+P1, %r15, %rcx ;               \
+        xorl   %ebp, %ebp ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r10 ;                         \
+        adoxq  %rbx, %r11 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        movq   0x8+P1, %rdx ;                    \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        adcxq  %rbp, %r15 ;                         \
+        adoxq  %rbp, %rcx ;                         \
+        adcq   %rbp, %rcx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   0x20+P1, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x28+P1, %rax, %rdx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rdx, %rcx ;                         \
+        movq   0x28+P1, %rdx ;                   \
+        mulxq  0x20+P1, %rbx, %rbp ;               \
+        mulxq  0x18+P1, %rax, %rdx ;               \
+        adcxq  %rax, %rcx ;                         \
+        adoxq  %rdx, %rbx ;                         \
+        movl   $0x0, %eax ;                         \
+        adcxq  %rax, %rbx ;                         \
+        adoxq  %rax, %rbp ;                         \
+        adcq   %rax, %rbp ;                         \
+        xorq   %rax, %rax ;                         \
+        movq   P1, %rdx ;                        \
+        mulxq  P1, %r8, %rax ;                     \
+        adcxq  %r9, %r9 ;                           \
+        adoxq  %rax, %r9 ;                          \
+        movq   0x8+P1, %rdx ;                    \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r10, %r10 ;                         \
+        adoxq  %rax, %r10 ;                         \
+        adcxq  %r11, %r11 ;                         \
+        adoxq  %rdx, %r11 ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r12, %r12 ;                         \
+        adoxq  %rax, %r12 ;                         \
+        adcxq  %r13, %r13 ;                         \
+        adoxq  %rdx, %r13 ;                         \
+        movq   0x18+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r14, %r14 ;                         \
+        adoxq  %rax, %r14 ;                         \
+        adcxq  %r15, %r15 ;                         \
+        adoxq  %rdx, %r15 ;                         \
+        movq   0x20+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %rcx, %rcx ;                         \
+        adoxq  %rax, %rcx ;                         \
+        adcxq  %rbx, %rbx ;                         \
+        adoxq  %rdx, %rbx ;                         \
+        movq   0x28+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rsi ;                     \
+        adcxq  %rbp, %rbp ;                         \
+        adoxq  %rax, %rbp ;                         \
+        movl   $0x0, %eax ;                         \
+        adcxq  %rax, %rsi ;                         \
+        adoxq  %rax, %rsi ;                         \
+        movq   %rbx, P0 ;                        \
+        movq   %r8, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r8, %rdx ;                          \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r8, %rax ;                      \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r8 ;                      \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r8 ;                          \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r9 ;                          \
+        sbbq   %r8, %r10 ;                          \
+        sbbq   %rbx, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        movq   %rdx, %r8 ;                          \
+        sbbq   $0x0, %r8 ;                          \
+        movq   %r9, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r9, %rdx ;                          \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r9, %rax ;                      \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r9 ;                      \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r9 ;                          \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r10 ;                         \
+        sbbq   %r9, %r11 ;                          \
+        sbbq   %rbx, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        movq   %rdx, %r9 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %r10, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r10, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r10, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r10 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r10 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r11 ;                         \
+        sbbq   %r10, %r12 ;                         \
+        sbbq   %rbx, %r13 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %rdx, %r10 ;                         \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %r11, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r11, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r11, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r11 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r11 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r12 ;                         \
+        sbbq   %r11, %r13 ;                         \
+        sbbq   %rbx, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %rdx, %r11 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r12, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r12, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r12, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r12 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r12 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r13 ;                         \
+        sbbq   %r12, %r8 ;                          \
+        sbbq   %rbx, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %rdx, %r12 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %r13, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r13, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r13, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r13 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r13 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r8 ;                          \
+        sbbq   %r13, %r9 ;                          \
+        sbbq   %rbx, %r10 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %rdx, %r13 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        movq   P0, %rbx ;                        \
+        addq   %r8, %r14 ;                          \
+        adcq   %r9, %r15 ;                          \
+        adcq   %r10, %rcx ;                         \
+        adcq   %r11, %rbx ;                         \
+        adcq   %r12, %rbp ;                         \
+        adcq   %r13, %rsi ;                         \
+        movl   $0x0, %r8d ;                         \
+        adcq   %r8, %r8 ;                           \
+        xorq   %r11, %r11 ;                         \
+        xorq   %r12, %r12 ;                         \
+        xorq   %r13, %r13 ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        addq   %r14, %rax ;                         \
+        movl   $0xffffffff, %r9d ;                  \
+        adcq   %r15, %r9 ;                          \
+        movl   $0x1, %r10d ;                        \
+        adcq   %rcx, %r10 ;                         \
+        adcq   %rbx, %r11 ;                         \
+        adcq   %rbp, %r12 ;                         \
+        adcq   %rsi, %r13 ;                         \
+        adcq   $0x0, %r8 ;                          \
+        cmovne %rax, %r14 ;                         \
+        cmovne %r9, %r15 ;                          \
+        cmovne %r10, %rcx ;                         \
+        cmovne %r11, %rbx ;                         \
+        cmovne %r12, %rbp ;                         \
+        cmovne %r13, %rsi ;                         \
+        movq   %r14, P0 ;                        \
+        movq   %r15, 0x8+P0 ;                    \
+        movq   %rcx, 0x10+P0 ;                   \
+        movq   %rbx, 0x18+P0 ;                   \
+        movq   %rbp, 0x20+P0 ;                   \
+        movq   %rsi, 0x28+P0
+
+// Almost-Montgomery variant which we use when an input to other muls
+// with the other argument fully reduced (which is always safe).
+
+#define amontsqr_p384(P0,P1)                    \
+        movq   P1, %rdx ;                        \
+        mulxq  0x8+P1, %r9, %r10 ;                 \
+        mulxq  0x18+P1, %r11, %r12 ;               \
+        mulxq  0x28+P1, %r13, %r14 ;               \
+        movq   0x18+P1, %rdx ;                   \
+        mulxq  0x20+P1, %r15, %rcx ;               \
+        xorl   %ebp, %ebp ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r10 ;                         \
+        adoxq  %rbx, %r11 ;                         \
+        mulxq  0x8+P1, %rax, %rbx ;                \
+        adcxq  %rax, %r11 ;                         \
+        adoxq  %rbx, %r12 ;                         \
+        movq   0x8+P1, %rdx ;                    \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x28+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        adcxq  %rbp, %r15 ;                         \
+        adoxq  %rbp, %rcx ;                         \
+        adcq   %rbp, %rcx ;                         \
+        xorl   %ebp, %ebp ;                         \
+        movq   0x20+P1, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                    \
+        adcxq  %rax, %r12 ;                         \
+        adoxq  %rbx, %r13 ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  0x18+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r13 ;                         \
+        adoxq  %rbx, %r14 ;                         \
+        mulxq  0x20+P1, %rax, %rbx ;               \
+        adcxq  %rax, %r14 ;                         \
+        adoxq  %rbx, %r15 ;                         \
+        mulxq  0x28+P1, %rax, %rdx ;               \
+        adcxq  %rax, %r15 ;                         \
+        adoxq  %rdx, %rcx ;                         \
+        movq   0x28+P1, %rdx ;                   \
+        mulxq  0x20+P1, %rbx, %rbp ;               \
+        mulxq  0x18+P1, %rax, %rdx ;               \
+        adcxq  %rax, %rcx ;                         \
+        adoxq  %rdx, %rbx ;                         \
+        movl   $0x0, %eax ;                         \
+        adcxq  %rax, %rbx ;                         \
+        adoxq  %rax, %rbp ;                         \
+        adcq   %rax, %rbp ;                         \
+        xorq   %rax, %rax ;                         \
+        movq   P1, %rdx ;                        \
+        mulxq  P1, %r8, %rax ;                     \
+        adcxq  %r9, %r9 ;                           \
+        adoxq  %rax, %r9 ;                          \
+        movq   0x8+P1, %rdx ;                    \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r10, %r10 ;                         \
+        adoxq  %rax, %r10 ;                         \
+        adcxq  %r11, %r11 ;                         \
+        adoxq  %rdx, %r11 ;                         \
+        movq   0x10+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r12, %r12 ;                         \
+        adoxq  %rax, %r12 ;                         \
+        adcxq  %r13, %r13 ;                         \
+        adoxq  %rdx, %r13 ;                         \
+        movq   0x18+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %r14, %r14 ;                         \
+        adoxq  %rax, %r14 ;                         \
+        adcxq  %r15, %r15 ;                         \
+        adoxq  %rdx, %r15 ;                         \
+        movq   0x20+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                     \
+        adcxq  %rcx, %rcx ;                         \
+        adoxq  %rax, %rcx ;                         \
+        adcxq  %rbx, %rbx ;                         \
+        adoxq  %rdx, %rbx ;                         \
+        movq   0x28+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rsi ;                     \
+        adcxq  %rbp, %rbp ;                         \
+        adoxq  %rax, %rbp ;                         \
+        movl   $0x0, %eax ;                         \
+        adcxq  %rax, %rsi ;                         \
+        adoxq  %rax, %rsi ;                         \
+        movq   %rbx, P0 ;                        \
+        movq   %r8, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r8, %rdx ;                          \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r8, %rax ;                      \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r8 ;                      \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r8 ;                          \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r9 ;                          \
+        sbbq   %r8, %r10 ;                          \
+        sbbq   %rbx, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        movq   %rdx, %r8 ;                          \
+        sbbq   $0x0, %r8 ;                          \
+        movq   %r9, %rdx ;                          \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r9, %rdx ;                          \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r9, %rax ;                      \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r9 ;                      \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r9 ;                          \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r10 ;                         \
+        sbbq   %r9, %r11 ;                          \
+        sbbq   %rbx, %r12 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        movq   %rdx, %r9 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %r10, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r10, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r10, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r10 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r10 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r11 ;                         \
+        sbbq   %r10, %r12 ;                         \
+        sbbq   %rbx, %r13 ;                         \
+        sbbq   $0x0, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %rdx, %r10 ;                         \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %r11, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r11, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r11, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r11 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r11 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r12 ;                         \
+        sbbq   %r11, %r13 ;                         \
+        sbbq   %rbx, %r8 ;                          \
+        sbbq   $0x0, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %rdx, %r11 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r12, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r12, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r12, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r12 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r12 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r13 ;                         \
+        sbbq   %r12, %r8 ;                          \
+        sbbq   %rbx, %r9 ;                          \
+        sbbq   $0x0, %r10 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %rdx, %r12 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %r13, %rdx ;                         \
+        shlq   $0x20, %rdx ;                        \
+        addq   %r13, %rdx ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        mulxq  %rax, %r13, %rax ;                     \
+        movl   $0xffffffff, %ebx ;                  \
+        mulxq  %rbx, %rbx, %r13 ;                     \
+        addq   %rbx, %rax ;                         \
+        adcq   %rdx, %r13 ;                         \
+        movl   $0x0, %ebx ;                         \
+        adcq   %rbx, %rbx ;                         \
+        subq   %rax, %r8 ;                          \
+        sbbq   %r13, %r9 ;                          \
+        sbbq   %rbx, %r10 ;                         \
+        sbbq   $0x0, %r11 ;                         \
+        sbbq   $0x0, %r12 ;                         \
+        movq   %rdx, %r13 ;                         \
+        sbbq   $0x0, %r13 ;                         \
+        movq   P0, %rbx ;                        \
+        addq   %r8, %r14 ;                          \
+        adcq   %r9, %r15 ;                          \
+        adcq   %r10, %rcx ;                         \
+        adcq   %r11, %rbx ;                         \
+        adcq   %r12, %rbp ;                         \
+        adcq   %r13, %rsi ;                         \
+        movl   $0x0, %r8d ;                         \
+        movq   $0xffffffff00000001, %rax ;          \
+        movl   $0xffffffff, %r9d ;                  \
+        movl   $0x1, %r10d ;                        \
+        cmovnc %r8, %rax ;                         \
+        cmovnc %r8, %r9 ;                          \
+        cmovnc %r8, %r10 ;                         \
+        addq   %rax, %r14 ;                        \
+        adcq   %r9, %r15 ;                         \
+        adcq   %r10, %rcx ;                        \
+        adcq   %r8, %rbx ;                         \
+        adcq   %r8, %rbp ;                         \
+        adcq   %r8, %rsi ;                         \
+        movq   %r14, P0 ;                        \
+        movq   %r15, 0x8+P0 ;                    \
+        movq   %rcx, 0x10+P0 ;                   \
+        movq   %rbx, 0x18+P0 ;                   \
+        movq   %rbp, 0x20+P0 ;                   \
+        movq   %rsi, 0x28+P0
+
+// Corresponds exactly to bignum_sub_p384
+
+#define sub_p384(P0,P1,P2)                      \
+        movq   P1, %rax ;                        \
+        subq   P2, %rax ;                        \
+        movq   0x8+P1, %rdx ;                    \
+        sbbq   0x8+P2, %rdx ;                    \
+        movq   0x10+P1, %r8 ;                    \
+        sbbq   0x10+P2, %r8 ;                    \
+        movq   0x18+P1, %r9 ;                    \
+        sbbq   0x18+P2, %r9 ;                    \
+        movq   0x20+P1, %r10 ;                   \
+        sbbq   0x20+P2, %r10 ;                   \
+        movq   0x28+P1, %r11 ;                   \
+        sbbq   0x28+P2, %r11 ;                   \
+        sbbq   %rcx, %rcx ;                         \
+        movl   $0xffffffff, %esi ;                  \
+        andq   %rsi, %rcx ;                         \
+        xorq   %rsi, %rsi ;                         \
+        subq   %rcx, %rsi ;                         \
+        subq   %rsi, %rax ;                         \
+        movq   %rax, P0 ;                        \
+        sbbq   %rcx, %rdx ;                         \
+        movq   %rdx, 0x8+P0 ;                    \
+        sbbq   %rax, %rax ;                         \
+        andq   %rsi, %rcx ;                         \
+        negq   %rax;                             \
+        sbbq   %rcx, %r8 ;                          \
+        movq   %r8, 0x10+P0 ;                    \
+        sbbq   $0x0, %r9 ;                          \
+        movq   %r9, 0x18+P0 ;                    \
+        sbbq   $0x0, %r10 ;                         \
+        movq   %r10, 0x20+P0 ;                   \
+        sbbq   $0x0, %r11 ;                         \
+        movq   %r11, 0x28+P0
+
+S2N_BN_SYMBOL(p384_montjmixadd):
+
+#if WINDOWS_ABI
+        pushq   %rdi
+        pushq   %rsi
+        movq    %rcx, %rdi
+        movq    %rdx, %rsi
+        movq    %r8, %rdx
+#endif
+
+// Save registers and make room on stack for temporary variables
+// Put the input arguments in non-volatile places on the stack
+
+        pushq  %rbx
+        pushq  %rbp
+        pushq  %r12
+        pushq  %r13
+        pushq  %r14
+        pushq  %r15
+
+        subq    $NSPACE, %rsp
+
+        movq    %rdi, input_z
+        movq    %rsi, input_x
+        movq    %rdx, input_y
+
+// Main code, just a sequence of basic field operations
+// 8 * multiply + 3 * square + 7 * subtract
+
+        amontsqr_p384(zp2,z_1)
+
+        movq    input_x, %rsi
+        movq    input_y, %rcx
+        montmul_p384(y2a,z_1,y_2)
+
+        movq    input_y, %rcx
+        montmul_p384(x2a,zp2,x_2)
+
+        montmul_p384(y2a,zp2,y2a)
+
+        movq    input_x, %rsi
+        sub_p384(xd,x2a,x_1)
+        movq    input_x, %rsi
+        sub_p384(yd,y2a,y_1)
+
+        amontsqr_p384(zz,xd)
+        montsqr_p384(ww,yd)
+
+        movq    input_x, %rsi
+        montmul_p384(zzx1,zz,x_1)
+        montmul_p384(zzx2,zz,x2a)
+
+        movq    input_z, %rdi
+        sub_p384(x_3,ww,zzx1)
+        sub_p384(t1,zzx2,zzx1)
+
+        movq    input_z, %rdi
+        movq    input_x, %rsi
+        montmul_p384(z_3,xd,z_1)
+
+        movq    input_z, %rdi
+        sub_p384(x_3,x_3,zzx2)
+
+        movq    input_z, %rdi
+        sub_p384(t2,zzx1,x_3)
+
+        movq    input_x, %rsi
+        montmul_p384(t1,t1,y_1)
+        montmul_p384(t2,yd,t2)
+
+        movq    input_z, %rdi
+        sub_p384(y_3,t2,t1)
+
+// Restore stack and registers
+
+        addq    $NSPACE, %rsp
+        popq    %r15
+        popq    %r14
+        popq    %r13
+        popq    %r12
+        popq    %rbp
+        popq    %rbx
+
+#if WINDOWS_ABI
+        popq   %rsi
+        popq   %rdi
+#endif
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack, "", %progbits
+#endif
