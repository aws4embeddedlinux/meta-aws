From 2ebc24c65fa39974f0e438849552947169121e59 Mon Sep 17 00:00:00 2001
From: Juneyoung Lee <aqjune@gmail.com>
Date: Wed, 24 May 2023 22:13:22 +0000
Subject: [PATCH] Add NEON versions of functions for RSA 2048 and 4096

s2n-bignum original commit: https://github.com/awslabs/s2n-bignum/commit/36a214d18bab26d63a895e89fbfe5be6d1f6f211
---
 arm/fastmul/bignum_emontredc_8n_neon.S | 1093 +++++++++++++++++++
 arm/fastmul/bignum_kmul_16_32_neon.S   |  835 ++++++++++++++
 arm/fastmul/bignum_kmul_32_64_neon.S   | 1387 ++++++++++++++++++++++++
 arm/fastmul/bignum_ksqr_16_32_neon.S   |  658 +++++++++++
 arm/fastmul/bignum_ksqr_32_64_neon.S   | 1075 ++++++++++++++++++
 5 files changed, 5048 insertions(+)
 create mode 100644 arm/fastmul/bignum_emontredc_8n_neon.S
 create mode 100644 arm/fastmul/bignum_kmul_16_32_neon.S
 create mode 100644 arm/fastmul/bignum_kmul_32_64_neon.S
 create mode 100644 arm/fastmul/bignum_ksqr_16_32_neon.S
 create mode 100644 arm/fastmul/bignum_ksqr_32_64_neon.S

diff --git a/arm/fastmul/bignum_emontredc_8n_neon.S b/arm/fastmul/bignum_emontredc_8n_neon.S
new file mode 100644
index 000000000..1fc7af398
--- /dev/null
+++ b/arm/fastmul/bignum_emontredc_8n_neon.S
@@ -0,0 +1,1093 @@
+// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+// SPDX-License-Identifier: Apache-2.0 OR ISC
+
+// ----------------------------------------------------------------------------
+// Extend Montgomery reduce in 8-digit blocks, results in input-output buffer
+// Inputs z[2*k], m[k], w; outputs function return (extra result bit) and z[2*k]
+//
+//    extern uint64_t bignum_emontredc_8n_neon
+//     (uint64_t k, uint64_t *z, uint64_t *m, uint64_t w);
+//
+// Functionally equivalent to bignum_emontredc (see that file for more detail).
+// But in general assumes that the input k is a multiple of 8.
+//
+// Standard ARM ABI: X0 = k, X1 = z, X2 = m, X3 = w, returns X0
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+					S2N_BN_SYM_VISIBILITY_DIRECTIVE(bignum_emontredc_8n_neon)
+					S2N_BN_SYM_PRIVACY_DIRECTIVE(bignum_emontredc_8n_neon)
+					.text
+					.balign 4
+
+
+S2N_BN_SYMBOL(bignum_emontredc_8n_neon):
+           stp x19, x20, [sp, #-16]!
+           stp x21, x22, [sp, #-16]!
+           stp x23, x24, [sp, #-16]!
+           stp x25, x26, [sp, #-16]!
+           stp x27, x28, [sp, #-16]!
+           sub sp, sp, #32
+           lsr x0, x0, #2
+           mov x26, x0
+           subs x12, x0, #1
+           bcc bignum_emontredc_8n_neon_end
+
+           stp x3, xzr, [sp]
+           stp x26, xzr, [sp, #16]
+           mov x28, xzr
+           lsl x0, x12, #5
+
+bignum_emontredc_8n_neon_outerloop:
+          ldp x3, xzr, [sp]
+          ldp x17, x19, [x1]
+          ldp x20, x21, [x1, #16]
+          ldp x8, x9, [x2]
+          ldp x10, x11, [x2, #16]
+          ldr q21, [x2, #16]
+
+          // Montgomery step 0
+
+          mul x4, x17, x3
+// NEON: Calculate x4 * (x10, x11) that does two 64x64->128-bit multiplications.
+dup v0.2d, x4
+uzp2    v3.4s, v21.4s, v0.4s
+xtn     v4.2s, v0.2d
+xtn     v5.2s, v21.2d
+          mul	x12, x4, x8
+          adds x17, x17, x12
+          umulh x12, x4, x8
+          mul	x13, x4, x9
+rev64   v1.4s, v21.4s
+umull   v6.2d, v4.2s, v5.2s
+umull   v7.2d, v4.2s, v3.2s
+uzp2    v16.4s, v0.4s, v0.4s
+mul     v0.4s, v1.4s, v0.4s
+movi    v2.2d, #0x000000ffffffff
+usra    v7.2d, v6.2d, #32
+umull   v1.2d, v16.2s, v3.2s
+uaddlp  v0.2d, v0.4s
+and     v2.16b, v7.16b, v2.16b
+umlal   v2.2d, v16.2s, v5.2s
+shl     v0.2d, v0.2d, #32
+usra    v1.2d, v7.2d, #32
+umlal   v0.2d, v4.2s, v5.2s
+mov x14, v0.d[0]
+mov x15, v0.d[1]
+          adcs x19, x19, x13
+          umulh x13, x4, x9
+          adcs x20, x20, x14
+usra    v1.2d, v2.2d, #32
+mov x14, v1.d[0]
+          adcs x21, x21, x15
+mov x15, v1.d[1]
+          adc x22, xzr, xzr
+          adds x19, x19, x12
+          mul x5, x19, x3 // hoisted from step 1
+          adcs x20, x20, x13
+          adcs x21, x21, x14
+          adc x22, x22, x15
+
+          // Montgomery step 1
+
+// NEON: Calculate x5 * (x10, x11) that does two 64x64->128-bit multiplications.
+dup v0.2d, x5
+uzp2    v3.4s, v21.4s, v0.4s
+xtn     v4.2s, v0.2d
+xtn     v5.2s, v21.2d
+
+          mul	x12, x5, x8
+          adds	x19, x19, x12
+          umulh	x12, x5, x8
+          mul	x13, x5, x9
+
+rev64   v1.4s, v21.4s
+umull   v6.2d, v4.2s, v5.2s
+umull   v7.2d, v4.2s, v3.2s
+uzp2    v16.4s, v0.4s, v0.4s
+mul     v0.4s, v1.4s, v0.4s
+movi    v2.2d, #0x000000ffffffff
+usra    v7.2d, v6.2d, #32
+umull   v1.2d, v16.2s, v3.2s
+uaddlp  v0.2d, v0.4s
+and     v2.16b, v7.16b, v2.16b
+umlal   v2.2d, v16.2s, v5.2s
+shl     v0.2d, v0.2d, #32
+usra    v1.2d, v7.2d, #32
+umlal   v0.2d, v4.2s, v5.2s
+mov x14, v0.d[0]
+mov x15, v0.d[1]
+          adcs	x20, x20, x13
+          umulh	x13, x5, x9
+          adcs x21, x21, x14
+usra    v1.2d, v2.2d, #32
+mov x14, v1.d[0]
+          adcs x22, x22, x15
+mov x15, v1.d[1]
+          adc x23, xzr, xzr
+          adds x20, x20, x12
+          mul	x6, x20, x3 // hoisted from step 2
+
+// NEON: For montgomery step 2,
+// calculate x6 * (x10, x11) that does two 64x64->128-bit multiplications.
+dup v0.2d, x6
+#define in1  v21
+#define in2  v0
+#define out_lo v0
+#define out_hi v1
+uzp2    v3.4s, in2.4s, in1.4s
+xtn     v4.2s, in1.2d
+xtn     v5.2s, in2.2d
+
+          adcs x21, x21, x13
+          adcs x22, x22, x14
+          adc x23, x23, x15
+
+          stp x4, x5, [x1]
+
+// hoisted from maddloop_neon_firstitr
+ldr q20, [x1]
+// q21 will be loaded later.
+
+ldr q22, [x2, #32]
+ldr q23, [x2, #48]
+
+        // Montgomery step 2
+
+rev64   v1.4s, in2.4s
+umull   v6.2d, v4.2s, v5.2s
+umull   v7.2d, v4.2s, v3.2s
+uzp2    v16.4s, in1.4s, in1.4s
+
+        mul	x12, x6, x8
+        adds	x20, x20, x12
+
+mul     v0.4s, v1.4s, in1.4s
+movi    v2.2d, #0x000000ffffffff
+usra    v7.2d, v6.2d, #32
+umull   out_hi.2d, v16.2s, v3.2s
+
+        umulh	x12, x6, x8
+        mul	x13, x6, x9
+
+uaddlp  v0.2d, v0.4s
+and     v2.16b, v7.16b, v2.16b
+umlal   v2.2d, v16.2s, v5.2s
+shl     out_lo.2d, v0.2d, #32
+
+        adcs	x21, x21, x13
+        umulh	x13, x6, x9
+
+usra    out_hi.2d, v7.2d, #32
+umlal   out_lo.2d, v4.2s, v5.2s
+mov x14, out_lo.d[0]
+mov x15, out_lo.d[1]
+
+usra    out_hi.2d, v2.2d, #32
+#undef in1
+#undef in2
+#undef out_lo
+#undef out_hi
+
+          adcs x22, x22, x14
+          adcs x23, x23, x15
+
+mov x14, v1.d[0]
+mov x15, v1.d[1]
+
+          adc	x24, xzr, xzr
+          adds	x21, x21, x12
+          mul	x7, x21, x3
+          adcs	x22, x22, x13
+          adcs	x23, x23, x14
+          adc	x24, x24, x15
+
+          stp x6, x7, [x1, #16]
+
+// hoisted from maddloop_neon_firstitr
+ldr q21, [x1, #16]
+
+// pre-calculate 2mul+2umulhs in maddloop_neon_firstitr
+// v25++v24 = hi and lo of (x4 * x8, x5 * x9)
+#define in1  v20
+#define in2  v22
+#define out_lo v24
+#define out_hi v25
+uzp2    v3.4s, in2.4s, in1.4s
+xtn     v4.2s, in1.2d
+
+          // Montgomery step 3
+
+           mul	x12, x7, x8
+           mul	x13, x7, x9
+
+xtn     v5.2s, in2.2d
+rev64   v1.4s, in2.4s
+umull   v6.2d, v4.2s, v5.2s
+umull   v7.2d, v4.2s, v3.2s
+
+           mul	x14, x7, x10
+           mul	x15, x7, x11
+
+uzp2    v16.4s, in1.4s, in1.4s
+mul     v0.4s, v1.4s, in1.4s
+movi    v2.2d, #0x000000ffffffff
+usra    v7.2d, v6.2d, #32
+umull   out_hi.2d, v16.2s, v3.2s
+uaddlp  v0.2d, v0.4s
+and     v2.16b, v7.16b, v2.16b
+umlal   v2.2d, v16.2s, v5.2s
+
+           adds	x21, x21, x12
+           umulh	x12, x7, x8
+           adcs	x22, x22, x13
+           umulh	x13, x7, x9
+
+shl     out_lo.2d, v0.2d, #32
+usra    out_hi.2d, v7.2d, #32
+umlal   out_lo.2d, v4.2s, v5.2s
+usra    out_hi.2d, v2.2d, #32
+#undef in1
+#undef in2
+#undef out_lo
+#undef out_hi
+
+           adcs	x23, x23, x14
+           umulh	x14, x7, x10
+           adcs	x24, x24, x15
+           umulh	x15, x7, x11
+
+// v27++v26 = hi and lo of (x6 * x10, x7 * x11)
+#define in1  v21
+#define in2  v23
+#define out_lo v26
+#define out_hi v27
+uzp2    v3.4s, in2.4s, in1.4s
+xtn     v4.2s, in1.2d
+xtn     v5.2s, in2.2d
+rev64   v1.4s, in2.4s
+
+// hoisted from maddloop_neon_firstitr and maddloop_x0one
+          ldp x8, x9, [x2, #32]
+          ldp x10, x11, [x2, #48]
+
+umull   v6.2d, v4.2s, v5.2s
+umull   v7.2d, v4.2s, v3.2s
+uzp2    v16.4s, in1.4s, in1.4s
+mul     v0.4s, v1.4s, in1.4s
+
+          adc	x25, xzr, xzr
+          adds	x12, x22, x12
+          adcs	x13, x23, x13
+          adcs	x14, x24, x14
+          adc	x15, x25, x15
+
+movi    v2.2d, #0x000000ffffffff
+usra    v7.2d, v6.2d, #32
+umull   out_hi.2d, v16.2s, v3.2s
+uaddlp  v0.2d, v0.4s
+and     v2.16b, v7.16b, v2.16b
+umlal   v2.2d, v16.2s, v5.2s
+shl     out_lo.2d, v0.2d, #32
+usra    out_hi.2d, v7.2d, #32
+umlal   out_lo.2d, v4.2s, v5.2s
+usra    out_hi.2d, v2.2d, #32
+#undef in1
+#undef in2
+#undef out_lo
+#undef out_hi
+
+          cbz x0, bignum_emontredc_8n_neon_madddone
+          mov x27, x0
+          cmp x0, #32
+          bne bignum_emontredc_8n_neon_maddloop_neon_firstitr
+
+bignum_emontredc_8n_neon_maddloop_x0one:
+         	add	x2, x2, #0x20
+         	add	x1, x1, #0x20
+         	mul	x17, x4, x8
+         	mul	x22, x5, x9
+         	mul	x23, x6, x10
+         	mul	x24, x7, x11
+         	umulh	x16, x4, x8
+         	adds	x22, x22, x16
+         	umulh	x16, x5, x9
+         	adcs	x23, x23, x16
+         	umulh	x16, x6, x10
+         	adcs	x24, x24, x16
+         	umulh	x16, x7, x11
+         	adc	x25, x16, xzr
+         	ldp	x20, x21, [x1]
+         	adds	x12, x12, x20
+         	adcs	x13, x13, x21
+         	ldp	x20, x21, [x1, #16]
+         	adcs	x14, x14, x20
+         	adcs	x15, x15, x21
+         	adc	x16, xzr, xzr
+         	adds	x19, x22, x17
+         	adcs	x22, x23, x22
+         	adcs	x23, x24, x23
+         	adcs	x24, x25, x24
+         	adc	x25, xzr, x25
+         	adds	x20, x22, x17
+         	adcs	x21, x23, x19
+         	adcs	x22, x24, x22
+         	adcs	x23, x25, x23
+         	adcs	x24, xzr, x24
+         	adc	x25, xzr, x25
+         	adds	x17, x17, x12
+         	adcs	x19, x19, x13
+         	adcs	x20, x20, x14
+         	adcs	x21, x21, x15
+         	adcs	x22, x22, x16
+         	adcs	x23, x23, xzr
+         	adcs	x24, x24, xzr
+         	adc	x25, x25, xzr
+         	subs	x15, x6, x7
+         	cneg	x15, x15, cc  // cc = lo, ul, last
+         	csetm	x12, cc  // cc = lo, ul, last
+         	subs	x13, x11, x10
+         	cneg	x13, x13, cc  // cc = lo, ul, last
+         	mul	x14, x15, x13
+         	umulh	x13, x15, x13
+         	cinv	x12, x12, cc  // cc = lo, ul, last
+         	cmn	x12, #0x1
+         	eor	x14, x14, x12
+         	adcs	x23, x23, x14
+         	eor	x13, x13, x12
+         	adcs	x24, x24, x13
+         	adc	x25, x25, x12
+         	subs	x15, x4, x5
+         	cneg	x15, x15, cc  // cc = lo, ul, last
+         	csetm	x12, cc  // cc = lo, ul, last
+         	subs	x13, x9, x8
+         	cneg	x13, x13, cc  // cc = lo, ul, last
+         	mul	x14, x15, x13
+         	umulh	x13, x15, x13
+         	cinv	x12, x12, cc  // cc = lo, ul, last
+         	cmn	x12, #0x1
+         	eor	x14, x14, x12
+         	adcs	x19, x19, x14
+         	eor	x13, x13, x12
+         	adcs	x20, x20, x13
+         	adcs	x21, x21, x12
+         	adcs	x22, x22, x12
+         	adcs	x23, x23, x12
+         	adcs	x24, x24, x12
+         	adc	x25, x25, x12
+         	subs	x15, x5, x7
+         	cneg	x15, x15, cc  // cc = lo, ul, last
+         	csetm	x12, cc  // cc = lo, ul, last
+         	subs	x13, x11, x9
+         	cneg	x13, x13, cc  // cc = lo, ul, last
+         	mul	x14, x15, x13
+         	umulh	x13, x15, x13
+         	cinv	x12, x12, cc  // cc = lo, ul, last
+         	cmn	x12, #0x1
+         	eor	x14, x14, x12
+         	adcs	x22, x22, x14
+         	eor	x13, x13, x12
+         	adcs	x23, x23, x13
+         	adcs	x24, x24, x12
+         	adc	x25, x25, x12
+         	subs	x15, x4, x6
+         	cneg	x15, x15, cc  // cc = lo, ul, last
+         	csetm	x12, cc  // cc = lo, ul, last
+         	subs	x13, x10, x8
+         	cneg	x13, x13, cc  // cc = lo, ul, last
+         	mul	x14, x15, x13
+         	umulh	x13, x15, x13
+         	cinv	x12, x12, cc  // cc = lo, ul, last
+         	cmn	x12, #0x1
+         	eor	x14, x14, x12
+         	adcs	x20, x20, x14
+         	eor	x13, x13, x12
+         	adcs	x21, x21, x13
+         	adcs	x22, x22, x12
+         	adcs	x23, x23, x12
+         	adcs	x24, x24, x12
+         	adc	x25, x25, x12
+         	subs	x15, x4, x7
+         	cneg	x15, x15, cc  // cc = lo, ul, last
+         	csetm	x12, cc  // cc = lo, ul, last
+         	subs	x13, x11, x8
+         	cneg	x13, x13, cc  // cc = lo, ul, last
+         	mul	x14, x15, x13
+         	umulh	x13, x15, x13
+         	cinv	x12, x12, cc  // cc = lo, ul, last
+         	cmn	x12, #0x1
+         	eor	x14, x14, x12
+         	adcs	x21, x21, x14
+         	eor	x13, x13, x12
+         	adcs	x22, x22, x13
+         	adcs	x23, x23, x12
+         	adcs	x24, x24, x12
+         	adc	x25, x25, x12
+         	subs	x15, x5, x6
+         	cneg	x15, x15, cc  // cc = lo, ul, last
+         	csetm	x12, cc  // cc = lo, ul, last
+         	subs	x13, x10, x9
+         	cneg	x13, x13, cc  // cc = lo, ul, last
+         	mul	x14, x15, x13
+         	umulh	x13, x15, x13
+         	cinv	x12, x12, cc  // cc = lo, ul, last
+         	cmn	x12, #0x1
+         	eor	x14, x14, x12
+         	adcs	x21, x21, x14
+         	eor	x13, x13, x12
+         	adcs	x22, x22, x13
+         	adcs	x13, x23, x12
+         	adcs	x14, x24, x12
+         	adc	x15, x25, x12
+         	mov	x12, x22
+         	stp	x17, x19, [x1]
+         	stp	x20, x21, [x1, #16]
+         	sub	x27, x27, #0x20
+          b bignum_emontredc_8n_neon_madddone
+
+
+bignum_emontredc_8n_neon_maddloop_neon_firstitr:
+
+mov x16, v25.d[0] //umulh x16,x4,x8
+mov x22, v24.d[1] //mul x22, x5, x9
+
+mov x20, v25.d[1] //umulh x20,x5,x9
+mov x23, v26.d[0] //mul x23, x6, x10
+
+mov x21, v27.d[0] //umulh x21,x6,x10
+mov x24, v26.d[1] //mul x24, x7, x11
+
+mov x3, v27.d[1] //umulh x3,x7,x11
+mov x17, v24.d[0] //mul x17, x4, x8
+
+          adds x22,x22,x16
+          adcs x23,x23,x20
+          adcs x24,x24,x21
+          adc x25,x3,xzr
+
+// pre-calculate the multiplications for the next iter.
+// v25 ++ v24 = hi, lo of (x4 * x8, x5 * x9)
+ldr q22, [x2, #64]
+ldr q23, [x2, #80]
+
+          add x2, x2, #32
+          add x1, x1, #32
+
+#define in1  v20
+#define in2  v22
+#define out_lo v24
+#define out_hi v25
+uzp2    v3.4s, in2.4s, in1.4s
+xtn     v4.2s, in1.2d
+xtn     v5.2s, in2.2d
+rev64   v1.4s, in2.4s
+
+          ldp x20,x21,[x1]
+          adds x12,x12,x20
+          adcs x13,x13,x21
+          ldp x20,x21,[x1,#16]
+
+umull   v6.2d, v4.2s, v5.2s
+umull   v7.2d, v4.2s, v3.2s
+uzp2    v16.4s, in1.4s, in1.4s
+mul     v0.4s, v1.4s, in1.4s
+
+          adcs x14,x14,x20
+          adcs x15,x15,x21
+          adc x16,xzr,xzr
+          adds x19,x22,x17
+
+movi    v2.2d, #0x000000ffffffff
+usra    v7.2d, v6.2d, #32
+umull   out_hi.2d, v16.2s, v3.2s
+uaddlp  v0.2d, v0.4s
+
+          adcs x22,x23,x22
+          adcs x23,x24,x23
+          adcs x24,x25,x24
+          adc x25,xzr,x25
+
+and     v2.16b, v7.16b, v2.16b
+umlal   v2.2d, v16.2s, v5.2s
+shl     out_lo.2d, v0.2d, #32
+usra    out_hi.2d, v7.2d, #32
+
+          adds x20,x22,x17
+          adcs x21,x23,x19
+          adcs x22,x24,x22
+          adcs x23,x25,x23
+
+umlal   out_lo.2d, v4.2s, v5.2s
+usra    out_hi.2d, v2.2d, #32
+#undef in1
+#undef in2
+#undef out_lo
+#undef out_hi
+
+          adcs x24,xzr,x24
+          adc x25,xzr,x25
+          adds x17,x17,x12
+          adcs x19,x19,x13
+
+#define in1  v21
+#define in2  v23
+#define out_lo v26
+#define out_hi v27
+uzp2    v3.4s, in2.4s, in1.4s
+xtn     v4.2s, in1.2d
+xtn     v5.2s, in2.2d
+rev64   v1.4s, in2.4s
+
+          adcs x20,x20,x14
+          adcs x21,x21,x15
+          adcs x22,x22,x16
+          adcs x23,x23,xzr
+
+umull   v6.2d, v4.2s, v5.2s
+umull   v7.2d, v4.2s, v3.2s
+uzp2    v16.4s, in1.4s, in1.4s
+mul     v0.4s, v1.4s, in1.4s
+
+          adcs x24,x24,xzr
+          adc x25,x25,xzr
+          subs x15,x6,x7
+          cneg x15,x15,cc
+
+movi    v2.2d, #0x000000ffffffff
+usra    v7.2d, v6.2d, #32
+umull   out_hi.2d, v16.2s, v3.2s
+uaddlp  v0.2d, v0.4s
+
+          csetm x12,cc
+          subs x13,x11,x10
+          cneg x13,x13,cc
+          mul x14,x15,x13
+
+and     v2.16b, v7.16b, v2.16b
+umlal   v2.2d, v16.2s, v5.2s
+shl     out_lo.2d, v0.2d, #32
+usra    out_hi.2d, v7.2d, #32
+
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+
+umlal   out_lo.2d, v4.2s, v5.2s
+usra    out_hi.2d, v2.2d, #32
+#undef in1
+#undef in2
+#undef out_lo
+#undef out_hi
+
+          adcs x23,x23,x14
+          eor x13,x13,x12
+          adcs x24,x24,x13
+          adc x25,x25,x12
+          subs x15,x4,x5
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x9,x8
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x19,x19,x14
+          eor x13,x13,x12
+          adcs x20,x20,x13
+          adcs x21,x21,x12
+          adcs x22,x22,x12
+          adcs x23,x23,x12
+          adcs x24,x24,x12
+          adc x25,x25,x12
+
+          stp x17,x19,[x1]
+
+mov x16, v25.d[0] // hi bits of (x4 * x8)
+mov x26, v27.d[0] // hi bits of (x6 * x10)
+mov x3, v25.d[1] // hi bits of (x5 * x9)
+mov x17, v27.d[1] // hi bits of (x6 * x10)
+
+          subs x15,x5,x7
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x11,x9
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x22,x22,x14
+          eor x13,x13,x12
+          adcs x23,x23,x13
+          adcs x24,x24,x12
+          adc x25,x25,x12
+          subs x15,x4,x6
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x10,x8
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x20,x20,x14
+          eor x13,x13,x12
+          adcs x21,x21,x13
+          adcs x22,x22,x12
+          adcs x23,x23,x12
+          adcs x24,x24,x12
+          adc x25,x25,x12
+          subs x15,x4,x7
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x11,x8
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x21,x21,x14
+          eor x13,x13,x12
+          adcs x22,x22,x13
+          adcs x23,x23,x12
+          adcs x24,x24,x12
+          adc x25,x25,x12
+          subs x15,x5,x6
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x10,x9
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x21,x21,x14
+
+          stp x20,x21,[x1,#16]
+mov x20, v24.d[1] // lo bits of (x5 * x9)
+mov x21, v26.d[0] // lo bits of (x6 * x10)
+
+          eor x13,x13,x12
+          adcs x22,x22,x13
+          adcs x13,x23,x12
+          adcs x14,x24,x12
+          adc x15,x25,x12
+          mov x12,x22
+
+mov x24, v26.d[1] // lo bits of (x7 * x11)
+
+           sub x27, x27, #32
+           cmp x27, #32
+           beq bignum_emontredc_8n_neon_maddloop_neon_last
+
+
+bignum_emontredc_8n_neon_maddloop_neon:
+          ldp x8, x9, [x2, #32]
+          ldp x10, x11, [x2, #48]
+
+// pre-calculate the multiplications for the next iter.
+// v25 ++ v24 = hi, lo of (x4 * x8, x5 * x9)
+ldr q22, [x2, #64]
+ldr q23, [x2, #80]
+
+          add x2, x2, #32
+          add x1, x1, #32
+
+          adds x22,x20,x16
+          adcs x23,x21,x3
+          adcs x24,x24,x26
+          adc x25,x17,xzr
+mov x17, v24.d[0] // lo bits of (x4 * x8)
+
+#define in1  v20
+#define in2  v22
+#define out_lo v24
+#define out_hi v25
+uzp2    v3.4s, in2.4s, in1.4s
+xtn     v4.2s, in1.2d
+xtn     v5.2s, in2.2d
+rev64   v1.4s, in2.4s
+
+          ldp x20,x21,[x1]
+          adds x12,x12,x20
+          adcs x13,x13,x21
+          ldp x20,x21,[x1,#16]
+
+umull   v6.2d, v4.2s, v5.2s
+umull   v7.2d, v4.2s, v3.2s
+uzp2    v16.4s, in1.4s, in1.4s
+mul     v0.4s, v1.4s, in1.4s
+
+          adcs x14,x14,x20
+          adcs x15,x15,x21
+          adc x16,xzr,xzr
+          adds x19,x22,x17
+
+movi    v2.2d, #0x000000ffffffff
+usra    v7.2d, v6.2d, #32
+umull   out_hi.2d, v16.2s, v3.2s
+uaddlp  v0.2d, v0.4s
+
+          adcs x22,x23,x22
+          adcs x23,x24,x23
+          adcs x24,x25,x24
+          adc x25,xzr,x25
+
+and     v2.16b, v7.16b, v2.16b
+umlal   v2.2d, v16.2s, v5.2s
+shl     out_lo.2d, v0.2d, #32
+usra    out_hi.2d, v7.2d, #32
+
+          adds x20,x22,x17
+          adcs x21,x23,x19
+          adcs x22,x24,x22
+          adcs x23,x25,x23
+
+umlal   out_lo.2d, v4.2s, v5.2s
+usra    out_hi.2d, v2.2d, #32
+#undef in1
+#undef in2
+#undef out_lo
+#undef out_hi
+
+          adcs x24,xzr,x24
+          adc x25,xzr,x25
+          adds x17,x17,x12
+          adcs x19,x19,x13
+
+#define in1  v21
+#define in2  v23
+#define out_lo v26
+#define out_hi v27
+uzp2    v3.4s, in2.4s, in1.4s
+xtn     v4.2s, in1.2d
+xtn     v5.2s, in2.2d
+rev64   v1.4s, in2.4s
+
+          adcs x20,x20,x14
+          adcs x21,x21,x15
+          adcs x22,x22,x16
+          adcs x23,x23,xzr
+
+umull   v6.2d, v4.2s, v5.2s
+umull   v7.2d, v4.2s, v3.2s
+uzp2    v16.4s, in1.4s, in1.4s
+mul     v0.4s, v1.4s, in1.4s
+
+          adcs x24,x24,xzr
+          adc x25,x25,xzr
+          subs x15,x6,x7
+          cneg x15,x15,cc
+
+movi    v2.2d, #0x000000ffffffff
+usra    v7.2d, v6.2d, #32
+umull   out_hi.2d, v16.2s, v3.2s
+uaddlp  v0.2d, v0.4s
+
+          csetm x12,cc
+          subs x13,x11,x10
+          cneg x13,x13,cc
+          mul x14,x15,x13
+
+and     v2.16b, v7.16b, v2.16b
+umlal   v2.2d, v16.2s, v5.2s
+shl     out_lo.2d, v0.2d, #32
+usra    out_hi.2d, v7.2d, #32
+
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+
+umlal   out_lo.2d, v4.2s, v5.2s
+usra    out_hi.2d, v2.2d, #32
+#undef in1
+#undef in2
+#undef out_lo
+#undef out_hi
+
+          adcs x23,x23,x14
+          eor x13,x13,x12
+          adcs x24,x24,x13
+          adc x25,x25,x12
+          subs x15,x4,x5
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x9,x8
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x19,x19,x14
+          eor x13,x13,x12
+          adcs x20,x20,x13
+          adcs x21,x21,x12
+          adcs x22,x22,x12
+          adcs x23,x23,x12
+          adcs x24,x24,x12
+          adc x25,x25,x12
+
+          stp x17,x19,[x1]
+
+mov x16, v25.d[0] // hi bits of (x4 * x8)
+mov x26, v27.d[0] // hi bits of (x6 * x10)
+mov x3, v25.d[1] // hi bits of (x5 * x9)
+mov x17, v27.d[1] // hi bits of (x6 * x10)
+
+          subs x15,x5,x7
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x11,x9
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x22,x22,x14
+          eor x13,x13,x12
+          adcs x23,x23,x13
+          adcs x24,x24,x12
+          adc x25,x25,x12
+          subs x15,x4,x6
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x10,x8
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x20,x20,x14
+          eor x13,x13,x12
+          adcs x21,x21,x13
+          adcs x22,x22,x12
+          adcs x23,x23,x12
+          adcs x24,x24,x12
+          adc x25,x25,x12
+          subs x15,x4,x7
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x11,x8
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x21,x21,x14
+          eor x13,x13,x12
+          adcs x22,x22,x13
+          adcs x23,x23,x12
+          adcs x24,x24,x12
+          adc x25,x25,x12
+          subs x15,x5,x6
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x10,x9
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x21,x21,x14
+
+          stp x20,x21,[x1,#16]
+mov x20, v24.d[1] // lo bits of (x5 * x9)
+mov x21, v26.d[0] // lo bits of (x6 * x10)
+
+          eor x13,x13,x12
+          adcs x22,x22,x13
+          adcs x13,x23,x12
+          adcs x14,x24,x12
+          adc x15,x25,x12
+          mov x12,x22
+
+mov x24, v26.d[1] // lo bits of (x7 * x11)
+
+           sub x27, x27, #32
+           cmp x27, #32
+           bne bignum_emontredc_8n_neon_maddloop_neon
+
+
+bignum_emontredc_8n_neon_maddloop_neon_last:
+          ldp x8, x9, [x2, #32]
+          ldp x10, x11, [x2, #48]
+
+          add x2, x2, #32
+          add x1, x1, #32
+
+          adds x22,x20,x16
+          adcs x23,x21,x3
+          adcs x24,x24,x26
+          adc x25,x17,xzr
+mov x17, v24.d[0] // lo bits of (x4 * x8)
+
+          ldp x20,x21,[x1]
+          adds x12,x12,x20
+          adcs x13,x13,x21
+          ldp x20,x21,[x1,#16]
+          adcs x14,x14,x20
+          adcs x15,x15,x21
+          adc x16,xzr,xzr
+          adds x19,x22,x17
+          adcs x22,x23,x22
+          adcs x23,x24,x23
+          adcs x24,x25,x24
+          adc x25,xzr,x25
+          adds x20,x22,x17
+          adcs x21,x23,x19
+          adcs x22,x24,x22
+          adcs x23,x25,x23
+          adcs x24,xzr,x24
+          adc x25,xzr,x25
+          adds x17,x17,x12
+          adcs x19,x19,x13
+          adcs x20,x20,x14
+          adcs x21,x21,x15
+          adcs x22,x22,x16
+          adcs x23,x23,xzr
+          adcs x24,x24,xzr
+          adc x25,x25,xzr
+          subs x15,x6,x7
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x11,x10
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x23,x23,x14
+          eor x13,x13,x12
+          adcs x24,x24,x13
+          adc x25,x25,x12
+          subs x15,x4,x5
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x9,x8
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x19,x19,x14
+          eor x13,x13,x12
+          adcs x20,x20,x13
+          adcs x21,x21,x12
+          adcs x22,x22,x12
+          adcs x23,x23,x12
+          adcs x24,x24,x12
+          adc x25,x25,x12
+          subs x15,x5,x7
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x11,x9
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x22,x22,x14
+          eor x13,x13,x12
+          adcs x23,x23,x13
+          adcs x24,x24,x12
+          adc x25,x25,x12
+          subs x15,x4,x6
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x10,x8
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x20,x20,x14
+          eor x13,x13,x12
+          adcs x21,x21,x13
+          adcs x22,x22,x12
+          adcs x23,x23,x12
+          adcs x24,x24,x12
+          adc x25,x25,x12
+          subs x15,x4,x7
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x11,x8
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x21,x21,x14
+          eor x13,x13,x12
+          adcs x22,x22,x13
+          adcs x23,x23,x12
+          adcs x24,x24,x12
+          adc x25,x25,x12
+          subs x15,x5,x6
+          cneg x15,x15,cc
+          csetm x12,cc
+          subs x13,x10,x9
+          cneg x13,x13,cc
+          mul x14,x15,x13
+          umulh x13,x15,x13
+          cinv x12,x12,cc
+          adds xzr,x12,#1
+          eor x14,x14,x12
+          adcs x21,x21,x14
+          eor x13,x13,x12
+          adcs x22,x22,x13
+          adcs x13,x23,x12
+          adcs x14,x24,x12
+          adc x15,x25,x12
+          mov x12,x22
+          stp x17,x19,[x1]
+          stp x20,x21,[x1,#16]
+           subs x27, x27, #64
+
+bignum_emontredc_8n_neon_madddone:
+           ldp x17, x19, [x1, #32]
+           ldp x20, x21, [x1, #48]
+           ldp x26, xzr, [sp, #16]
+           adds xzr, x28, x28
+           adcs x17, x17, x12
+           adcs x19, x19, x13
+           adcs x20, x20, x14
+           adcs x21, x21, x15
+           csetm x28, cs
+           stp x17, x19, [x1, #32]
+           stp x20, x21, [x1, #48]
+           sub x1, x1, x0
+           sub x2, x2, x0
+           add x1, x1, #32
+           subs x26, x26, #1
+           stp x26, xzr, [sp, #16]
+           bne bignum_emontredc_8n_neon_outerloop
+           neg x0, x28
+
+bignum_emontredc_8n_neon_end:
+           add sp, sp, #32
+
+           ldp x27, x28, [sp], #16
+           ldp x25, x26, [sp], #16
+           ldp x23, x24, [sp], #16
+           ldp x21, x22, [sp], #16
+           ldp x19, x20, [sp], #16
+           ret
+
diff --git a/arm/fastmul/bignum_kmul_16_32_neon.S b/arm/fastmul/bignum_kmul_16_32_neon.S
new file mode 100644
index 000000000..a3cb89bda
--- /dev/null
+++ b/arm/fastmul/bignum_kmul_16_32_neon.S
@@ -0,0 +1,835 @@
+// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+// SPDX-License-Identifier: Apache-2.0 OR ISC
+
+// ----------------------------------------------------------------------------
+// Multiply z := x * y
+// Inputs x[16], y[16]; output z[32]; temporary buffer t[>=32]
+//
+//    extern void bignum_kmul_16_32_neon
+//     (uint64_t z[static 32], uint64_t x[static 16], uint64_t y[static 16],
+//      uint64_t t[static 32])
+//
+// This is a Karatsuba-style function multiplying half-sized results
+// internally and using temporary buffer t for intermediate results.
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = y, X3 = t
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(bignum_kmul_16_32_neon)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(bignum_kmul_16_32_neon)
+        .text
+        .balign 4
+
+// Subroutine-safe copies of the output, inputs and temporary buffer pointers
+
+#define z x25
+#define x x26
+#define y x27
+#define t x28
+
+// More variables for sign masks, with s also necessarily subroutine-safe
+
+#define s x29
+#define m x19
+
+S2N_BN_SYMBOL(bignum_kmul_16_32_neon):
+
+// Save registers, including return address
+
+        stp     x19, x20, [sp, #-16]!
+        stp     x21, x22, [sp, #-16]!
+        stp     x23, x24, [sp, #-16]!
+        stp     x25, x26, [sp, #-16]!
+        stp     x27, x28, [sp, #-16]!
+        stp     x29, x30, [sp, #-16]!
+
+// Move parameters into subroutine-safe places
+
+        mov     z, x0
+        mov     x, x1
+        mov     y, x2
+        mov     t, x3
+
+// Compute L = x_lo * y_lo in bottom half of buffer (size 8 x 8 -> 16)
+
+        bl      bignum_kmul_16_32_neon_local_mul_8_16
+
+// Compute absolute difference [t..] = |x_lo - x_hi|
+// and the sign s = sgn(x_lo - x_hi) as a bitmask (all 1s for negative)
+
+        ldp     x10, x11, [x]
+        ldp     x8, x9, [x, #64]
+        subs    x10, x10, x8
+        sbcs    x11, x11, x9
+        ldp     x12, x13, [x, #16]
+        ldp     x8, x9, [x, #80]
+        sbcs    x12, x12, x8
+        sbcs    x13, x13, x9
+        ldp     x14, x15, [x, #32]
+        ldp     x8, x9, [x, #96]
+        sbcs    x14, x14, x8
+        sbcs    x15, x15, x9
+        ldp     x16, x17, [x, #48]
+        ldp     x8, x9, [x, #112]
+        sbcs    x16, x16, x8
+        sbcs    x17, x17, x9
+        csetm   s, cc
+        adds    xzr, s, s
+        eor     x10, x10, s
+        adcs    x10, x10, xzr
+        eor     x11, x11, s
+        adcs    x11, x11, xzr
+        stp     x10, x11, [t]
+        eor     x12, x12, s
+        adcs    x12, x12, xzr
+        eor     x13, x13, s
+        adcs    x13, x13, xzr
+        stp     x12, x13, [t, #16]
+        eor     x14, x14, s
+        adcs    x14, x14, xzr
+        eor     x15, x15, s
+        adcs    x15, x15, xzr
+        stp     x14, x15, [t, #32]
+        eor     x16, x16, s
+        adcs    x16, x16, xzr
+        eor     x17, x17, s
+        adcs    x17, x17, xzr
+        stp     x16, x17, [t, #48]
+
+// Compute H = x_hi * y_hi in top half of buffer (size 8 x 8 -> 16)
+
+        add     x0, z, #128
+        add     x1, x, #64
+        add     x2, y, #64
+        bl      bignum_kmul_16_32_neon_local_mul_8_16
+
+// Compute the other absolute difference [t+8..] = |y_hi - y_lo|
+// Collect the combined product sign bitmask (all 1s for negative) in s
+
+        ldp     x10, x11, [y]
+        ldp     x8, x9, [y, #64]
+        subs    x10, x8, x10
+        sbcs    x11, x9, x11
+        ldp     x12, x13, [y, #16]
+        ldp     x8, x9, [y, #80]
+        sbcs    x12, x8, x12
+        sbcs    x13, x9, x13
+        ldp     x14, x15, [y, #32]
+        ldp     x8, x9, [y, #96]
+        sbcs    x14, x8, x14
+        sbcs    x15, x9, x15
+        ldp     x16, x17, [y, #48]
+        ldp     x8, x9, [y, #112]
+        sbcs    x16, x8, x16
+        sbcs    x17, x9, x17
+        csetm   m, cc
+        adds    xzr, m, m
+        eor     x10, x10, m
+        adcs    x10, x10, xzr
+        eor     x11, x11, m
+        adcs    x11, x11, xzr
+        stp     x10, x11, [t, #64]
+        eor     x12, x12, m
+        adcs    x12, x12, xzr
+        eor     x13, x13, m
+        adcs    x13, x13, xzr
+        stp     x12, x13, [t, #80]
+        eor     x14, x14, m
+        adcs    x14, x14, xzr
+        eor     x15, x15, m
+        adcs    x15, x15, xzr
+        stp     x14, x15, [t, #96]
+        eor     x16, x16, m
+        adcs    x16, x16, xzr
+        eor     x17, x17, m
+        adcs    x17, x17, xzr
+        stp     x16, x17, [t, #112]
+        eor     s, s, m
+
+// Compute H' = H + L_top in place of H (it cannot overflow)
+// First add 8-sized block then propagate carry through next 8
+
+        ldp     x10, x11, [z, #128]
+        ldp     x12, x13, [z, #64]
+        adds    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, #128]
+
+        ldp     x10, x11, [z, #128+16]
+        ldp     x12, x13, [z, #64+16]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, #128+16]
+
+        ldp     x10, x11, [z, #128+32]
+        ldp     x12, x13, [z, #64+32]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, #128+32]
+
+        ldp     x10, x11, [z, #128+48]
+        ldp     x12, x13, [z, #64+48]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, #128+48]
+
+        ldp     x10, x11, [z, #128+64]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [z, #128+64]
+
+        ldp     x10, x11, [z, #128+80]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [z, #128+80]
+
+        ldp     x10, x11, [z, #128+96]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [z, #128+96]
+
+        ldp     x10, x11, [z, #128+112]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [z, #128+112]
+
+// Compute M = |x_lo - x_hi| * |y_hi - y_lo| in [t+16...], size 16
+
+        add     x0, t, #128
+        mov     x1, t
+        add     x2, t, #64
+        bl      bignum_kmul_16_32_neon_local_mul_8_16
+
+// Add the interlocking H' and L_bot terms, storing in registers x15..x0
+// Intercept the carry at the 8 + 16 = 24 position and store it in x.
+// (Note that we no longer need the input x was pointing at.)
+
+        ldp     x0, x1, [z]
+        ldp     x16, x17, [z, #128]
+        adds    x0, x0, x16
+        adcs    x1, x1, x17
+        ldp     x2, x3, [z, #16]
+        ldp     x16, x17, [z, #144]
+        adcs    x2, x2, x16
+        adcs    x3, x3, x17
+        ldp     x4, x5, [z, #32]
+        ldp     x16, x17, [z, #160]
+        adcs    x4, x4, x16
+        adcs    x5, x5, x17
+        ldp     x6, x7, [z, #48]
+        ldp     x16, x17, [z, #176]
+        adcs    x6, x6, x16
+        adcs    x7, x7, x17
+        ldp     x8, x9, [z, #128]
+        ldp     x16, x17, [z, #192]
+        adcs    x8, x8, x16
+        adcs    x9, x9, x17
+        ldp     x10, x11, [z, #144]
+        ldp     x16, x17, [z, #208]
+        adcs    x10, x10, x16
+        adcs    x11, x11, x17
+        ldp     x12, x13, [z, #160]
+        ldp     x16, x17, [z, #224]
+        adcs    x12, x12, x16
+        adcs    x13, x13, x17
+        ldp     x14, x15, [z, #176]
+        ldp     x16, x17, [z, #240]
+        adcs    x14, x14, x16
+        adcs    x15, x15, x17
+
+        cset    x, cs
+
+// Add the sign-adjusted mid-term cross product M
+
+        cmn     s, s
+
+        ldp     x16, x17, [t, #128]
+        eor     x16, x16, s
+        adcs    x0, x0, x16
+        eor     x17, x17, s
+        adcs    x1, x1, x17
+        stp     x0, x1, [z, #64]
+        ldp     x16, x17, [t, #144]
+        eor     x16, x16, s
+        adcs    x2, x2, x16
+        eor     x17, x17, s
+        adcs    x3, x3, x17
+        stp     x2, x3, [z, #80]
+        ldp     x16, x17, [t, #160]
+        eor     x16, x16, s
+        adcs    x4, x4, x16
+        eor     x17, x17, s
+        adcs    x5, x5, x17
+        stp     x4, x5, [z, #96]
+        ldp     x16, x17, [t, #176]
+        eor     x16, x16, s
+        adcs    x6, x6, x16
+        eor     x17, x17, s
+        adcs    x7, x7, x17
+        stp     x6, x7, [z, #112]
+        ldp     x16, x17, [t, #192]
+        eor     x16, x16, s
+        adcs    x8, x8, x16
+        eor     x17, x17, s
+        adcs    x9, x9, x17
+        stp     x8, x9, [z, #128]
+        ldp     x16, x17, [t, #208]
+        eor     x16, x16, s
+        adcs    x10, x10, x16
+        eor     x17, x17, s
+        adcs    x11, x11, x17
+        stp     x10, x11, [z, #144]
+        ldp     x16, x17, [t, #224]
+        eor     x16, x16, s
+        adcs    x12, x12, x16
+        eor     x17, x17, s
+        adcs    x13, x13, x17
+        stp     x12, x13, [z, #160]
+        ldp     x16, x17, [t, #240]
+        eor     x16, x16, s
+        adcs    x14, x14, x16
+        eor     x17, x17, s
+        adcs    x15, x15, x17
+        stp     x14, x15, [z, #176]
+
+// Get the next digits effectively resulting so far starting at 24
+
+        adcs    y, s, x
+        adc     t, s, xzr
+
+// Now the final 8 digits of padding; the first one is special in using y
+// and also in getting the carry chain started
+
+        ldp     x10, x11, [z, #192]
+        adds    x10, x10, y
+        adcs    x11, x11, t
+        stp     x10, x11, [z, #192]
+        ldp     x10, x11, [z, #208]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, #208]
+        ldp     x10, x11, [z, #224]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, #224]
+        ldp     x10, x11, [z, #240]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, #240]
+
+// Restore registers and return
+
+        ldp     x29, x30, [sp], #16
+        ldp     x27, x28, [sp], #16
+        ldp     x25, x26, [sp], #16
+        ldp     x23, x24, [sp], #16
+        ldp     x21, x22, [sp], #16
+        ldp     x19, x20, [sp], #16
+
+        ret
+
+// ----------------------------------------------------------------------------
+// Local copy of bignum_mul_8_16_neon without the scratch register save/restore
+// ----------------------------------------------------------------------------
+
+bignum_kmul_16_32_neon_local_mul_8_16:
+        ldp x3, x4, [x1]
+        ldr q0, [x1]
+        ldp x7, x8, [x2]
+        ldr q1, [x2]
+        ldp x5, x6, [x1, #16]
+        ldr q2, [x1, #16]
+        ldp x9, x10, [x2, #16]
+        ldr q3, [x2, #16]
+        uzp1 v4.4s, v1.4s, v0.4s
+        rev64 v1.4s, v1.4s
+        uzp1 v5.4s, v0.4s, v0.4s
+        mul v0.4s, v1.4s, v0.4s
+        uaddlp v0.2d, v0.4s
+        shl v0.2d, v0.2d, #32
+        umlal v0.2d, v5.2s, v4.2s
+        mov x11, v0.d[0]
+        mov x15, v0.d[1]
+        uzp1 v0.4s, v3.4s, v2.4s
+        rev64 v1.4s, v3.4s
+        uzp1 v3.4s, v2.4s, v2.4s
+        mul v1.4s, v1.4s, v2.4s
+        uaddlp v1.2d, v1.4s
+        shl v1.2d, v1.2d, #32
+        umlal v1.2d, v3.2s, v0.2s
+        mov x16, v1.d[0]
+        mov x17, v1.d[1]
+        ldr q0, [x1, #32]
+        ldr q1, [x2, #32]
+        ldr q2, [x1, #48]
+        ldr q3, [x2, #48]
+        umulh x19, x3, x7
+        adds x15, x15, x19
+        umulh x19, x4, x8
+        adcs x16, x16, x19
+        umulh x19, x5, x9
+        adcs x17, x17, x19
+        umulh x19, x6, x10
+        uzp1 v4.4s, v1.4s, v0.4s
+        rev64 v1.4s, v1.4s
+        uzp1 v5.4s, v0.4s, v0.4s
+        mul v0.4s, v1.4s, v0.4s
+        uaddlp v0.2d, v0.4s
+        shl v0.2d, v0.2d, #32
+        umlal v0.2d, v5.2s, v4.2s
+        adc x19, x19, xzr
+        adds x12, x15, x11
+        adcs x15, x16, x15
+        adcs x16, x17, x16
+        adcs x17, x19, x17
+        adc x19, xzr, x19
+        adds x13, x15, x11
+        adcs x14, x16, x12
+        adcs x15, x17, x15
+        adcs x16, x19, x16
+        adcs x17, xzr, x17
+        adc x19, xzr, x19
+        subs x24, x5, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x9
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x16, x16, x22
+        eor x21, x21, x20
+        adcs x17, x17, x21
+        adc x19, x19, x20
+        subs x24, x3, x4
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x8, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x12, x12, x22
+        eor x21, x21, x20
+        adcs x13, x13, x21
+        adcs x14, x14, x20
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x15, x15, x22
+        eor x21, x21, x20
+        adcs x16, x16, x21
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x13, x13, x22
+        eor x21, x21, x20
+        adcs x14, x14, x21
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        ldp x3, x4, [x1, #32]
+        stp x11, x12, [x0]
+        ldp x7, x8, [x2, #32]
+        stp x13, x14, [x0, #16]
+        ldp x5, x6, [x1, #48]
+        stp x15, x16, [x0, #32]
+        ldp x9, x10, [x2, #48]
+        stp x17, x19, [x0, #48]
+        mov x11, v0.d[0]
+        mov x15, v0.d[1]
+        uzp1 v0.4s, v3.4s, v2.4s
+        rev64 v1.4s, v3.4s
+        uzp1 v3.4s, v2.4s, v2.4s
+        mul v1.4s, v1.4s, v2.4s
+        uaddlp v1.2d, v1.4s
+        shl v1.2d, v1.2d, #32
+        umlal v1.2d, v3.2s, v0.2s
+        mov x16, v1.d[0]
+        mov x17, v1.d[1]
+        umulh x19, x3, x7
+        adds x15, x15, x19
+        umulh x19, x4, x8
+        adcs x16, x16, x19
+        umulh x19, x5, x9
+        adcs x17, x17, x19
+        umulh x19, x6, x10
+        adc x19, x19, xzr
+        adds x12, x15, x11
+        adcs x15, x16, x15
+        adcs x16, x17, x16
+        adcs x17, x19, x17
+        adc x19, xzr, x19
+        adds x13, x15, x11
+        adcs x14, x16, x12
+        adcs x15, x17, x15
+        adcs x16, x19, x16
+        adcs x17, xzr, x17
+        adc x19, xzr, x19
+        ldp x22, x21, [x0, #32]
+        adds x11, x11, x22
+        adcs x12, x12, x21
+        ldp x22, x21, [x0, #48]
+        adcs x13, x13, x22
+        adcs x14, x14, x21
+        adcs x15, x15, xzr
+        adcs x16, x16, xzr
+        adcs x17, x17, xzr
+        adc x19, x19, xzr
+        subs x24, x5, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x9
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x16, x16, x22
+        eor x21, x21, x20
+        adcs x17, x17, x21
+        adc x19, x19, x20
+        subs x24, x3, x4
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x8, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x12, x12, x22
+        eor x21, x21, x20
+        adcs x13, x13, x21
+        adcs x14, x14, x20
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x15, x15, x22
+        eor x21, x21, x20
+        adcs x16, x16, x21
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x13, x13, x22
+        eor x21, x21, x20
+        adcs x14, x14, x21
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        ldp x22, x21, [x1]
+        subs x3, x3, x22
+        sbcs x4, x4, x21
+        ldp x22, x21, [x1, #16]
+        sbcs x5, x5, x22
+        sbcs x6, x6, x21
+        csetm x24, cc
+        stp x11, x12, [x0, #64]
+        ldp x22, x21, [x2]
+        subs x7, x22, x7
+        sbcs x8, x21, x8
+        ldp x22, x21, [x2, #16]
+        sbcs x9, x22, x9
+        sbcs x10, x21, x10
+        csetm x1, cc
+        stp x13, x14, [x0, #80]
+        eor x3, x3, x24
+        subs x3, x3, x24
+        eor x4, x4, x24
+        sbcs x4, x4, x24
+        eor x5, x5, x24
+        sbcs x5, x5, x24
+        eor x6, x6, x24
+        sbc x6, x6, x24
+        stp x15, x16, [x0, #96]
+        eor x7, x7, x1
+        subs x7, x7, x1
+        eor x8, x8, x1
+        sbcs x8, x8, x1
+        eor x9, x9, x1
+        sbcs x9, x9, x1
+        eor x10, x10, x1
+        sbc x10, x10, x1
+        stp x17, x19, [x0, #112]
+        eor x1, x1, x24
+        mul x11, x3, x7
+        mul x15, x4, x8
+        mul x16, x5, x9
+        mul x17, x6, x10
+        umulh x19, x3, x7
+        adds x15, x15, x19
+        umulh x19, x4, x8
+        adcs x16, x16, x19
+        umulh x19, x5, x9
+        adcs x17, x17, x19
+        umulh x19, x6, x10
+        adc x19, x19, xzr
+        adds x12, x15, x11
+        adcs x15, x16, x15
+        adcs x16, x17, x16
+        adcs x17, x19, x17
+        adc x19, xzr, x19
+        adds x13, x15, x11
+        adcs x14, x16, x12
+        adcs x15, x17, x15
+        adcs x16, x19, x16
+        adcs x17, xzr, x17
+        adc x19, xzr, x19
+        subs x24, x5, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x9
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x16, x16, x22
+        eor x21, x21, x20
+        adcs x17, x17, x21
+        adc x19, x19, x20
+        subs x24, x3, x4
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x8, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x12, x12, x22
+        eor x21, x21, x20
+        adcs x13, x13, x21
+        adcs x14, x14, x20
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x15, x15, x22
+        eor x21, x21, x20
+        adcs x16, x16, x21
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x13, x13, x22
+        eor x21, x21, x20
+        adcs x14, x14, x21
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        ldp x3, x4, [x0]
+        ldp x7, x8, [x0, #64]
+        adds x3, x3, x7
+        adcs x4, x4, x8
+        ldp x5, x6, [x0, #16]
+        ldp x9, x10, [x0, #80]
+        adcs x5, x5, x9
+        adcs x6, x6, x10
+        ldp x20, x21, [x0, #96]
+        adcs x7, x7, x20
+        adcs x8, x8, x21
+        ldp x22, x23, [x0, #112]
+        adcs x9, x9, x22
+        adcs x10, x10, x23
+        adcs x24, x1, xzr
+        adc x2, x1, xzr
+        cmn x1, #0x1
+        eor x11, x11, x1
+        adcs x3, x11, x3
+        eor x12, x12, x1
+        adcs x4, x12, x4
+        eor x13, x13, x1
+        adcs x5, x13, x5
+        eor x14, x14, x1
+        adcs x6, x14, x6
+        eor x15, x15, x1
+        adcs x7, x15, x7
+        eor x16, x16, x1
+        adcs x8, x16, x8
+        eor x17, x17, x1
+        adcs x9, x17, x9
+        eor x19, x19, x1
+        adcs x10, x19, x10
+        adcs x20, x20, x24
+        adcs x21, x21, x2
+        adcs x22, x22, x2
+        adc x23, x23, x2
+        stp x3, x4, [x0, #32]
+        stp x5, x6, [x0, #48]
+        stp x7, x8, [x0, #64]
+        stp x9, x10, [x0, #80]
+        stp x20, x21, [x0, #96]
+        stp x22, x23, [x0, #112]
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack,"",%progbits
+#endif
+
diff --git a/arm/fastmul/bignum_kmul_32_64_neon.S b/arm/fastmul/bignum_kmul_32_64_neon.S
new file mode 100644
index 000000000..ce17e8fbb
--- /dev/null
+++ b/arm/fastmul/bignum_kmul_32_64_neon.S
@@ -0,0 +1,1387 @@
+// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+// SPDX-License-Identifier: Apache-2.0 OR ISC
+
+// ----------------------------------------------------------------------------
+// Multiply z := x * y
+// Inputs x[32], y[32]; output z[64]; temporary buffer t[>=96]
+//
+//    extern void bignum_kmul_32_64_neon
+//     (uint64_t z[static 64], uint64_t x[static 32], uint64_t y[static 32],
+//      uint64_t t[static 96])
+//
+// This is a Karatsuba-style function multiplying half-sized results
+// internally and using temporary buffer t for intermediate results.
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = y, X3 = t
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(bignum_kmul_32_64_neon)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(bignum_kmul_32_64_neon)
+        .text
+        .balign 4
+
+#define K 16
+#define L 8 // this is (K/2)
+
+#define z x19
+#define x x20
+#define y x21
+#define t x22
+
+#define c x16
+
+S2N_BN_SYMBOL(bignum_kmul_32_64_neon):
+
+// Save extra registers and return address, store parameters safely
+
+        stp     x19, x20, [sp, -16]!
+        stp     x21, x22, [sp, -16]!
+        stp     x23, x24, [sp, -16]!
+        stp     x25, x26, [sp, -16]!
+        stp     x27, x28, [sp, -16]!
+        stp     x29, x30, [sp, -16]!
+
+        mov     z, x0
+        mov     x, x1
+        mov     y, x2
+        mov     t, x3
+
+// Compute L = x_lo * y_lo in bottom half of buffer (size 16 x 16 -> 32)
+
+        bl      bignum_kmul_32_64_neon_local_kmul_16_32
+
+// Compute H = x_hi * y_hi in top half of buffer (size 16 x 16 -> 32)
+
+        add     x0, z, #16*K
+        add     x1, x, #8*K
+        add     x2, y, #8*K
+        mov     x3, t
+        bl      bignum_kmul_32_64_neon_local_kmul_16_32
+
+// Compute absolute difference [t..] = |x_lo - x_hi|
+// and the sign x = sgn(x_lo - x_hi) as a bitmask (all 1s for negative)
+// Note that we overwrite the pointer x itself with this sign,
+// which is safe since we no longer need it.
+
+        ldp     x0, x1, [x, #128]
+        ldp     x16, x17, [x]
+        subs    x0, x0, x16
+        sbcs    x1, x1, x17
+
+        ldp     x2, x3, [x, #144]
+        ldp     x16, x17, [x, #16]
+        sbcs    x2, x2, x16
+        sbcs    x3, x3, x17
+
+        ldp     x4, x5, [x, #160]
+        ldp     x16, x17, [x, #32]
+        sbcs    x4, x4, x16
+        sbcs    x5, x5, x17
+
+        ldp     x6, x7, [x, #176]
+        ldp     x16, x17, [x, #48]
+        sbcs    x6, x6, x16
+        sbcs    x7, x7, x17
+
+        ldp     x8, x9, [x, #192]
+        ldp     x16, x17, [x, #64]
+        sbcs    x8, x8, x16
+        sbcs    x9, x9, x17
+
+        ldp     x10, x11, [x, #208]
+        ldp     x16, x17, [x, #80]
+        sbcs    x10, x10, x16
+        sbcs    x11, x11, x17
+
+        ldp     x12, x13, [x, #224]
+        ldp     x16, x17, [x, #96]
+        sbcs    x12, x12, x16
+        sbcs    x13, x13, x17
+
+        ldp     x14, x15, [x, #240]
+        ldp     x16, x17, [x, #112]
+        sbcs    x14, x14, x16
+        sbcs    x15, x15, x17
+
+        sbc     x, xzr, xzr
+
+        adds    xzr, x, x
+
+        eor     x0, x0, x
+        adcs    x0, x0, xzr
+        eor     x1, x1, x
+        adcs    x1, x1, xzr
+        stp     x0, x1, [t]
+
+        eor     x2, x2, x
+        adcs    x2, x2, xzr
+        eor     x3, x3, x
+        adcs    x3, x3, xzr
+        stp     x2, x3, [t, #16]
+
+        eor     x4, x4, x
+        adcs    x4, x4, xzr
+        eor     x5, x5, x
+        adcs    x5, x5, xzr
+        stp     x4, x5, [t, #32]
+
+        eor     x6, x6, x
+        adcs    x6, x6, xzr
+        eor     x7, x7, x
+        adcs    x7, x7, xzr
+        stp     x6, x7, [t, #48]
+
+        eor     x8, x8, x
+        adcs    x8, x8, xzr
+        eor     x9, x9, x
+        adcs    x9, x9, xzr
+        stp     x8, x9, [t, #64]
+
+        eor     x10, x10, x
+        adcs    x10, x10, xzr
+        eor     x11, x11, x
+        adcs    x11, x11, xzr
+        stp     x10, x11, [t, #80]
+
+        eor     x12, x12, x
+        adcs    x12, x12, xzr
+        eor     x13, x13, x
+        adcs    x13, x13, xzr
+        stp     x12, x13, [t, #96]
+
+        eor     x14, x14, x
+        adcs    x14, x14, xzr
+        eor     x15, x15, x
+        adc     x15, x15, xzr
+        stp     x14, x15, [t, #112]
+
+// Compute the other absolute difference [t+8*K..] = |y_hi - y_lo|
+// Collect the combined product sign bitmask (all 1s for negative) as
+// y = sgn((x_lo - x_hi) * (y_hi - y_lo)), overwriting the y pointer.
+
+        ldp     x0, x1, [y]
+        ldp     x16, x17, [y, #128]
+        subs    x0, x0, x16
+        sbcs    x1, x1, x17
+
+        ldp     x2, x3, [y, #16]
+        ldp     x16, x17, [y, #144]
+        sbcs    x2, x2, x16
+        sbcs    x3, x3, x17
+
+        ldp     x4, x5, [y, #32]
+        ldp     x16, x17, [y, #160]
+        sbcs    x4, x4, x16
+        sbcs    x5, x5, x17
+
+        ldp     x6, x7, [y, #48]
+        ldp     x16, x17, [y, #176]
+        sbcs    x6, x6, x16
+        sbcs    x7, x7, x17
+
+        ldp     x8, x9, [y, #64]
+        ldp     x16, x17, [y, #192]
+        sbcs    x8, x8, x16
+        sbcs    x9, x9, x17
+
+        ldp     x10, x11, [y, #80]
+        ldp     x16, x17, [y, #208]
+        sbcs    x10, x10, x16
+        sbcs    x11, x11, x17
+
+        ldp     x12, x13, [y, #96]
+        ldp     x16, x17, [y, #224]
+        sbcs    x12, x12, x16
+        sbcs    x13, x13, x17
+
+        ldp     x14, x15, [y, #112]
+        ldp     x16, x17, [y, #240]
+        sbcs    x14, x14, x16
+        sbcs    x15, x15, x17
+
+        sbc     y, xzr, xzr
+
+        adds    xzr, y, y
+
+        eor     x0, x0, y
+        adcs    x0, x0, xzr
+        eor     x1, x1, y
+        adcs    x1, x1, xzr
+        stp     x0, x1, [t, #128]
+
+        eor     x2, x2, y
+        adcs    x2, x2, xzr
+        eor     x3, x3, y
+        adcs    x3, x3, xzr
+        stp     x2, x3, [t, #128+16]
+
+        eor     x4, x4, y
+        adcs    x4, x4, xzr
+        eor     x5, x5, y
+        adcs    x5, x5, xzr
+        stp     x4, x5, [t, #128+32]
+
+        eor     x6, x6, y
+        adcs    x6, x6, xzr
+        eor     x7, x7, y
+        adcs    x7, x7, xzr
+        stp     x6, x7, [t, #128+48]
+
+        eor     x8, x8, y
+        adcs    x8, x8, xzr
+        eor     x9, x9, y
+        adcs    x9, x9, xzr
+        stp     x8, x9, [t, #128+64]
+
+        eor     x10, x10, y
+        adcs    x10, x10, xzr
+        eor     x11, x11, y
+        adcs    x11, x11, xzr
+        stp     x10, x11, [t, #128+80]
+
+        eor     x12, x12, y
+        adcs    x12, x12, xzr
+        eor     x13, x13, y
+        adcs    x13, x13, xzr
+        stp     x12, x13, [t, #128+96]
+
+        eor     x14, x14, y
+        adcs    x14, x14, xzr
+        eor     x15, x15, y
+        adc     x15, x15, xzr
+        stp     x14, x15, [t, #128+112]
+
+        eor     y, y, x
+
+// Compute H' = H + L_top in place of H (it cannot overflow)
+
+        ldp     x0, x1, [z, #16*16]
+        ldp     x2, x3, [z, #16*L]
+        adds    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*16]
+
+        ldp     x0, x1, [z, #16*17]
+        ldp     x2, x3, [z, #16*9]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*17]
+
+        ldp     x0, x1, [z, #16*18]
+        ldp     x2, x3, [z, #16*10]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*18]
+
+        ldp     x0, x1, [z, #16*19]
+        ldp     x2, x3, [z, #16*11]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*19]
+
+        ldp     x0, x1, [z, #16*20]
+        ldp     x2, x3, [z, #16*12]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*20]
+
+        ldp     x0, x1, [z, #16*21]
+        ldp     x2, x3, [z, #16*13]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*21]
+
+        ldp     x0, x1, [z, #16*22]
+        ldp     x2, x3, [z, #16*14]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*22]
+
+        ldp     x0, x1, [z, #16*23]
+        ldp     x2, x3, [z, #16*15]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*23]
+
+        ldp     x0, x1, [z, #16*24]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*24]
+
+        ldp     x0, x1, [z, #16*25]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*25]
+
+        ldp     x0, x1, [z, #16*26]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*26]
+
+        ldp     x0, x1, [z, #16*27]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*27]
+
+        ldp     x0, x1, [z, #16*28]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*28]
+
+        ldp     x0, x1, [z, #16*29]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*29]
+
+        ldp     x0, x1, [z, #16*30]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*30]
+
+        ldp     x0, x1, [z, #16*31]
+        adcs    x0, x0, xzr
+        adc     x1, x1, xzr
+        stp     x0, x1, [z, #16*31]
+
+// Compute M = |x_lo - x_hi| * |y_hi - y_lo|, size 32
+
+        add     x0, t, #16*K
+        mov     x1, t
+        add     x2, t, #8*K
+        add     x3, t, #32*K
+        bl      bignum_kmul_32_64_neon_local_kmul_16_32
+
+// Add the interlocking H' and L_bot terms
+// Intercept the carry at the 3k position and store it in x.
+// Again, we no longer need the input x was pointing at.
+
+        ldp     x0, x1, [z, #16*16]
+        ldp     x2, x3, [z]
+        adds    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*8]
+
+        ldp     x0, x1, [z, #16*17]
+        ldp     x2, x3, [z, #16*1]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*9]
+
+        ldp     x0, x1, [z, #16*18]
+        ldp     x2, x3, [z, #16*2]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*10]
+
+        ldp     x0, x1, [z, #16*19]
+        ldp     x2, x3, [z, #16*3]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*11]
+
+        ldp     x0, x1, [z, #16*20]
+        ldp     x2, x3, [z, #16*4]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*12]
+
+        ldp     x0, x1, [z, #16*21]
+        ldp     x2, x3, [z, #16*5]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*13]
+
+        ldp     x0, x1, [z, #16*22]
+        ldp     x2, x3, [z, #16*6]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*14]
+
+        ldp     x0, x1, [z, #16*23]
+        ldp     x2, x3, [z, #16*7]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*15]
+
+        ldp     x0, x1, [z, #16*16]
+        ldp     x2, x3, [z, #16*24]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*16]
+
+        ldp     x0, x1, [z, #16*17]
+        ldp     x2, x3, [z, #16*25]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*17]
+
+        ldp     x0, x1, [z, #16*18]
+        ldp     x2, x3, [z, #16*26]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*18]
+
+        ldp     x0, x1, [z, #16*19]
+        ldp     x2, x3, [z, #16*27]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*19]
+
+        ldp     x0, x1, [z, #16*20]
+        ldp     x2, x3, [z, #16*28]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*20]
+
+        ldp     x0, x1, [z, #16*21]
+        ldp     x2, x3, [z, #16*29]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*21]
+
+        ldp     x0, x1, [z, #16*22]
+        ldp     x2, x3, [z, #16*30]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*22]
+
+        ldp     x0, x1, [z, #16*23]
+        ldp     x2, x3, [z, #16*31]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*23]
+
+        cset      x, cs
+
+// Add the sign-adjusted mid-term cross product M
+
+        cmn     y, y
+
+        ldp     x0, x1, [z, #128]
+        ldp     x2, x3, [t, #128+128]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #128]
+
+        ldp     x0, x1, [z, #144]
+        ldp     x2, x3, [t, #128+144]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #144]
+
+        ldp     x0, x1, [z, #160]
+        ldp     x2, x3, [t, #128+160]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #160]
+
+        ldp     x0, x1, [z, #176]
+        ldp     x2, x3, [t, #128+176]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #176]
+
+        ldp     x0, x1, [z, #192]
+        ldp     x2, x3, [t, #128+192]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #192]
+
+        ldp     x0, x1, [z, #208]
+        ldp     x2, x3, [t, #128+208]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #208]
+
+        ldp     x0, x1, [z, #224]
+        ldp     x2, x3, [t, #128+224]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #224]
+
+        ldp     x0, x1, [z, #240]
+        ldp     x2, x3, [t, #128+240]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #240]
+
+        ldp     x0, x1, [z, #256]
+        ldp     x2, x3, [t, #128+256]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #256]
+
+        ldp     x0, x1, [z, #272]
+        ldp     x2, x3, [t, #128+272]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #272]
+
+        ldp     x0, x1, [z, #288]
+        ldp     x2, x3, [t, #128+288]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #288]
+
+        ldp     x0, x1, [z, #304]
+        ldp     x2, x3, [t, #128+304]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #304]
+
+        ldp     x0, x1, [z, #320]
+        ldp     x2, x3, [t, #128+320]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #320]
+
+        ldp     x0, x1, [z, #336]
+        ldp     x2, x3, [t, #128+336]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #336]
+
+        ldp     x0, x1, [z, #352]
+        ldp     x2, x3, [t, #128+352]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #352]
+
+        ldp     x0, x1, [z, #368]
+        ldp     x2, x3, [t, #128+368]
+        eor     x2, x2, y
+        adcs    x0, x0, x2
+        eor     x3, x3, y
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #368]
+
+// Get the next digits effectively resulting so far starting at 3k
+// [...,c,c,c,c,x]
+
+        adcs    x, y, x
+        adc     c, y, xzr
+
+// Now propagate through the top quarter of the result
+
+        ldp     x0, x1, [z, #16*24]
+        adds    x0, x0, x
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*24]
+
+        ldp     x0, x1, [z, #16*25]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*25]
+
+        ldp     x0, x1, [z, #16*26]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*26]
+
+        ldp     x0, x1, [z, #16*27]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*27]
+
+        ldp     x0, x1, [z, #16*28]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*28]
+
+        ldp     x0, x1, [z, #16*29]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*29]
+
+        ldp     x0, x1, [z, #16*30]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*30]
+
+        ldp     x0, x1, [z, #16*31]
+        adcs    x0, x0, c
+        adc     x1, x1, c
+        stp     x0, x1, [z, #16*31]
+
+// Restore and return
+
+        ldp     x29, x30, [sp], #16
+        ldp     x27, x28, [sp], #16
+        ldp     x25, x26, [sp], #16
+        ldp     x23, x24, [sp], #16
+        ldp     x21, x22, [sp], #16
+        ldp     x19, x20, [sp], #16
+        ret
+
+// Local copy of bignum_kmul_16_32_neon, identical to main one except that it
+// only preserves the key registers we need to be stable in the main code.
+// This includes in turn a copy of bignum_mul_8_16_neon.
+
+bignum_kmul_32_64_neon_local_kmul_16_32:
+        stp     x19, x20, [sp, -16]!
+        stp     x21, x22, [sp, -16]!
+        stp     x23, x30, [sp, -16]!
+        mov     x25, x0
+        mov     x26, x1
+        mov     x27, x2
+        mov     x28, x3
+        bl      bignum_kmul_32_64_neon_local_mul_8_16
+        ldp     x10, x11, [x26]
+        ldp     x8, x9, [x26, #64]
+        subs    x10, x10, x8
+        sbcs    x11, x11, x9
+        ldp     x12, x13, [x26, #16]
+        ldp     x8, x9, [x26, #80]
+        sbcs    x12, x12, x8
+        sbcs    x13, x13, x9
+        ldp     x14, x15, [x26, #32]
+        ldp     x8, x9, [x26, #96]
+        sbcs    x14, x14, x8
+        sbcs    x15, x15, x9
+        ldp     x16, x17, [x26, #48]
+        ldp     x8, x9, [x26, #112]
+        sbcs    x16, x16, x8
+        sbcs    x17, x17, x9
+        csetm   x29, cc
+        cmn     x29, x29
+        eor     x10, x10, x29
+        adcs    x10, x10, xzr
+        eor     x11, x11, x29
+        adcs    x11, x11, xzr
+        stp     x10, x11, [x28]
+        eor     x12, x12, x29
+        adcs    x12, x12, xzr
+        eor     x13, x13, x29
+        adcs    x13, x13, xzr
+        stp     x12, x13, [x28, #16]
+        eor     x14, x14, x29
+        adcs    x14, x14, xzr
+        eor     x15, x15, x29
+        adcs    x15, x15, xzr
+        stp     x14, x15, [x28, #32]
+        eor     x16, x16, x29
+        adcs    x16, x16, xzr
+        eor     x17, x17, x29
+        adcs    x17, x17, xzr
+        stp     x16, x17, [x28, #48]
+        add     x0, x25, #0x80
+        add     x1, x26, #0x40
+        add     x2, x27, #0x40
+        bl      bignum_kmul_32_64_neon_local_mul_8_16
+        ldp     x10, x11, [x27]
+        ldp     x8, x9, [x27, #64]
+        subs    x10, x8, x10
+        sbcs    x11, x9, x11
+        ldp     x12, x13, [x27, #16]
+        ldp     x8, x9, [x27, #80]
+        sbcs    x12, x8, x12
+        sbcs    x13, x9, x13
+        ldp     x14, x15, [x27, #32]
+        ldp     x8, x9, [x27, #96]
+        sbcs    x14, x8, x14
+        sbcs    x15, x9, x15
+        ldp     x16, x17, [x27, #48]
+        ldp     x8, x9, [x27, #112]
+        sbcs    x16, x8, x16
+        sbcs    x17, x9, x17
+        csetm   x19, cc
+        cmn     x19, x19
+        eor     x10, x10, x19
+        adcs    x10, x10, xzr
+        eor     x11, x11, x19
+        adcs    x11, x11, xzr
+        stp     x10, x11, [x28, #64]
+        eor     x12, x12, x19
+        adcs    x12, x12, xzr
+        eor     x13, x13, x19
+        adcs    x13, x13, xzr
+        stp     x12, x13, [x28, #80]
+        eor     x14, x14, x19
+        adcs    x14, x14, xzr
+        eor     x15, x15, x19
+        adcs    x15, x15, xzr
+        stp     x14, x15, [x28, #96]
+        eor     x16, x16, x19
+        adcs    x16, x16, xzr
+        eor     x17, x17, x19
+        adcs    x17, x17, xzr
+        stp     x16, x17, [x28, #112]
+        eor     x29, x29, x19
+        ldp     x10, x11, [x25, #128]
+        ldp     x12, x13, [x25, #64]
+        adds    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [x25, #128]
+        ldp     x10, x11, [x25, #144]
+        ldp     x12, x13, [x25, #80]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [x25, #144]
+        ldp     x10, x11, [x25, #160]
+        ldp     x12, x13, [x25, #96]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [x25, #160]
+        ldp     x10, x11, [x25, #176]
+        ldp     x12, x13, [x25, #112]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [x25, #176]
+        ldp     x10, x11, [x25, #192]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [x25, #192]
+        ldp     x10, x11, [x25, #208]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [x25, #208]
+        ldp     x10, x11, [x25, #224]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [x25, #224]
+        ldp     x10, x11, [x25, #240]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [x25, #240]
+        add     x0, x28, #0x80
+        mov     x1, x28
+        add     x2, x28, #0x40
+        bl      bignum_kmul_32_64_neon_local_mul_8_16
+        ldp     x0, x1, [x25]
+        ldp     x16, x17, [x25, #128]
+        adds    x0, x0, x16
+        adcs    x1, x1, x17
+        ldp     x2, x3, [x25, #16]
+        ldp     x16, x17, [x25, #144]
+        adcs    x2, x2, x16
+        adcs    x3, x3, x17
+        ldp     x4, x5, [x25, #32]
+        ldp     x16, x17, [x25, #160]
+        adcs    x4, x4, x16
+        adcs    x5, x5, x17
+        ldp     x6, x7, [x25, #48]
+        ldp     x16, x17, [x25, #176]
+        adcs    x6, x6, x16
+        adcs    x7, x7, x17
+        ldp     x8, x9, [x25, #128]
+        ldp     x16, x17, [x25, #192]
+        adcs    x8, x8, x16
+        adcs    x9, x9, x17
+        ldp     x10, x11, [x25, #144]
+        ldp     x16, x17, [x25, #208]
+        adcs    x10, x10, x16
+        adcs    x11, x11, x17
+        ldp     x12, x13, [x25, #160]
+        ldp     x16, x17, [x25, #224]
+        adcs    x12, x12, x16
+        adcs    x13, x13, x17
+        ldp     x14, x15, [x25, #176]
+        ldp     x16, x17, [x25, #240]
+        adcs    x14, x14, x16
+        adcs    x15, x15, x17
+        cset    x26, cs
+        cmn     x29, x29
+        ldp     x16, x17, [x28, #128]
+        eor     x16, x16, x29
+        adcs    x0, x0, x16
+        eor     x17, x17, x29
+        adcs    x1, x1, x17
+        stp     x0, x1, [x25, #64]
+        ldp     x16, x17, [x28, #144]
+        eor     x16, x16, x29
+        adcs    x2, x2, x16
+        eor     x17, x17, x29
+        adcs    x3, x3, x17
+        stp     x2, x3, [x25, #80]
+        ldp     x16, x17, [x28, #160]
+        eor     x16, x16, x29
+        adcs    x4, x4, x16
+        eor     x17, x17, x29
+        adcs    x5, x5, x17
+        stp     x4, x5, [x25, #96]
+        ldp     x16, x17, [x28, #176]
+        eor     x16, x16, x29
+        adcs    x6, x6, x16
+        eor     x17, x17, x29
+        adcs    x7, x7, x17
+        stp     x6, x7, [x25, #112]
+        ldp     x16, x17, [x28, #192]
+        eor     x16, x16, x29
+        adcs    x8, x8, x16
+        eor     x17, x17, x29
+        adcs    x9, x9, x17
+        stp     x8, x9, [x25, #128]
+        ldp     x16, x17, [x28, #208]
+        eor     x16, x16, x29
+        adcs    x10, x10, x16
+        eor     x17, x17, x29
+        adcs    x11, x11, x17
+        stp     x10, x11, [x25, #144]
+        ldp     x16, x17, [x28, #224]
+        eor     x16, x16, x29
+        adcs    x12, x12, x16
+        eor     x17, x17, x29
+        adcs    x13, x13, x17
+        stp     x12, x13, [x25, #160]
+        ldp     x16, x17, [x28, #240]
+        eor     x16, x16, x29
+        adcs    x14, x14, x16
+        eor     x17, x17, x29
+        adcs    x15, x15, x17
+        stp     x14, x15, [x25, #176]
+        adcs    x27, x29, x26
+        adc     x28, x29, xzr
+        ldp     x10, x11, [x25, #192]
+        adds    x10, x10, x27
+        adcs    x11, x11, x28
+        stp     x10, x11, [x25, #192]
+        ldp     x10, x11, [x25, #208]
+        adcs    x10, x10, x28
+        adcs    x11, x11, x28
+        stp     x10, x11, [x25, #208]
+        ldp     x10, x11, [x25, #224]
+        adcs    x10, x10, x28
+        adcs    x11, x11, x28
+        stp     x10, x11, [x25, #224]
+        ldp     x10, x11, [x25, #240]
+        adcs    x10, x10, x28
+        adcs    x11, x11, x28
+        stp     x10, x11, [x25, #240]
+        ldp     x23, x30, [sp], #16
+        ldp     x21, x22, [sp], #16
+        ldp     x19, x20, [sp], #16
+        ret
+
+bignum_kmul_32_64_neon_local_mul_8_16:
+        ldp x3, x4, [x1]
+        ldr q0, [x1]
+        ldp x7, x8, [x2]
+        ldr q1, [x2]
+        ldp x5, x6, [x1, #16]
+        ldr q2, [x1, #16]
+        ldp x9, x10, [x2, #16]
+        ldr q3, [x2, #16]
+        uzp1 v4.4s, v1.4s, v0.4s
+        rev64 v1.4s, v1.4s
+        uzp1 v5.4s, v0.4s, v0.4s
+        mul v0.4s, v1.4s, v0.4s
+        uaddlp v0.2d, v0.4s
+        shl v0.2d, v0.2d, #32
+        umlal v0.2d, v5.2s, v4.2s
+        mov x11, v0.d[0]
+        mov x15, v0.d[1]
+        uzp1 v0.4s, v3.4s, v2.4s
+        rev64 v1.4s, v3.4s
+        uzp1 v3.4s, v2.4s, v2.4s
+        mul v1.4s, v1.4s, v2.4s
+        uaddlp v1.2d, v1.4s
+        shl v1.2d, v1.2d, #32
+        umlal v1.2d, v3.2s, v0.2s
+        mov x16, v1.d[0]
+        mov x17, v1.d[1]
+        ldr q0, [x1, #32]
+        ldr q1, [x2, #32]
+        ldr q2, [x1, #48]
+        ldr q3, [x2, #48]
+        umulh x19, x3, x7
+        adds x15, x15, x19
+        umulh x19, x4, x8
+        adcs x16, x16, x19
+        umulh x19, x5, x9
+        adcs x17, x17, x19
+        umulh x19, x6, x10
+        uzp1 v4.4s, v1.4s, v0.4s
+        rev64 v1.4s, v1.4s
+        uzp1 v5.4s, v0.4s, v0.4s
+        mul v0.4s, v1.4s, v0.4s
+        uaddlp v0.2d, v0.4s
+        shl v0.2d, v0.2d, #32
+        umlal v0.2d, v5.2s, v4.2s
+        adc x19, x19, xzr
+        adds x12, x15, x11
+        adcs x15, x16, x15
+        adcs x16, x17, x16
+        adcs x17, x19, x17
+        adc x19, xzr, x19
+        adds x13, x15, x11
+        adcs x14, x16, x12
+        adcs x15, x17, x15
+        adcs x16, x19, x16
+        adcs x17, xzr, x17
+        adc x19, xzr, x19
+        subs x24, x5, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x9
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x16, x16, x22
+        eor x21, x21, x20
+        adcs x17, x17, x21
+        adc x19, x19, x20
+        subs x24, x3, x4
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x8, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x12, x12, x22
+        eor x21, x21, x20
+        adcs x13, x13, x21
+        adcs x14, x14, x20
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x15, x15, x22
+        eor x21, x21, x20
+        adcs x16, x16, x21
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x13, x13, x22
+        eor x21, x21, x20
+        adcs x14, x14, x21
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        ldp x3, x4, [x1, #32]
+        stp x11, x12, [x0]
+        ldp x7, x8, [x2, #32]
+        stp x13, x14, [x0, #16]
+        ldp x5, x6, [x1, #48]
+        stp x15, x16, [x0, #32]
+        ldp x9, x10, [x2, #48]
+        stp x17, x19, [x0, #48]
+        mov x11, v0.d[0]
+        mov x15, v0.d[1]
+        uzp1 v0.4s, v3.4s, v2.4s
+        rev64 v1.4s, v3.4s
+        uzp1 v3.4s, v2.4s, v2.4s
+        mul v1.4s, v1.4s, v2.4s
+        uaddlp v1.2d, v1.4s
+        shl v1.2d, v1.2d, #32
+        umlal v1.2d, v3.2s, v0.2s
+        mov x16, v1.d[0]
+        mov x17, v1.d[1]
+        umulh x19, x3, x7
+        adds x15, x15, x19
+        umulh x19, x4, x8
+        adcs x16, x16, x19
+        umulh x19, x5, x9
+        adcs x17, x17, x19
+        umulh x19, x6, x10
+        adc x19, x19, xzr
+        adds x12, x15, x11
+        adcs x15, x16, x15
+        adcs x16, x17, x16
+        adcs x17, x19, x17
+        adc x19, xzr, x19
+        adds x13, x15, x11
+        adcs x14, x16, x12
+        adcs x15, x17, x15
+        adcs x16, x19, x16
+        adcs x17, xzr, x17
+        adc x19, xzr, x19
+        ldp x22, x21, [x0, #32]
+        adds x11, x11, x22
+        adcs x12, x12, x21
+        ldp x22, x21, [x0, #48]
+        adcs x13, x13, x22
+        adcs x14, x14, x21
+        adcs x15, x15, xzr
+        adcs x16, x16, xzr
+        adcs x17, x17, xzr
+        adc x19, x19, xzr
+        subs x24, x5, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x9
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x16, x16, x22
+        eor x21, x21, x20
+        adcs x17, x17, x21
+        adc x19, x19, x20
+        subs x24, x3, x4
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x8, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x12, x12, x22
+        eor x21, x21, x20
+        adcs x13, x13, x21
+        adcs x14, x14, x20
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x15, x15, x22
+        eor x21, x21, x20
+        adcs x16, x16, x21
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x13, x13, x22
+        eor x21, x21, x20
+        adcs x14, x14, x21
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        ldp x22, x21, [x1]
+        subs x3, x3, x22
+        sbcs x4, x4, x21
+        ldp x22, x21, [x1, #16]
+        sbcs x5, x5, x22
+        sbcs x6, x6, x21
+        csetm x24, cc
+        stp x11, x12, [x0, #64]
+        ldp x22, x21, [x2]
+        subs x7, x22, x7
+        sbcs x8, x21, x8
+        ldp x22, x21, [x2, #16]
+        sbcs x9, x22, x9
+        sbcs x10, x21, x10
+        csetm x1, cc
+        stp x13, x14, [x0, #80]
+        eor x3, x3, x24
+        subs x3, x3, x24
+        eor x4, x4, x24
+        sbcs x4, x4, x24
+        eor x5, x5, x24
+        sbcs x5, x5, x24
+        eor x6, x6, x24
+        sbc x6, x6, x24
+        stp x15, x16, [x0, #96]
+        eor x7, x7, x1
+        subs x7, x7, x1
+        eor x8, x8, x1
+        sbcs x8, x8, x1
+        eor x9, x9, x1
+        sbcs x9, x9, x1
+        eor x10, x10, x1
+        sbc x10, x10, x1
+        stp x17, x19, [x0, #112]
+        eor x1, x1, x24
+        mul x11, x3, x7
+        mul x15, x4, x8
+        mul x16, x5, x9
+        mul x17, x6, x10
+        umulh x19, x3, x7
+        adds x15, x15, x19
+        umulh x19, x4, x8
+        adcs x16, x16, x19
+        umulh x19, x5, x9
+        adcs x17, x17, x19
+        umulh x19, x6, x10
+        adc x19, x19, xzr
+        adds x12, x15, x11
+        adcs x15, x16, x15
+        adcs x16, x17, x16
+        adcs x17, x19, x17
+        adc x19, xzr, x19
+        adds x13, x15, x11
+        adcs x14, x16, x12
+        adcs x15, x17, x15
+        adcs x16, x19, x16
+        adcs x17, xzr, x17
+        adc x19, xzr, x19
+        subs x24, x5, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x9
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x16, x16, x22
+        eor x21, x21, x20
+        adcs x17, x17, x21
+        adc x19, x19, x20
+        subs x24, x3, x4
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x8, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x12, x12, x22
+        eor x21, x21, x20
+        adcs x13, x13, x21
+        adcs x14, x14, x20
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x15, x15, x22
+        eor x21, x21, x20
+        adcs x16, x16, x21
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x13, x13, x22
+        eor x21, x21, x20
+        adcs x14, x14, x21
+        adcs x15, x15, x20
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x3, x6
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x10, x7
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        subs x24, x4, x5
+        cneg x24, x24, cc
+        csetm x20, cc
+        subs x21, x9, x8
+        cneg x21, x21, cc
+        mul x22, x24, x21
+        umulh x21, x24, x21
+        cinv x20, x20, cc
+        cmn x20, #0x1
+        eor x22, x22, x20
+        adcs x14, x14, x22
+        eor x21, x21, x20
+        adcs x15, x15, x21
+        adcs x16, x16, x20
+        adcs x17, x17, x20
+        adc x19, x19, x20
+        ldp x3, x4, [x0]
+        ldp x7, x8, [x0, #64]
+        adds x3, x3, x7
+        adcs x4, x4, x8
+        ldp x5, x6, [x0, #16]
+        ldp x9, x10, [x0, #80]
+        adcs x5, x5, x9
+        adcs x6, x6, x10
+        ldp x20, x21, [x0, #96]
+        adcs x7, x7, x20
+        adcs x8, x8, x21
+        ldp x22, x23, [x0, #112]
+        adcs x9, x9, x22
+        adcs x10, x10, x23
+        adcs x24, x1, xzr
+        adc x2, x1, xzr
+        cmn x1, #0x1
+        eor x11, x11, x1
+        adcs x3, x11, x3
+        eor x12, x12, x1
+        adcs x4, x12, x4
+        eor x13, x13, x1
+        adcs x5, x13, x5
+        eor x14, x14, x1
+        adcs x6, x14, x6
+        eor x15, x15, x1
+        adcs x7, x15, x7
+        eor x16, x16, x1
+        adcs x8, x16, x8
+        eor x17, x17, x1
+        adcs x9, x17, x9
+        eor x19, x19, x1
+        adcs x10, x19, x10
+        adcs x20, x20, x24
+        adcs x21, x21, x2
+        adcs x22, x22, x2
+        adc x23, x23, x2
+        stp x3, x4, [x0, #32]
+        stp x5, x6, [x0, #48]
+        stp x7, x8, [x0, #64]
+        stp x9, x10, [x0, #80]
+        stp x20, x21, [x0, #96]
+        stp x22, x23, [x0, #112]
+        ret
+
+
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack,"",%progbits
+#endif
+
diff --git a/arm/fastmul/bignum_ksqr_16_32_neon.S b/arm/fastmul/bignum_ksqr_16_32_neon.S
new file mode 100644
index 000000000..bc7fca069
--- /dev/null
+++ b/arm/fastmul/bignum_ksqr_16_32_neon.S
@@ -0,0 +1,658 @@
+// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+// SPDX-License-Identifier: Apache-2.0 OR ISC
+
+// ----------------------------------------------------------------------------
+// Square, z := x^2
+// Input x[16]; output z[32]; temporary buffer t[>=24]
+//
+//    extern void bignum_ksqr_16_32_neon
+//     (uint64_t z[static 32], uint64_t x[static 16], uint64_t t[static 24]);
+//
+// This is a Karatsuba-style function squaring half-sized results
+// and using temporary buffer t for intermediate results.
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = t
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(bignum_ksqr_16_32_neon)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(bignum_ksqr_16_32_neon)
+        .text
+        .balign 4
+
+// Subroutine-safe copies of the output, inputs and temporary buffer pointers
+
+#define z x23
+#define x x24
+#define t x25
+
+// More variables for sign masks, with s also necessarily subroutine-safe
+
+#define s x19
+
+
+S2N_BN_SYMBOL(bignum_ksqr_16_32_neon):
+
+// Save registers, including return address
+
+        stp     x19, x20, [sp, #-16]!
+        stp     x21, x22, [sp, #-16]!
+        stp     x23, x24, [sp, #-16]!
+        stp     x25, x30, [sp, #-16]!
+
+// Move parameters into subroutine-safe places
+
+        mov     z, x0
+        mov     x, x1
+        mov     t, x2
+
+// Compute L = x_lo * y_lo in bottom half of buffer (size 8 x 8 -> 16)
+
+        bl      bignum_ksqr_16_32_neon_local_sqr_8_16
+
+// Compute absolute difference [t..] = |x_lo - x_hi|
+
+        ldp     x10, x11, [x]
+        ldp     x8, x9, [x, #64]
+        subs    x10, x10, x8
+        sbcs    x11, x11, x9
+        ldp     x12, x13, [x, #16]
+        ldp     x8, x9, [x, #80]
+        sbcs    x12, x12, x8
+        sbcs    x13, x13, x9
+        ldp     x14, x15, [x, #32]
+        ldp     x8, x9, [x, #96]
+        sbcs    x14, x14, x8
+        sbcs    x15, x15, x9
+        ldp     x16, x17, [x, #48]
+        ldp     x8, x9, [x, #112]
+        sbcs    x16, x16, x8
+        sbcs    x17, x17, x9
+        csetm   s, cc
+        adds    xzr, s, s
+        eor     x10, x10, s
+        adcs    x10, x10, xzr
+        eor     x11, x11, s
+        adcs    x11, x11, xzr
+        stp     x10, x11, [t]
+        eor     x12, x12, s
+        adcs    x12, x12, xzr
+        eor     x13, x13, s
+        adcs    x13, x13, xzr
+        stp     x12, x13, [t, #16]
+        eor     x14, x14, s
+        adcs    x14, x14, xzr
+        eor     x15, x15, s
+        adcs    x15, x15, xzr
+        stp     x14, x15, [t, #32]
+        eor     x16, x16, s
+        adcs    x16, x16, xzr
+        eor     x17, x17, s
+        adcs    x17, x17, xzr
+        stp     x16, x17, [t, #48]
+
+// Compute H = x_hi * y_hi in top half of buffer (size 8 x 8 -> 16)
+
+        add     x0, z, #128
+        add     x1, x, #64
+        bl      bignum_ksqr_16_32_neon_local_sqr_8_16
+
+// Compute H' = H + L_top in place of H (it cannot overflow)
+// First add 8-sized block then propagate carry through next 8
+
+        ldp     x10, x11, [z, #128]
+        ldp     x12, x13, [z, #64]
+        adds    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, #128]
+
+        ldp     x10, x11, [z, #128+16]
+        ldp     x12, x13, [z, #64+16]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, #128+16]
+
+        ldp     x10, x11, [z, #128+32]
+        ldp     x12, x13, [z, #64+32]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, #128+32]
+
+        ldp     x10, x11, [z, #128+48]
+        ldp     x12, x13, [z, #64+48]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, #128+48]
+
+        ldp     x10, x11, [z, #128+64]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [z, #128+64]
+
+        ldp     x10, x11, [z, #128+80]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [z, #128+80]
+
+        ldp     x10, x11, [z, #128+96]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [z, #128+96]
+
+        ldp     x10, x11, [z, #128+112]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [z, #128+112]
+
+// Compute M = |x_lo - x_hi| * |y_hi - y_lo| in [t+8...], size 16
+
+        add     x0, t, #64
+        mov     x1, t
+        bl      bignum_ksqr_16_32_neon_local_sqr_8_16
+
+// Add the interlocking H' and L_bot terms, storing in registers x15..x0
+// Intercept the carry at the 8 + 16 = 24 position and store it in x.
+// (Note that we no longer need the input x was pointing at.)
+
+        ldp     x0, x1, [z]
+        ldp     x16, x17, [z, #128]
+        adds    x0, x0, x16
+        adcs    x1, x1, x17
+        ldp     x2, x3, [z, #16]
+        ldp     x16, x17, [z, #144]
+        adcs    x2, x2, x16
+        adcs    x3, x3, x17
+        ldp     x4, x5, [z, #32]
+        ldp     x16, x17, [z, #160]
+        adcs    x4, x4, x16
+        adcs    x5, x5, x17
+        ldp     x6, x7, [z, #48]
+        ldp     x16, x17, [z, #176]
+        adcs    x6, x6, x16
+        adcs    x7, x7, x17
+        ldp     x8, x9, [z, #128]
+        ldp     x16, x17, [z, #192]
+        adcs    x8, x8, x16
+        adcs    x9, x9, x17
+        ldp     x10, x11, [z, #144]
+        ldp     x16, x17, [z, #208]
+        adcs    x10, x10, x16
+        adcs    x11, x11, x17
+        ldp     x12, x13, [z, #160]
+        ldp     x16, x17, [z, #224]
+        adcs    x12, x12, x16
+        adcs    x13, x13, x17
+        ldp     x14, x15, [z, #176]
+        ldp     x16, x17, [z, #240]
+        adcs    x14, x14, x16
+        adcs    x15, x15, x17
+        cset    x, cs
+
+// Subtract the mid-term cross product M
+
+        ldp     x16, x17, [t, #64]
+        subs    x0, x0, x16
+        sbcs    x1, x1, x17
+        stp     x0, x1, [z, #64]
+        ldp     x16, x17, [t, #80]
+        sbcs    x2, x2, x16
+        sbcs    x3, x3, x17
+        stp     x2, x3, [z, #80]
+        ldp     x16, x17, [t, #96]
+        sbcs    x4, x4, x16
+        sbcs    x5, x5, x17
+        stp     x4, x5, [z, #96]
+        ldp     x16, x17, [t, #112]
+        sbcs    x6, x6, x16
+        sbcs    x7, x7, x17
+        stp     x6, x7, [z, #112]
+        ldp     x16, x17, [t, #128]
+        sbcs    x8, x8, x16
+        sbcs    x9, x9, x17
+        stp     x8, x9, [z, #128]
+        ldp     x16, x17, [t, #144]
+        sbcs    x10, x10, x16
+        sbcs    x11, x11, x17
+        stp     x10, x11, [z, #144]
+        ldp     x16, x17, [t, #160]
+        sbcs    x12, x12, x16
+        sbcs    x13, x13, x17
+        stp     x12, x13, [z, #160]
+        ldp     x16, x17, [t, #176]
+        sbcs    x14, x14, x16
+        sbcs    x15, x15, x17
+        stp     x14, x15, [z, #176]
+
+// Get the next digits effectively resulting so far starting at 24
+
+        sbcs    x, x, xzr
+        csetm   t, cc
+
+// Now the final 8 digits of padding; the first one is special in using x
+// and also in getting the carry chain started
+
+        ldp     x10, x11, [z, #192]
+        adds    x10, x10, x
+        adcs    x11, x11, t
+        stp     x10, x11, [z, #192]
+        ldp     x10, x11, [z, #208]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, #208]
+        ldp     x10, x11, [z, #224]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, #224]
+        ldp     x10, x11, [z, #240]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, #240]
+
+// Restore registers and return
+
+        ldp     x25, x30, [sp], #16
+        ldp     x23, x24, [sp], #16
+        ldp     x21, x22, [sp], #16
+        ldp     x19, x20, [sp], #16
+
+        ret
+
+// -----------------------------------------------------------------------------
+// Local 8x8->16 squaring routine, shared to reduce code size. Effectively
+// the same as bignum_sqr_8_16_neon without the scratch register preservation.
+// -----------------------------------------------------------------------------
+
+bignum_ksqr_16_32_neon_local_sqr_8_16:
+// Load registers.
+        ldp	x2, x3, [x1]
+ldr	q20, [x1]
+        ldp	x4, x5, [x1, #16]
+ldr	q21, [x1, #16]
+        ldp	x6, x7, [x1, #32]
+ldr	q22, [x1, #32]
+        ldp	x8, x9, [x1, #48]
+ldr	q23, [x1, #48]
+movi	v30.2d, #0xffffffff
+
+        mul	x17, x2, x4
+        mul	x14, x3, x5
+
+// Scalar+NEON: square the lower half with a near-clone of bignum_sqr_4_8
+// NEON: prepare 64x64->128 squaring of two 64-bit ints (x2, x3)
+ext	v1.16b, v20.16b, v20.16b, #8
+        umulh	x20, x2, x4
+shrn	v2.2s, v20.2d, #32
+        subs	x21, x2, x3
+zip1	v0.2s, v20.2s, v1.2s
+        cneg	x21, x21, cc  // cc = lo, ul, last
+umull	v5.2d, v2.2s, v2.2s
+        csetm	x11, cc  // cc = lo, ul, last
+umull	v6.2d, v2.2s, v0.2s
+        subs	x12, x5, x4
+umull	v3.2d, v0.2s, v0.2s
+        cneg	x12, x12, cc  // cc = lo, ul, last
+mov	v1.16b, v6.16b
+        mul	x13, x21, x12
+usra	v1.2d, v3.2d, #32
+        umulh	x12, x21, x12
+and	v4.16b, v1.16b, v30.16b
+        cinv	x11, x11, cc  // cc = lo, ul, last
+add	v4.2d, v4.2d, v6.2d
+        eor	x13, x13, x11
+usra	v5.2d, v4.2d, #32
+        eor	x12, x12, x11
+sli	v3.2d, v4.2d, #32
+        adds	x19, x17, x20
+usra	v5.2d, v1.2d, #32
+        adc	x20, x20, xzr
+  // NEON: prepare 64x64->128 squaring of two 64-bit ints (x4, x5)
+  ext	v1.16b, v21.16b, v21.16b, #8
+        umulh	x21, x3, x5
+  shrn	v2.2s, v21.2d, #32
+        adds	x19, x19, x14
+  zip1	v0.2s, v21.2s, v1.2s
+        adcs	x20, x20, x21
+        adc	x21, x21, xzr
+        adds	x20, x20, x14
+        adc	x21, x21, xzr
+        cmn	x11, #0x1
+        adcs	x19, x19, x13
+mov	x13, v3.d[1] // mul     x13, x3, x3
+        adcs	x20, x20, x12
+mov	x14, v5.d[1] // umulh   x14, x3, x3
+        adc	x21, x21, x11
+mov	x12, v3.d[0] // mul     x12, x2, x2
+        adds	x17, x17, x17
+mov	x11, v5.d[0] // umulh   x11, x2, x2
+        adcs	x19, x19, x19
+  umull	v5.2d, v2.2s, v2.2s
+        adcs	x20, x20, x20
+  umull	v6.2d, v2.2s, v0.2s
+        adcs	x21, x21, x21
+  umull	v3.2d, v0.2s, v0.2s
+        adc	x10, xzr, xzr
+  mov	v1.16b, v6.16b
+
+        mul	x15, x2, x3
+  usra	v1.2d, v3.2d, #32
+        umulh	x16, x2, x3
+  and	v4.16b, v1.16b, v30.16b
+        adds	x11, x11, x15
+  add	v4.2d, v4.2d, v6.2d
+        adcs	x13, x13, x16
+  usra	v5.2d, v4.2d, #32
+        adc	x14, x14, xzr
+  sli	v3.2d, v4.2d, #32
+        adds	x11, x11, x15
+  usra	v5.2d, v1.2d, #32
+        adcs	x13, x13, x16
+        adc	x14, x14, xzr
+        stp	x12, x11, [x0]
+  mov	x11, v5.d[0] // umulh   x11, x4, x4
+        adds	x17, x17, x13
+  mov	x13, v3.d[1] // mul     x13, x5, x5
+        adcs	x19, x19, x14
+  mov	x14, v5.d[1] // umulh   x14, x5, x5
+        adcs	x20, x20, xzr
+  mov	x12, v3.d[0] // mul     x12, x4, x4
+        adcs	x21, x21, xzr
+// NEON: prepare muls in the upper half
+ext	v1.16b, v22.16b, v22.16b, #8
+        adc	x10, x10, xzr
+shrn	v2.2s, v22.2d, #32
+        stp	x17, x19, [x0, #16]
+zip1	v0.2s, v22.2s, v1.2s
+        mul	x15, x4, x5
+umull	v5.2d, v2.2s, v2.2s
+        umulh	x16, x4, x5
+umull	v6.2d, v2.2s, v0.2s
+        adds	x11, x11, x15
+umull	v3.2d, v0.2s, v0.2s
+        adcs	x13, x13, x16
+mov	v1.16b, v6.16b
+        adc	x14, x14, xzr
+usra	v1.2d, v3.2d, #32
+        adds	x11, x11, x15
+and	v4.16b, v1.16b, v30.16b
+        adcs	x13, x13, x16
+add	v4.2d, v4.2d, v6.2d
+        adc	x14, x14, xzr
+usra	v5.2d, v4.2d, #32
+        adds	x12, x12, x20
+sli	v3.2d, v4.2d, #32
+        adcs	x11, x11, x21
+usra	v5.2d, v1.2d, #32
+        stp	x12, x11, [x0, #32]
+  // NEON: prepare muls in the upper half
+  ext	v1.16b, v23.16b, v23.16b, #8
+        adcs	x13, x13, x10
+  shrn	v2.2s, v23.2d, #32
+        adc	x14, x14, xzr
+  zip1	v0.2s, v23.2s, v1.2s
+        stp	x13, x14, [x0, #48]
+
+// Scalar: square the upper half with a slight variant of the previous block
+        mul	x17, x6, x8
+  umull	v16.2d, v2.2s, v2.2s
+        mul	x14, x7, x9
+  umull	v6.2d, v2.2s, v0.2s
+        umulh	x20, x6, x8
+  umull	v18.2d, v0.2s, v0.2s
+        subs	x21, x6, x7
+        cneg	x21, x21, cc  // cc = lo, ul, last
+  mov	v1.16b, v6.16b
+        csetm	x11, cc  // cc = lo, ul, last
+        subs	x12, x9, x8
+        cneg	x12, x12, cc  // cc = lo, ul, last
+  usra	v1.2d, v18.2d, #32
+        mul	x13, x21, x12
+  and	v4.16b, v1.16b, v30.16b
+        umulh	x12, x21, x12
+  add	v4.2d, v4.2d, v6.2d
+        cinv	x11, x11, cc  // cc = lo, ul, last
+        eor	x13, x13, x11
+        eor	x12, x12, x11
+  usra	v16.2d, v4.2d, #32
+        adds	x19, x17, x20
+        adc	x20, x20, xzr
+  sli	v18.2d, v4.2d, #32
+        umulh	x21, x7, x9
+        adds	x19, x19, x14
+        adcs	x20, x20, x21
+        adc	x21, x21, xzr
+        adds	x20, x20, x14
+mov	x14, v5.d[1]
+        adc	x21, x21, xzr
+        cmn	x11, #0x1
+        adcs	x19, x19, x13
+mov	x13, v3.d[1]
+        adcs	x20, x20, x12
+mov	x12, v3.d[0]
+        adc	x21, x21, x11
+mov	x11, v5.d[0]
+        adds	x17, x17, x17
+        adcs	x19, x19, x19
+  usra	v16.2d, v1.2d, #32
+        adcs	x20, x20, x20
+        adcs	x21, x21, x21
+        adc	x10, xzr, xzr
+// NEON: two mul+umulhs for the next stage
+uzp2	v17.4s, v21.4s, v23.4s
+        mul	x15, x6, x7
+xtn	v4.2s, v23.2d
+        umulh	x16, x6, x7
+  mov	x22, v16.d[0]
+        adds	x11, x11, x15
+        adcs	x13, x13, x16
+xtn	v5.2s, v21.2d
+        adc	x14, x14, xzr
+        adds	x11, x11, x15
+rev64	v1.4s, v21.4s
+        adcs	x13, x13, x16
+        adc	x14, x14, xzr
+        stp	x12, x11, [x0, #64]
+        adds	x17, x17, x13
+  mov	x13, v18.d[1]
+        adcs	x19, x19, x14
+  mov	x14, v16.d[1]
+        adcs	x20, x20, xzr
+  mov	x12, v18.d[0]
+        adcs	x21, x21, xzr
+        adc	x10, x10, xzr
+umull	v6.2d, v4.2s, v5.2s
+        stp	x17, x19, [x0, #80]
+umull	v7.2d, v4.2s, v17.2s
+        mul	x15, x8, x9
+uzp2	v16.4s, v23.4s, v23.4s
+        umulh	x16, x8, x9
+mul	v0.4s, v1.4s, v23.4s
+        adds	x11, x22, x15
+        adcs	x13, x13, x16
+usra	v7.2d, v6.2d, #32
+        adc	x14, x14, xzr
+        adds	x11, x11, x15
+umull	v1.2d, v16.2s, v17.2s
+        adcs	x13, x13, x16
+        adc	x14, x14, xzr
+uaddlp	v0.2d, v0.4s
+        adds	x12, x12, x20
+        adcs	x11, x11, x21
+and	v2.16b, v7.16b, v30.16b
+umlal	v2.2d, v16.2s, v5.2s
+shl	v0.2d, v0.2d, #32
+usra	v1.2d, v7.2d, #32
+umlal	v0.2d, v4.2s, v5.2s
+mov	x16, v0.d[1]
+mov	x15, v0.d[0]
+usra	v1.2d, v2.2d, #32
+mov	x20, v1.d[0]
+mov	x21, v1.d[1]
+        stp	x12, x11, [x0, #96]
+        adcs	x13, x13, x10
+        adc	x14, x14, xzr
+        stp	x13, x14, [x0, #112]
+
+// Now get the cross-product in [s7,...,s0] and double it as [c,s7,...,s0]
+
+        mul	x10, x2, x6
+        mul	x14, x3, x7
+        umulh	x17, x2, x6
+        adds	x14, x14, x17
+        umulh	x17, x3, x7
+        adcs	x15, x15, x17
+        adcs	x16, x16, x20
+        adc	x17, x21, xzr
+        adds	x11, x14, x10
+        adcs	x14, x15, x14
+        adcs	x15, x16, x15
+        adcs	x16, x17, x16
+        adc	x17, xzr, x17
+        adds	x12, x14, x10
+        adcs	x13, x15, x11
+        adcs	x14, x16, x14
+        adcs	x15, x17, x15
+        adcs	x16, xzr, x16
+        adc	x17, xzr, x17
+        subs	x22, x4, x5
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x9, x8
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x15, x15, x21
+        eor	x20, x20, x19
+        adcs	x16, x16, x20
+        adc	x17, x17, x19
+        subs	x22, x2, x3
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x7, x6
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x11, x11, x21
+        eor	x20, x20, x19
+        adcs	x12, x12, x20
+        adcs	x13, x13, x19
+        adcs	x14, x14, x19
+        adcs	x15, x15, x19
+        adcs	x16, x16, x19
+        adc	x17, x17, x19
+        subs	x22, x3, x5
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x9, x7
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x14, x14, x21
+        eor	x20, x20, x19
+        adcs	x15, x15, x20
+        adcs	x16, x16, x19
+        adc	x17, x17, x19
+        subs	x22, x2, x4
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x8, x6
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x12, x12, x21
+        eor	x20, x20, x19
+        adcs	x13, x13, x20
+        adcs	x14, x14, x19
+        adcs	x15, x15, x19
+        adcs	x16, x16, x19
+        adc	x17, x17, x19
+        subs	x22, x2, x5
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x9, x6
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x13, x13, x21
+        eor	x20, x20, x19
+        adcs	x14, x14, x20
+        adcs	x15, x15, x19
+        adcs	x16, x16, x19
+        adc	x17, x17, x19
+        subs	x22, x3, x4
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x8, x7
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x13, x13, x21
+        eor	x20, x20, x19
+        adcs	x14, x14, x20
+        adcs	x15, x15, x19
+        adcs	x16, x16, x19
+        adc	x17, x17, x19
+        adds	x10, x10, x10
+        adcs	x11, x11, x11
+        adcs	x12, x12, x12
+        adcs	x13, x13, x13
+        adcs	x14, x14, x14
+        adcs	x15, x15, x15
+        adcs	x16, x16, x16
+        adcs	x17, x17, x17
+        adc	x19, xzr, xzr
+
+// Add it back to the buffer
+
+        ldp	x2, x3, [x0, #32]
+        adds	x10, x10, x2
+        adcs	x11, x11, x3
+        stp	x10, x11, [x0, #32]
+
+        ldp	x2, x3, [x0, #48]
+        adcs	x12, x12, x2
+        adcs	x13, x13, x3
+        stp	x12, x13, [x0, #48]
+
+        ldp	x2, x3, [x0, #64]
+        adcs	x14, x14, x2
+        adcs	x15, x15, x3
+        stp	x14, x15, [x0, #64]
+
+        ldp	x2, x3, [x0, #80]
+        adcs	x16, x16, x2
+        adcs	x17, x17, x3
+        stp	x16, x17, [x0, #80]
+
+        ldp	x2, x3, [x0, #96]
+        adcs	x2, x2, x19
+        adcs	x3, x3, xzr
+        stp	x2, x3, [x0, #96]
+
+        ldp	x2, x3, [x0, #112]
+        adcs	x2, x2, xzr
+        adc	x3, x3, xzr
+        stp	x2, x3, [x0, #112]
+
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack,"",%progbits
+#endif
diff --git a/arm/fastmul/bignum_ksqr_32_64_neon.S b/arm/fastmul/bignum_ksqr_32_64_neon.S
new file mode 100644
index 000000000..83e611c5b
--- /dev/null
+++ b/arm/fastmul/bignum_ksqr_32_64_neon.S
@@ -0,0 +1,1075 @@
+// Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+// SPDX-License-Identifier: Apache-2.0 OR ISC
+
+// ----------------------------------------------------------------------------
+// Square, z := x^2
+// Input x[32]; output z[64]; temporary buffer t[>=72]
+//
+//    extern void bignum_ksqr_32_64_neon
+//     (uint64_t z[static 64], uint64_t x[static 32], uint64_t t[static 72]);
+//
+// This is a Karatsuba-style function squaring half-sized results
+// and using temporary buffer t for intermediate results.
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = t
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(bignum_ksqr_32_64_neon)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(bignum_ksqr_32_64_neon)
+        .text
+        .balign 4
+
+#define K 16
+#define L 8 // (K/2)
+
+#define z x19
+#define x x20
+#define t x21
+
+#define c x16
+
+
+S2N_BN_SYMBOL(bignum_ksqr_32_64_neon):
+
+// Save extra registers and return address, store parameters safely
+
+        stp     x19, x20, [sp, #-16]!
+        stp     x21, x30, [sp, #-16]!
+
+        mov     z, x0
+        mov     x, x1
+        mov     t, x2
+
+// Compute L = x_lo * y_lo in bottom half of buffer (size 16 x 16 -> 32)
+
+        bl      bignum_ksqr_32_64_neon_local_ksqr_16_32
+
+// Compute H = x_hi * y_hi in top half of buffer (size 16 x 16 -> 32)
+
+        add     x0, z, #16*K
+        add     x1, x, #8*K
+        mov     x2, t
+        bl      bignum_ksqr_32_64_neon_local_ksqr_16_32
+
+// Compute H' = H + L_top in place of H (it cannot overflow)
+
+        ldp     x0, x1, [z, #16*16]
+        ldp     x2, x3, [z, #16*8]
+        adds    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*16]
+
+        ldp     x0, x1, [z, #16*17]
+        ldp     x2, x3, [z, #16*9]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*17]
+
+        ldp     x0, x1, [z, #16*18]
+        ldp     x2, x3, [z, #16*10]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*18]
+
+        ldp     x0, x1, [z, #16*19]
+        ldp     x2, x3, [z, #16*11]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*19]
+
+        ldp     x0, x1, [z, #16*20]
+        ldp     x2, x3, [z, #16*12]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*20]
+
+        ldp     x0, x1, [z, #16*21]
+        ldp     x2, x3, [z, #16*13]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*21]
+
+        ldp     x0, x1, [z, #16*22]
+        ldp     x2, x3, [z, #16*14]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*22]
+
+        ldp     x0, x1, [z, #16*23]
+        ldp     x2, x3, [z, #16*15]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*23]
+
+        ldp     x0, x1, [z, #16*24]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*24]
+
+        ldp     x0, x1, [z, #16*25]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*25]
+
+        ldp     x0, x1, [z, #16*26]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*26]
+
+        ldp     x0, x1, [z, #16*27]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*27]
+
+        ldp     x0, x1, [z, #16*28]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*28]
+
+        ldp     x0, x1, [z, #16*29]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*29]
+
+        ldp     x0, x1, [z, #16*30]
+        adcs    x0, x0, xzr
+        adcs    x1, x1, xzr
+        stp     x0, x1, [z, #16*30]
+
+        ldp     x0, x1, [z, #16*31]
+        adcs    x0, x0, xzr
+        adc     x1, x1, xzr
+        stp     x0, x1, [z, #16*31]
+
+// Compute absolute difference [t..] = |x_lo - x_hi|
+
+        ldp     x0, x1, [x, #128]
+        ldp     x16, x17, [x]
+        subs    x0, x0, x16
+        sbcs    x1, x1, x17
+
+        ldp     x2, x3, [x, #144]
+        ldp     x16, x17, [x, #16]
+        sbcs    x2, x2, x16
+        sbcs    x3, x3, x17
+
+        ldp     x4, x5, [x, #160]
+        ldp     x16, x17, [x, #32]
+        sbcs    x4, x4, x16
+        sbcs    x5, x5, x17
+
+        ldp     x6, x7, [x, #176]
+        ldp     x16, x17, [x, #48]
+        sbcs    x6, x6, x16
+        sbcs    x7, x7, x17
+
+        ldp     x8, x9, [x, #192]
+        ldp     x16, x17, [x, #64]
+        sbcs    x8, x8, x16
+        sbcs    x9, x9, x17
+
+        ldp     x10, x11, [x, #208]
+        ldp     x16, x17, [x, #80]
+        sbcs    x10, x10, x16
+        sbcs    x11, x11, x17
+
+        ldp     x12, x13, [x, #224]
+        ldp     x16, x17, [x, #96]
+        sbcs    x12, x12, x16
+        sbcs    x13, x13, x17
+
+        ldp     x14, x15, [x, #240]
+        ldp     x16, x17, [x, #112]
+        sbcs    x14, x14, x16
+        sbcs    x15, x15, x17
+
+        sbc     c, xzr, xzr
+
+        adds    xzr, c, c
+
+        eor     x0, x0, c
+        adcs    x0, x0, xzr
+        eor     x1, x1, c
+        adcs    x1, x1, xzr
+        stp     x0, x1, [t]
+
+        eor     x2, x2, c
+        adcs    x2, x2, xzr
+        eor     x3, x3, c
+        adcs    x3, x3, xzr
+        stp     x2, x3, [t, #16]
+
+        eor     x4, x4, c
+        adcs    x4, x4, xzr
+        eor     x5, x5, c
+        adcs    x5, x5, xzr
+        stp     x4, x5, [t, #32]
+
+        eor     x6, x6, c
+        adcs    x6, x6, xzr
+        eor     x7, x7, c
+        adcs    x7, x7, xzr
+        stp     x6, x7, [t, #48]
+
+        eor     x8, x8, c
+        adcs    x8, x8, xzr
+        eor     x9, x9, c
+        adcs    x9, x9, xzr
+        stp     x8, x9, [t, #64]
+
+        eor     x10, x10, c
+        adcs    x10, x10, xzr
+        eor     x11, x11, c
+        adcs    x11, x11, xzr
+        stp     x10, x11, [t, #80]
+
+        eor     x12, x12, c
+        adcs    x12, x12, xzr
+        eor     x13, x13, c
+        adcs    x13, x13, xzr
+        stp     x12, x13, [t, #96]
+
+        eor     x14, x14, c
+        adcs    x14, x14, xzr
+        eor     x15, x15, c
+        adc     x15, x15, xzr
+        stp     x14, x15, [t, #112]
+
+// Compute M = |x_lo - x_hi|^2, size 32
+
+        add     x0, t, #8*K
+        mov     x1, t
+        add     x2, t, #24*K
+        bl      bignum_ksqr_32_64_neon_local_ksqr_16_32
+
+// Add the interlocking H' and L_bot terms
+// Intercept the carry at the 3k position and store it in x.
+// (Note that we no longer need the input x was pointing at.)
+
+        ldp     x0, x1, [z, #16*16]
+        ldp     x2, x3, [z]
+        adds    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*8]
+
+        ldp     x0, x1, [z, #16*17]
+        ldp     x2, x3, [z, #16*1]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*9]
+
+        ldp     x0, x1, [z, #16*18]
+        ldp     x2, x3, [z, #16*2]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*10]
+
+        ldp     x0, x1, [z, #16*19]
+        ldp     x2, x3, [z, #16*3]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*11]
+
+        ldp     x0, x1, [z, #16*20]
+        ldp     x2, x3, [z, #16*4]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*12]
+
+        ldp     x0, x1, [z, #16*21]
+        ldp     x2, x3, [z, #16*5]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*13]
+
+        ldp     x0, x1, [z, #16*22]
+        ldp     x2, x3, [z, #16*6]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*14]
+
+        ldp     x0, x1, [z, #16*23]
+        ldp     x2, x3, [z, #16*7]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*15]
+
+        ldp     x0, x1, [z, #16*16]
+        ldp     x2, x3, [z, #16*24]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*16]
+
+        ldp     x0, x1, [z, #16*17]
+        ldp     x2, x3, [z, #16*25]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*17]
+
+        ldp     x0, x1, [z, #16*18]
+        ldp     x2, x3, [z, #16*26]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*18]
+
+        ldp     x0, x1, [z, #16*19]
+        ldp     x2, x3, [z, #16*27]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*19]
+
+        ldp     x0, x1, [z, #16*20]
+        ldp     x2, x3, [z, #16*28]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*20]
+
+        ldp     x0, x1, [z, #16*21]
+        ldp     x2, x3, [z, #16*29]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*21]
+
+        ldp     x0, x1, [z, #16*22]
+        ldp     x2, x3, [z, #16*30]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*22]
+
+        ldp     x0, x1, [z, #16*23]
+        ldp     x2, x3, [z, #16*31]
+        adcs    x0, x0, x2
+        adcs    x1, x1, x3
+        stp     x0, x1, [z, #16*23]
+
+        cset      x, cs
+
+// Subtract the mid-term cross product M
+
+        ldp     x0, x1, [z, #16*L]
+        ldp     x2, x3, [t, #16*L]
+        subs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*L]
+
+        ldp     x0, x1, [z, #16*9]
+        ldp     x2, x3, [t, #16*9]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*9]
+
+        ldp     x0, x1, [z, #16*10]
+        ldp     x2, x3, [t, #16*10]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*10]
+
+        ldp     x0, x1, [z, #16*11]
+        ldp     x2, x3, [t, #16*11]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*11]
+
+        ldp     x0, x1, [z, #16*12]
+        ldp     x2, x3, [t, #16*12]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*12]
+
+        ldp     x0, x1, [z, #16*13]
+        ldp     x2, x3, [t, #16*13]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*13]
+
+        ldp     x0, x1, [z, #16*14]
+        ldp     x2, x3, [t, #16*14]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*14]
+
+        ldp     x0, x1, [z, #16*15]
+        ldp     x2, x3, [t, #16*15]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*15]
+
+        ldp     x0, x1, [z, #16*16]
+        ldp     x2, x3, [t, #16*16]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*16]
+
+        ldp     x0, x1, [z, #16*17]
+        ldp     x2, x3, [t, #16*17]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*17]
+
+        ldp     x0, x1, [z, #16*18]
+        ldp     x2, x3, [t, #16*18]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*18]
+
+        ldp     x0, x1, [z, #16*19]
+        ldp     x2, x3, [t, #16*19]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*19]
+
+        ldp     x0, x1, [z, #16*20]
+        ldp     x2, x3, [t, #16*20]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*20]
+
+        ldp     x0, x1, [z, #16*21]
+        ldp     x2, x3, [t, #16*21]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*21]
+
+        ldp     x0, x1, [z, #16*22]
+        ldp     x2, x3, [t, #16*22]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*22]
+
+        ldp     x0, x1, [z, #16*23]
+        ldp     x2, x3, [t, #16*23]
+        sbcs    x0, x0, x2
+        sbcs    x1, x1, x3
+        stp     x0, x1, [z, #16*23]
+
+// Get the next digits effectively resulting so far starting at 3k
+// [...,c,c,c,c,x]
+
+        sbcs    x, x, xzr
+        csetm   c, cc
+
+// Now propagate through the top quarter of the result
+
+        ldp     x0, x1, [z, #16*24]
+        adds    x0, x0, x
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*24]
+
+        ldp     x0, x1, [z, #16*25]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*25]
+
+        ldp     x0, x1, [z, #16*26]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*26]
+
+        ldp     x0, x1, [z, #16*27]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*27]
+
+        ldp     x0, x1, [z, #16*28]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*28]
+
+        ldp     x0, x1, [z, #16*29]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*29]
+
+        ldp     x0, x1, [z, #16*30]
+        adcs    x0, x0, c
+        adcs    x1, x1, c
+        stp     x0, x1, [z, #16*30]
+
+        ldp     x0, x1, [z, #16*31]
+        adcs    x0, x0, c
+        adc     x1, x1, c
+        stp     x0, x1, [z, #16*31]
+
+// Restore
+
+        ldp     x21, x30, [sp], #16
+        ldp     x19, x20, [sp], #16
+
+        ret
+
+// Local copy of bignum_ksqr_16_32, identical to main one.
+// This includes in turn a copy of bignum_sqr_8_16.
+
+bignum_ksqr_32_64_neon_local_ksqr_16_32:
+        stp     x19, x20, [sp, #-16]!
+        stp     x21, x22, [sp, #-16]!
+        stp     x23, x24, [sp, #-16]!
+        stp     x25, x30, [sp, #-16]!
+        mov     x23, x0
+        mov     x24, x1
+        mov     x25, x2
+        bl      bignum_ksqr_32_64_neon_local_sqr_8_16
+        ldp     x10, x11, [x24]
+        ldp     x8, x9, [x24, #64]
+        subs    x10, x10, x8
+        sbcs    x11, x11, x9
+        ldp     x12, x13, [x24, #16]
+        ldp     x8, x9, [x24, #80]
+        sbcs    x12, x12, x8
+        sbcs    x13, x13, x9
+        ldp     x14, x15, [x24, #32]
+        ldp     x8, x9, [x24, #96]
+        sbcs    x14, x14, x8
+        sbcs    x15, x15, x9
+        ldp     x16, x17, [x24, #48]
+        ldp     x8, x9, [x24, #112]
+        sbcs    x16, x16, x8
+        sbcs    x17, x17, x9
+        csetm   x19, cc
+        cmn     x19, x19
+        eor     x10, x10, x19
+        adcs    x10, x10, xzr
+        eor     x11, x11, x19
+        adcs    x11, x11, xzr
+        stp     x10, x11, [x25]
+        eor     x12, x12, x19
+        adcs    x12, x12, xzr
+        eor     x13, x13, x19
+        adcs    x13, x13, xzr
+        stp     x12, x13, [x25, #16]
+        eor     x14, x14, x19
+        adcs    x14, x14, xzr
+        eor     x15, x15, x19
+        adcs    x15, x15, xzr
+        stp     x14, x15, [x25, #32]
+        eor     x16, x16, x19
+        adcs    x16, x16, xzr
+        eor     x17, x17, x19
+        adcs    x17, x17, xzr
+        stp     x16, x17, [x25, #48]
+        add     x0, x23, #0x80
+        add     x1, x24, #0x40
+        bl      bignum_ksqr_32_64_neon_local_sqr_8_16
+        ldp     x10, x11, [x23, #128]
+        ldp     x12, x13, [x23, #64]
+        adds    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [x23, #128]
+        ldp     x10, x11, [x23, #144]
+        ldp     x12, x13, [x23, #80]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [x23, #144]
+        ldp     x10, x11, [x23, #160]
+        ldp     x12, x13, [x23, #96]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [x23, #160]
+        ldp     x10, x11, [x23, #176]
+        ldp     x12, x13, [x23, #112]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [x23, #176]
+        ldp     x10, x11, [x23, #192]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [x23, #192]
+        ldp     x10, x11, [x23, #208]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [x23, #208]
+        ldp     x10, x11, [x23, #224]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [x23, #224]
+        ldp     x10, x11, [x23, #240]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [x23, #240]
+        add     x0, x25, #0x40
+        mov     x1, x25
+        bl      bignum_ksqr_32_64_neon_local_sqr_8_16
+        ldp     x0, x1, [x23]
+        ldp     x16, x17, [x23, #128]
+        adds    x0, x0, x16
+        adcs    x1, x1, x17
+        ldp     x2, x3, [x23, #16]
+        ldp     x16, x17, [x23, #144]
+        adcs    x2, x2, x16
+        adcs    x3, x3, x17
+        ldp     x4, x5, [x23, #32]
+        ldp     x16, x17, [x23, #160]
+        adcs    x4, x4, x16
+        adcs    x5, x5, x17
+        ldp     x6, x7, [x23, #48]
+        ldp     x16, x17, [x23, #176]
+        adcs    x6, x6, x16
+        adcs    x7, x7, x17
+        ldp     x8, x9, [x23, #128]
+        ldp     x16, x17, [x23, #192]
+        adcs    x8, x8, x16
+        adcs    x9, x9, x17
+        ldp     x10, x11, [x23, #144]
+        ldp     x16, x17, [x23, #208]
+        adcs    x10, x10, x16
+        adcs    x11, x11, x17
+        ldp     x12, x13, [x23, #160]
+        ldp     x16, x17, [x23, #224]
+        adcs    x12, x12, x16
+        adcs    x13, x13, x17
+        ldp     x14, x15, [x23, #176]
+        ldp     x16, x17, [x23, #240]
+        adcs    x14, x14, x16
+        adcs    x15, x15, x17
+        cset    x24, cs
+        ldp     x16, x17, [x25, #64]
+        subs    x0, x0, x16
+        sbcs    x1, x1, x17
+        stp     x0, x1, [x23, #64]
+        ldp     x16, x17, [x25, #80]
+        sbcs    x2, x2, x16
+        sbcs    x3, x3, x17
+        stp     x2, x3, [x23, #80]
+        ldp     x16, x17, [x25, #96]
+        sbcs    x4, x4, x16
+        sbcs    x5, x5, x17
+        stp     x4, x5, [x23, #96]
+        ldp     x16, x17, [x25, #112]
+        sbcs    x6, x6, x16
+        sbcs    x7, x7, x17
+        stp     x6, x7, [x23, #112]
+        ldp     x16, x17, [x25, #128]
+        sbcs    x8, x8, x16
+        sbcs    x9, x9, x17
+        stp     x8, x9, [x23, #128]
+        ldp     x16, x17, [x25, #144]
+        sbcs    x10, x10, x16
+        sbcs    x11, x11, x17
+        stp     x10, x11, [x23, #144]
+        ldp     x16, x17, [x25, #160]
+        sbcs    x12, x12, x16
+        sbcs    x13, x13, x17
+        stp     x12, x13, [x23, #160]
+        ldp     x16, x17, [x25, #176]
+        sbcs    x14, x14, x16
+        sbcs    x15, x15, x17
+        stp     x14, x15, [x23, #176]
+        sbcs    x24, x24, xzr
+        csetm   x25, cc
+        ldp     x10, x11, [x23, #192]
+        adds    x10, x10, x24
+        adcs    x11, x11, x25
+        stp     x10, x11, [x23, #192]
+        ldp     x10, x11, [x23, #208]
+        adcs    x10, x10, x25
+        adcs    x11, x11, x25
+        stp     x10, x11, [x23, #208]
+        ldp     x10, x11, [x23, #224]
+        adcs    x10, x10, x25
+        adcs    x11, x11, x25
+        stp     x10, x11, [x23, #224]
+        ldp     x10, x11, [x23, #240]
+        adcs    x10, x10, x25
+        adcs    x11, x11, x25
+        stp     x10, x11, [x23, #240]
+        ldp     x25, x30, [sp], #16
+        ldp     x23, x24, [sp], #16
+        ldp     x21, x22, [sp], #16
+        ldp     x19, x20, [sp], #16
+        ret
+
+bignum_ksqr_32_64_neon_local_sqr_8_16:
+// Load registers.
+        ldp	x2, x3, [x1]
+ldr	q20, [x1]
+        ldp	x4, x5, [x1, #16]
+ldr	q21, [x1, #16]
+        ldp	x6, x7, [x1, #32]
+ldr	q22, [x1, #32]
+        ldp	x8, x9, [x1, #48]
+ldr	q23, [x1, #48]
+movi	v30.2d, #0xffffffff
+
+        mul	x17, x2, x4
+        mul	x14, x3, x5
+
+// Scalar+NEON: square the lower half with a near-clone of bignum_sqr_4_8
+// NEON: prepare 64x64->128 squaring of two 64-bit ints (x2, x3)
+ext	v1.16b, v20.16b, v20.16b, #8
+        umulh	x20, x2, x4
+shrn	v2.2s, v20.2d, #32
+        subs	x21, x2, x3
+zip1	v0.2s, v20.2s, v1.2s
+        cneg	x21, x21, cc  // cc = lo, ul, last
+umull	v5.2d, v2.2s, v2.2s
+        csetm	x11, cc  // cc = lo, ul, last
+umull	v6.2d, v2.2s, v0.2s
+        subs	x12, x5, x4
+umull	v3.2d, v0.2s, v0.2s
+        cneg	x12, x12, cc  // cc = lo, ul, last
+mov	v1.16b, v6.16b
+        mul	x13, x21, x12
+usra	v1.2d, v3.2d, #32
+        umulh	x12, x21, x12
+and	v4.16b, v1.16b, v30.16b
+        cinv	x11, x11, cc  // cc = lo, ul, last
+add	v4.2d, v4.2d, v6.2d
+        eor	x13, x13, x11
+usra	v5.2d, v4.2d, #32
+        eor	x12, x12, x11
+sli	v3.2d, v4.2d, #32
+        adds	x19, x17, x20
+usra	v5.2d, v1.2d, #32
+        adc	x20, x20, xzr
+  // NEON: prepare 64x64->128 squaring of two 64-bit ints (x4, x5)
+  ext	v1.16b, v21.16b, v21.16b, #8
+        umulh	x21, x3, x5
+  shrn	v2.2s, v21.2d, #32
+        adds	x19, x19, x14
+  zip1	v0.2s, v21.2s, v1.2s
+        adcs	x20, x20, x21
+        adc	x21, x21, xzr
+        adds	x20, x20, x14
+        adc	x21, x21, xzr
+        cmn	x11, #0x1
+        adcs	x19, x19, x13
+mov	x13, v3.d[1] // mul     x13, x3, x3
+        adcs	x20, x20, x12
+mov	x14, v5.d[1] // umulh   x14, x3, x3
+        adc	x21, x21, x11
+mov	x12, v3.d[0] // mul     x12, x2, x2
+        adds	x17, x17, x17
+mov	x11, v5.d[0] // umulh   x11, x2, x2
+        adcs	x19, x19, x19
+  umull	v5.2d, v2.2s, v2.2s
+        adcs	x20, x20, x20
+  umull	v6.2d, v2.2s, v0.2s
+        adcs	x21, x21, x21
+  umull	v3.2d, v0.2s, v0.2s
+        adc	x10, xzr, xzr
+  mov	v1.16b, v6.16b
+
+        mul	x15, x2, x3
+  usra	v1.2d, v3.2d, #32
+        umulh	x16, x2, x3
+  and	v4.16b, v1.16b, v30.16b
+        adds	x11, x11, x15
+  add	v4.2d, v4.2d, v6.2d
+        adcs	x13, x13, x16
+  usra	v5.2d, v4.2d, #32
+        adc	x14, x14, xzr
+  sli	v3.2d, v4.2d, #32
+        adds	x11, x11, x15
+  usra	v5.2d, v1.2d, #32
+        adcs	x13, x13, x16
+        adc	x14, x14, xzr
+        stp	x12, x11, [x0]
+  mov	x11, v5.d[0] // umulh   x11, x4, x4
+        adds	x17, x17, x13
+  mov	x13, v3.d[1] // mul     x13, x5, x5
+        adcs	x19, x19, x14
+  mov	x14, v5.d[1] // umulh   x14, x5, x5
+        adcs	x20, x20, xzr
+  mov	x12, v3.d[0] // mul     x12, x4, x4
+        adcs	x21, x21, xzr
+// NEON: prepare muls in the upper half
+ext	v1.16b, v22.16b, v22.16b, #8
+        adc	x10, x10, xzr
+shrn	v2.2s, v22.2d, #32
+        stp	x17, x19, [x0, #16]
+zip1	v0.2s, v22.2s, v1.2s
+        mul	x15, x4, x5
+umull	v5.2d, v2.2s, v2.2s
+        umulh	x16, x4, x5
+umull	v6.2d, v2.2s, v0.2s
+        adds	x11, x11, x15
+umull	v3.2d, v0.2s, v0.2s
+        adcs	x13, x13, x16
+mov	v1.16b, v6.16b
+        adc	x14, x14, xzr
+usra	v1.2d, v3.2d, #32
+        adds	x11, x11, x15
+and	v4.16b, v1.16b, v30.16b
+        adcs	x13, x13, x16
+add	v4.2d, v4.2d, v6.2d
+        adc	x14, x14, xzr
+usra	v5.2d, v4.2d, #32
+        adds	x12, x12, x20
+sli	v3.2d, v4.2d, #32
+        adcs	x11, x11, x21
+usra	v5.2d, v1.2d, #32
+        stp	x12, x11, [x0, #32]
+  // NEON: prepare muls in the upper half
+  ext	v1.16b, v23.16b, v23.16b, #8
+        adcs	x13, x13, x10
+  shrn	v2.2s, v23.2d, #32
+        adc	x14, x14, xzr
+  zip1	v0.2s, v23.2s, v1.2s
+        stp	x13, x14, [x0, #48]
+
+// Scalar: square the upper half with a slight variant of the previous block
+        mul	x17, x6, x8
+  umull	v16.2d, v2.2s, v2.2s
+        mul	x14, x7, x9
+  umull	v6.2d, v2.2s, v0.2s
+        umulh	x20, x6, x8
+  umull	v18.2d, v0.2s, v0.2s
+        subs	x21, x6, x7
+        cneg	x21, x21, cc  // cc = lo, ul, last
+  mov	v1.16b, v6.16b
+        csetm	x11, cc  // cc = lo, ul, last
+        subs	x12, x9, x8
+        cneg	x12, x12, cc  // cc = lo, ul, last
+  usra	v1.2d, v18.2d, #32
+        mul	x13, x21, x12
+  and	v4.16b, v1.16b, v30.16b
+        umulh	x12, x21, x12
+  add	v4.2d, v4.2d, v6.2d
+        cinv	x11, x11, cc  // cc = lo, ul, last
+        eor	x13, x13, x11
+        eor	x12, x12, x11
+  usra	v16.2d, v4.2d, #32
+        adds	x19, x17, x20
+        adc	x20, x20, xzr
+  sli	v18.2d, v4.2d, #32
+        umulh	x21, x7, x9
+        adds	x19, x19, x14
+        adcs	x20, x20, x21
+        adc	x21, x21, xzr
+        adds	x20, x20, x14
+mov	x14, v5.d[1]
+        adc	x21, x21, xzr
+        cmn	x11, #0x1
+        adcs	x19, x19, x13
+mov	x13, v3.d[1]
+        adcs	x20, x20, x12
+mov	x12, v3.d[0]
+        adc	x21, x21, x11
+mov	x11, v5.d[0]
+        adds	x17, x17, x17
+        adcs	x19, x19, x19
+  usra	v16.2d, v1.2d, #32
+        adcs	x20, x20, x20
+        adcs	x21, x21, x21
+        adc	x10, xzr, xzr
+// NEON: two mul+umulhs for the next stage
+uzp2	v17.4s, v21.4s, v23.4s
+        mul	x15, x6, x7
+xtn	v4.2s, v23.2d
+        umulh	x16, x6, x7
+  mov	x22, v16.d[0]
+        adds	x11, x11, x15
+        adcs	x13, x13, x16
+xtn	v5.2s, v21.2d
+        adc	x14, x14, xzr
+        adds	x11, x11, x15
+rev64	v1.4s, v21.4s
+        adcs	x13, x13, x16
+        adc	x14, x14, xzr
+        stp	x12, x11, [x0, #64]
+        adds	x17, x17, x13
+  mov	x13, v18.d[1]
+        adcs	x19, x19, x14
+  mov	x14, v16.d[1]
+        adcs	x20, x20, xzr
+  mov	x12, v18.d[0]
+        adcs	x21, x21, xzr
+        adc	x10, x10, xzr
+umull	v6.2d, v4.2s, v5.2s
+        stp	x17, x19, [x0, #80]
+umull	v7.2d, v4.2s, v17.2s
+        mul	x15, x8, x9
+uzp2	v16.4s, v23.4s, v23.4s
+        umulh	x16, x8, x9
+mul	v0.4s, v1.4s, v23.4s
+        adds	x11, x22, x15
+        adcs	x13, x13, x16
+usra	v7.2d, v6.2d, #32
+        adc	x14, x14, xzr
+        adds	x11, x11, x15
+umull	v1.2d, v16.2s, v17.2s
+        adcs	x13, x13, x16
+        adc	x14, x14, xzr
+uaddlp	v0.2d, v0.4s
+        adds	x12, x12, x20
+        adcs	x11, x11, x21
+and	v2.16b, v7.16b, v30.16b
+umlal	v2.2d, v16.2s, v5.2s
+shl	v0.2d, v0.2d, #32
+usra	v1.2d, v7.2d, #32
+umlal	v0.2d, v4.2s, v5.2s
+mov	x16, v0.d[1]
+mov	x15, v0.d[0]
+usra	v1.2d, v2.2d, #32
+mov	x20, v1.d[0]
+mov	x21, v1.d[1]
+        stp	x12, x11, [x0, #96]
+        adcs	x13, x13, x10
+        adc	x14, x14, xzr
+        stp	x13, x14, [x0, #112]
+
+// Now get the cross-product in [s7,...,s0] and double it as [c,s7,...,s0]
+
+        mul	x10, x2, x6
+        mul	x14, x3, x7
+        umulh	x17, x2, x6
+        adds	x14, x14, x17
+        umulh	x17, x3, x7
+        adcs	x15, x15, x17
+        adcs	x16, x16, x20
+        adc	x17, x21, xzr
+        adds	x11, x14, x10
+        adcs	x14, x15, x14
+        adcs	x15, x16, x15
+        adcs	x16, x17, x16
+        adc	x17, xzr, x17
+        adds	x12, x14, x10
+        adcs	x13, x15, x11
+        adcs	x14, x16, x14
+        adcs	x15, x17, x15
+        adcs	x16, xzr, x16
+        adc	x17, xzr, x17
+        subs	x22, x4, x5
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x9, x8
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x15, x15, x21
+        eor	x20, x20, x19
+        adcs	x16, x16, x20
+        adc	x17, x17, x19
+        subs	x22, x2, x3
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x7, x6
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x11, x11, x21
+        eor	x20, x20, x19
+        adcs	x12, x12, x20
+        adcs	x13, x13, x19
+        adcs	x14, x14, x19
+        adcs	x15, x15, x19
+        adcs	x16, x16, x19
+        adc	x17, x17, x19
+        subs	x22, x3, x5
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x9, x7
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x14, x14, x21
+        eor	x20, x20, x19
+        adcs	x15, x15, x20
+        adcs	x16, x16, x19
+        adc	x17, x17, x19
+        subs	x22, x2, x4
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x8, x6
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x12, x12, x21
+        eor	x20, x20, x19
+        adcs	x13, x13, x20
+        adcs	x14, x14, x19
+        adcs	x15, x15, x19
+        adcs	x16, x16, x19
+        adc	x17, x17, x19
+        subs	x22, x2, x5
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x9, x6
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x13, x13, x21
+        eor	x20, x20, x19
+        adcs	x14, x14, x20
+        adcs	x15, x15, x19
+        adcs	x16, x16, x19
+        adc	x17, x17, x19
+        subs	x22, x3, x4
+        cneg	x22, x22, cc  // cc = lo, ul, last
+        csetm	x19, cc  // cc = lo, ul, last
+        subs	x20, x8, x7
+        cneg	x20, x20, cc  // cc = lo, ul, last
+        mul	x21, x22, x20
+        umulh	x20, x22, x20
+        cinv	x19, x19, cc  // cc = lo, ul, last
+        cmn	x19, #0x1
+        eor	x21, x21, x19
+        adcs	x13, x13, x21
+        eor	x20, x20, x19
+        adcs	x14, x14, x20
+        adcs	x15, x15, x19
+        adcs	x16, x16, x19
+        adc	x17, x17, x19
+        adds	x10, x10, x10
+        adcs	x11, x11, x11
+        adcs	x12, x12, x12
+        adcs	x13, x13, x13
+        adcs	x14, x14, x14
+        adcs	x15, x15, x15
+        adcs	x16, x16, x16
+        adcs	x17, x17, x17
+        adc	x19, xzr, xzr
+
+// Add it back to the buffer
+
+        ldp	x2, x3, [x0, #32]
+        adds	x10, x10, x2
+        adcs	x11, x11, x3
+        stp	x10, x11, [x0, #32]
+
+        ldp	x2, x3, [x0, #48]
+        adcs	x12, x12, x2
+        adcs	x13, x13, x3
+        stp	x12, x13, [x0, #48]
+
+        ldp	x2, x3, [x0, #64]
+        adcs	x14, x14, x2
+        adcs	x15, x15, x3
+        stp	x14, x15, [x0, #64]
+
+        ldp	x2, x3, [x0, #80]
+        adcs	x16, x16, x2
+        adcs	x17, x17, x3
+        stp	x16, x17, [x0, #80]
+
+        ldp	x2, x3, [x0, #96]
+        adcs	x2, x2, x19
+        adcs	x3, x3, xzr
+        stp	x2, x3, [x0, #96]
+
+        ldp	x2, x3, [x0, #112]
+        adcs	x2, x2, xzr
+        adc	x3, x3, xzr
+        stp	x2, x3, [x0, #112]
+
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack,"",%progbits
+#endif
