From 4ff910280c86874563515d29b076ac54c2f709f1 Mon Sep 17 00:00:00 2001
From: Nevine Ebeid <nebeid@amazon.com>
Date: Wed, 19 Jul 2023 11:51:30 -0400
Subject: [PATCH] Update build files in generated-src.

---
 .../fipsmodule/aesv8-gcm-armv8-unroll8.S      | 744 ++++++-------
 .../crypto/fipsmodule/aesv8-gcm-armv8.S       |  16 +-
 .../fipsmodule/aesv8-gcm-armv8-unroll8.S      | 744 ++++++-------
 .../crypto/fipsmodule/aesv8-gcm-armv8.S       |  16 +-
 .../crypto/fipsmodule/aesni-gcm-avx512.S      | 998 +++++++++---------
 .../crypto/fipsmodule/aesni-gcm-x86_64.S      |  16 +-
 .../crypto/fipsmodule/aesni-gcm-avx512.S      | 998 +++++++++---------
 .../crypto/fipsmodule/aesni-gcm-x86_64.S      |  16 +-
 .../fipsmodule/aesv8-gcm-armv8-unroll8.S      | 744 ++++++-------
 .../crypto/fipsmodule/aesv8-gcm-armv8.S       |  16 +-
 .../crypto/fipsmodule/aesni-gcm-x86_64.asm    |  16 +-
 11 files changed, 2168 insertions(+), 2156 deletions(-)

diff --git a/generated-src/ios-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S b/generated-src/ios-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S
index f8e26e44e..795b76637 100644
--- a/generated-src/ios-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S
+++ b/generated-src/ios-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S
@@ -34,7 +34,7 @@ _aesv8_gcm_8x_enc_128:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -70,7 +70,7 @@ _aesv8_gcm_8x_enc_128:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				  	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				  	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 6
 	add	v30.4s, v30.4s, v31.4s		//CTR block 6
@@ -96,7 +96,7 @@ _aesv8_gcm_8x_enc_128:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 0
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 1
@@ -139,7 +139,7 @@ _aesv8_gcm_8x_enc_128:
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 3
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 3
 	aese	v0.16b, v26.16b
@@ -180,7 +180,7 @@ _aesv8_gcm_8x_enc_128:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 5
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 5
@@ -214,7 +214,7 @@ _aesv8_gcm_8x_enc_128:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 6
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 7
@@ -251,7 +251,7 @@ _aesv8_gcm_8x_enc_128:
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 8
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 8
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	aese	v3.16b, v26.16b						//AES block 8k+11 - round 9
 	aese	v4.16b, v28.16b
@@ -319,29 +319,29 @@ _aesv8_gcm_8x_enc_128:
 
 L128_enc_main_loop:	//main	loop start
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 	rev64	v8.16b, v8.16b						//GHASH block 8k
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5 (t0, t1, t2 and t3 free)
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
 
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 
@@ -360,9 +360,9 @@ L128_enc_main_loop:	//main	loop start
 	pmull2	v9.1q, v11.2d, v20.2d				//GHASH block 8k+3 - high
 
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h3l | h3h
+	ldr	q25, [x6, #80]				//load h3l | h3h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 0
@@ -402,7 +402,7 @@ L128_enc_main_loop:	//main	loop start
 	trn1	v29.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 1
 	aese	v3.16b, v27.16b
@@ -447,19 +447,19 @@ L128_enc_main_loop:	//main	loop start
 
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 3
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4 (t0, t1, and t2 free)
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 3
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 3
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
@@ -502,7 +502,7 @@ L128_enc_main_loop:	//main	loop start
 
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 4
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	trn1	v13.2d, v15.2d, v14.2d				//GHASH block 8k+6, 8k+7 - mid
 
 	pmull2	v16.1q, v12.2d, v24.2d				//GHASH block 8k+4 - mid
@@ -568,7 +568,7 @@ L128_enc_main_loop:	//main	loop start
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 6
 
 .long	0xce153652	//eor3 v18.16b, v18.16b, v21.16b, v13.16b			//GHASH block 8k+6, 8k+7 - mid
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 .long	0xce0b3231	//eor3 v17.16b, v17.16b, v11.16b, v12.16b			//GHASH block 8k+6, 8k+7 - high
 
 	aese	v2.16b, v27.16b
@@ -609,7 +609,7 @@ L128_enc_main_loop:	//main	loop start
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 8
 .long	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	ext	v29.16b, v17.16b, v17.16b, #8				//MODULO - other top alignment
 	rev32	v23.16b, v30.16b					//CTR block 8k+18
@@ -673,21 +673,21 @@ L128_enc_main_loop:	//main	loop start
 
 L128_enc_prepretail:	//PREPRETAIL
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h6k | h5k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h6k | h5k
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
 
@@ -710,7 +710,7 @@ L128_enc_prepretail:	//PREPRETAIL
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
 
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
 	pmull2	v18.1q, v8.2d, v24.2d				//GHASH block 8k	- mid
@@ -759,12 +759,12 @@ L128_enc_prepretail:	//PREPRETAIL
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 1
 	pmull	v20.1q, v11.1d, v20.1d				//GHASH block 8k+3 - low
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
@@ -778,8 +778,8 @@ L128_enc_prepretail:	//PREPRETAIL
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
 
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 1
 
@@ -811,11 +811,11 @@ L128_enc_prepretail:	//PREPRETAIL
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 2
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 2
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	trn1	v16.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
 	aese	v0.16b, v26.16b
@@ -879,7 +879,7 @@ L128_enc_prepretail:	//PREPRETAIL
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 4
 
 	pmull2	v12.1q, v15.2d, v20.2d				//GHASH block 8k+7 - high
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	pmull	v20.1q, v15.1d, v20.1d				//GHASH block 8k+7 - low
 
 .long	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
@@ -929,7 +929,7 @@ L128_enc_prepretail:	//PREPRETAIL
 
 	pmull	v21.1q, v17.1d, v16.1d		 	//MODULO - top 64b align with mid
 .long	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 7
@@ -975,7 +975,7 @@ L128_enc_prepretail:	//PREPRETAIL
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 8
 
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 	aese	v6.16b, v26.16b						//AES block 8k+14 - round 9
 	aese	v2.16b, v26.16b						//AES block 8k+10 - round 9
 
@@ -993,16 +993,16 @@ L128_enc_tail:	//TAIL
 	ldr	q8, [x0], #16				//AES block 8k+8 - load plaintext
 
 	mov	v29.16b, v27.16b
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 .long	0xce007509	//eor3 v9.16b, v8.16b, v0.16b, v29.16b			//AES block 8k+8 - result
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 	cmp	x5, #112
 	b.gt	L128_enc_blocks_more_than_7
@@ -1057,7 +1057,7 @@ L128_enc_tail:	//TAIL
 	mov	v6.16b, v1.16b
 
 	cmp	x5, #32
-	ldr	q24, [x3, #96]					//load h4k | h3k
+	ldr	q24, [x6, #64]					//load h4k | h3k
 	b.gt	L128_enc_blocks_more_than_2
 
 	cmp	x5, #16
@@ -1066,7 +1066,7 @@ L128_enc_tail:	//TAIL
 	mov	v7.16b, v1.16b
 	b.gt	L128_enc_blocks_more_than_1
 
-	ldr	q21, [x3, #48]					//load h2k | h1k
+	ldr	q21, [x6, #16]					//load h2k | h1k
 	sub	v30.4s, v30.4s, v31.4s
 	b	L128_enc_blocks_less_than_1
 L128_enc_blocks_more_than_7:	//blocks	left >  7
@@ -1169,7 +1169,7 @@ L128_enc_blocks_more_than_3:	//blocks	left >  3
 
 	st1	{ v9.16b}, [x2], #16			  	//AES final-3 block - store result
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
@@ -1178,7 +1178,7 @@ L128_enc_blocks_more_than_3:	//blocks	left >  3
 	movi	v16.8b, #0						//supress further partial tag feed in
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	pmull	v26.1q, v8.1d, v25.1d				//GHASH final-3 block - low
 
 	ldr	q9, [x0], #16				//AES final-2 block - load plaintext
@@ -1206,7 +1206,7 @@ L128_enc_blocks_more_than_2:	//blocks	left >  2
 	ldr	q9, [x0], #16				//AES final-1 block - load plaintext
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-2 block - mid
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 	movi	v16.8b, #0						//supress further partial tag feed in
 
@@ -1226,7 +1226,7 @@ L128_enc_blocks_more_than_1:	//blocks	left >  1
 
 	st1	{ v9.16b}, [x2], #16			  	//AES final-1 block - store result
 
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 	ldr	q9, [x0], #16				//AES final block - load plaintext
@@ -1241,7 +1241,7 @@ L128_enc_blocks_more_than_1:	//blocks	left >  1
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
 
@@ -1262,16 +1262,16 @@ L128_enc_blocks_less_than_1:	//blocks	left <= 1
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
 	and	x1, x1, #127			 	//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 	cmp	x1, #64
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
 
 	mov	v0.d[1], x14
 	mov	v0.d[0], x13					//ctr0b is mask for last block
@@ -1288,7 +1288,7 @@ L128_enc_blocks_less_than_1:	//blocks	left <= 1
 	ins	v16.d[0], v8.d[1]					//GHASH final block - mid
 
 	eor	v16.8b, v16.8b, v8.8b				//GHASH final block - mid
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	pmull	v16.1q, v16.1d, v21.1d				//GHASH final block - mid
@@ -1339,7 +1339,7 @@ _aesv8_gcm_8x_dec_128:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -1350,7 +1350,7 @@ _aesv8_gcm_8x_dec_128:
 	mov	x5, x9
 	ld1	{ v0.16b}, [x16]					//CTR block 0
 
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	sub	x5, x5, #1		//byte_len - 1
 
 	mov	x15, #0x100000000				//set up counter increment
@@ -1414,7 +1414,7 @@ _aesv8_gcm_8x_dec_128:
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 1
@@ -1452,7 +1452,7 @@ _aesv8_gcm_8x_dec_128:
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 3
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 3
 
@@ -1487,7 +1487,7 @@ _aesv8_gcm_8x_dec_128:
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 4
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 5
 	aese	v3.16b, v28.16b
@@ -1538,7 +1538,7 @@ _aesv8_gcm_8x_dec_128:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 7
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 7
@@ -1573,7 +1573,7 @@ _aesv8_gcm_8x_dec_128:
 	aese	v1.16b, v26.16b						//AES block 1 - round 9
 	aese	v6.16b, v26.16b						//AES block 6 - round 9
 
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 	aese	v4.16b, v26.16b						//AES block 4 - round 9
 	aese	v3.16b, v26.16b						//AES block 3 - round 9
 
@@ -1626,9 +1626,9 @@ _aesv8_gcm_8x_dec_128:
 	b.ge	L128_dec_prepretail					//do prepretail
 
 L128_dec_main_loop:	//main	loop start
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
@@ -1636,9 +1636,9 @@ L128_dec_main_loop:	//main	loop start
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
@@ -1647,12 +1647,12 @@ L128_dec_main_loop:	//main	loop start
 
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	pmull2	v16.1q, v9.2d, v23.2d				//GHASH block 8k+1 - high
 	pmull2	v17.1q, v8.2d, v25.2d				//GHASH block 8k - high
@@ -1695,7 +1695,7 @@ L128_dec_main_loop:	//main	loop start
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
 .long	0xce1d2631	//eor3 v17.16b, v17.16b, v29.16b, v9.16b			//GHASH block 8k+2, 8k+3 - high
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	trn1	v29.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 1
@@ -1704,9 +1704,9 @@ L128_dec_main_loop:	//main	loop start
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	pmull2	v18.1q, v8.2d, v24.2d				//GHASH block 8k	- mid
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	pmull	v24.1q, v8.1d, v24.1d				//GHASH block 8k+1 - mid
 	aese	v6.16b, v27.16b
@@ -1734,9 +1734,9 @@ L128_dec_main_loop:	//main	loop start
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 2
 	eor	v10.16b, v10.16b, v29.16b				//GHASH block 8k+2, 8k+3 - mid
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v18.16b, v18.16b, v24.16b				//GHASH block 8k+1 - mid
@@ -1761,12 +1761,12 @@ L128_dec_main_loop:	//main	loop start
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 3
 	trn2	v12.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
@@ -1813,7 +1813,7 @@ L128_dec_main_loop:	//main	loop start
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 4
 	trn2	v14.2d, v15.2d, v14.2d				//GHASH block 8k+6, 8k+7 - mid
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	pmull2	v16.1q, v12.2d, v24.2d				//GHASH block 8k+4 - mid
@@ -1867,7 +1867,7 @@ L128_dec_main_loop:	//main	loop start
 .long	0xce0b3231	//eor3 v17.16b, v17.16b, v11.16b, v12.16b			//GHASH block 8k+6, 8k+7 - high
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	ldr	d16, [x10]			//MODULO - load modulo constant
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+6, 8k+7 - low
@@ -1931,7 +1931,7 @@ L128_dec_main_loop:	//main	loop start
 
 	aese	v0.16b, v26.16b						//AES block 8k+8 - round 9
 	aese	v1.16b, v26.16b						//AES block 8k+9 - round 9
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	aese	v6.16b, v26.16b						//AES block 8k+14 - round 9
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
@@ -1982,19 +1982,19 @@ L128_dec_prepretail:	//PREPRETAIL
 
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5
 
@@ -2002,8 +2002,8 @@ L128_dec_prepretail:	//PREPRETAIL
 
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
@@ -2063,13 +2063,13 @@ L128_dec_prepretail:	//PREPRETAIL
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 1
@@ -2082,9 +2082,9 @@ L128_dec_prepretail:	//PREPRETAIL
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 
@@ -2112,13 +2112,13 @@ L128_dec_prepretail:	//PREPRETAIL
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
 	trn2	v12.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 3
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	pmull2	v10.1q, v13.2d, v23.2d				//GHASH block 8k+5 - high
 	pmull	v23.1q, v13.1d, v23.1d				//GHASH block 8k+5 - low
 
@@ -2173,7 +2173,7 @@ L128_dec_prepretail:	//PREPRETAIL
 	pmull2	v13.1q, v14.2d, v21.2d				//GHASH block 8k+6 - mid
 	pmull	v21.1q, v14.1d, v21.1d				//GHASH block 8k+7 - mid
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 .long	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 5
@@ -2224,7 +2224,7 @@ L128_dec_prepretail:	//PREPRETAIL
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 7
 .long	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	pmull	v29.1q, v17.1d, v16.1d		 	//MODULO - top 64b align with mid
 	aese	v3.16b, v27.16b
@@ -2246,7 +2246,7 @@ L128_dec_prepretail:	//PREPRETAIL
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
 
 .long	0xce1d5652	//eor3 v18.16b, v18.16b, v29.16b, v21.16b			//MODULO - fold into mid
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 8
@@ -2290,15 +2290,15 @@ L128_dec_tail:	//TAIL
 
 	cmp	x5, #112
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 	ldr	q9, [x0], #16				//AES block 8k+8 - load ciphertext
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
 
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 
@@ -2354,7 +2354,7 @@ L128_dec_tail:	//TAIL
 	mov	v7.16b, v6.16b
 	cmp	x5, #32
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	mov	v6.16b, v1.16b
 	b.gt	L128_dec_blocks_more_than_2
 
@@ -2365,7 +2365,7 @@ L128_dec_tail:	//TAIL
 	b.gt	L128_dec_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	L128_dec_blocks_less_than_1
 L128_dec_blocks_more_than_7:	//blocks	left >  7
 	rev64	v8.16b, v9.16b						//GHASH final-7 block
@@ -2468,9 +2468,9 @@ L128_dec_blocks_more_than_3:	//blocks	left >  3
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-3 block - mid
 
@@ -2495,7 +2495,7 @@ L128_dec_blocks_more_than_2:	//blocks	left >  2
 	st1	{ v12.16b}, [x2], #16			 	//AES final-2 block - store result
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 	movi	v16.8b, #0						//supress further partial tag feed in
 
@@ -2520,7 +2520,7 @@ L128_dec_blocks_more_than_1:	//blocks	left >  1
 	st1	{ v12.16b}, [x2], #16			 	//AES final-1 block - store result
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
@@ -2534,7 +2534,7 @@ L128_dec_blocks_more_than_1:	//blocks	left >  1
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-1 block - high
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
 .long	0xce07752c	//eor3 v12.16b, v9.16b, v7.16b, v29.16b				//AES final block - result
@@ -2554,20 +2554,20 @@ L128_dec_blocks_less_than_1:	//blocks	left <= 1
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
 
 	mov	v0.d[1], x14
 	mov	v0.d[0], x13					//ctr0b is mask for last block
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
 
@@ -2636,7 +2636,7 @@ _aesv8_gcm_8x_enc_192:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -2672,7 +2672,7 @@ _aesv8_gcm_8x_enc_192:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	add	x5, x5, x0
 
@@ -2699,7 +2699,7 @@ _aesv8_gcm_8x_enc_192:
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 0
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 1
@@ -2739,7 +2739,7 @@ _aesv8_gcm_8x_enc_192:
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 3
 
@@ -2782,7 +2782,7 @@ _aesv8_gcm_8x_enc_192:
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 5
 
@@ -2820,7 +2820,7 @@ _aesv8_gcm_8x_enc_192:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 6
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 7
@@ -2869,7 +2869,7 @@ _aesv8_gcm_8x_enc_192:
 	ld1	{ v19.16b}, [x3]
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 9
@@ -2912,7 +2912,7 @@ _aesv8_gcm_8x_enc_192:
 
 	aese	v4.16b, v28.16b						//AES block 12 - round 11
 	aese	v7.16b, v28.16b						//AES block 15 - round 11
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	aese	v1.16b, v28.16b						//AES block 9 - round 11
 	aese	v5.16b, v28.16b						//AES block 13 - round 11
@@ -2965,21 +2965,21 @@ _aesv8_gcm_8x_enc_192:
 
 L192_enc_main_loop:	//main	loop start
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4 (t0, t1, and t2 free)
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 	rev64	v8.16b, v8.16b						//GHASH block 8k
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
@@ -3010,7 +3010,7 @@ L192_enc_main_loop:	//main	loop start
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	pmull2	v17.1q, v8.2d, v25.2d				//GHASH block 8k - high
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
@@ -3023,8 +3023,8 @@ L192_enc_main_loop:	//main	loop start
 	trn1	v18.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 1
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	pmull2	v29.1q, v10.2d, v22.2d				//GHASH block 8k+2 - high
 	pmull	v19.1q, v8.1d, v25.1d				//GHASH block 8k - low
@@ -3076,14 +3076,14 @@ L192_enc_main_loop:	//main	loop start
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 3
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	pmull2	v18.1q, v8.2d, v24.2d				//GHASH block 8k - mid
@@ -3129,12 +3129,12 @@ L192_enc_main_loop:	//main	loop start
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 5
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7 (t0, t1, t2 and t3 free)
@@ -3149,8 +3149,8 @@ L192_enc_main_loop:	//main	loop start
 
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 5
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 5
@@ -3197,7 +3197,7 @@ L192_enc_main_loop:	//main	loop start
 	eor	v14.16b, v14.16b, v13.16b				//GHASH block 8k+6, 8k+7 - mid
 
 	pmull2	v16.1q, v12.2d, v24.2d				//GHASH block 8k+4 - mid
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull	v24.1q, v12.1d, v24.1d				//GHASH block 8k+5 - mid
 
 	aese	v4.16b, v27.16b
@@ -3244,7 +3244,7 @@ L192_enc_main_loop:	//main	loop start
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 8
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 8
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+6, 8k+7 - low
 	rev32	v20.16b, v30.16b					//CTR block 8k+16
@@ -3282,7 +3282,7 @@ L192_enc_main_loop:	//main	loop start
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 10
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 10
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 	ext	v29.16b, v17.16b, v17.16b, #8				//MODULO - other top alignment
 
 	aese	v0.16b, v27.16b
@@ -3355,26 +3355,26 @@ L192_enc_main_loop:	//main	loop start
 
 L192_enc_prepretail:	//PREPRETAIL
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
@@ -3408,7 +3408,7 @@ L192_enc_prepretail:	//PREPRETAIL
 	trn2	v8.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	pmull	v23.1q, v9.1d, v23.1d				//GHASH block 8k+1 - low
 	eor	v17.16b, v17.16b, v16.16b				//GHASH block 8k+1 - high
@@ -3468,7 +3468,7 @@ L192_enc_prepretail:	//PREPRETAIL
 
 	eor	v10.16b, v10.16b, v29.16b				//GHASH block 8k+2, 8k+3 - mid
 	pmull	v24.1q, v8.1d, v24.1d				//GHASH block 8k+1 - mid
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 3
@@ -3482,17 +3482,17 @@ L192_enc_prepretail:	//PREPRETAIL
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 3
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 3
@@ -3526,14 +3526,14 @@ L192_enc_prepretail:	//PREPRETAIL
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7 (t0, t1, t2 and t3 free)
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 5
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	pmull2	v11.1q, v14.2d, v22.2d				//GHASH block 8k+6 - high
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
@@ -3605,7 +3605,7 @@ L192_enc_prepretail:	//PREPRETAIL
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 7
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 7
@@ -3647,7 +3647,7 @@ L192_enc_prepretail:	//PREPRETAIL
 
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 9
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 9
 
@@ -3665,7 +3665,7 @@ L192_enc_prepretail:	//PREPRETAIL
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 9
 
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 10
@@ -3702,18 +3702,18 @@ L192_enc_prepretail:	//PREPRETAIL
 
 L192_enc_tail:	//TAIL
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	sub	x5, x4, x0 	//main_end_input_ptr is number of bytes left to process
 
 	ldr	q8, [x0], #16				//AES block 8k+8 - l3ad plaintext
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	mov	v29.16b, v26.16b
 
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	cmp	x5, #112
@@ -3771,7 +3771,7 @@ L192_enc_tail:	//TAIL
 	mov	v6.16b, v1.16b
 	sub	v30.4s, v30.4s, v31.4s
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	cmp	x5, #32
 	b.gt	L192_enc_blocks_more_than_2
 
@@ -3782,7 +3782,7 @@ L192_enc_tail:	//TAIL
 	b.gt	L192_enc_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	L192_enc_blocks_less_than_1
 L192_enc_blocks_more_than_7:	//blocks	left >  7
 	st1	{ v9.16b}, [x2], #16			 	//AES final-7 block  - store result
@@ -3881,7 +3881,7 @@ L192_enc_blocks_more_than_4:	//blocks	left >  4
 .long	0xce047529	//eor3 v9.16b, v9.16b, v4.16b, v29.16b			//AES final-3 block - result
 L192_enc_blocks_more_than_3:	//blocks	left >  3
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	st1	{ v9.16b}, [x2], #16			 	//AES final-3 block - store result
 
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
@@ -3890,7 +3890,7 @@ L192_enc_blocks_more_than_3:	//blocks	left >  3
 	movi	v16.8b, #0						//supress further partial tag feed in
 
 	ldr	q9, [x0], #16				//AES final-2 block - load plaintext
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
@@ -3913,7 +3913,7 @@ L192_enc_blocks_more_than_2:	//blocks	left >  2
 	st1	{ v9.16b}, [x2], #16			 	//AES final-2 block - store result
 
 	rev64	v8.16b, v9.16b						//GHASH final-2 block
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
@@ -3936,7 +3936,7 @@ L192_enc_blocks_more_than_2:	//blocks	left >  2
 .long	0xce067529	//eor3 v9.16b, v9.16b, v6.16b, v29.16b			//AES final-1 block - result
 L192_enc_blocks_more_than_1:	//blocks	left >  1
 
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	st1	{ v9.16b}, [x2], #16			 	//AES final-1 block - store result
 
@@ -3952,7 +3952,7 @@ L192_enc_blocks_more_than_1:	//blocks	left >  1
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 
 	ldr	q9, [x0], #16				//AES final block - load plaintext
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
 
@@ -3965,7 +3965,7 @@ L192_enc_blocks_more_than_1:	//blocks	left >  1
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-1 block - high
 L192_enc_blocks_less_than_1:	//blocks	left <= 1
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
 	sub	x1, x1, #128				//bit_length -= 128
@@ -3974,15 +3974,15 @@ L192_enc_blocks_less_than_1:	//blocks	left <= 1
 
 	and	x1, x1, #127				//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
 
 	mov	v0.d[1], x14
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
@@ -4052,7 +4052,7 @@ _aesv8_gcm_8x_dec_192:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -4086,7 +4086,7 @@ _aesv8_gcm_8x_dec_192:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 6
 	add	v30.4s, v30.4s, v31.4s		//CTR block 6
@@ -4111,7 +4111,7 @@ _aesv8_gcm_8x_dec_192:
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 0
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 1
@@ -4155,7 +4155,7 @@ _aesv8_gcm_8x_dec_192:
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 3
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 3
 	aese	v5.16b, v26.16b
@@ -4196,7 +4196,7 @@ _aesv8_gcm_8x_dec_192:
 
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 5
@@ -4232,7 +4232,7 @@ _aesv8_gcm_8x_dec_192:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 6
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	add	v30.4s, v30.4s, v31.4s		//CTR block 7
 
@@ -4283,7 +4283,7 @@ _aesv8_gcm_8x_dec_192:
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 9
@@ -4323,7 +4323,7 @@ _aesv8_gcm_8x_dec_192:
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 10
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 10
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	aese	v0.16b, v28.16b						//AES block 0 - round 11
 	aese	v1.16b, v28.16b						//AES block 1 - round 11
@@ -4380,16 +4380,16 @@ _aesv8_gcm_8x_dec_192:
 
 L192_dec_main_loop:	//main	loop start
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
@@ -4422,14 +4422,14 @@ L192_dec_main_loop:	//main	loop start
 
 	pmull	v19.1q, v8.1d, v25.1d				//GHASH block 8k - low
 	pmull2	v16.1q, v9.2d, v23.2d				//GHASH block 8k+1 - high
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
 	pmull	v23.1q, v9.1d, v23.1d				//GHASH block 8k+1 - low
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	aese	v0.16b, v27.16b
@@ -4452,8 +4452,8 @@ L192_dec_main_loop:	//main	loop start
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	trn2	v8.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 
 	eor	v17.16b, v17.16b, v16.16b				//GHASH block 8k+1 - high
@@ -4484,9 +4484,9 @@ L192_dec_main_loop:	//main	loop start
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 2
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 2
@@ -4506,7 +4506,7 @@ L192_dec_main_loop:	//main	loop start
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 3
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	eor	v10.16b, v10.16b, v29.16b				//GHASH block 8k+2, 8k+3 - mid
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
@@ -4545,16 +4545,16 @@ L192_dec_main_loop:	//main	loop start
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 4
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 5
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
@@ -4576,8 +4576,8 @@ L192_dec_main_loop:	//main	loop start
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	pmull2	v10.1q, v13.2d, v23.2d				//GHASH block 8k+5 - high
 	pmull	v23.1q, v13.1d, v23.1d				//GHASH block 8k+5 - low
 
@@ -4617,7 +4617,7 @@ L192_dec_main_loop:	//main	loop start
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 7
 
@@ -4675,7 +4675,7 @@ L192_dec_main_loop:	//main	loop start
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 9
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 9
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 .long	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
 	ldp	q8, q9, [x0], #32			//AES block 8k+8, 8k+9 - load ciphertext
@@ -4710,7 +4710,7 @@ L192_dec_main_loop:	//main	loop start
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 10
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 10
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	ldp	q14, q15, [x0], #32			//AES block 8k+14, 8k+15 - load ciphertext
 	aese	v4.16b, v27.16b
@@ -4768,13 +4768,13 @@ L192_dec_main_loop:	//main	loop start
 	b.lt	L192_dec_main_loop
 
 L192_dec_prepretail:	//PREPRETAIL
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
@@ -4787,9 +4787,9 @@ L192_dec_prepretail:	//PREPRETAIL
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 
@@ -4816,7 +4816,7 @@ L192_dec_prepretail:	//PREPRETAIL
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 1
@@ -4845,8 +4845,8 @@ L192_dec_prepretail:	//PREPRETAIL
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 2
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
@@ -4892,20 +4892,20 @@ L192_dec_prepretail:	//PREPRETAIL
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 3
 
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 3
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 	pmull	v21.1q, v10.1d, v21.1d				//GHASH block 8k+3 - mid
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	eor	v18.16b, v18.16b, v24.16b				//GHASH block 8k+1 - mid
 	aese	v2.16b, v26.16b
@@ -4938,8 +4938,8 @@ L192_dec_prepretail:	//PREPRETAIL
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 4
 
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	trn1	v16.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
 
 	aese	v7.16b, v28.16b
@@ -4949,7 +4949,7 @@ L192_dec_prepretail:	//PREPRETAIL
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 5
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 5
 	aese	v5.16b, v28.16b
@@ -5009,7 +5009,7 @@ L192_dec_prepretail:	//PREPRETAIL
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull	v21.1q, v14.1d, v21.1d				//GHASH block 8k+7 - mid
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
@@ -5056,7 +5056,7 @@ L192_dec_prepretail:	//PREPRETAIL
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 8
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 8
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 .long	0xce1d5652	//eor3 v18.16b, v18.16b, v29.16b, v21.16b			//MODULO - fold into mid
 	aese	v7.16b, v26.16b
@@ -5079,7 +5079,7 @@ L192_dec_prepretail:	//PREPRETAIL
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 9
 
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 	ext	v21.16b, v18.16b, v18.16b, #8			 	//MODULO - other mid alignment
 
 	aese	v2.16b, v27.16b
@@ -5118,16 +5118,16 @@ L192_dec_tail:	//TAIL
 
 	sub	x5, x4, x0 	//main_end_input_ptr is number of bytes left to process
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	ldr	q9, [x0], #16				//AES block 8k+8 - load ciphertext
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	mov	v29.16b, v26.16b
 
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
@@ -5186,7 +5186,7 @@ L192_dec_tail:	//TAIL
 	cmp	x5, #32
 
 	mov	v6.16b, v1.16b
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	b.gt	L192_dec_blocks_more_than_2
 
 	sub	v30.4s, v30.4s, v31.4s
@@ -5196,7 +5196,7 @@ L192_dec_tail:	//TAIL
 	b.gt	L192_dec_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	L192_dec_blocks_less_than_1
 L192_dec_blocks_more_than_7:	//blocks	left >  7
 	rev64	v8.16b, v9.16b						//GHASH final-7 block
@@ -5290,7 +5290,7 @@ L192_dec_blocks_more_than_4:	//blocks	left >  4
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-4 block - high
 L192_dec_blocks_more_than_3:	//blocks	left >  3
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
 	ldr	q9, [x0], #16				//AES final-2 block - load ciphertext
@@ -5309,7 +5309,7 @@ L192_dec_blocks_more_than_3:	//blocks	left >  3
 .long	0xce05752c	//eor3 v12.16b, v9.16b, v5.16b, v29.16b				//AES final-2 block - result
 
 	eor	v19.16b, v19.16b, v26.16b					//GHASH final-3 block - low
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-3 block - mid
 
@@ -5319,7 +5319,7 @@ L192_dec_blocks_more_than_3:	//blocks	left >  3
 L192_dec_blocks_more_than_2:	//blocks	left >  2
 
 	rev64	v8.16b, v9.16b						//GHASH final-2 block
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
@@ -5346,12 +5346,12 @@ L192_dec_blocks_more_than_1:	//blocks	left >  1
 
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 	ldr	q9, [x0], #16				//AES final block - load ciphertext
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
 	movi	v16.8b, #0						//supress further partial tag feed in
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	pmull	v26.1q, v8.1d, v22.1d				//GHASH final-1 block - low
 	ins	v27.d[0], v8.d[1]					//GHASH final-1 block - mid
@@ -5380,17 +5380,17 @@ L192_dec_blocks_less_than_1:	//blocks	left <= 1
 	str	q30, [x16]					//store the updated counter
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 
 	and	x1, x1, #127				//bit_length %= 128
 
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	mov	v0.d[1], x14
@@ -5458,7 +5458,7 @@ _aesv8_gcm_8x_enc_256:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -5497,7 +5497,7 @@ _aesv8_gcm_8x_enc_256:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 6
 	add	v30.4s, v30.4s, v31.4s		//CTR block 6
@@ -5522,7 +5522,7 @@ _aesv8_gcm_8x_enc_256:
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 0
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 1
@@ -5567,7 +5567,7 @@ _aesv8_gcm_8x_enc_256:
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 3
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 3
@@ -5607,7 +5607,7 @@ _aesv8_gcm_8x_enc_256:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 5
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 5
@@ -5641,7 +5641,7 @@ _aesv8_gcm_8x_enc_256:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 6
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 7
@@ -5685,7 +5685,7 @@ _aesv8_gcm_8x_enc_256:
 	ld1	{ v19.16b}, [x3]
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 9
@@ -5727,7 +5727,7 @@ _aesv8_gcm_8x_enc_256:
 
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 11
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 11
 
@@ -5746,7 +5746,7 @@ _aesv8_gcm_8x_enc_256:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 11
 
 	add	v30.4s, v30.4s, v31.4s		//CTR block 7
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 12
@@ -5824,17 +5824,17 @@ _aesv8_gcm_8x_enc_256:
 	b.ge	L256_enc_prepretail					//do prepretail
 
 L256_enc_main_loop:	//main	loop start
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
@@ -5844,9 +5844,9 @@ L256_enc_main_loop:	//main	loop start
 
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	aese	v3.16b, v26.16b
@@ -5869,7 +5869,7 @@ L256_enc_main_loop:	//main	loop start
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
@@ -5931,7 +5931,7 @@ L256_enc_main_loop:	//main	loop start
 
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 
 	aese	v2.16b, v26.16b
@@ -5947,9 +5947,9 @@ L256_enc_main_loop:	//main	loop start
 
 	pmull	v20.1q, v11.1d, v20.1d				//GHASH block 8k+3 - low
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	trn1	v16.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
@@ -5978,7 +5978,7 @@ L256_enc_main_loop:	//main	loop start
 
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
@@ -6028,21 +6028,21 @@ L256_enc_main_loop:	//main	loop start
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 6
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 7
 	aese	v3.16b, v27.16b
@@ -6105,7 +6105,7 @@ L256_enc_main_loop:	//main	loop start
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 9
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 9
 	aese	v3.16b, v26.16b
@@ -6148,7 +6148,7 @@ L256_enc_main_loop:	//main	loop start
 
 .long	0xce0b3231	//eor3 v17.16b, v17.16b, v11.16b, v12.16b			//GHASH block 8k+6, 8k+7 - high
 
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 	rev32	v20.16b, v30.16b					//CTR block 8k+16
 
 	ext	v21.16b, v17.16b, v17.16b, #8			 	//MODULO - other top alignment
@@ -6189,7 +6189,7 @@ L256_enc_main_loop:	//main	loop start
 
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 12
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 	aese	v7.16b, v27.16b						//AES block 8k+15 - round 13
 
 	ldp	q10, q11, [x0], #32			//AES block 8k+10, 8k+11 - load plaintext
@@ -6254,7 +6254,7 @@ L256_enc_main_loop:	//main	loop start
 
 L256_enc_prepretail:	//PREPRETAIL
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
@@ -6263,8 +6263,8 @@ L256_enc_prepretail:	//PREPRETAIL
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 
@@ -6293,20 +6293,20 @@ L256_enc_prepretail:	//PREPRETAIL
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 1
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 1
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 1
 
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
@@ -6342,7 +6342,7 @@ L256_enc_prepretail:	//PREPRETAIL
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	trn1	v18.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 	pmull2	v17.1q, v8.2d, v25.2d				//GHASH block 8k - high
 
@@ -6420,11 +6420,11 @@ L256_enc_prepretail:	//PREPRETAIL
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 5
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 	pmull	v21.1q, v10.1d, v21.1d				//GHASH block 8k+3 - mid
@@ -6450,8 +6450,8 @@ L256_enc_prepretail:	//PREPRETAIL
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 6
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 6
 	aese	v3.16b, v26.16b
@@ -6466,12 +6466,12 @@ L256_enc_prepretail:	//PREPRETAIL
 
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 7
 	aese	v4.16b, v27.16b
@@ -6534,7 +6534,7 @@ L256_enc_prepretail:	//PREPRETAIL
 .long	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
 .long	0xce082a31	//eor3 v17.16b, v17.16b, v8.16b, v10.16b			//GHASH block 8k+4, 8k+5 - high
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 9
 	aese	v0.16b, v26.16b
@@ -6584,7 +6584,7 @@ L256_enc_prepretail:	//PREPRETAIL
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 11
 
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 	ext	v21.16b, v17.16b, v17.16b, #8			 	//MODULO - other top alignment
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 11
@@ -6605,7 +6605,7 @@ L256_enc_prepretail:	//PREPRETAIL
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 11
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 12
@@ -6642,17 +6642,17 @@ L256_enc_prepretail:	//PREPRETAIL
 	aese	v6.16b, v27.16b						//AES block 8k+14 - round 13
 L256_enc_tail:	//TAIL
 
-	ldp	q24, q25, [x3, #192]			//load h8l | h8h
+	ldp	q24, q25, [x6, #160]			//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	sub	x5, x4, x0		//main_end_input_ptr is number of bytes left to process
 
 	ldr	q8, [x0], #16				//AES block 8k+8 - load plaintext
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	mov	v29.16b, v28.16b
@@ -6708,7 +6708,7 @@ L256_enc_tail:	//TAIL
 
 	cmp	x5, #32
 	mov	v7.16b, v6.16b
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	mov	v6.16b, v1.16b
 	sub	v30.4s, v30.4s, v31.4s
@@ -6721,7 +6721,7 @@ L256_enc_tail:	//TAIL
 	b.gt	L256_enc_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	L256_enc_blocks_less_than_1
 L256_enc_blocks_more_than_7:	//blocks	left >  7
 	st1	{ v9.16b}, [x2], #16				//AES final-7 block  - store result
@@ -6823,7 +6823,7 @@ L256_enc_blocks_more_than_3:	//blocks	left >  3
 
 	st1	{ v9.16b}, [x2], #16				//AES final-3 block - store result
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
 
@@ -6834,7 +6834,7 @@ L256_enc_blocks_more_than_3:	//blocks	left >  3
 
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-3 block - high
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-3 block - mid
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-3 block - mid
 	ldr	q9, [x0], #16				//AES final-2 block - load plaintext
@@ -6849,7 +6849,7 @@ L256_enc_blocks_more_than_3:	//blocks	left >  3
 	eor	v19.16b, v19.16b, v26.16b					//GHASH final-3 block - low
 L256_enc_blocks_more_than_2:	//blocks	left >  2
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 
 	st1	{ v9.16b}, [x2], #16			 	//AES final-2 block - store result
@@ -6879,7 +6879,7 @@ L256_enc_blocks_more_than_1:	//blocks	left >  1
 
 	st1	{ v9.16b}, [x2], #16				//AES final-1 block - store result
 
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 	ldr	q9, [x0], #16				//AES final block - load plaintext
@@ -6896,7 +6896,7 @@ L256_enc_blocks_more_than_1:	//blocks	left >  1
 	pmull	v26.1q, v8.1d, v22.1d				//GHASH final-1 block - low
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	eor	v19.16b, v19.16b, v26.16b					//GHASH final-1 block - low
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
@@ -6912,18 +6912,18 @@ L256_enc_blocks_less_than_1:	//blocks	left <= 1
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x14, x6, xzr, lt
-	csel	x13, x7, x6, lt
+	csel	x14, x7, xzr, lt
+	csel	x13, x8, x7, lt
 
 	mov	v0.d[0], x13					//ctr0b is mask for last block
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
@@ -6990,7 +6990,7 @@ _aesv8_gcm_8x_dec_256:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -7016,7 +7016,7 @@ _aesv8_gcm_8x_dec_256:
 
 	rev32	v2.16b, v30.16b				//CTR block 2
 	add	v30.4s, v30.4s, v31.4s		//CTR block 2
-	ldp	q26, q27, [x8, #0]				  	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				  	//load rk0, rk1
 
 	rev32	v3.16b, v30.16b				//CTR block 3
 	add	v30.4s, v30.4s, v31.4s		//CTR block 3
@@ -7051,7 +7051,7 @@ _aesv8_gcm_8x_dec_256:
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 0
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b		        //AES block 7 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b		        //AES block 6 - round 1
@@ -7090,7 +7090,7 @@ _aesv8_gcm_8x_dec_256:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 2
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 2
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 3
@@ -7136,7 +7136,7 @@ _aesv8_gcm_8x_dec_256:
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 5
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 5
 	aese	v7.16b, v28.16b
@@ -7171,7 +7171,7 @@ _aesv8_gcm_8x_dec_256:
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 6
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 7
@@ -7218,7 +7218,7 @@ _aesv8_gcm_8x_dec_256:
 	ld1	{ v19.16b}, [x3]
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	add	x4, x0, x1, lsr #3 //end_input_ptr
 	add	x5, x5, x0
 
@@ -7258,7 +7258,7 @@ _aesv8_gcm_8x_dec_256:
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 10
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 10
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 11
@@ -7280,7 +7280,7 @@ _aesv8_gcm_8x_dec_256:
 
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 11
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 12
@@ -7358,13 +7358,13 @@ _aesv8_gcm_8x_dec_256:
 
 L256_dec_main_loop:	//main	loop start
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
@@ -7396,12 +7396,12 @@ L256_dec_main_loop:	//main	loop start
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 0
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	eor	v8.16b, v8.16b, v19.16b					//PRE 1
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
@@ -7445,7 +7445,7 @@ L256_dec_main_loop:	//main	loop start
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	pmull2	v29.1q, v10.2d, v22.2d				//GHASH block 8k+2 - high
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
@@ -7493,12 +7493,12 @@ L256_dec_main_loop:	//main	loop start
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 4
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
 	pmull	v22.1q, v10.1d, v22.1d				//GHASH block 8k+2 - low
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
@@ -7554,9 +7554,9 @@ L256_dec_main_loop:	//main	loop start
 	pmull	v21.1q, v10.1d, v21.1d				//GHASH block 8k+3 - mid
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
 	eor	v18.16b, v18.16b, v24.16b				//GHASH block 8k+1 - mid
@@ -7565,11 +7565,11 @@ L256_dec_main_loop:	//main	loop start
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 	aese	v7.16b, v27.16b
@@ -7582,8 +7582,8 @@ L256_dec_main_loop:	//main	loop start
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 7
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 7
 	aese	v4.16b, v27.16b
@@ -7617,7 +7617,7 @@ L256_dec_main_loop:	//main	loop start
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 8
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	pmull	v22.1q, v14.1d, v22.1d				//GHASH block 8k+6 - low
 	trn2	v14.2d, v15.2d, v14.2d				//GHASH block 8k+6, 8k+7 - mid
 
@@ -7684,7 +7684,7 @@ L256_dec_main_loop:	//main	loop start
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+16
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 11
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 11
@@ -7724,7 +7724,7 @@ L256_dec_main_loop:	//main	loop start
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 11
 
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 12
 	aese	v3.16b, v26.16b
@@ -7784,30 +7784,30 @@ L256_dec_main_loop:	//main	loop start
 	b.lt	L256_dec_main_loop
 
 L256_dec_prepretail:	//PREPRETAIL
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	aese	v0.16b, v26.16b
@@ -7831,7 +7831,7 @@ L256_dec_prepretail:	//PREPRETAIL
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
 	eor	v8.16b, v8.16b, v19.16b					//PRE 1
@@ -7891,7 +7891,7 @@ L256_dec_prepretail:	//PREPRETAIL
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 3
 	pmull2	v9.1q, v11.2d, v20.2d				//GHASH block 8k+3 - high
@@ -7922,9 +7922,9 @@ L256_dec_prepretail:	//PREPRETAIL
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 4
 
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 4
@@ -7964,11 +7964,11 @@ L256_dec_prepretail:	//PREPRETAIL
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5
@@ -7979,8 +7979,8 @@ L256_dec_prepretail:	//PREPRETAIL
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 6
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 6
 
@@ -8010,7 +8010,7 @@ L256_dec_prepretail:	//PREPRETAIL
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 6
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull	v22.1q, v14.1d, v22.1d				//GHASH block 8k+6 - low
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
@@ -8073,7 +8073,7 @@ L256_dec_prepretail:	//PREPRETAIL
 	pmull	v21.1q, v14.1d, v21.1d				//GHASH block 8k+7 - mid
 	pmull	v20.1q, v15.1d, v20.1d				//GHASH block 8k+7 - low
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 .long	0xce195e73	//eor3 v19.16b, v19.16b, v25.16b, v23.16b			//GHASH block 8k+4, 8k+5 - low
 .long	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
 
@@ -8110,7 +8110,7 @@ L256_dec_prepretail:	//PREPRETAIL
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 10
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 10
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 
 	ext	v21.16b, v17.16b, v17.16b, #8				//MODULO - other top alignment
 
@@ -8153,7 +8153,7 @@ L256_dec_prepretail:	//PREPRETAIL
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 12
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b	        	//AES block 8k+9 - round 12
 
@@ -8180,15 +8180,15 @@ L256_dec_tail:	//TAIL
 
 	ldr	q9, [x0], #16				//AES block 8k+8 - load ciphertext
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 	mov	v29.16b, v28.16b
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 .long	0xce00752c	//eor3 v12.16b, v9.16b, v0.16b, v29.16b				//AES block 8k+8 - result
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	b.gt	L256_dec_blocks_more_than_7
@@ -8238,7 +8238,7 @@ L256_dec_tail:	//TAIL
 	mov	v5.16b, v1.16b
 	b.gt	L256_dec_blocks_more_than_3
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	sub	v30.4s, v30.4s, v31.4s
 	mov	v7.16b, v6.16b
 
@@ -8253,7 +8253,7 @@ L256_dec_tail:	//TAIL
 	b.gt	L256_dec_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	L256_dec_blocks_less_than_1
 L256_dec_blocks_more_than_7:	//blocks	left >  7
 	rev64	v8.16b, v9.16b						//GHASH final-7 block
@@ -8348,13 +8348,13 @@ L256_dec_blocks_more_than_4:	//blocks	left >  4
 .long	0xce04752c	//eor3 v12.16b, v9.16b, v4.16b, v29.16b				//AES final-3 block - result
 L256_dec_blocks_more_than_3:	//blocks	left >  3
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
 	ldr	q9, [x0], #16				//AES final-2 block - load ciphertext
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
 	st1	{ v12.16b}, [x2], #16			 	//AES final-3 block - store result
@@ -8378,7 +8378,7 @@ L256_dec_blocks_more_than_2:	//blocks	left >  2
 
 	rev64	v8.16b, v9.16b						//GHASH final-2 block
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 	ldr	q9, [x0], #16				//AES final-1 block - load ciphertext
 
@@ -8406,14 +8406,14 @@ L256_dec_blocks_more_than_1:	//blocks	left >  1
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-1 block - mid
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 	ldr	q9, [x0], #16				//AES final block - load ciphertext
 	st1	{ v12.16b}, [x2], #16			 	//AES final-1 block - store result
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	pmull	v26.1q, v8.1d, v22.1d				//GHASH final-1 block - low
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
@@ -8432,7 +8432,7 @@ L256_dec_blocks_more_than_1:	//blocks	left >  1
 L256_dec_blocks_less_than_1:	//blocks	left <= 1
 
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
 	sub	x1, x1, #128				//bit_length -= 128
@@ -8443,18 +8443,18 @@ L256_dec_blocks_less_than_1:	//blocks	left <= 1
 
 	and	x1, x1, #127			 	//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x14, x6, xzr, lt
-	csel	x13, x7, x6, lt
+	csel	x14, x7, xzr, lt
+	csel	x13, x8, x7, lt
 
 	mov	v0.d[0], x13					//ctr0b is mask for last block
 	mov	v0.d[1], x14
 
 	and	v9.16b, v9.16b, v0.16b					//possibly partial last block has zeroes in highest bits
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	bif	v12.16b, v26.16b, v0.16b					//insert existing bytes in top end of result before storing
 
diff --git a/generated-src/ios-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S b/generated-src/ios-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S
index 50305ceb7..f56f032ad 100644
--- a/generated-src/ios-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S
+++ b/generated-src/ios-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S
@@ -90,7 +90,7 @@ _aes_gcm_enc_kernel:
 	ldr	q23, [x8, #80]                                 // load rk5
 	aese	v1.16b, v19.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 1
-	ldr	q14, [x3, #80]                         // load h3l | h3h
+	ldr	q14, [x6, #48]                              // load h3l | h3h
 	ext	v14.16b, v14.16b, v14.16b, #8
 	aese	v3.16b, v18.16b
 	aesmc	v3.16b, v3.16b          // AES block 3 - round 0
@@ -99,14 +99,14 @@ _aes_gcm_enc_kernel:
 	ldr	q22, [x8, #64]                                 // load rk4
 	aese	v1.16b, v20.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 2
-	ldr	q13, [x3, #64]                         // load h2l | h2h
+	ldr	q13, [x6, #32]                              // load h2l | h2h
 	ext	v13.16b, v13.16b, v13.16b, #8
 	aese	v3.16b, v19.16b
 	aesmc	v3.16b, v3.16b          // AES block 3 - round 1
 	ldr	q30, [x8, #192]                               // load rk12
 	aese	v2.16b, v20.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 2
-	ldr	q15, [x3, #112]                        // load h4l | h4h
+	ldr	q15, [x6, #80]                              // load h4l | h4h
 	ext	v15.16b, v15.16b, v15.16b, #8
 	aese	v1.16b, v21.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 3
@@ -149,7 +149,7 @@ _aes_gcm_enc_kernel:
 	ldr	q27, [x8, #144]                                // load rk9
 	aese	v0.16b, v24.16b
 	aesmc	v0.16b, v0.16b          // AES block 0 - round 6
-	ldr	q12, [x3, #32]                         // load h1l | h1h
+	ldr	q12, [x6]                                   // load h1l | h1h
 	ext	v12.16b, v12.16b, v12.16b, #8
 	aese	v2.16b, v24.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 6
@@ -850,15 +850,15 @@ _aes_gcm_dec_kernel:
 	ldr	q19, [x8, #16]                                 // load rk1
 	aese	v0.16b, v18.16b
 	aesmc	v0.16b, v0.16b          // AES block 0 - round 0
-	ldr	q14, [x3, #80]                         // load h3l | h3h
+	ldr	q14, [x6, #48]                              // load h3l | h3h
 	ext	v14.16b, v14.16b, v14.16b, #8
 	aese	v3.16b, v18.16b
 	aesmc	v3.16b, v3.16b          // AES block 3 - round 0
-	ldr	q15, [x3, #112]                        // load h4l | h4h
+	ldr	q15, [x6, #80]                              // load h4l | h4h
 	ext	v15.16b, v15.16b, v15.16b, #8
 	aese	v1.16b, v18.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 0
-	ldr	q13, [x3, #64]                         // load h2l | h2h
+	ldr	q13, [x6, #32]                              // load h2l | h2h
 	ext	v13.16b, v13.16b, v13.16b, #8
 	aese	v2.16b, v18.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 0
@@ -878,7 +878,7 @@ _aes_gcm_dec_kernel:
 	ldr	q30, [x8, #192]                               // load rk12
 	aese	v0.16b, v20.16b
 	aesmc	v0.16b, v0.16b          // AES block 0 - round 2
-	ldr	q12, [x3, #32]                         // load h1l | h1h
+	ldr	q12, [x6]                                   // load h1l | h1h
 	ext	v12.16b, v12.16b, v12.16b, #8
 	aese	v2.16b, v20.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 2
diff --git a/generated-src/linux-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S b/generated-src/linux-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S
index c44988d6e..3d3e40380 100644
--- a/generated-src/linux-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S
+++ b/generated-src/linux-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S
@@ -34,7 +34,7 @@ aesv8_gcm_8x_enc_128:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -70,7 +70,7 @@ aesv8_gcm_8x_enc_128:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				  	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				  	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 6
 	add	v30.4s, v30.4s, v31.4s		//CTR block 6
@@ -96,7 +96,7 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 0
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 1
@@ -139,7 +139,7 @@ aesv8_gcm_8x_enc_128:
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 3
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 3
 	aese	v0.16b, v26.16b
@@ -180,7 +180,7 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 5
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 5
@@ -214,7 +214,7 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 6
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 7
@@ -251,7 +251,7 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 8
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 8
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	aese	v3.16b, v26.16b						//AES block 8k+11 - round 9
 	aese	v4.16b, v28.16b
@@ -319,29 +319,29 @@ aesv8_gcm_8x_enc_128:
 
 .L128_enc_main_loop:	//main	loop start
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 	rev64	v8.16b, v8.16b						//GHASH block 8k
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5 (t0, t1, t2 and t3 free)
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
 
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 
@@ -360,9 +360,9 @@ aesv8_gcm_8x_enc_128:
 	pmull2	v9.1q, v11.2d, v20.2d				//GHASH block 8k+3 - high
 
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h3l | h3h
+	ldr	q25, [x6, #80]				//load h3l | h3h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 0
@@ -402,7 +402,7 @@ aesv8_gcm_8x_enc_128:
 	trn1	v29.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 1
 	aese	v3.16b, v27.16b
@@ -447,19 +447,19 @@ aesv8_gcm_8x_enc_128:
 
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 3
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4 (t0, t1, and t2 free)
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 3
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 3
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
@@ -502,7 +502,7 @@ aesv8_gcm_8x_enc_128:
 
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 4
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	trn1	v13.2d, v15.2d, v14.2d				//GHASH block 8k+6, 8k+7 - mid
 
 	pmull2	v16.1q, v12.2d, v24.2d				//GHASH block 8k+4 - mid
@@ -568,7 +568,7 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 6
 
 .inst	0xce153652	//eor3 v18.16b, v18.16b, v21.16b, v13.16b			//GHASH block 8k+6, 8k+7 - mid
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 .inst	0xce0b3231	//eor3 v17.16b, v17.16b, v11.16b, v12.16b			//GHASH block 8k+6, 8k+7 - high
 
 	aese	v2.16b, v27.16b
@@ -609,7 +609,7 @@ aesv8_gcm_8x_enc_128:
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 8
 .inst	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	ext	v29.16b, v17.16b, v17.16b, #8				//MODULO - other top alignment
 	rev32	v23.16b, v30.16b					//CTR block 8k+18
@@ -673,21 +673,21 @@ aesv8_gcm_8x_enc_128:
 
 .L128_enc_prepretail:	//PREPRETAIL
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h6k | h5k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h6k | h5k
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
 
@@ -710,7 +710,7 @@ aesv8_gcm_8x_enc_128:
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
 
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
 	pmull2	v18.1q, v8.2d, v24.2d				//GHASH block 8k	- mid
@@ -759,12 +759,12 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 1
 	pmull	v20.1q, v11.1d, v20.1d				//GHASH block 8k+3 - low
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
@@ -778,8 +778,8 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
 
 .inst	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 1
 
@@ -811,11 +811,11 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 2
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 2
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	trn1	v16.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
 	aese	v0.16b, v26.16b
@@ -879,7 +879,7 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 4
 
 	pmull2	v12.1q, v15.2d, v20.2d				//GHASH block 8k+7 - high
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	pmull	v20.1q, v15.1d, v20.1d				//GHASH block 8k+7 - low
 
 .inst	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
@@ -929,7 +929,7 @@ aesv8_gcm_8x_enc_128:
 
 	pmull	v21.1q, v17.1d, v16.1d		 	//MODULO - top 64b align with mid
 .inst	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 7
@@ -975,7 +975,7 @@ aesv8_gcm_8x_enc_128:
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 8
 
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 	aese	v6.16b, v26.16b						//AES block 8k+14 - round 9
 	aese	v2.16b, v26.16b						//AES block 8k+10 - round 9
 
@@ -993,16 +993,16 @@ aesv8_gcm_8x_enc_128:
 	ldr	q8, [x0], #16				//AES block 8k+8 - load plaintext
 
 	mov	v29.16b, v27.16b
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 .inst	0xce007509	//eor3 v9.16b, v8.16b, v0.16b, v29.16b			//AES block 8k+8 - result
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 	cmp	x5, #112
 	b.gt	.L128_enc_blocks_more_than_7
@@ -1057,7 +1057,7 @@ aesv8_gcm_8x_enc_128:
 	mov	v6.16b, v1.16b
 
 	cmp	x5, #32
-	ldr	q24, [x3, #96]					//load h4k | h3k
+	ldr	q24, [x6, #64]					//load h4k | h3k
 	b.gt	.L128_enc_blocks_more_than_2
 
 	cmp	x5, #16
@@ -1066,7 +1066,7 @@ aesv8_gcm_8x_enc_128:
 	mov	v7.16b, v1.16b
 	b.gt	.L128_enc_blocks_more_than_1
 
-	ldr	q21, [x3, #48]					//load h2k | h1k
+	ldr	q21, [x6, #16]					//load h2k | h1k
 	sub	v30.4s, v30.4s, v31.4s
 	b	.L128_enc_blocks_less_than_1
 .L128_enc_blocks_more_than_7:	//blocks	left >  7
@@ -1169,7 +1169,7 @@ aesv8_gcm_8x_enc_128:
 
 	st1	{ v9.16b}, [x2], #16			  	//AES final-3 block - store result
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
@@ -1178,7 +1178,7 @@ aesv8_gcm_8x_enc_128:
 	movi	v16.8b, #0						//supress further partial tag feed in
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	pmull	v26.1q, v8.1d, v25.1d				//GHASH final-3 block - low
 
 	ldr	q9, [x0], #16				//AES final-2 block - load plaintext
@@ -1206,7 +1206,7 @@ aesv8_gcm_8x_enc_128:
 	ldr	q9, [x0], #16				//AES final-1 block - load plaintext
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-2 block - mid
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 	movi	v16.8b, #0						//supress further partial tag feed in
 
@@ -1226,7 +1226,7 @@ aesv8_gcm_8x_enc_128:
 
 	st1	{ v9.16b}, [x2], #16			  	//AES final-1 block - store result
 
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 	ldr	q9, [x0], #16				//AES final block - load plaintext
@@ -1241,7 +1241,7 @@ aesv8_gcm_8x_enc_128:
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
 
@@ -1262,16 +1262,16 @@ aesv8_gcm_8x_enc_128:
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
 	and	x1, x1, #127			 	//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 	cmp	x1, #64
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
 
 	mov	v0.d[1], x14
 	mov	v0.d[0], x13					//ctr0b is mask for last block
@@ -1288,7 +1288,7 @@ aesv8_gcm_8x_enc_128:
 	ins	v16.d[0], v8.d[1]					//GHASH final block - mid
 
 	eor	v16.8b, v16.8b, v8.8b				//GHASH final block - mid
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	pmull	v16.1q, v16.1d, v21.1d				//GHASH final block - mid
@@ -1339,7 +1339,7 @@ aesv8_gcm_8x_dec_128:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -1350,7 +1350,7 @@ aesv8_gcm_8x_dec_128:
 	mov	x5, x9
 	ld1	{ v0.16b}, [x16]					//CTR block 0
 
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	sub	x5, x5, #1		//byte_len - 1
 
 	mov	x15, #0x100000000				//set up counter increment
@@ -1414,7 +1414,7 @@ aesv8_gcm_8x_dec_128:
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 1
@@ -1452,7 +1452,7 @@ aesv8_gcm_8x_dec_128:
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 3
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 3
 
@@ -1487,7 +1487,7 @@ aesv8_gcm_8x_dec_128:
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 4
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 5
 	aese	v3.16b, v28.16b
@@ -1538,7 +1538,7 @@ aesv8_gcm_8x_dec_128:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 7
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 7
@@ -1573,7 +1573,7 @@ aesv8_gcm_8x_dec_128:
 	aese	v1.16b, v26.16b						//AES block 1 - round 9
 	aese	v6.16b, v26.16b						//AES block 6 - round 9
 
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 	aese	v4.16b, v26.16b						//AES block 4 - round 9
 	aese	v3.16b, v26.16b						//AES block 3 - round 9
 
@@ -1626,9 +1626,9 @@ aesv8_gcm_8x_dec_128:
 	b.ge	.L128_dec_prepretail					//do prepretail
 
 .L128_dec_main_loop:	//main	loop start
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
@@ -1636,9 +1636,9 @@ aesv8_gcm_8x_dec_128:
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
@@ -1647,12 +1647,12 @@ aesv8_gcm_8x_dec_128:
 
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	pmull2	v16.1q, v9.2d, v23.2d				//GHASH block 8k+1 - high
 	pmull2	v17.1q, v8.2d, v25.2d				//GHASH block 8k - high
@@ -1695,7 +1695,7 @@ aesv8_gcm_8x_dec_128:
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
 .inst	0xce1d2631	//eor3 v17.16b, v17.16b, v29.16b, v9.16b			//GHASH block 8k+2, 8k+3 - high
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	trn1	v29.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 1
@@ -1704,9 +1704,9 @@ aesv8_gcm_8x_dec_128:
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	pmull2	v18.1q, v8.2d, v24.2d				//GHASH block 8k	- mid
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	pmull	v24.1q, v8.1d, v24.1d				//GHASH block 8k+1 - mid
 	aese	v6.16b, v27.16b
@@ -1734,9 +1734,9 @@ aesv8_gcm_8x_dec_128:
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 2
 	eor	v10.16b, v10.16b, v29.16b				//GHASH block 8k+2, 8k+3 - mid
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v18.16b, v18.16b, v24.16b				//GHASH block 8k+1 - mid
@@ -1761,12 +1761,12 @@ aesv8_gcm_8x_dec_128:
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
 .inst	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 3
 	trn2	v12.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
@@ -1813,7 +1813,7 @@ aesv8_gcm_8x_dec_128:
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 4
 	trn2	v14.2d, v15.2d, v14.2d				//GHASH block 8k+6, 8k+7 - mid
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	pmull2	v16.1q, v12.2d, v24.2d				//GHASH block 8k+4 - mid
@@ -1867,7 +1867,7 @@ aesv8_gcm_8x_dec_128:
 .inst	0xce0b3231	//eor3 v17.16b, v17.16b, v11.16b, v12.16b			//GHASH block 8k+6, 8k+7 - high
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	ldr	d16, [x10]			//MODULO - load modulo constant
 .inst	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+6, 8k+7 - low
@@ -1931,7 +1931,7 @@ aesv8_gcm_8x_dec_128:
 
 	aese	v0.16b, v26.16b						//AES block 8k+8 - round 9
 	aese	v1.16b, v26.16b						//AES block 8k+9 - round 9
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	aese	v6.16b, v26.16b						//AES block 8k+14 - round 9
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
@@ -1982,19 +1982,19 @@ aesv8_gcm_8x_dec_128:
 
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5
 
@@ -2002,8 +2002,8 @@ aesv8_gcm_8x_dec_128:
 
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
@@ -2063,13 +2063,13 @@ aesv8_gcm_8x_dec_128:
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 .inst	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 1
@@ -2082,9 +2082,9 @@ aesv8_gcm_8x_dec_128:
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 .inst	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 
@@ -2112,13 +2112,13 @@ aesv8_gcm_8x_dec_128:
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
 	trn2	v12.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 3
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	pmull2	v10.1q, v13.2d, v23.2d				//GHASH block 8k+5 - high
 	pmull	v23.1q, v13.1d, v23.1d				//GHASH block 8k+5 - low
 
@@ -2173,7 +2173,7 @@ aesv8_gcm_8x_dec_128:
 	pmull2	v13.1q, v14.2d, v21.2d				//GHASH block 8k+6 - mid
 	pmull	v21.1q, v14.1d, v21.1d				//GHASH block 8k+7 - mid
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 .inst	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 5
@@ -2224,7 +2224,7 @@ aesv8_gcm_8x_dec_128:
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 7
 .inst	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	pmull	v29.1q, v17.1d, v16.1d		 	//MODULO - top 64b align with mid
 	aese	v3.16b, v27.16b
@@ -2246,7 +2246,7 @@ aesv8_gcm_8x_dec_128:
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
 
 .inst	0xce1d5652	//eor3 v18.16b, v18.16b, v29.16b, v21.16b			//MODULO - fold into mid
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 8
@@ -2290,15 +2290,15 @@ aesv8_gcm_8x_dec_128:
 
 	cmp	x5, #112
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 	ldr	q9, [x0], #16				//AES block 8k+8 - load ciphertext
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
 
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 
@@ -2354,7 +2354,7 @@ aesv8_gcm_8x_dec_128:
 	mov	v7.16b, v6.16b
 	cmp	x5, #32
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	mov	v6.16b, v1.16b
 	b.gt	.L128_dec_blocks_more_than_2
 
@@ -2365,7 +2365,7 @@ aesv8_gcm_8x_dec_128:
 	b.gt	.L128_dec_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	.L128_dec_blocks_less_than_1
 .L128_dec_blocks_more_than_7:	//blocks	left >  7
 	rev64	v8.16b, v9.16b						//GHASH final-7 block
@@ -2468,9 +2468,9 @@ aesv8_gcm_8x_dec_128:
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-3 block - mid
 
@@ -2495,7 +2495,7 @@ aesv8_gcm_8x_dec_128:
 	st1	{ v12.16b}, [x2], #16			 	//AES final-2 block - store result
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 	movi	v16.8b, #0						//supress further partial tag feed in
 
@@ -2520,7 +2520,7 @@ aesv8_gcm_8x_dec_128:
 	st1	{ v12.16b}, [x2], #16			 	//AES final-1 block - store result
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
@@ -2534,7 +2534,7 @@ aesv8_gcm_8x_dec_128:
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-1 block - high
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
 .inst	0xce07752c	//eor3 v12.16b, v9.16b, v7.16b, v29.16b				//AES final block - result
@@ -2554,20 +2554,20 @@ aesv8_gcm_8x_dec_128:
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
 
 	mov	v0.d[1], x14
 	mov	v0.d[0], x13					//ctr0b is mask for last block
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
 
@@ -2636,7 +2636,7 @@ aesv8_gcm_8x_enc_192:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -2672,7 +2672,7 @@ aesv8_gcm_8x_enc_192:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	add	x5, x5, x0
 
@@ -2699,7 +2699,7 @@ aesv8_gcm_8x_enc_192:
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 0
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 1
@@ -2739,7 +2739,7 @@ aesv8_gcm_8x_enc_192:
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 3
 
@@ -2782,7 +2782,7 @@ aesv8_gcm_8x_enc_192:
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 5
 
@@ -2820,7 +2820,7 @@ aesv8_gcm_8x_enc_192:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 6
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 7
@@ -2869,7 +2869,7 @@ aesv8_gcm_8x_enc_192:
 	ld1	{ v19.16b}, [x3]
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 9
@@ -2912,7 +2912,7 @@ aesv8_gcm_8x_enc_192:
 
 	aese	v4.16b, v28.16b						//AES block 12 - round 11
 	aese	v7.16b, v28.16b						//AES block 15 - round 11
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	aese	v1.16b, v28.16b						//AES block 9 - round 11
 	aese	v5.16b, v28.16b						//AES block 13 - round 11
@@ -2965,21 +2965,21 @@ aesv8_gcm_8x_enc_192:
 
 .L192_enc_main_loop:	//main	loop start
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4 (t0, t1, and t2 free)
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 	rev64	v8.16b, v8.16b						//GHASH block 8k
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
@@ -3010,7 +3010,7 @@ aesv8_gcm_8x_enc_192:
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	pmull2	v17.1q, v8.2d, v25.2d				//GHASH block 8k - high
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
@@ -3023,8 +3023,8 @@ aesv8_gcm_8x_enc_192:
 	trn1	v18.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 1
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	pmull2	v29.1q, v10.2d, v22.2d				//GHASH block 8k+2 - high
 	pmull	v19.1q, v8.1d, v25.1d				//GHASH block 8k - low
@@ -3076,14 +3076,14 @@ aesv8_gcm_8x_enc_192:
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 3
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	pmull2	v18.1q, v8.2d, v24.2d				//GHASH block 8k - mid
@@ -3129,12 +3129,12 @@ aesv8_gcm_8x_enc_192:
 .inst	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 5
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7 (t0, t1, t2 and t3 free)
@@ -3149,8 +3149,8 @@ aesv8_gcm_8x_enc_192:
 
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 5
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 5
@@ -3197,7 +3197,7 @@ aesv8_gcm_8x_enc_192:
 	eor	v14.16b, v14.16b, v13.16b				//GHASH block 8k+6, 8k+7 - mid
 
 	pmull2	v16.1q, v12.2d, v24.2d				//GHASH block 8k+4 - mid
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull	v24.1q, v12.1d, v24.1d				//GHASH block 8k+5 - mid
 
 	aese	v4.16b, v27.16b
@@ -3244,7 +3244,7 @@ aesv8_gcm_8x_enc_192:
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 8
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 8
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 .inst	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+6, 8k+7 - low
 	rev32	v20.16b, v30.16b					//CTR block 8k+16
@@ -3282,7 +3282,7 @@ aesv8_gcm_8x_enc_192:
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 10
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 10
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 	ext	v29.16b, v17.16b, v17.16b, #8				//MODULO - other top alignment
 
 	aese	v0.16b, v27.16b
@@ -3355,26 +3355,26 @@ aesv8_gcm_8x_enc_192:
 
 .L192_enc_prepretail:	//PREPRETAIL
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
@@ -3408,7 +3408,7 @@ aesv8_gcm_8x_enc_192:
 	trn2	v8.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	pmull	v23.1q, v9.1d, v23.1d				//GHASH block 8k+1 - low
 	eor	v17.16b, v17.16b, v16.16b				//GHASH block 8k+1 - high
@@ -3468,7 +3468,7 @@ aesv8_gcm_8x_enc_192:
 
 	eor	v10.16b, v10.16b, v29.16b				//GHASH block 8k+2, 8k+3 - mid
 	pmull	v24.1q, v8.1d, v24.1d				//GHASH block 8k+1 - mid
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 3
@@ -3482,17 +3482,17 @@ aesv8_gcm_8x_enc_192:
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 3
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 3
@@ -3526,14 +3526,14 @@ aesv8_gcm_8x_enc_192:
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7 (t0, t1, t2 and t3 free)
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 5
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	pmull2	v11.1q, v14.2d, v22.2d				//GHASH block 8k+6 - high
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
@@ -3605,7 +3605,7 @@ aesv8_gcm_8x_enc_192:
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 7
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 7
@@ -3647,7 +3647,7 @@ aesv8_gcm_8x_enc_192:
 
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 9
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 9
 
@@ -3665,7 +3665,7 @@ aesv8_gcm_8x_enc_192:
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 9
 
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 10
@@ -3702,18 +3702,18 @@ aesv8_gcm_8x_enc_192:
 
 .L192_enc_tail:	//TAIL
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	sub	x5, x4, x0 	//main_end_input_ptr is number of bytes left to process
 
 	ldr	q8, [x0], #16				//AES block 8k+8 - l3ad plaintext
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	mov	v29.16b, v26.16b
 
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	cmp	x5, #112
@@ -3771,7 +3771,7 @@ aesv8_gcm_8x_enc_192:
 	mov	v6.16b, v1.16b
 	sub	v30.4s, v30.4s, v31.4s
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	cmp	x5, #32
 	b.gt	.L192_enc_blocks_more_than_2
 
@@ -3782,7 +3782,7 @@ aesv8_gcm_8x_enc_192:
 	b.gt	.L192_enc_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	.L192_enc_blocks_less_than_1
 .L192_enc_blocks_more_than_7:	//blocks	left >  7
 	st1	{ v9.16b}, [x2], #16			 	//AES final-7 block  - store result
@@ -3881,7 +3881,7 @@ aesv8_gcm_8x_enc_192:
 .inst	0xce047529	//eor3 v9.16b, v9.16b, v4.16b, v29.16b			//AES final-3 block - result
 .L192_enc_blocks_more_than_3:	//blocks	left >  3
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	st1	{ v9.16b}, [x2], #16			 	//AES final-3 block - store result
 
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
@@ -3890,7 +3890,7 @@ aesv8_gcm_8x_enc_192:
 	movi	v16.8b, #0						//supress further partial tag feed in
 
 	ldr	q9, [x0], #16				//AES final-2 block - load plaintext
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
@@ -3913,7 +3913,7 @@ aesv8_gcm_8x_enc_192:
 	st1	{ v9.16b}, [x2], #16			 	//AES final-2 block - store result
 
 	rev64	v8.16b, v9.16b						//GHASH final-2 block
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
@@ -3936,7 +3936,7 @@ aesv8_gcm_8x_enc_192:
 .inst	0xce067529	//eor3 v9.16b, v9.16b, v6.16b, v29.16b			//AES final-1 block - result
 .L192_enc_blocks_more_than_1:	//blocks	left >  1
 
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	st1	{ v9.16b}, [x2], #16			 	//AES final-1 block - store result
 
@@ -3952,7 +3952,7 @@ aesv8_gcm_8x_enc_192:
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 
 	ldr	q9, [x0], #16				//AES final block - load plaintext
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
 
@@ -3965,7 +3965,7 @@ aesv8_gcm_8x_enc_192:
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-1 block - high
 .L192_enc_blocks_less_than_1:	//blocks	left <= 1
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
 	sub	x1, x1, #128				//bit_length -= 128
@@ -3974,15 +3974,15 @@ aesv8_gcm_8x_enc_192:
 
 	and	x1, x1, #127				//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
 
 	mov	v0.d[1], x14
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
@@ -4052,7 +4052,7 @@ aesv8_gcm_8x_dec_192:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -4086,7 +4086,7 @@ aesv8_gcm_8x_dec_192:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 6
 	add	v30.4s, v30.4s, v31.4s		//CTR block 6
@@ -4111,7 +4111,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 0
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 1
@@ -4155,7 +4155,7 @@ aesv8_gcm_8x_dec_192:
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 3
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 3
 	aese	v5.16b, v26.16b
@@ -4196,7 +4196,7 @@ aesv8_gcm_8x_dec_192:
 
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 5
@@ -4232,7 +4232,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 6
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	add	v30.4s, v30.4s, v31.4s		//CTR block 7
 
@@ -4283,7 +4283,7 @@ aesv8_gcm_8x_dec_192:
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 9
@@ -4323,7 +4323,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 10
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 10
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	aese	v0.16b, v28.16b						//AES block 0 - round 11
 	aese	v1.16b, v28.16b						//AES block 1 - round 11
@@ -4380,16 +4380,16 @@ aesv8_gcm_8x_dec_192:
 
 .L192_dec_main_loop:	//main	loop start
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
@@ -4422,14 +4422,14 @@ aesv8_gcm_8x_dec_192:
 
 	pmull	v19.1q, v8.1d, v25.1d				//GHASH block 8k - low
 	pmull2	v16.1q, v9.2d, v23.2d				//GHASH block 8k+1 - high
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
 	pmull	v23.1q, v9.1d, v23.1d				//GHASH block 8k+1 - low
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	aese	v0.16b, v27.16b
@@ -4452,8 +4452,8 @@ aesv8_gcm_8x_dec_192:
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	trn2	v8.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 
 	eor	v17.16b, v17.16b, v16.16b				//GHASH block 8k+1 - high
@@ -4484,9 +4484,9 @@ aesv8_gcm_8x_dec_192:
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 2
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 2
@@ -4506,7 +4506,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 3
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	eor	v10.16b, v10.16b, v29.16b				//GHASH block 8k+2, 8k+3 - mid
 .inst	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
@@ -4545,16 +4545,16 @@ aesv8_gcm_8x_dec_192:
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 4
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 5
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
@@ -4576,8 +4576,8 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	pmull2	v10.1q, v13.2d, v23.2d				//GHASH block 8k+5 - high
 	pmull	v23.1q, v13.1d, v23.1d				//GHASH block 8k+5 - low
 
@@ -4617,7 +4617,7 @@ aesv8_gcm_8x_dec_192:
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 7
 
@@ -4675,7 +4675,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 9
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 9
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 .inst	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
 	ldp	q8, q9, [x0], #32			//AES block 8k+8, 8k+9 - load ciphertext
@@ -4710,7 +4710,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 10
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 10
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	ldp	q14, q15, [x0], #32			//AES block 8k+14, 8k+15 - load ciphertext
 	aese	v4.16b, v27.16b
@@ -4768,13 +4768,13 @@ aesv8_gcm_8x_dec_192:
 	b.lt	.L192_dec_main_loop
 
 .L192_dec_prepretail:	//PREPRETAIL
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
@@ -4787,9 +4787,9 @@ aesv8_gcm_8x_dec_192:
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 
@@ -4816,7 +4816,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 1
@@ -4845,8 +4845,8 @@ aesv8_gcm_8x_dec_192:
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 2
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
@@ -4892,20 +4892,20 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 3
 
 .inst	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 3
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 	pmull	v21.1q, v10.1d, v21.1d				//GHASH block 8k+3 - mid
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	eor	v18.16b, v18.16b, v24.16b				//GHASH block 8k+1 - mid
 	aese	v2.16b, v26.16b
@@ -4938,8 +4938,8 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 4
 
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	trn1	v16.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
 
 	aese	v7.16b, v28.16b
@@ -4949,7 +4949,7 @@ aesv8_gcm_8x_dec_192:
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 5
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 5
 	aese	v5.16b, v28.16b
@@ -5009,7 +5009,7 @@ aesv8_gcm_8x_dec_192:
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull	v21.1q, v14.1d, v21.1d				//GHASH block 8k+7 - mid
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
@@ -5056,7 +5056,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 8
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 8
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 .inst	0xce1d5652	//eor3 v18.16b, v18.16b, v29.16b, v21.16b			//MODULO - fold into mid
 	aese	v7.16b, v26.16b
@@ -5079,7 +5079,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 9
 
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 	ext	v21.16b, v18.16b, v18.16b, #8			 	//MODULO - other mid alignment
 
 	aese	v2.16b, v27.16b
@@ -5118,16 +5118,16 @@ aesv8_gcm_8x_dec_192:
 
 	sub	x5, x4, x0 	//main_end_input_ptr is number of bytes left to process
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	ldr	q9, [x0], #16				//AES block 8k+8 - load ciphertext
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	mov	v29.16b, v26.16b
 
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
@@ -5186,7 +5186,7 @@ aesv8_gcm_8x_dec_192:
 	cmp	x5, #32
 
 	mov	v6.16b, v1.16b
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	b.gt	.L192_dec_blocks_more_than_2
 
 	sub	v30.4s, v30.4s, v31.4s
@@ -5196,7 +5196,7 @@ aesv8_gcm_8x_dec_192:
 	b.gt	.L192_dec_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	.L192_dec_blocks_less_than_1
 .L192_dec_blocks_more_than_7:	//blocks	left >  7
 	rev64	v8.16b, v9.16b						//GHASH final-7 block
@@ -5290,7 +5290,7 @@ aesv8_gcm_8x_dec_192:
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-4 block - high
 .L192_dec_blocks_more_than_3:	//blocks	left >  3
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
 	ldr	q9, [x0], #16				//AES final-2 block - load ciphertext
@@ -5309,7 +5309,7 @@ aesv8_gcm_8x_dec_192:
 .inst	0xce05752c	//eor3 v12.16b, v9.16b, v5.16b, v29.16b				//AES final-2 block - result
 
 	eor	v19.16b, v19.16b, v26.16b					//GHASH final-3 block - low
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-3 block - mid
 
@@ -5319,7 +5319,7 @@ aesv8_gcm_8x_dec_192:
 .L192_dec_blocks_more_than_2:	//blocks	left >  2
 
 	rev64	v8.16b, v9.16b						//GHASH final-2 block
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
@@ -5346,12 +5346,12 @@ aesv8_gcm_8x_dec_192:
 
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 	ldr	q9, [x0], #16				//AES final block - load ciphertext
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
 	movi	v16.8b, #0						//supress further partial tag feed in
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	pmull	v26.1q, v8.1d, v22.1d				//GHASH final-1 block - low
 	ins	v27.d[0], v8.d[1]					//GHASH final-1 block - mid
@@ -5380,17 +5380,17 @@ aesv8_gcm_8x_dec_192:
 	str	q30, [x16]					//store the updated counter
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 
 	and	x1, x1, #127				//bit_length %= 128
 
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	mov	v0.d[1], x14
@@ -5458,7 +5458,7 @@ aesv8_gcm_8x_enc_256:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -5497,7 +5497,7 @@ aesv8_gcm_8x_enc_256:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 6
 	add	v30.4s, v30.4s, v31.4s		//CTR block 6
@@ -5522,7 +5522,7 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 0
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 1
@@ -5567,7 +5567,7 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 3
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 3
@@ -5607,7 +5607,7 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 5
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 5
@@ -5641,7 +5641,7 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 6
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 7
@@ -5685,7 +5685,7 @@ aesv8_gcm_8x_enc_256:
 	ld1	{ v19.16b}, [x3]
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 9
@@ -5727,7 +5727,7 @@ aesv8_gcm_8x_enc_256:
 
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 11
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 11
 
@@ -5746,7 +5746,7 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 11
 
 	add	v30.4s, v30.4s, v31.4s		//CTR block 7
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 12
@@ -5824,17 +5824,17 @@ aesv8_gcm_8x_enc_256:
 	b.ge	.L256_enc_prepretail					//do prepretail
 
 .L256_enc_main_loop:	//main	loop start
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
@@ -5844,9 +5844,9 @@ aesv8_gcm_8x_enc_256:
 
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	aese	v3.16b, v26.16b
@@ -5869,7 +5869,7 @@ aesv8_gcm_8x_enc_256:
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
@@ -5931,7 +5931,7 @@ aesv8_gcm_8x_enc_256:
 
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 
 	aese	v2.16b, v26.16b
@@ -5947,9 +5947,9 @@ aesv8_gcm_8x_enc_256:
 
 	pmull	v20.1q, v11.1d, v20.1d				//GHASH block 8k+3 - low
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	trn1	v16.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
@@ -5978,7 +5978,7 @@ aesv8_gcm_8x_enc_256:
 
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
@@ -6028,21 +6028,21 @@ aesv8_gcm_8x_enc_256:
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 6
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
 .inst	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 7
 	aese	v3.16b, v27.16b
@@ -6105,7 +6105,7 @@ aesv8_gcm_8x_enc_256:
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 9
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 9
 	aese	v3.16b, v26.16b
@@ -6148,7 +6148,7 @@ aesv8_gcm_8x_enc_256:
 
 .inst	0xce0b3231	//eor3 v17.16b, v17.16b, v11.16b, v12.16b			//GHASH block 8k+6, 8k+7 - high
 
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 	rev32	v20.16b, v30.16b					//CTR block 8k+16
 
 	ext	v21.16b, v17.16b, v17.16b, #8			 	//MODULO - other top alignment
@@ -6189,7 +6189,7 @@ aesv8_gcm_8x_enc_256:
 
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 12
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 	aese	v7.16b, v27.16b						//AES block 8k+15 - round 13
 
 	ldp	q10, q11, [x0], #32			//AES block 8k+10, 8k+11 - load plaintext
@@ -6254,7 +6254,7 @@ aesv8_gcm_8x_enc_256:
 
 .L256_enc_prepretail:	//PREPRETAIL
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
@@ -6263,8 +6263,8 @@ aesv8_gcm_8x_enc_256:
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 
@@ -6293,20 +6293,20 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 1
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 1
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 1
 
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
@@ -6342,7 +6342,7 @@ aesv8_gcm_8x_enc_256:
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	trn1	v18.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 	pmull2	v17.1q, v8.2d, v25.2d				//GHASH block 8k - high
 
@@ -6420,11 +6420,11 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 5
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 	pmull	v21.1q, v10.1d, v21.1d				//GHASH block 8k+3 - mid
@@ -6450,8 +6450,8 @@ aesv8_gcm_8x_enc_256:
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 6
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 6
 	aese	v3.16b, v26.16b
@@ -6466,12 +6466,12 @@ aesv8_gcm_8x_enc_256:
 
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 7
 	aese	v4.16b, v27.16b
@@ -6534,7 +6534,7 @@ aesv8_gcm_8x_enc_256:
 .inst	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
 .inst	0xce082a31	//eor3 v17.16b, v17.16b, v8.16b, v10.16b			//GHASH block 8k+4, 8k+5 - high
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 9
 	aese	v0.16b, v26.16b
@@ -6584,7 +6584,7 @@ aesv8_gcm_8x_enc_256:
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 11
 
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 	ext	v21.16b, v17.16b, v17.16b, #8			 	//MODULO - other top alignment
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 11
@@ -6605,7 +6605,7 @@ aesv8_gcm_8x_enc_256:
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 11
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 12
@@ -6642,17 +6642,17 @@ aesv8_gcm_8x_enc_256:
 	aese	v6.16b, v27.16b						//AES block 8k+14 - round 13
 .L256_enc_tail:	//TAIL
 
-	ldp	q24, q25, [x3, #192]			//load h8l | h8h
+	ldp	q24, q25, [x6, #160]			//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	sub	x5, x4, x0		//main_end_input_ptr is number of bytes left to process
 
 	ldr	q8, [x0], #16				//AES block 8k+8 - load plaintext
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	mov	v29.16b, v28.16b
@@ -6708,7 +6708,7 @@ aesv8_gcm_8x_enc_256:
 
 	cmp	x5, #32
 	mov	v7.16b, v6.16b
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	mov	v6.16b, v1.16b
 	sub	v30.4s, v30.4s, v31.4s
@@ -6721,7 +6721,7 @@ aesv8_gcm_8x_enc_256:
 	b.gt	.L256_enc_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	.L256_enc_blocks_less_than_1
 .L256_enc_blocks_more_than_7:	//blocks	left >  7
 	st1	{ v9.16b}, [x2], #16				//AES final-7 block  - store result
@@ -6823,7 +6823,7 @@ aesv8_gcm_8x_enc_256:
 
 	st1	{ v9.16b}, [x2], #16				//AES final-3 block - store result
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
 
@@ -6834,7 +6834,7 @@ aesv8_gcm_8x_enc_256:
 
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-3 block - high
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-3 block - mid
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-3 block - mid
 	ldr	q9, [x0], #16				//AES final-2 block - load plaintext
@@ -6849,7 +6849,7 @@ aesv8_gcm_8x_enc_256:
 	eor	v19.16b, v19.16b, v26.16b					//GHASH final-3 block - low
 .L256_enc_blocks_more_than_2:	//blocks	left >  2
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 
 	st1	{ v9.16b}, [x2], #16			 	//AES final-2 block - store result
@@ -6879,7 +6879,7 @@ aesv8_gcm_8x_enc_256:
 
 	st1	{ v9.16b}, [x2], #16				//AES final-1 block - store result
 
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 	ldr	q9, [x0], #16				//AES final block - load plaintext
@@ -6896,7 +6896,7 @@ aesv8_gcm_8x_enc_256:
 	pmull	v26.1q, v8.1d, v22.1d				//GHASH final-1 block - low
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	eor	v19.16b, v19.16b, v26.16b					//GHASH final-1 block - low
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
@@ -6912,18 +6912,18 @@ aesv8_gcm_8x_enc_256:
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x14, x6, xzr, lt
-	csel	x13, x7, x6, lt
+	csel	x14, x7, xzr, lt
+	csel	x13, x8, x7, lt
 
 	mov	v0.d[0], x13					//ctr0b is mask for last block
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
@@ -6990,7 +6990,7 @@ aesv8_gcm_8x_dec_256:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -7016,7 +7016,7 @@ aesv8_gcm_8x_dec_256:
 
 	rev32	v2.16b, v30.16b				//CTR block 2
 	add	v30.4s, v30.4s, v31.4s		//CTR block 2
-	ldp	q26, q27, [x8, #0]				  	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				  	//load rk0, rk1
 
 	rev32	v3.16b, v30.16b				//CTR block 3
 	add	v30.4s, v30.4s, v31.4s		//CTR block 3
@@ -7051,7 +7051,7 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 0
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b		        //AES block 7 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b		        //AES block 6 - round 1
@@ -7090,7 +7090,7 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 2
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 2
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 3
@@ -7136,7 +7136,7 @@ aesv8_gcm_8x_dec_256:
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 5
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 5
 	aese	v7.16b, v28.16b
@@ -7171,7 +7171,7 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 6
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 7
@@ -7218,7 +7218,7 @@ aesv8_gcm_8x_dec_256:
 	ld1	{ v19.16b}, [x3]
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	add	x4, x0, x1, lsr #3 //end_input_ptr
 	add	x5, x5, x0
 
@@ -7258,7 +7258,7 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 10
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 10
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 11
@@ -7280,7 +7280,7 @@ aesv8_gcm_8x_dec_256:
 
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 11
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 12
@@ -7358,13 +7358,13 @@ aesv8_gcm_8x_dec_256:
 
 .L256_dec_main_loop:	//main	loop start
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
@@ -7396,12 +7396,12 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 0
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	eor	v8.16b, v8.16b, v19.16b					//PRE 1
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
@@ -7445,7 +7445,7 @@ aesv8_gcm_8x_dec_256:
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	pmull2	v29.1q, v10.2d, v22.2d				//GHASH block 8k+2 - high
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
@@ -7493,12 +7493,12 @@ aesv8_gcm_8x_dec_256:
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 4
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
 	pmull	v22.1q, v10.1d, v22.1d				//GHASH block 8k+2 - low
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
@@ -7554,9 +7554,9 @@ aesv8_gcm_8x_dec_256:
 	pmull	v21.1q, v10.1d, v21.1d				//GHASH block 8k+3 - mid
 .inst	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
 	eor	v18.16b, v18.16b, v24.16b				//GHASH block 8k+1 - mid
@@ -7565,11 +7565,11 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 .inst	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 	aese	v7.16b, v27.16b
@@ -7582,8 +7582,8 @@ aesv8_gcm_8x_dec_256:
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 7
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 7
 	aese	v4.16b, v27.16b
@@ -7617,7 +7617,7 @@ aesv8_gcm_8x_dec_256:
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 8
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	pmull	v22.1q, v14.1d, v22.1d				//GHASH block 8k+6 - low
 	trn2	v14.2d, v15.2d, v14.2d				//GHASH block 8k+6, 8k+7 - mid
 
@@ -7684,7 +7684,7 @@ aesv8_gcm_8x_dec_256:
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+16
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 11
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 11
@@ -7724,7 +7724,7 @@ aesv8_gcm_8x_dec_256:
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 11
 
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 12
 	aese	v3.16b, v26.16b
@@ -7784,30 +7784,30 @@ aesv8_gcm_8x_dec_256:
 	b.lt	.L256_dec_main_loop
 
 .L256_dec_prepretail:	//PREPRETAIL
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	aese	v0.16b, v26.16b
@@ -7831,7 +7831,7 @@ aesv8_gcm_8x_dec_256:
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
 	eor	v8.16b, v8.16b, v19.16b					//PRE 1
@@ -7891,7 +7891,7 @@ aesv8_gcm_8x_dec_256:
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 3
 	pmull2	v9.1q, v11.2d, v20.2d				//GHASH block 8k+3 - high
@@ -7922,9 +7922,9 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 4
 
 .inst	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 4
@@ -7964,11 +7964,11 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5
@@ -7979,8 +7979,8 @@ aesv8_gcm_8x_dec_256:
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 6
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 6
 
@@ -8010,7 +8010,7 @@ aesv8_gcm_8x_dec_256:
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 6
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull	v22.1q, v14.1d, v22.1d				//GHASH block 8k+6 - low
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
@@ -8073,7 +8073,7 @@ aesv8_gcm_8x_dec_256:
 	pmull	v21.1q, v14.1d, v21.1d				//GHASH block 8k+7 - mid
 	pmull	v20.1q, v15.1d, v20.1d				//GHASH block 8k+7 - low
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 .inst	0xce195e73	//eor3 v19.16b, v19.16b, v25.16b, v23.16b			//GHASH block 8k+4, 8k+5 - low
 .inst	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
 
@@ -8110,7 +8110,7 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 10
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 10
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 
 	ext	v21.16b, v17.16b, v17.16b, #8				//MODULO - other top alignment
 
@@ -8153,7 +8153,7 @@ aesv8_gcm_8x_dec_256:
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 12
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b	        	//AES block 8k+9 - round 12
 
@@ -8180,15 +8180,15 @@ aesv8_gcm_8x_dec_256:
 
 	ldr	q9, [x0], #16				//AES block 8k+8 - load ciphertext
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 	mov	v29.16b, v28.16b
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 .inst	0xce00752c	//eor3 v12.16b, v9.16b, v0.16b, v29.16b				//AES block 8k+8 - result
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	b.gt	.L256_dec_blocks_more_than_7
@@ -8238,7 +8238,7 @@ aesv8_gcm_8x_dec_256:
 	mov	v5.16b, v1.16b
 	b.gt	.L256_dec_blocks_more_than_3
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	sub	v30.4s, v30.4s, v31.4s
 	mov	v7.16b, v6.16b
 
@@ -8253,7 +8253,7 @@ aesv8_gcm_8x_dec_256:
 	b.gt	.L256_dec_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	.L256_dec_blocks_less_than_1
 .L256_dec_blocks_more_than_7:	//blocks	left >  7
 	rev64	v8.16b, v9.16b						//GHASH final-7 block
@@ -8348,13 +8348,13 @@ aesv8_gcm_8x_dec_256:
 .inst	0xce04752c	//eor3 v12.16b, v9.16b, v4.16b, v29.16b				//AES final-3 block - result
 .L256_dec_blocks_more_than_3:	//blocks	left >  3
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
 	ldr	q9, [x0], #16				//AES final-2 block - load ciphertext
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
 	st1	{ v12.16b}, [x2], #16			 	//AES final-3 block - store result
@@ -8378,7 +8378,7 @@ aesv8_gcm_8x_dec_256:
 
 	rev64	v8.16b, v9.16b						//GHASH final-2 block
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 	ldr	q9, [x0], #16				//AES final-1 block - load ciphertext
 
@@ -8406,14 +8406,14 @@ aesv8_gcm_8x_dec_256:
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-1 block - mid
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 	ldr	q9, [x0], #16				//AES final block - load ciphertext
 	st1	{ v12.16b}, [x2], #16			 	//AES final-1 block - store result
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	pmull	v26.1q, v8.1d, v22.1d				//GHASH final-1 block - low
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
@@ -8432,7 +8432,7 @@ aesv8_gcm_8x_dec_256:
 .L256_dec_blocks_less_than_1:	//blocks	left <= 1
 
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
 	sub	x1, x1, #128				//bit_length -= 128
@@ -8443,18 +8443,18 @@ aesv8_gcm_8x_dec_256:
 
 	and	x1, x1, #127			 	//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x14, x6, xzr, lt
-	csel	x13, x7, x6, lt
+	csel	x14, x7, xzr, lt
+	csel	x13, x8, x7, lt
 
 	mov	v0.d[0], x13					//ctr0b is mask for last block
 	mov	v0.d[1], x14
 
 	and	v9.16b, v9.16b, v0.16b					//possibly partial last block has zeroes in highest bits
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	bif	v12.16b, v26.16b, v0.16b					//insert existing bytes in top end of result before storing
 
diff --git a/generated-src/linux-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S b/generated-src/linux-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S
index a2672c0d3..1d452ddf4 100644
--- a/generated-src/linux-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S
+++ b/generated-src/linux-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S
@@ -90,7 +90,7 @@ aes_gcm_enc_kernel:
 	ldr	q23, [x8, #80]                                 // load rk5
 	aese	v1.16b, v19.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 1
-	ldr	q14, [x3, #80]                         // load h3l | h3h
+	ldr	q14, [x6, #48]                              // load h3l | h3h
 	ext	v14.16b, v14.16b, v14.16b, #8
 	aese	v3.16b, v18.16b
 	aesmc	v3.16b, v3.16b          // AES block 3 - round 0
@@ -99,14 +99,14 @@ aes_gcm_enc_kernel:
 	ldr	q22, [x8, #64]                                 // load rk4
 	aese	v1.16b, v20.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 2
-	ldr	q13, [x3, #64]                         // load h2l | h2h
+	ldr	q13, [x6, #32]                              // load h2l | h2h
 	ext	v13.16b, v13.16b, v13.16b, #8
 	aese	v3.16b, v19.16b
 	aesmc	v3.16b, v3.16b          // AES block 3 - round 1
 	ldr	q30, [x8, #192]                               // load rk12
 	aese	v2.16b, v20.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 2
-	ldr	q15, [x3, #112]                        // load h4l | h4h
+	ldr	q15, [x6, #80]                              // load h4l | h4h
 	ext	v15.16b, v15.16b, v15.16b, #8
 	aese	v1.16b, v21.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 3
@@ -149,7 +149,7 @@ aes_gcm_enc_kernel:
 	ldr	q27, [x8, #144]                                // load rk9
 	aese	v0.16b, v24.16b
 	aesmc	v0.16b, v0.16b          // AES block 0 - round 6
-	ldr	q12, [x3, #32]                         // load h1l | h1h
+	ldr	q12, [x6]                                   // load h1l | h1h
 	ext	v12.16b, v12.16b, v12.16b, #8
 	aese	v2.16b, v24.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 6
@@ -850,15 +850,15 @@ aes_gcm_dec_kernel:
 	ldr	q19, [x8, #16]                                 // load rk1
 	aese	v0.16b, v18.16b
 	aesmc	v0.16b, v0.16b          // AES block 0 - round 0
-	ldr	q14, [x3, #80]                         // load h3l | h3h
+	ldr	q14, [x6, #48]                              // load h3l | h3h
 	ext	v14.16b, v14.16b, v14.16b, #8
 	aese	v3.16b, v18.16b
 	aesmc	v3.16b, v3.16b          // AES block 3 - round 0
-	ldr	q15, [x3, #112]                        // load h4l | h4h
+	ldr	q15, [x6, #80]                              // load h4l | h4h
 	ext	v15.16b, v15.16b, v15.16b, #8
 	aese	v1.16b, v18.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 0
-	ldr	q13, [x3, #64]                         // load h2l | h2h
+	ldr	q13, [x6, #32]                              // load h2l | h2h
 	ext	v13.16b, v13.16b, v13.16b, #8
 	aese	v2.16b, v18.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 0
@@ -878,7 +878,7 @@ aes_gcm_dec_kernel:
 	ldr	q30, [x8, #192]                               // load rk12
 	aese	v0.16b, v20.16b
 	aesmc	v0.16b, v0.16b          // AES block 0 - round 2
-	ldr	q12, [x3, #32]                         // load h1l | h1h
+	ldr	q12, [x6]                                   // load h1l | h1h
 	ext	v12.16b, v12.16b, v12.16b, #8
 	aese	v2.16b, v20.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 2
diff --git a/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-avx512.S b/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-avx512.S
index f30a8fe42..369ddf929 100644
--- a/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-avx512.S
+++ b/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-avx512.S
@@ -2066,7 +2066,7 @@ gcm_setiv_avx512:
 	cmpq	$12,%rcx
 	je	iv_len_12_init_IV
 	vpxor	%xmm2,%xmm2,%xmm2
-	leaq	96(%rsi),%r13
+	leaq	80(%rsi),%r13
 	movq	%rdx,%r10
 	movq	%rcx,%r11
 	orq	%r11,%r11
@@ -4009,7 +4009,7 @@ aes_gcm_encrypt_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -4215,7 +4215,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_achfkmnqFwjgbDD
 
@@ -5365,7 +5365,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -5564,7 +5564,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -5762,7 +5762,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -5961,7 +5961,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -6181,7 +6181,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -6407,7 +6407,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -6639,7 +6639,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -6872,7 +6872,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -7126,7 +7126,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -7388,7 +7388,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -7656,7 +7656,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -7925,7 +7925,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -8211,7 +8211,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -8501,7 +8501,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -8797,7 +8797,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -9094,7 +9094,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_aGAffhBljtiFsea:
 
@@ -9802,7 +9802,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -10001,7 +10001,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -10199,7 +10199,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -10398,7 +10398,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -10618,7 +10618,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -10844,7 +10844,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -11076,7 +11076,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -11309,7 +11309,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -11563,7 +11563,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -11825,7 +11825,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -12093,7 +12093,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -12362,7 +12362,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -12648,7 +12648,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -12938,7 +12938,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -13234,7 +13234,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -13531,7 +13531,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_erAoEayjDqpuhEu:
 
@@ -14050,7 +14050,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -14242,7 +14242,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -14459,7 +14459,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -14677,7 +14677,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -14915,7 +14915,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -15162,7 +15162,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -15419,7 +15419,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -15677,7 +15677,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -15952,7 +15952,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -16235,7 +16235,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -16528,7 +16528,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -16822,7 +16822,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -17129,7 +17129,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -17440,7 +17440,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -17761,7 +17761,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -18083,7 +18083,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_ifytbdtuElzEqkG:
 
@@ -18230,7 +18230,7 @@ aes_gcm_encrypt_avx512:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_mgjxphyGhnqeEta
 	vmovdqu64	640(%rsp),%zmm3
@@ -18503,7 +18503,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -18702,7 +18702,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -18900,7 +18900,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -19099,7 +19099,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -19319,7 +19319,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -19545,7 +19545,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -19777,7 +19777,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -20010,7 +20010,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -20264,7 +20264,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -20526,7 +20526,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -20794,7 +20794,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -21063,7 +21063,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -21349,7 +21349,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -21639,7 +21639,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -21935,7 +21935,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -22232,7 +22232,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_eruDeitqttsEEhG:
 
@@ -22465,7 +22465,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -22579,7 +22579,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -22718,7 +22718,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -22858,7 +22858,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -23016,7 +23016,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%xmm29,%xmm3,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -23183,7 +23183,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%ymm29,%ymm3,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -23360,7 +23360,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -23538,7 +23538,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -23732,7 +23732,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%xmm29,%xmm4,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -23934,7 +23934,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%ymm29,%ymm4,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -24146,7 +24146,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -24359,7 +24359,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -24584,7 +24584,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%xmm29,%xmm5,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -24813,7 +24813,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%ymm29,%ymm5,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -25052,7 +25052,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -25292,7 +25292,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_icDnnqvChyBsuli:
 
@@ -25396,7 +25396,7 @@ aes_gcm_encrypt_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -25612,7 +25612,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_EclAduckuFhozAl
 
@@ -26809,7 +26809,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -27012,7 +27012,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -27214,7 +27214,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -27417,7 +27417,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -27643,7 +27643,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -27875,7 +27875,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -28113,7 +28113,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -28352,7 +28352,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -28614,7 +28614,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -28884,7 +28884,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -29160,7 +29160,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -29437,7 +29437,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -29733,7 +29733,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -30033,7 +30033,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -30339,7 +30339,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -30646,7 +30646,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_bCffyflcoaBxCzy:
 
@@ -31380,7 +31380,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -31583,7 +31583,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -31785,7 +31785,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -31988,7 +31988,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -32214,7 +32214,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -32446,7 +32446,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -32684,7 +32684,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -32923,7 +32923,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -33185,7 +33185,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -33455,7 +33455,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -33731,7 +33731,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -34008,7 +34008,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -34304,7 +34304,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -34604,7 +34604,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -34910,7 +34910,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -35217,7 +35217,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_ECtspjaqpoxwhnx:
 
@@ -35751,7 +35751,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -35947,7 +35947,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -36168,7 +36168,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -36390,7 +36390,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -36634,7 +36634,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -36887,7 +36887,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -37150,7 +37150,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -37414,7 +37414,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -37697,7 +37697,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -37988,7 +37988,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -38289,7 +38289,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -38591,7 +38591,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -38908,7 +38908,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -39229,7 +39229,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -39560,7 +39560,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -39892,7 +39892,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_pybhdxzahdqcprl:
 
@@ -40039,7 +40039,7 @@ aes_gcm_encrypt_avx512:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_dzmCrsBiciGnliE
 	vmovdqu64	640(%rsp),%zmm3
@@ -40316,7 +40316,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -40519,7 +40519,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -40721,7 +40721,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -40924,7 +40924,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -41150,7 +41150,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -41382,7 +41382,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -41620,7 +41620,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -41859,7 +41859,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -42121,7 +42121,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -42391,7 +42391,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -42667,7 +42667,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -42944,7 +42944,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -43240,7 +43240,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -43540,7 +43540,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -43846,7 +43846,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -44153,7 +44153,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_ayefrejzGqbkfya:
 
@@ -44390,7 +44390,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -44508,7 +44508,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -44651,7 +44651,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -44795,7 +44795,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -44959,7 +44959,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%xmm29,%xmm3,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -45132,7 +45132,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%ymm29,%ymm3,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -45315,7 +45315,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -45499,7 +45499,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -45701,7 +45701,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%xmm29,%xmm4,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -45911,7 +45911,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%ymm29,%ymm4,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -46131,7 +46131,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -46352,7 +46352,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -46587,7 +46587,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%xmm29,%xmm5,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -46826,7 +46826,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%ymm29,%ymm5,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -47075,7 +47075,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -47325,7 +47325,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_qxCpdapFxyCuqwj:
 
@@ -47429,7 +47429,7 @@ aes_gcm_encrypt_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -47655,7 +47655,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_slhsqgEufGclFec
 
@@ -48902,7 +48902,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -49109,7 +49109,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -49315,7 +49315,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -49522,7 +49522,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -49754,7 +49754,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -49992,7 +49992,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -50236,7 +50236,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -50481,7 +50481,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -50751,7 +50751,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -51029,7 +51029,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -51313,7 +51313,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -51598,7 +51598,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -51904,7 +51904,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -52214,7 +52214,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -52530,7 +52530,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -52847,7 +52847,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_CFywctAlrBmkufB:
 
@@ -53609,7 +53609,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -53816,7 +53816,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -54022,7 +54022,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -54229,7 +54229,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -54461,7 +54461,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -54699,7 +54699,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -54943,7 +54943,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -55188,7 +55188,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -55458,7 +55458,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -55736,7 +55736,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -56020,7 +56020,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -56305,7 +56305,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -56611,7 +56611,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -56921,7 +56921,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -57237,7 +57237,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -57554,7 +57554,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_iuDhkykBcvvzBFb:
 
@@ -58104,7 +58104,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -58304,7 +58304,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -58529,7 +58529,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -58755,7 +58755,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -59005,7 +59005,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -59264,7 +59264,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -59533,7 +59533,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -59803,7 +59803,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -60094,7 +60094,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -60393,7 +60393,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -60702,7 +60702,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -61012,7 +61012,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -61339,7 +61339,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -61670,7 +61670,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -62011,7 +62011,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -62353,7 +62353,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_niAfluBnEgrukbj:
 
@@ -62500,7 +62500,7 @@ aes_gcm_encrypt_avx512:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_BiAvfDwrflaDzBx
 	vmovdqu64	640(%rsp),%zmm3
@@ -62781,7 +62781,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -62988,7 +62988,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -63194,7 +63194,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -63401,7 +63401,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -63633,7 +63633,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -63871,7 +63871,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -64115,7 +64115,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -64360,7 +64360,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -64630,7 +64630,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -64908,7 +64908,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -65192,7 +65192,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -65477,7 +65477,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -65783,7 +65783,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -66093,7 +66093,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -66409,7 +66409,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -66726,7 +66726,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_uqdvluxFgGqdFqv:
 
@@ -66967,7 +66967,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -67089,7 +67089,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -67236,7 +67236,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -67384,7 +67384,7 @@ aes_gcm_encrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -67554,7 +67554,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%xmm29,%xmm3,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -67733,7 +67733,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%ymm29,%ymm3,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -67922,7 +67922,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -68112,7 +68112,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -68322,7 +68322,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%xmm29,%xmm4,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -68540,7 +68540,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%ymm29,%ymm4,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -68768,7 +68768,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -68997,7 +68997,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -69242,7 +69242,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%xmm29,%xmm5,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -69491,7 +69491,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%ymm29,%ymm5,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -69750,7 +69750,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -70010,7 +70010,7 @@ aes_gcm_encrypt_avx512:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_mlladecCGcaEame:
 
@@ -70216,7 +70216,7 @@ aes_gcm_decrypt_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -70422,7 +70422,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_yDAnEECuuGxfwvr
 
@@ -71572,7 +71572,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -71771,7 +71771,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -71969,7 +71969,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -72168,7 +72168,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -72388,7 +72388,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -72614,7 +72614,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -72846,7 +72846,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -73079,7 +73079,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -73333,7 +73333,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -73595,7 +73595,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -73863,7 +73863,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -74132,7 +74132,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -74418,7 +74418,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -74708,7 +74708,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -75004,7 +75004,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -75301,7 +75301,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_vqjlBldpifEzCAi:
 
@@ -76009,7 +76009,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -76208,7 +76208,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -76406,7 +76406,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -76605,7 +76605,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -76825,7 +76825,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -77051,7 +77051,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -77283,7 +77283,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -77516,7 +77516,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -77770,7 +77770,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -78032,7 +78032,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -78300,7 +78300,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -78569,7 +78569,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -78855,7 +78855,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -79145,7 +79145,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -79441,7 +79441,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -79738,7 +79738,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_usEFihDgqghhogg:
 
@@ -80257,7 +80257,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -80449,7 +80449,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -80666,7 +80666,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -80884,7 +80884,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -81122,7 +81122,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -81369,7 +81369,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -81626,7 +81626,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -81884,7 +81884,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -82159,7 +82159,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -82442,7 +82442,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -82735,7 +82735,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -83029,7 +83029,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -83336,7 +83336,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -83647,7 +83647,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -83968,7 +83968,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -84290,7 +84290,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_pCDjmBApGDgFGhw:
 
@@ -84437,7 +84437,7 @@ aes_gcm_decrypt_avx512:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_lurmstfAeByrDpz
 	vmovdqu64	640(%rsp),%zmm3
@@ -84710,7 +84710,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -84909,7 +84909,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -85107,7 +85107,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -85306,7 +85306,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -85526,7 +85526,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -85752,7 +85752,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -85984,7 +85984,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -86217,7 +86217,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -86471,7 +86471,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -86733,7 +86733,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -87001,7 +87001,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -87270,7 +87270,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -87556,7 +87556,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -87846,7 +87846,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -88142,7 +88142,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -88439,7 +88439,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_qbevliloqkkkFsD:
 
@@ -88672,7 +88672,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm6,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -88786,7 +88786,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm6,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -88925,7 +88925,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -89065,7 +89065,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -89223,7 +89223,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%xmm29,%xmm7,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -89390,7 +89390,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%ymm29,%ymm7,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -89567,7 +89567,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -89745,7 +89745,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -89939,7 +89939,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%xmm29,%xmm10,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -90141,7 +90141,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%ymm29,%ymm10,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -90353,7 +90353,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -90566,7 +90566,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -90791,7 +90791,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%xmm29,%xmm11,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -91020,7 +91020,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%ymm29,%ymm11,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -91259,7 +91259,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -91499,7 +91499,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_fvqkmnelfBwdflt:
 
@@ -91603,7 +91603,7 @@ aes_gcm_decrypt_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -91819,7 +91819,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_mbihlziFEFsDoGE
 
@@ -93016,7 +93016,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -93219,7 +93219,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -93421,7 +93421,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -93624,7 +93624,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -93850,7 +93850,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -94082,7 +94082,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -94320,7 +94320,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -94559,7 +94559,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -94821,7 +94821,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -95091,7 +95091,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -95367,7 +95367,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -95644,7 +95644,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -95940,7 +95940,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -96240,7 +96240,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -96546,7 +96546,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -96853,7 +96853,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_rerkgBbyampldto:
 
@@ -97587,7 +97587,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -97790,7 +97790,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -97992,7 +97992,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -98195,7 +98195,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -98421,7 +98421,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -98653,7 +98653,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -98891,7 +98891,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -99130,7 +99130,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -99392,7 +99392,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -99662,7 +99662,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -99938,7 +99938,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -100215,7 +100215,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -100511,7 +100511,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -100811,7 +100811,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -101117,7 +101117,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -101424,7 +101424,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_emEtFnwcsvbsGee:
 
@@ -101958,7 +101958,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -102154,7 +102154,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -102375,7 +102375,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -102597,7 +102597,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -102841,7 +102841,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -103094,7 +103094,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -103357,7 +103357,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -103621,7 +103621,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -103904,7 +103904,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -104195,7 +104195,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -104496,7 +104496,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -104798,7 +104798,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -105115,7 +105115,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -105436,7 +105436,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -105767,7 +105767,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -106099,7 +106099,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_ciyykzjryphtjAc:
 
@@ -106246,7 +106246,7 @@ aes_gcm_decrypt_avx512:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_yBFabBiEpjEBBsr
 	vmovdqu64	640(%rsp),%zmm3
@@ -106523,7 +106523,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -106726,7 +106726,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -106928,7 +106928,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -107131,7 +107131,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -107357,7 +107357,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -107589,7 +107589,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -107827,7 +107827,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -108066,7 +108066,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -108328,7 +108328,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -108598,7 +108598,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -108874,7 +108874,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -109151,7 +109151,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -109447,7 +109447,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -109747,7 +109747,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -110053,7 +110053,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -110360,7 +110360,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_EFFoGallwwbomEy:
 
@@ -110597,7 +110597,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm6,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -110715,7 +110715,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm6,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -110858,7 +110858,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -111002,7 +111002,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -111166,7 +111166,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%xmm29,%xmm7,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -111339,7 +111339,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%ymm29,%ymm7,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -111522,7 +111522,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -111706,7 +111706,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -111908,7 +111908,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%xmm29,%xmm10,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -112118,7 +112118,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%ymm29,%ymm10,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -112338,7 +112338,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -112559,7 +112559,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -112794,7 +112794,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%xmm29,%xmm11,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -113033,7 +113033,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%ymm29,%ymm11,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -113282,7 +113282,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -113532,7 +113532,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_pGqEmoznEqGhujq:
 
@@ -113636,7 +113636,7 @@ aes_gcm_decrypt_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -113862,7 +113862,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_iDAhziwpjqoADaj
 
@@ -115109,7 +115109,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -115316,7 +115316,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -115522,7 +115522,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -115729,7 +115729,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -115961,7 +115961,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -116199,7 +116199,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -116443,7 +116443,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -116688,7 +116688,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -116958,7 +116958,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -117236,7 +117236,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -117520,7 +117520,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -117805,7 +117805,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -118111,7 +118111,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -118421,7 +118421,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -118737,7 +118737,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -119054,7 +119054,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_fdoxuvdoEsDrnFi:
 
@@ -119816,7 +119816,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -120023,7 +120023,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -120229,7 +120229,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -120436,7 +120436,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -120668,7 +120668,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -120906,7 +120906,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -121150,7 +121150,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -121395,7 +121395,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -121665,7 +121665,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -121943,7 +121943,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -122227,7 +122227,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -122512,7 +122512,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -122818,7 +122818,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -123128,7 +123128,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -123444,7 +123444,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -123761,7 +123761,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_pBmCpEpokBigCud:
 
@@ -124311,7 +124311,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -124511,7 +124511,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -124736,7 +124736,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -124962,7 +124962,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -125212,7 +125212,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -125471,7 +125471,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -125740,7 +125740,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -126010,7 +126010,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -126301,7 +126301,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -126600,7 +126600,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -126909,7 +126909,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -127219,7 +127219,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -127546,7 +127546,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -127877,7 +127877,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -128218,7 +128218,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -128560,7 +128560,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_cGvBxlvhpkhxlhv:
 
@@ -128707,7 +128707,7 @@ aes_gcm_decrypt_avx512:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	.L_skip_hkeys_precomputation_wDAhpcxxDdecsFn
 	vmovdqu64	640(%rsp),%zmm3
@@ -128988,7 +128988,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -129195,7 +129195,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -129401,7 +129401,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -129608,7 +129608,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -129840,7 +129840,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -130078,7 +130078,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -130322,7 +130322,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -130567,7 +130567,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -130837,7 +130837,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -131115,7 +131115,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -131399,7 +131399,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -131684,7 +131684,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -131990,7 +131990,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -132300,7 +132300,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -132616,7 +132616,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -132933,7 +132933,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_xGuCpnrvibyoyay:
 
@@ -133174,7 +133174,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm6,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -133296,7 +133296,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm6,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -133443,7 +133443,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -133591,7 +133591,7 @@ aes_gcm_decrypt_avx512:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -133761,7 +133761,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%xmm29,%xmm7,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -133940,7 +133940,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%ymm29,%ymm7,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -134129,7 +134129,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -134319,7 +134319,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -134529,7 +134529,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%xmm29,%xmm10,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -134747,7 +134747,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%ymm29,%ymm10,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -134975,7 +134975,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -135204,7 +135204,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -135449,7 +135449,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%xmm29,%xmm11,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -135698,7 +135698,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%ymm29,%ymm11,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -135957,7 +135957,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -136217,7 +136217,7 @@ aes_gcm_decrypt_avx512:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 .L_small_initial_partial_block_fpGgFAenBuAyutw:
 
diff --git a/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S b/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
index dc6e9e9a1..acb4c0086 100644
--- a/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
+++ b/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
@@ -380,17 +380,18 @@ aesni_gcm_decrypt:
 
 	vzeroupper
 
+	movq	16(%rbp),%r12
 	vmovdqu	(%r8),%xmm1
 	addq	$-128,%rsp
 	movl	12(%r8),%ebx
 	leaq	.Lbswap_mask(%rip),%r11
 	leaq	-128(%rcx),%r14
 	movq	$0xf80,%r15
-	vmovdqu	(%r9),%xmm8
+	vmovdqu	(%r12),%xmm8
 	andq	$-128,%rsp
 	vmovdqu	(%r11),%xmm0
 	leaq	128(%rcx),%rcx
-	leaq	32+32(%r9),%r9
+	leaq	32(%r9),%r9
 	movl	240-128(%rcx),%r10d
 	vpshufb	%xmm0,%xmm8,%xmm8
 
@@ -435,6 +436,7 @@ aesni_gcm_decrypt:
 
 	call	_aesni_ctr32_ghash_6x
 
+	movq	16(%rbp),%r12
 	vmovups	%xmm9,-96(%rsi)
 	vmovups	%xmm10,-80(%rsi)
 	vmovups	%xmm11,-64(%rsi)
@@ -443,7 +445,7 @@ aesni_gcm_decrypt:
 	vmovups	%xmm14,-16(%rsi)
 
 	vpshufb	(%r11),%xmm8,%xmm8
-	vmovdqu	%xmm8,-64(%r9)
+	vmovdqu	%xmm8,(%r12)
 
 	vzeroupper
 	leaq	-40(%rbp),%rsp
@@ -655,8 +657,9 @@ aesni_gcm_encrypt:
 
 	call	_aesni_ctr32_6x
 
-	vmovdqu	(%r9),%xmm8
-	leaq	32+32(%r9),%r9
+	movq	16(%rbp),%r12
+	leaq	32(%r9),%r9
+	vmovdqu	(%r12),%xmm8
 	subq	$12,%rdx
 	movq	$192,%rax
 	vpshufb	%xmm0,%xmm8,%xmm8
@@ -834,8 +837,9 @@ aesni_gcm_encrypt:
 	vpclmulqdq	$0x10,%xmm3,%xmm8,%xmm8
 	vpxor	%xmm7,%xmm2,%xmm2
 	vpxor	%xmm2,%xmm8,%xmm8
+	movq	16(%rbp),%r12
 	vpshufb	(%r11),%xmm8,%xmm8
-	vmovdqu	%xmm8,-64(%r9)
+	vmovdqu	%xmm8,(%r12)
 
 	vzeroupper
 	leaq	-40(%rbp),%rsp
diff --git a/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-avx512.S b/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-avx512.S
index 089a588b6..b5a6414e0 100644
--- a/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-avx512.S
+++ b/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-avx512.S
@@ -2048,7 +2048,7 @@ L$setiv_seh_prolog_end:
 	cmpq	$12,%rcx
 	je	iv_len_12_init_IV
 	vpxor	%xmm2,%xmm2,%xmm2
-	leaq	96(%rsi),%r13
+	leaq	80(%rsi),%r13
 	movq	%rdx,%r10
 	movq	%rcx,%r11
 	orq	%r11,%r11
@@ -3978,7 +3978,7 @@ L$aes_gcm_encrypt_128_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -4184,7 +4184,7 @@ L$_next_16_ok_yByFrylbFDFnFCp:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_achfkmnqFwjgbDD
 
@@ -5334,7 +5334,7 @@ L$_16_blocks_ok_azzgqhumkfnyDqm:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -5533,7 +5533,7 @@ L$_16_blocks_ok_yekhBCebufcAiFh:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -5731,7 +5731,7 @@ L$_16_blocks_ok_usjywjwllaabozc:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -5930,7 +5930,7 @@ L$_16_blocks_ok_xobkzaAwcplaFgb:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -6150,7 +6150,7 @@ L$_16_blocks_ok_bpsqdGAhjeggABn:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -6376,7 +6376,7 @@ L$_16_blocks_ok_qmgDCpkysmqcgnB:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -6608,7 +6608,7 @@ L$_16_blocks_ok_jaFyvjvpAfzmwyg:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -6841,7 +6841,7 @@ L$_16_blocks_ok_FbwsrgpDGDmccid:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -7095,7 +7095,7 @@ L$_16_blocks_ok_dtxuExFwmpsGEiG:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -7357,7 +7357,7 @@ L$_16_blocks_ok_damgrhyFxffganz:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -7625,7 +7625,7 @@ L$_16_blocks_ok_gnGEkpgDpmugvpk:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -7894,7 +7894,7 @@ L$_16_blocks_ok_qkecuzhoaAuxmmC:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -8180,7 +8180,7 @@ L$_16_blocks_ok_BjhkFcriuCnuFez:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -8470,7 +8470,7 @@ L$_16_blocks_ok_kGBwgppdvolmGmc:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -8766,7 +8766,7 @@ L$_16_blocks_ok_EBkkfjcEDyEptfo:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -9063,7 +9063,7 @@ L$_16_blocks_ok_BlcvjlyDGzsAttk:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_aGAffhBljtiFsea:
 
@@ -9771,7 +9771,7 @@ L$_16_blocks_ok_dbajrbEcjsFpceD:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -9970,7 +9970,7 @@ L$_16_blocks_ok_kgpAeeaoAnozgEF:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -10168,7 +10168,7 @@ L$_16_blocks_ok_kblsDeoCDCisntD:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -10367,7 +10367,7 @@ L$_16_blocks_ok_eGcBplCnDqdtGiy:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -10587,7 +10587,7 @@ L$_16_blocks_ok_bgsqDFmekFAimag:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -10813,7 +10813,7 @@ L$_16_blocks_ok_oaGuttEwoetbnjp:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -11045,7 +11045,7 @@ L$_16_blocks_ok_FvhiAqmdFpdFmlp:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -11278,7 +11278,7 @@ L$_16_blocks_ok_hwGtCmqmcvackpz:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -11532,7 +11532,7 @@ L$_16_blocks_ok_DDnhmxjezrilein:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -11794,7 +11794,7 @@ L$_16_blocks_ok_zCijhbGCeraapou:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -12062,7 +12062,7 @@ L$_16_blocks_ok_aafwvnrniBpBhGh:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -12331,7 +12331,7 @@ L$_16_blocks_ok_szlfmGmeuofoAra:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -12617,7 +12617,7 @@ L$_16_blocks_ok_knBrwwsfezoBuDz:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -12907,7 +12907,7 @@ L$_16_blocks_ok_xfkAqxxGjDnhBjB:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -13203,7 +13203,7 @@ L$_16_blocks_ok_myvDpkrqCoAukhb:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -13500,7 +13500,7 @@ L$_16_blocks_ok_zEAEoetgkvqojFa:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_erAoEayjDqpuhEu:
 
@@ -14019,7 +14019,7 @@ L$_16_blocks_ok_CfAjeyGwbnghnsF:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -14211,7 +14211,7 @@ L$_16_blocks_ok_sbkoxvmnmihnaig:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -14428,7 +14428,7 @@ L$_16_blocks_ok_zopCCjajxtsjEdG:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -14646,7 +14646,7 @@ L$_16_blocks_ok_utgfjaowycovqbp:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -14884,7 +14884,7 @@ L$_16_blocks_ok_wugoGjfryfqCjFa:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -15131,7 +15131,7 @@ L$_16_blocks_ok_bpCexfjrkbCbhBc:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -15388,7 +15388,7 @@ L$_16_blocks_ok_ifByzBizpdBxFnD:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -15646,7 +15646,7 @@ L$_16_blocks_ok_cjwhqEvpCfjCcEa:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -15921,7 +15921,7 @@ L$_16_blocks_ok_xiomBjDmsdhvtig:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -16204,7 +16204,7 @@ L$_16_blocks_ok_cEyikykuFcExlBe:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -16497,7 +16497,7 @@ L$_16_blocks_ok_gsBoGfzrmwqlomo:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -16791,7 +16791,7 @@ L$_16_blocks_ok_CAvgqgqjrtonFws:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -17098,7 +17098,7 @@ L$_16_blocks_ok_zqBffksAbxFoiFr:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -17409,7 +17409,7 @@ L$_16_blocks_ok_mBiifnhuGFDpfDy:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -17730,7 +17730,7 @@ L$_16_blocks_ok_zDGlqyFvuaglkeB:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -18052,7 +18052,7 @@ L$_16_blocks_ok_uwtqqfwgewBdjhg:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_ifytbdtuElzEqkG:
 
@@ -18199,7 +18199,7 @@ L$_message_below_32_blocks_pzwgkGgbplFqzaB:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_mgjxphyGhnqeEta
 	vmovdqu64	640(%rsp),%zmm3
@@ -18472,7 +18472,7 @@ L$_16_blocks_ok_zCjdttbyboeGxFb:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -18671,7 +18671,7 @@ L$_16_blocks_ok_fhFvhqpaozkgyzE:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -18869,7 +18869,7 @@ L$_16_blocks_ok_hjBmpccGhruhCnv:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -19068,7 +19068,7 @@ L$_16_blocks_ok_bBrsEuBDcsAcscn:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -19288,7 +19288,7 @@ L$_16_blocks_ok_nygdGeFptfwzvpw:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -19514,7 +19514,7 @@ L$_16_blocks_ok_adtbeheumiAkmlw:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -19746,7 +19746,7 @@ L$_16_blocks_ok_aDdoAskralEtovy:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -19979,7 +19979,7 @@ L$_16_blocks_ok_hjBdmnrbjjzAbzC:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -20233,7 +20233,7 @@ L$_16_blocks_ok_szBmuqzxwjxBawF:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -20495,7 +20495,7 @@ L$_16_blocks_ok_xhlcvtlyGczsicp:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -20763,7 +20763,7 @@ L$_16_blocks_ok_CkhBiupnDlzBoGx:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -21032,7 +21032,7 @@ L$_16_blocks_ok_svvcxnisrDiilsD:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -21318,7 +21318,7 @@ L$_16_blocks_ok_oDDmorFzihnoffg:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -21608,7 +21608,7 @@ L$_16_blocks_ok_liipuseeafvnkfi:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -21904,7 +21904,7 @@ L$_16_blocks_ok_cuygxmuthGeaeby:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -22201,7 +22201,7 @@ L$_16_blocks_ok_AgkAgztElEpGqer:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_eruDeitqttsEEhG:
 
@@ -22434,7 +22434,7 @@ L$_small_initial_num_blocks_is_1_Arjlgemsqpaxhfj:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -22548,7 +22548,7 @@ L$_small_initial_num_blocks_is_2_Arjlgemsqpaxhfj:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -22687,7 +22687,7 @@ L$_small_initial_num_blocks_is_3_Arjlgemsqpaxhfj:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -22827,7 +22827,7 @@ L$_small_initial_num_blocks_is_4_Arjlgemsqpaxhfj:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -22985,7 +22985,7 @@ L$_small_initial_num_blocks_is_5_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%xmm29,%xmm3,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -23152,7 +23152,7 @@ L$_small_initial_num_blocks_is_6_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%ymm29,%ymm3,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -23329,7 +23329,7 @@ L$_small_initial_num_blocks_is_7_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -23507,7 +23507,7 @@ L$_small_initial_num_blocks_is_8_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -23701,7 +23701,7 @@ L$_small_initial_num_blocks_is_9_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%xmm29,%xmm4,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -23903,7 +23903,7 @@ L$_small_initial_num_blocks_is_10_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%ymm29,%ymm4,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -24115,7 +24115,7 @@ L$_small_initial_num_blocks_is_11_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -24328,7 +24328,7 @@ L$_small_initial_num_blocks_is_12_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -24553,7 +24553,7 @@ L$_small_initial_num_blocks_is_13_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%xmm29,%xmm5,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -24782,7 +24782,7 @@ L$_small_initial_num_blocks_is_14_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%ymm29,%ymm5,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -25021,7 +25021,7 @@ L$_small_initial_num_blocks_is_15_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -25261,7 +25261,7 @@ L$_small_initial_num_blocks_is_16_Arjlgemsqpaxhfj:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_icDnnqvChyBsuli:
 
@@ -25365,7 +25365,7 @@ L$aes_gcm_encrypt_192_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -25581,7 +25581,7 @@ L$_next_16_ok_lelEEvckqsGkuGn:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_EclAduckuFhozAl
 
@@ -26778,7 +26778,7 @@ L$_16_blocks_ok_czjqmrcuGbkhjtu:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -26981,7 +26981,7 @@ L$_16_blocks_ok_tCDuaqxntEtBCqr:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -27183,7 +27183,7 @@ L$_16_blocks_ok_AxfvkflbDBEFEmp:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -27386,7 +27386,7 @@ L$_16_blocks_ok_xkpgotEfuidCEnC:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -27612,7 +27612,7 @@ L$_16_blocks_ok_pxAyyxhuewraobh:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -27844,7 +27844,7 @@ L$_16_blocks_ok_rlBkdasaFkzjByu:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -28082,7 +28082,7 @@ L$_16_blocks_ok_keqkskoubnuElfA:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -28321,7 +28321,7 @@ L$_16_blocks_ok_hwCFDDlqwBqrdyx:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -28583,7 +28583,7 @@ L$_16_blocks_ok_ybEEnfpGmbdDyaC:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -28853,7 +28853,7 @@ L$_16_blocks_ok_opfbCaznAiAepnv:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -29129,7 +29129,7 @@ L$_16_blocks_ok_qxFolltldGnscDg:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -29406,7 +29406,7 @@ L$_16_blocks_ok_nvmdGffBdmtukpe:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -29702,7 +29702,7 @@ L$_16_blocks_ok_zGEqEwwbyegFygC:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -30002,7 +30002,7 @@ L$_16_blocks_ok_hGfdBnfArvqgnDo:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -30308,7 +30308,7 @@ L$_16_blocks_ok_AhbxhfFAjAuyeFk:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -30615,7 +30615,7 @@ L$_16_blocks_ok_njybzcioxuyaaaD:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_bCffyflcoaBxCzy:
 
@@ -31349,7 +31349,7 @@ L$_16_blocks_ok_BvAqyjatyidEnnt:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -31552,7 +31552,7 @@ L$_16_blocks_ok_nbawutokAutAqum:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -31754,7 +31754,7 @@ L$_16_blocks_ok_CwkxGelBrtqaaxv:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -31957,7 +31957,7 @@ L$_16_blocks_ok_gFpynBlybCeGalG:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -32183,7 +32183,7 @@ L$_16_blocks_ok_xwErcCwicbEwFqC:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -32415,7 +32415,7 @@ L$_16_blocks_ok_baDecrAptncCCuf:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -32653,7 +32653,7 @@ L$_16_blocks_ok_iltrljarpeDchus:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -32892,7 +32892,7 @@ L$_16_blocks_ok_eyzjCojxduufqEi:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -33154,7 +33154,7 @@ L$_16_blocks_ok_bwdCwgCmnErFeDe:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -33424,7 +33424,7 @@ L$_16_blocks_ok_eGGpBsfFnpwwbub:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -33700,7 +33700,7 @@ L$_16_blocks_ok_eddhoEuAgjbBjFF:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -33977,7 +33977,7 @@ L$_16_blocks_ok_bfsFAnmADrmmioq:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -34273,7 +34273,7 @@ L$_16_blocks_ok_eqddxBoxqiwCsny:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -34573,7 +34573,7 @@ L$_16_blocks_ok_DAGxccpeauyqpCa:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -34879,7 +34879,7 @@ L$_16_blocks_ok_xrzdkvEbdpatlsn:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -35186,7 +35186,7 @@ L$_16_blocks_ok_nhkzxmwsyGuskoi:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_ECtspjaqpoxwhnx:
 
@@ -35720,7 +35720,7 @@ L$_16_blocks_ok_ycAFtgAvrzFpmud:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -35916,7 +35916,7 @@ L$_16_blocks_ok_unaFqvbBnCelmgG:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -36137,7 +36137,7 @@ L$_16_blocks_ok_FzufylrxyerzBEy:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -36359,7 +36359,7 @@ L$_16_blocks_ok_FtupvahihsnvuAd:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -36603,7 +36603,7 @@ L$_16_blocks_ok_uBhGhomDazsjBak:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -36856,7 +36856,7 @@ L$_16_blocks_ok_mBfhrGpovoncBkc:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -37119,7 +37119,7 @@ L$_16_blocks_ok_FvpewBABrfyByvd:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -37383,7 +37383,7 @@ L$_16_blocks_ok_FsoptjzAkrqyAAr:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -37666,7 +37666,7 @@ L$_16_blocks_ok_iABBxfvotBEkECx:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -37957,7 +37957,7 @@ L$_16_blocks_ok_jEngtqCkuniGdjp:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -38258,7 +38258,7 @@ L$_16_blocks_ok_beoirgaAxslixji:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -38560,7 +38560,7 @@ L$_16_blocks_ok_sxrCycfBickEpCs:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -38877,7 +38877,7 @@ L$_16_blocks_ok_sesGGmqiCkypotq:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -39198,7 +39198,7 @@ L$_16_blocks_ok_jqifyxAoeoxkDuE:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -39529,7 +39529,7 @@ L$_16_blocks_ok_CBqhusrmEugbwks:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -39861,7 +39861,7 @@ L$_16_blocks_ok_hDfCleGEdmpzBiw:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_pybhdxzahdqcprl:
 
@@ -40008,7 +40008,7 @@ L$_message_below_32_blocks_jzxBnczDBxGvzop:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_dzmCrsBiciGnliE
 	vmovdqu64	640(%rsp),%zmm3
@@ -40285,7 +40285,7 @@ L$_16_blocks_ok_BsFiEfmuvxGEGuk:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -40488,7 +40488,7 @@ L$_16_blocks_ok_DrefbggoCuhFosm:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -40690,7 +40690,7 @@ L$_16_blocks_ok_oskEeEmCEGeqECv:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -40893,7 +40893,7 @@ L$_16_blocks_ok_aAxBGtfyfEadAkB:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -41119,7 +41119,7 @@ L$_16_blocks_ok_bpEikxmsheidfwq:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -41351,7 +41351,7 @@ L$_16_blocks_ok_otEmDDixbpFEmvy:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -41589,7 +41589,7 @@ L$_16_blocks_ok_kEvFawDBkeclidj:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -41828,7 +41828,7 @@ L$_16_blocks_ok_nfBegzmtymkjkuE:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -42090,7 +42090,7 @@ L$_16_blocks_ok_zjmfGFrkFzfxxez:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -42360,7 +42360,7 @@ L$_16_blocks_ok_BvDkzdlGxbqBdwD:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -42636,7 +42636,7 @@ L$_16_blocks_ok_wfjezxDvGpDnoFf:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -42913,7 +42913,7 @@ L$_16_blocks_ok_pbckDbEtDdqavpn:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -43209,7 +43209,7 @@ L$_16_blocks_ok_oCotpBuspdAtjpe:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -43509,7 +43509,7 @@ L$_16_blocks_ok_bbvjuqrsjgdyCBn:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -43815,7 +43815,7 @@ L$_16_blocks_ok_GriwFAotfyoEekC:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -44122,7 +44122,7 @@ L$_16_blocks_ok_sjAcjwAAtCgmwjr:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_ayefrejzGqbkfya:
 
@@ -44359,7 +44359,7 @@ L$_small_initial_num_blocks_is_1_wjgmgrFcljfrexe:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -44477,7 +44477,7 @@ L$_small_initial_num_blocks_is_2_wjgmgrFcljfrexe:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -44620,7 +44620,7 @@ L$_small_initial_num_blocks_is_3_wjgmgrFcljfrexe:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -44764,7 +44764,7 @@ L$_small_initial_num_blocks_is_4_wjgmgrFcljfrexe:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -44928,7 +44928,7 @@ L$_small_initial_num_blocks_is_5_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%xmm29,%xmm3,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -45101,7 +45101,7 @@ L$_small_initial_num_blocks_is_6_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%ymm29,%ymm3,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -45284,7 +45284,7 @@ L$_small_initial_num_blocks_is_7_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -45468,7 +45468,7 @@ L$_small_initial_num_blocks_is_8_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -45670,7 +45670,7 @@ L$_small_initial_num_blocks_is_9_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%xmm29,%xmm4,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -45880,7 +45880,7 @@ L$_small_initial_num_blocks_is_10_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%ymm29,%ymm4,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -46100,7 +46100,7 @@ L$_small_initial_num_blocks_is_11_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -46321,7 +46321,7 @@ L$_small_initial_num_blocks_is_12_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -46556,7 +46556,7 @@ L$_small_initial_num_blocks_is_13_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%xmm29,%xmm5,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -46795,7 +46795,7 @@ L$_small_initial_num_blocks_is_14_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%ymm29,%ymm5,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -47044,7 +47044,7 @@ L$_small_initial_num_blocks_is_15_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -47294,7 +47294,7 @@ L$_small_initial_num_blocks_is_16_wjgmgrFcljfrexe:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_qxCpdapFxyCuqwj:
 
@@ -47398,7 +47398,7 @@ L$aes_gcm_encrypt_256_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -47624,7 +47624,7 @@ L$_next_16_ok_FolitFcvmzDtzbD:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_slhsqgEufGclFec
 
@@ -48871,7 +48871,7 @@ L$_16_blocks_ok_igvodhikativhxs:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -49078,7 +49078,7 @@ L$_16_blocks_ok_vsprwaoekjwbkng:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -49284,7 +49284,7 @@ L$_16_blocks_ok_pdiFfjCElAtekEv:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -49491,7 +49491,7 @@ L$_16_blocks_ok_giftEyoltvfgggA:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -49723,7 +49723,7 @@ L$_16_blocks_ok_orpkewzlnxCGshz:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -49961,7 +49961,7 @@ L$_16_blocks_ok_orictFjAdfigdzk:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -50205,7 +50205,7 @@ L$_16_blocks_ok_ivtabDnDqnrGEcy:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -50450,7 +50450,7 @@ L$_16_blocks_ok_uBiojDdgtEoAfGd:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -50720,7 +50720,7 @@ L$_16_blocks_ok_FqperxgfhBwCqDo:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -50998,7 +50998,7 @@ L$_16_blocks_ok_bvimoanuboioxom:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -51282,7 +51282,7 @@ L$_16_blocks_ok_DcdigDqdkAmpala:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -51567,7 +51567,7 @@ L$_16_blocks_ok_ijmafkyicqbAgov:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -51873,7 +51873,7 @@ L$_16_blocks_ok_xewjdgAADiucjCd:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -52183,7 +52183,7 @@ L$_16_blocks_ok_uxvkthhndspgdct:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -52499,7 +52499,7 @@ L$_16_blocks_ok_fdeajBtuhuyobdz:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -52816,7 +52816,7 @@ L$_16_blocks_ok_mxnyyrjuxpBhloh:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_CFywctAlrBmkufB:
 
@@ -53578,7 +53578,7 @@ L$_16_blocks_ok_alDzwCfDlrwfuue:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -53785,7 +53785,7 @@ L$_16_blocks_ok_nCqcfaumojsjgbp:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -53991,7 +53991,7 @@ L$_16_blocks_ok_uwpbmorybawstbl:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -54198,7 +54198,7 @@ L$_16_blocks_ok_vadkquwycFnaotd:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -54430,7 +54430,7 @@ L$_16_blocks_ok_aFkFaFcofvloukl:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -54668,7 +54668,7 @@ L$_16_blocks_ok_hyGBuzayqDhhsut:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -54912,7 +54912,7 @@ L$_16_blocks_ok_wfwrxhyCBsGqfaa:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -55157,7 +55157,7 @@ L$_16_blocks_ok_nwCspduhyDCpabc:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -55427,7 +55427,7 @@ L$_16_blocks_ok_FtfeaayDywckyfd:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -55705,7 +55705,7 @@ L$_16_blocks_ok_rwkpzgCdusgbwpC:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -55989,7 +55989,7 @@ L$_16_blocks_ok_lwGByppsljaznxt:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -56274,7 +56274,7 @@ L$_16_blocks_ok_jbqznyehrlCBlqk:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -56580,7 +56580,7 @@ L$_16_blocks_ok_zfoiakgFjhncFgz:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -56890,7 +56890,7 @@ L$_16_blocks_ok_boaouDrBeEmAnwp:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -57206,7 +57206,7 @@ L$_16_blocks_ok_mFdcfdxbaoeAcmw:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -57523,7 +57523,7 @@ L$_16_blocks_ok_dhDlEwplftmrFtf:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_iuDhkykBcvvzBFb:
 
@@ -58073,7 +58073,7 @@ L$_16_blocks_ok_cmjbanhfxFrrojy:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -58273,7 +58273,7 @@ L$_16_blocks_ok_EgjyoropybwcGcn:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -58498,7 +58498,7 @@ L$_16_blocks_ok_wGGmGvscmpGfnny:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -58724,7 +58724,7 @@ L$_16_blocks_ok_lnowafuogaacgct:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -58974,7 +58974,7 @@ L$_16_blocks_ok_trmgpGgtzmsExiu:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -59233,7 +59233,7 @@ L$_16_blocks_ok_FwaeBcDAewBtpAB:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -59502,7 +59502,7 @@ L$_16_blocks_ok_AnyscuqxAspkzsl:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -59772,7 +59772,7 @@ L$_16_blocks_ok_cgqpkbbBmprdEnv:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -60063,7 +60063,7 @@ L$_16_blocks_ok_ovcajrDEfpdjwcF:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -60362,7 +60362,7 @@ L$_16_blocks_ok_xyisBwjDghCtkcq:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -60671,7 +60671,7 @@ L$_16_blocks_ok_oCaueqhtnkiqikA:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -60981,7 +60981,7 @@ L$_16_blocks_ok_xwjsvxAnBhmckaz:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -61308,7 +61308,7 @@ L$_16_blocks_ok_jfgktdduAaBgqFv:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -61639,7 +61639,7 @@ L$_16_blocks_ok_xdtrxodfgwcifbm:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -61980,7 +61980,7 @@ L$_16_blocks_ok_FrBtEqtdGyajfFu:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -62322,7 +62322,7 @@ L$_16_blocks_ok_ofhxurlakbuiiab:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_niAfluBnEgrukbj:
 
@@ -62469,7 +62469,7 @@ L$_message_below_32_blocks_ralurfzeatcGxDF:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_BiAvfDwrflaDzBx
 	vmovdqu64	640(%rsp),%zmm3
@@ -62750,7 +62750,7 @@ L$_16_blocks_ok_zDsbocmrpEvnicC:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -62957,7 +62957,7 @@ L$_16_blocks_ok_fkqtFBuohiwoapu:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -63163,7 +63163,7 @@ L$_16_blocks_ok_myfxreEhmAEiFvd:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -63370,7 +63370,7 @@ L$_16_blocks_ok_EshcbGrbbBjGmFs:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -63602,7 +63602,7 @@ L$_16_blocks_ok_rBzncCcAACDmBwu:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%xmm29,%xmm3,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -63840,7 +63840,7 @@ L$_16_blocks_ok_yghnlDweoeGyiyD:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%ymm29,%ymm3,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -64084,7 +64084,7 @@ L$_16_blocks_ok_stoalvbzsyrkrBC:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -64329,7 +64329,7 @@ L$_16_blocks_ok_miFDzcCBFGrssiv:
 	vpshufb	%zmm29,%zmm0,%zmm17
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -64599,7 +64599,7 @@ L$_16_blocks_ok_lkCdskAdsidpkuw:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%xmm29,%xmm4,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -64877,7 +64877,7 @@ L$_16_blocks_ok_hktAeBlvDcCnios:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%ymm29,%ymm4,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -65161,7 +65161,7 @@ L$_16_blocks_ok_bblFcfwEgdzswCm:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -65446,7 +65446,7 @@ L$_16_blocks_ok_qmmgmehghErCGvF:
 	vpshufb	%zmm29,%zmm3,%zmm19
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -65752,7 +65752,7 @@ L$_16_blocks_ok_dulzkutdgjakGvB:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%xmm29,%xmm5,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -66062,7 +66062,7 @@ L$_16_blocks_ok_nntbrGkellunBas:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%ymm29,%ymm5,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -66378,7 +66378,7 @@ L$_16_blocks_ok_gqGDtzmCceFkfal:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -66695,7 +66695,7 @@ L$_16_blocks_ok_nnArmAxpgvlqCpA:
 	vpshufb	%zmm29,%zmm4,%zmm20
 	vpshufb	%zmm29,%zmm5,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_uqdvluxFgGqdFqv:
 
@@ -66936,7 +66936,7 @@ L$_small_initial_num_blocks_is_1_hdjaAabmubhzgrE:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm0,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -67058,7 +67058,7 @@ L$_small_initial_num_blocks_is_2_hdjaAabmubhzgrE:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm0,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -67205,7 +67205,7 @@ L$_small_initial_num_blocks_is_3_hdjaAabmubhzgrE:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -67353,7 +67353,7 @@ L$_small_initial_num_blocks_is_4_hdjaAabmubhzgrE:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -67523,7 +67523,7 @@ L$_small_initial_num_blocks_is_5_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%xmm29,%xmm3,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -67702,7 +67702,7 @@ L$_small_initial_num_blocks_is_6_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%ymm29,%ymm3,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -67891,7 +67891,7 @@ L$_small_initial_num_blocks_is_7_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -68081,7 +68081,7 @@ L$_small_initial_num_blocks_is_8_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm0,%zmm6
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -68291,7 +68291,7 @@ L$_small_initial_num_blocks_is_9_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%xmm29,%xmm4,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -68509,7 +68509,7 @@ L$_small_initial_num_blocks_is_10_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%ymm29,%ymm4,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -68737,7 +68737,7 @@ L$_small_initial_num_blocks_is_11_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -68966,7 +68966,7 @@ L$_small_initial_num_blocks_is_12_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm3,%zmm7
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -69211,7 +69211,7 @@ L$_small_initial_num_blocks_is_13_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%xmm29,%xmm5,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -69460,7 +69460,7 @@ L$_small_initial_num_blocks_is_14_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%ymm29,%ymm5,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -69719,7 +69719,7 @@ L$_small_initial_num_blocks_is_15_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -69979,7 +69979,7 @@ L$_small_initial_num_blocks_is_16_hdjaAabmubhzgrE:
 	vpshufb	%zmm29,%zmm4,%zmm10
 	vpshufb	%zmm29,%zmm5,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_mlladecCGcaEame:
 
@@ -70173,7 +70173,7 @@ L$aes_gcm_decrypt_128_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -70379,7 +70379,7 @@ L$_next_16_ok_DkBvliAEspzoabf:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_yDAnEECuuGxfwvr
 
@@ -71529,7 +71529,7 @@ L$_16_blocks_ok_lClAAkfGiaxqtqb:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -71728,7 +71728,7 @@ L$_16_blocks_ok_dxfcclgCzfhujoB:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -71926,7 +71926,7 @@ L$_16_blocks_ok_GzfdDtolkqgqFel:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -72125,7 +72125,7 @@ L$_16_blocks_ok_tFlldonsxgiBeEi:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -72345,7 +72345,7 @@ L$_16_blocks_ok_lAbhcGDwivukqtE:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -72571,7 +72571,7 @@ L$_16_blocks_ok_xsoFcrclantxpei:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -72803,7 +72803,7 @@ L$_16_blocks_ok_xeeduBscFEzvdva:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -73036,7 +73036,7 @@ L$_16_blocks_ok_kBlrofzDjhoFnxv:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -73290,7 +73290,7 @@ L$_16_blocks_ok_whsqDBkDGaknbAC:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -73552,7 +73552,7 @@ L$_16_blocks_ok_GbBbalFokmrvvlx:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -73820,7 +73820,7 @@ L$_16_blocks_ok_ldEsDEbywdmplpt:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -74089,7 +74089,7 @@ L$_16_blocks_ok_rAsEscwvsFrjwEn:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -74375,7 +74375,7 @@ L$_16_blocks_ok_yuCmdhwEwEhlsnq:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -74665,7 +74665,7 @@ L$_16_blocks_ok_oEwrswoqGyjlsqe:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -74961,7 +74961,7 @@ L$_16_blocks_ok_CtjhmwDvAgBsAry:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -75258,7 +75258,7 @@ L$_16_blocks_ok_ejwsGBcDyFeryCA:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_vqjlBldpifEzCAi:
 
@@ -75966,7 +75966,7 @@ L$_16_blocks_ok_kounvuokEjmfgDl:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -76165,7 +76165,7 @@ L$_16_blocks_ok_GkcjorkhgDBFApE:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -76363,7 +76363,7 @@ L$_16_blocks_ok_GlGoAfCtaxDnccC:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -76562,7 +76562,7 @@ L$_16_blocks_ok_zwfpgGyijsBkpeE:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -76782,7 +76782,7 @@ L$_16_blocks_ok_CizAwbkEgozyasa:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -77008,7 +77008,7 @@ L$_16_blocks_ok_yuzbpkwFyzjuBAz:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -77240,7 +77240,7 @@ L$_16_blocks_ok_fDccaFllCyjzgaw:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -77473,7 +77473,7 @@ L$_16_blocks_ok_yuxjCAwGGjlocDt:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -77727,7 +77727,7 @@ L$_16_blocks_ok_FrborCeuBByFkga:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -77989,7 +77989,7 @@ L$_16_blocks_ok_uqpvEzAtlprmDsg:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -78257,7 +78257,7 @@ L$_16_blocks_ok_wyBrnxyfcdFguiF:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -78526,7 +78526,7 @@ L$_16_blocks_ok_nbfsGzmFjniAhpc:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -78812,7 +78812,7 @@ L$_16_blocks_ok_BlpixnjkGtBtzBl:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -79102,7 +79102,7 @@ L$_16_blocks_ok_nlkisqljGgnlewr:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -79398,7 +79398,7 @@ L$_16_blocks_ok_FewkqxwDmrjetmG:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -79695,7 +79695,7 @@ L$_16_blocks_ok_hEoxzbghGBmpbpw:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_usEFihDgqghhogg:
 
@@ -80214,7 +80214,7 @@ L$_16_blocks_ok_iCcBbEaCnnBtiGz:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -80406,7 +80406,7 @@ L$_16_blocks_ok_spdFufbAAGcAxFf:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -80623,7 +80623,7 @@ L$_16_blocks_ok_sBAazunogzDjqho:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -80841,7 +80841,7 @@ L$_16_blocks_ok_sekyjhofosAtkyB:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -81079,7 +81079,7 @@ L$_16_blocks_ok_zdkGskjaniDljeq:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -81326,7 +81326,7 @@ L$_16_blocks_ok_mrylAcnDjuqklnd:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -81583,7 +81583,7 @@ L$_16_blocks_ok_ektccsvjwlnFwnw:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -81841,7 +81841,7 @@ L$_16_blocks_ok_GGmuDhkjBtqxcEd:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -82116,7 +82116,7 @@ L$_16_blocks_ok_lDwlixsAzhAgDkG:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -82399,7 +82399,7 @@ L$_16_blocks_ok_BbbzknmqtuDuEfg:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -82692,7 +82692,7 @@ L$_16_blocks_ok_jatgakEfrDmqCyG:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -82986,7 +82986,7 @@ L$_16_blocks_ok_tovGfhABebkuFEt:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -83293,7 +83293,7 @@ L$_16_blocks_ok_ChCrwqCswoEpicz:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -83604,7 +83604,7 @@ L$_16_blocks_ok_GzibzgsizEbkyAE:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -83925,7 +83925,7 @@ L$_16_blocks_ok_DExqfkaBzzhxtrd:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -84247,7 +84247,7 @@ L$_16_blocks_ok_fanaekDAulfkhcb:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_pCDjmBApGDgFGhw:
 
@@ -84394,7 +84394,7 @@ L$_message_below_32_blocks_icBhFhCkojGgnBc:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_lurmstfAeByrDpz
 	vmovdqu64	640(%rsp),%zmm3
@@ -84667,7 +84667,7 @@ L$_16_blocks_ok_erjqEcdgnsabCAp:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -84866,7 +84866,7 @@ L$_16_blocks_ok_AibviGpsltwhwck:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -85064,7 +85064,7 @@ L$_16_blocks_ok_cwyoDiaxggCofzt:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -85263,7 +85263,7 @@ L$_16_blocks_ok_fqeFwlbvdGyejoA:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -85483,7 +85483,7 @@ L$_16_blocks_ok_cjnavuxfcgGzzCb:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -85709,7 +85709,7 @@ L$_16_blocks_ok_DndbknmyrzkriDg:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -85941,7 +85941,7 @@ L$_16_blocks_ok_jtGaGqFaokuwcFo:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -86174,7 +86174,7 @@ L$_16_blocks_ok_BCegvazduGiwBqv:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -86428,7 +86428,7 @@ L$_16_blocks_ok_nGczFFdvDDdbdAl:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -86690,7 +86690,7 @@ L$_16_blocks_ok_oulxbBotdhvdFbg:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -86958,7 +86958,7 @@ L$_16_blocks_ok_nCertFgkfoCxtun:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -87227,7 +87227,7 @@ L$_16_blocks_ok_DtwkcFbdCfdcCrh:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -87513,7 +87513,7 @@ L$_16_blocks_ok_ndumifgEEuiqDiF:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -87803,7 +87803,7 @@ L$_16_blocks_ok_osDGzgifEhqjECm:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -88099,7 +88099,7 @@ L$_16_blocks_ok_tiCBFudBnEgekda:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -88396,7 +88396,7 @@ L$_16_blocks_ok_ennneCoBjzBsijF:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_qbevliloqkkkFsD:
 
@@ -88629,7 +88629,7 @@ L$_small_initial_num_blocks_is_1_fAioGdenAmmvupb:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm6,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -88743,7 +88743,7 @@ L$_small_initial_num_blocks_is_2_fAioGdenAmmvupb:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm6,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -88882,7 +88882,7 @@ L$_small_initial_num_blocks_is_3_fAioGdenAmmvupb:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -89022,7 +89022,7 @@ L$_small_initial_num_blocks_is_4_fAioGdenAmmvupb:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -89180,7 +89180,7 @@ L$_small_initial_num_blocks_is_5_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%xmm29,%xmm7,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -89347,7 +89347,7 @@ L$_small_initial_num_blocks_is_6_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%ymm29,%ymm7,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -89524,7 +89524,7 @@ L$_small_initial_num_blocks_is_7_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -89702,7 +89702,7 @@ L$_small_initial_num_blocks_is_8_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -89896,7 +89896,7 @@ L$_small_initial_num_blocks_is_9_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%xmm29,%xmm10,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -90098,7 +90098,7 @@ L$_small_initial_num_blocks_is_10_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%ymm29,%ymm10,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -90310,7 +90310,7 @@ L$_small_initial_num_blocks_is_11_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -90523,7 +90523,7 @@ L$_small_initial_num_blocks_is_12_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -90748,7 +90748,7 @@ L$_small_initial_num_blocks_is_13_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%xmm29,%xmm11,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -90977,7 +90977,7 @@ L$_small_initial_num_blocks_is_14_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%ymm29,%ymm11,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -91216,7 +91216,7 @@ L$_small_initial_num_blocks_is_15_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -91456,7 +91456,7 @@ L$_small_initial_num_blocks_is_16_fAioGdenAmmvupb:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_fvqkmnelfBwdflt:
 
@@ -91560,7 +91560,7 @@ L$aes_gcm_decrypt_192_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -91776,7 +91776,7 @@ L$_next_16_ok_bmCGDqjpElhfFfq:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_mbihlziFEFsDoGE
 
@@ -92973,7 +92973,7 @@ L$_16_blocks_ok_nlADBBgdbvxiiEb:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -93176,7 +93176,7 @@ L$_16_blocks_ok_uvnjGlBDyvrfirm:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -93378,7 +93378,7 @@ L$_16_blocks_ok_FgovsDdCfEGrkbF:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -93581,7 +93581,7 @@ L$_16_blocks_ok_DlimwiDzackronx:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -93807,7 +93807,7 @@ L$_16_blocks_ok_qGrgsssqhFxDdtg:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -94039,7 +94039,7 @@ L$_16_blocks_ok_ufEGEnqpAFAEymx:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -94277,7 +94277,7 @@ L$_16_blocks_ok_xgpGrqoEEApwzGE:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -94516,7 +94516,7 @@ L$_16_blocks_ok_DBafwcnsvcxAbsv:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -94778,7 +94778,7 @@ L$_16_blocks_ok_muonozkGretEzbg:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -95048,7 +95048,7 @@ L$_16_blocks_ok_tcxAtedExcFvxwb:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -95324,7 +95324,7 @@ L$_16_blocks_ok_oCyoemhjBbobeot:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -95601,7 +95601,7 @@ L$_16_blocks_ok_rechbAAmkFuppsn:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -95897,7 +95897,7 @@ L$_16_blocks_ok_llFkwrFhuxfvsGD:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -96197,7 +96197,7 @@ L$_16_blocks_ok_euGgDuqlvgCFoFG:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -96503,7 +96503,7 @@ L$_16_blocks_ok_pFsoEbjdpyaFdzt:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -96810,7 +96810,7 @@ L$_16_blocks_ok_lxzkkenajCqycbF:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_rerkgBbyampldto:
 
@@ -97544,7 +97544,7 @@ L$_16_blocks_ok_mklqBGCbyBjeEom:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -97747,7 +97747,7 @@ L$_16_blocks_ok_ADzEaGzEEnztayt:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -97949,7 +97949,7 @@ L$_16_blocks_ok_lcaBxDbeGChbeFD:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -98152,7 +98152,7 @@ L$_16_blocks_ok_pawpbdkivckxDwC:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -98378,7 +98378,7 @@ L$_16_blocks_ok_DaxgvFmGcDpdBDr:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -98610,7 +98610,7 @@ L$_16_blocks_ok_GCBuEfGizfDEkbf:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -98848,7 +98848,7 @@ L$_16_blocks_ok_sxxwCglaApctqvC:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -99087,7 +99087,7 @@ L$_16_blocks_ok_CnnuddjEBnFGdsj:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -99349,7 +99349,7 @@ L$_16_blocks_ok_jwawBbqsGrnbEEd:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -99619,7 +99619,7 @@ L$_16_blocks_ok_bEhtipvqjwytqAA:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -99895,7 +99895,7 @@ L$_16_blocks_ok_peywgEttBymhlkG:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -100172,7 +100172,7 @@ L$_16_blocks_ok_pfftEtegsrsinbs:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -100468,7 +100468,7 @@ L$_16_blocks_ok_hoEpuvlFtAdDDCj:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -100768,7 +100768,7 @@ L$_16_blocks_ok_kDibsGzbehdlyln:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -101074,7 +101074,7 @@ L$_16_blocks_ok_vejCgbGykbnkAnl:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -101381,7 +101381,7 @@ L$_16_blocks_ok_oEmrkvwdwsmBgef:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_emEtFnwcsvbsGee:
 
@@ -101915,7 +101915,7 @@ L$_16_blocks_ok_EztzACczExrozqe:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -102111,7 +102111,7 @@ L$_16_blocks_ok_ddpheeylmysesqA:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -102332,7 +102332,7 @@ L$_16_blocks_ok_vAzgdsEEohhszlv:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -102554,7 +102554,7 @@ L$_16_blocks_ok_ciiDnbwsdfFhyEA:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -102798,7 +102798,7 @@ L$_16_blocks_ok_AGvFmhBetCxAviv:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -103051,7 +103051,7 @@ L$_16_blocks_ok_sjympigbCCDhsDn:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -103314,7 +103314,7 @@ L$_16_blocks_ok_puBiejaewnoDvka:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -103578,7 +103578,7 @@ L$_16_blocks_ok_eaeCeiduedGDdDq:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -103861,7 +103861,7 @@ L$_16_blocks_ok_zgrBucdeiivwwje:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -104152,7 +104152,7 @@ L$_16_blocks_ok_rqjyEzzCiBijwho:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -104453,7 +104453,7 @@ L$_16_blocks_ok_udirAnChEpiDCdb:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -104755,7 +104755,7 @@ L$_16_blocks_ok_nCrveguADGnpgFu:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -105072,7 +105072,7 @@ L$_16_blocks_ok_FuAeDsuGfAcCbnh:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -105393,7 +105393,7 @@ L$_16_blocks_ok_FvEhyckDsphilDy:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -105724,7 +105724,7 @@ L$_16_blocks_ok_lpConoqwylkjlwn:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -106056,7 +106056,7 @@ L$_16_blocks_ok_vhaFwxkrByAhtie:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_ciyykzjryphtjAc:
 
@@ -106203,7 +106203,7 @@ L$_message_below_32_blocks_efvnrtvwAsfehEC:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_yBFabBiEpjEBBsr
 	vmovdqu64	640(%rsp),%zmm3
@@ -106480,7 +106480,7 @@ L$_16_blocks_ok_yqjovttCDEvpyyd:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -106683,7 +106683,7 @@ L$_16_blocks_ok_dunlemEBzoyBoxa:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -106885,7 +106885,7 @@ L$_16_blocks_ok_gknxnDbcehnficG:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -107088,7 +107088,7 @@ L$_16_blocks_ok_vkChDblsuoFkgEp:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -107314,7 +107314,7 @@ L$_16_blocks_ok_aGCpdetktlAtivE:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -107546,7 +107546,7 @@ L$_16_blocks_ok_DlEhcmhmAqggthl:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -107784,7 +107784,7 @@ L$_16_blocks_ok_szxcAmcFcFxFikD:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -108023,7 +108023,7 @@ L$_16_blocks_ok_nGgmonbofwfdiqp:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -108285,7 +108285,7 @@ L$_16_blocks_ok_isErwnbzGxuwnib:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -108555,7 +108555,7 @@ L$_16_blocks_ok_ylkmjtxhbazdAht:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -108831,7 +108831,7 @@ L$_16_blocks_ok_amFqbyqnsgkbEyu:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -109108,7 +109108,7 @@ L$_16_blocks_ok_Gxdljjoscahpipo:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -109404,7 +109404,7 @@ L$_16_blocks_ok_BzbwlusABaejjjy:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -109704,7 +109704,7 @@ L$_16_blocks_ok_wfxluBeiqgADmFb:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -110010,7 +110010,7 @@ L$_16_blocks_ok_vyklFkDwzsnvgsC:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -110317,7 +110317,7 @@ L$_16_blocks_ok_cwmmduuojwChbzc:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_EFFoGallwwbomEy:
 
@@ -110554,7 +110554,7 @@ L$_small_initial_num_blocks_is_1_sFoDGktxtpnDmhn:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm6,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -110672,7 +110672,7 @@ L$_small_initial_num_blocks_is_2_sFoDGktxtpnDmhn:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm6,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -110815,7 +110815,7 @@ L$_small_initial_num_blocks_is_3_sFoDGktxtpnDmhn:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -110959,7 +110959,7 @@ L$_small_initial_num_blocks_is_4_sFoDGktxtpnDmhn:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -111123,7 +111123,7 @@ L$_small_initial_num_blocks_is_5_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%xmm29,%xmm7,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -111296,7 +111296,7 @@ L$_small_initial_num_blocks_is_6_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%ymm29,%ymm7,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -111479,7 +111479,7 @@ L$_small_initial_num_blocks_is_7_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -111663,7 +111663,7 @@ L$_small_initial_num_blocks_is_8_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -111865,7 +111865,7 @@ L$_small_initial_num_blocks_is_9_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%xmm29,%xmm10,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -112075,7 +112075,7 @@ L$_small_initial_num_blocks_is_10_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%ymm29,%ymm10,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -112295,7 +112295,7 @@ L$_small_initial_num_blocks_is_11_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -112516,7 +112516,7 @@ L$_small_initial_num_blocks_is_12_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -112751,7 +112751,7 @@ L$_small_initial_num_blocks_is_13_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%xmm29,%xmm11,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -112990,7 +112990,7 @@ L$_small_initial_num_blocks_is_14_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%ymm29,%ymm11,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -113239,7 +113239,7 @@ L$_small_initial_num_blocks_is_15_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -113489,7 +113489,7 @@ L$_small_initial_num_blocks_is_16_sFoDGktxtpnDmhn:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_pGqEmoznEqGhujq:
 
@@ -113593,7 +113593,7 @@ L$aes_gcm_decrypt_256_avx512:
 
 	vmovdqu64	16(%rsi),%xmm3
 
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	vmovdqu64	240(%r10),%xmm4
 
 
@@ -113819,7 +113819,7 @@ L$_next_16_ok_tpefFeFucnbumCh:
 	vmovdqa64	%zmm10,832(%rsp)
 	vmovdqa64	%zmm11,896(%rsp)
 	vmovdqa64	%zmm12,960(%rsp)
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_iDAhziwpjqoADaj
 
@@ -115066,7 +115066,7 @@ L$_16_blocks_ok_BpzosFahboxovuF:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -115273,7 +115273,7 @@ L$_16_blocks_ok_idijgbEnolbjmvb:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -115479,7 +115479,7 @@ L$_16_blocks_ok_wghnihbAoEsnemr:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -115686,7 +115686,7 @@ L$_16_blocks_ok_CjzGmeouGBagvfs:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -115918,7 +115918,7 @@ L$_16_blocks_ok_DgBblneEbhavoAc:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -116156,7 +116156,7 @@ L$_16_blocks_ok_sswuqofDefGijpp:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -116400,7 +116400,7 @@ L$_16_blocks_ok_lncoDbxzFvwogbC:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -116645,7 +116645,7 @@ L$_16_blocks_ok_ExCdtGvwDseyezE:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -116915,7 +116915,7 @@ L$_16_blocks_ok_qnvdfsmvntyhGuo:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -117193,7 +117193,7 @@ L$_16_blocks_ok_ucjmDCDgtvwsblB:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -117477,7 +117477,7 @@ L$_16_blocks_ok_tGfszqdtairfiAy:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -117762,7 +117762,7 @@ L$_16_blocks_ok_GBxxxGGdrBGGAzv:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -118068,7 +118068,7 @@ L$_16_blocks_ok_rFdlFzmcbwfmCFo:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -118378,7 +118378,7 @@ L$_16_blocks_ok_yDllfugovhaluis:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -118694,7 +118694,7 @@ L$_16_blocks_ok_pincAkEEiiwgxGh:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -119011,7 +119011,7 @@ L$_16_blocks_ok_dBDAoEoFjhwcanb:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_fdoxuvdoEsDrnFi:
 
@@ -119773,7 +119773,7 @@ L$_16_blocks_ok_fekfutzigacvqDc:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -119980,7 +119980,7 @@ L$_16_blocks_ok_zEwleqntmDxAeyd:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -120186,7 +120186,7 @@ L$_16_blocks_ok_acnffEtijrEjnxo:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -120393,7 +120393,7 @@ L$_16_blocks_ok_uGhvhwlitDofjoE:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -120625,7 +120625,7 @@ L$_16_blocks_ok_BwnlahcxoBDAelu:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -120863,7 +120863,7 @@ L$_16_blocks_ok_ymfljrqweowoCvG:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -121107,7 +121107,7 @@ L$_16_blocks_ok_ijxrtlxzmzgCbiE:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -121352,7 +121352,7 @@ L$_16_blocks_ok_zzfxscwhyoakGqc:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -121622,7 +121622,7 @@ L$_16_blocks_ok_hswtkcnEneBfnil:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -121900,7 +121900,7 @@ L$_16_blocks_ok_EBzDixsnrGlAsGi:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -122184,7 +122184,7 @@ L$_16_blocks_ok_qyEwjvzrfEfrwlG:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -122469,7 +122469,7 @@ L$_16_blocks_ok_bcstjouersAefmz:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -122775,7 +122775,7 @@ L$_16_blocks_ok_rymwDrficveEDaj:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -123085,7 +123085,7 @@ L$_16_blocks_ok_kzfnwbigglfewrl:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -123401,7 +123401,7 @@ L$_16_blocks_ok_zpEbDAveGDqklle:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -123718,7 +123718,7 @@ L$_16_blocks_ok_bjFGibBdktCEryt:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_pBmCpEpokBigCud:
 
@@ -124268,7 +124268,7 @@ L$_16_blocks_ok_GsxmuksbpmpGjAF:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -124468,7 +124468,7 @@ L$_16_blocks_ok_EjdqvCnEusieimt:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -124693,7 +124693,7 @@ L$_16_blocks_ok_uctbCqtlugkklDD:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -124919,7 +124919,7 @@ L$_16_blocks_ok_gaqeqvovBwleCnk:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -125169,7 +125169,7 @@ L$_16_blocks_ok_ocpzeCAdEaCuwqG:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -125428,7 +125428,7 @@ L$_16_blocks_ok_tCpEhfGhdbguevv:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -125697,7 +125697,7 @@ L$_16_blocks_ok_yhnxntsqCvqmnAv:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -125967,7 +125967,7 @@ L$_16_blocks_ok_qhecknjsAigbdvl:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -126258,7 +126258,7 @@ L$_16_blocks_ok_bFfGEAqbwowecqr:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -126557,7 +126557,7 @@ L$_16_blocks_ok_rvpoAkotkmdfoGD:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -126866,7 +126866,7 @@ L$_16_blocks_ok_vfjpDwaAwwnfAie:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -127176,7 +127176,7 @@ L$_16_blocks_ok_sxuCEDavBFjsEdv:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -127503,7 +127503,7 @@ L$_16_blocks_ok_qqAerGvEyeduCgs:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -127834,7 +127834,7 @@ L$_16_blocks_ok_tiwfklfdCbEnvFe:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -128175,7 +128175,7 @@ L$_16_blocks_ok_BatgsGhBnhqnqnx:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -128517,7 +128517,7 @@ L$_16_blocks_ok_CuxvqEazAfGjsCp:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_cGvBxlvhpkhxlhv:
 
@@ -128664,7 +128664,7 @@ L$_message_below_32_blocks_keEetjmxflGqBfv:
 	subq	$256,%r8
 	addq	$256,%rax
 	movl	%r8d,%r10d
-	leaq	96(%rsi),%r12
+	leaq	80(%rsi),%r12
 	testq	%r14,%r14
 	jnz	L$_skip_hkeys_precomputation_wDAhpcxxDdecsFn
 	vmovdqu64	640(%rsp),%zmm3
@@ -128945,7 +128945,7 @@ L$_16_blocks_ok_cChwjnmCkfzrqax:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%xmm29,%xmm17,%xmm17
 	vextracti32x4	$0,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -129152,7 +129152,7 @@ L$_16_blocks_ok_DqmtBvrcgAqmgdw:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%ymm29,%ymm17,%ymm17
 	vextracti32x4	$1,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -129358,7 +129358,7 @@ L$_16_blocks_ok_hfDuCGGGEpbgAAo:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$2,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -129565,7 +129565,7 @@ L$_16_blocks_ok_wxaujbwbDrFxuhe:
 	vmovdqu8	%zmm17,%zmm17{%k1}{z}
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vextracti32x4	$3,%zmm17,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -129797,7 +129797,7 @@ L$_16_blocks_ok_tEuoxeaCCDdhEFB:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%xmm29,%xmm19,%xmm19
 	vextracti32x4	$0,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -130035,7 +130035,7 @@ L$_16_blocks_ok_prosxFkubabgvzg:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%ymm29,%ymm19,%ymm19
 	vextracti32x4	$1,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -130279,7 +130279,7 @@ L$_16_blocks_ok_aeeqyBehlbvnfnk:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$2,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -130524,7 +130524,7 @@ L$_16_blocks_ok_rboylvBCxohyFxr:
 	vpshufb	%zmm29,%zmm17,%zmm17
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vextracti32x4	$3,%zmm19,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -130794,7 +130794,7 @@ L$_16_blocks_ok_kwzbcrnlszssDoA:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%xmm29,%xmm20,%xmm20
 	vextracti32x4	$0,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -131072,7 +131072,7 @@ L$_16_blocks_ok_hrbjfpBdCjiGnfs:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%ymm29,%ymm20,%ymm20
 	vextracti32x4	$1,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -131356,7 +131356,7 @@ L$_16_blocks_ok_gffyuiCaEymxbgx:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$2,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -131641,7 +131641,7 @@ L$_16_blocks_ok_hAjEfcezvfywBbB:
 	vpshufb	%zmm29,%zmm19,%zmm19
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vextracti32x4	$3,%zmm20,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -131947,7 +131947,7 @@ L$_16_blocks_ok_jsBqmgCzCrGvyyA:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%xmm29,%xmm21,%xmm21
 	vextracti32x4	$0,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -132257,7 +132257,7 @@ L$_16_blocks_ok_muGuwhaFlxCtAii:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%ymm29,%ymm21,%ymm21
 	vextracti32x4	$1,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -132573,7 +132573,7 @@ L$_16_blocks_ok_EpbiipkiGBkrvEx:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$2,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -132890,7 +132890,7 @@ L$_16_blocks_ok_etuAklrEovqCDpq:
 	vpshufb	%zmm29,%zmm20,%zmm20
 	vpshufb	%zmm29,%zmm21,%zmm21
 	vextracti32x4	$3,%zmm21,%xmm7
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_xGuCpnrvibyoyay:
 
@@ -133131,7 +133131,7 @@ L$_small_initial_num_blocks_is_1_vkDeiBlhaznkthD:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%xmm29,%xmm6,%xmm6
 	vextracti32x4	$0,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 
 
 	cmpq	$16,%r8
@@ -133253,7 +133253,7 @@ L$_small_initial_num_blocks_is_2_vkDeiBlhaznkthD:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%ymm29,%ymm6,%ymm6
 	vextracti32x4	$1,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (2 - 1),%r8
 
 
@@ -133400,7 +133400,7 @@ L$_small_initial_num_blocks_is_3_vkDeiBlhaznkthD:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$2,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (3 - 1),%r8
 
 
@@ -133548,7 +133548,7 @@ L$_small_initial_num_blocks_is_4_vkDeiBlhaznkthD:
 	vmovdqu8	%zmm0,%zmm0{%k1}{z}
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vextracti32x4	$3,%zmm6,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (4 - 1),%r8
 
 
@@ -133718,7 +133718,7 @@ L$_small_initial_num_blocks_is_5_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%xmm29,%xmm7,%xmm7
 	vextracti32x4	$0,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (5 - 1),%r8
 
 
@@ -133897,7 +133897,7 @@ L$_small_initial_num_blocks_is_6_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%ymm29,%ymm7,%ymm7
 	vextracti32x4	$1,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (6 - 1),%r8
 
 
@@ -134086,7 +134086,7 @@ L$_small_initial_num_blocks_is_7_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$2,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (7 - 1),%r8
 
 
@@ -134276,7 +134276,7 @@ L$_small_initial_num_blocks_is_8_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm6,%zmm6
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vextracti32x4	$3,%zmm7,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (8 - 1),%r8
 
 
@@ -134486,7 +134486,7 @@ L$_small_initial_num_blocks_is_9_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%xmm29,%xmm10,%xmm10
 	vextracti32x4	$0,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (9 - 1),%r8
 
 
@@ -134704,7 +134704,7 @@ L$_small_initial_num_blocks_is_10_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%ymm29,%ymm10,%ymm10
 	vextracti32x4	$1,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (10 - 1),%r8
 
 
@@ -134932,7 +134932,7 @@ L$_small_initial_num_blocks_is_11_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$2,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (11 - 1),%r8
 
 
@@ -135161,7 +135161,7 @@ L$_small_initial_num_blocks_is_12_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm7,%zmm7
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vextracti32x4	$3,%zmm10,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (12 - 1),%r8
 
 
@@ -135406,7 +135406,7 @@ L$_small_initial_num_blocks_is_13_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%xmm29,%xmm11,%xmm11
 	vextracti32x4	$0,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (13 - 1),%r8
 
 
@@ -135655,7 +135655,7 @@ L$_small_initial_num_blocks_is_14_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%ymm29,%ymm11,%ymm11
 	vextracti32x4	$1,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (14 - 1),%r8
 
 
@@ -135914,7 +135914,7 @@ L$_small_initial_num_blocks_is_15_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$2,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (15 - 1),%r8
 
 
@@ -136174,7 +136174,7 @@ L$_small_initial_num_blocks_is_16_vkDeiBlhaznkthD:
 	vpshufb	%zmm29,%zmm10,%zmm10
 	vpshufb	%zmm29,%zmm11,%zmm11
 	vextracti32x4	$3,%zmm11,%xmm13
-	leaq	96(%rsi),%r10
+	leaq	80(%rsi),%r10
 	subq	$16 * (16 - 1),%r8
 L$_small_initial_partial_block_fpGgFAenBuAyutw:
 
diff --git a/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S b/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
index 2ddbddb8e..2cef522ba 100644
--- a/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
+++ b/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
@@ -379,17 +379,18 @@ _aesni_gcm_decrypt:
 
 	vzeroupper
 
+	movq	16(%rbp),%r12
 	vmovdqu	(%r8),%xmm1
 	addq	$-128,%rsp
 	movl	12(%r8),%ebx
 	leaq	L$bswap_mask(%rip),%r11
 	leaq	-128(%rcx),%r14
 	movq	$0xf80,%r15
-	vmovdqu	(%r9),%xmm8
+	vmovdqu	(%r12),%xmm8
 	andq	$-128,%rsp
 	vmovdqu	(%r11),%xmm0
 	leaq	128(%rcx),%rcx
-	leaq	32+32(%r9),%r9
+	leaq	32(%r9),%r9
 	movl	240-128(%rcx),%r10d
 	vpshufb	%xmm0,%xmm8,%xmm8
 
@@ -434,6 +435,7 @@ L$dec_no_key_aliasing:
 
 	call	_aesni_ctr32_ghash_6x
 
+	movq	16(%rbp),%r12
 	vmovups	%xmm9,-96(%rsi)
 	vmovups	%xmm10,-80(%rsi)
 	vmovups	%xmm11,-64(%rsi)
@@ -442,7 +444,7 @@ L$dec_no_key_aliasing:
 	vmovups	%xmm14,-16(%rsi)
 
 	vpshufb	(%r11),%xmm8,%xmm8
-	vmovdqu	%xmm8,-64(%r9)
+	vmovdqu	%xmm8,(%r12)
 
 	vzeroupper
 	leaq	-40(%rbp),%rsp
@@ -646,8 +648,9 @@ L$enc_no_key_aliasing:
 
 	call	_aesni_ctr32_6x
 
-	vmovdqu	(%r9),%xmm8
-	leaq	32+32(%r9),%r9
+	movq	16(%rbp),%r12
+	leaq	32(%r9),%r9
+	vmovdqu	(%r12),%xmm8
 	subq	$12,%rdx
 	movq	$192,%rax
 	vpshufb	%xmm0,%xmm8,%xmm8
@@ -825,8 +828,9 @@ L$enc_no_key_aliasing:
 	vpclmulqdq	$0x10,%xmm3,%xmm8,%xmm8
 	vpxor	%xmm7,%xmm2,%xmm2
 	vpxor	%xmm2,%xmm8,%xmm8
+	movq	16(%rbp),%r12
 	vpshufb	(%r11),%xmm8,%xmm8
-	vmovdqu	%xmm8,-64(%r9)
+	vmovdqu	%xmm8,(%r12)
 
 	vzeroupper
 	leaq	-40(%rbp),%rsp
diff --git a/generated-src/win-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S b/generated-src/win-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S
index c7550cca7..ae7a502ba 100644
--- a/generated-src/win-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S
+++ b/generated-src/win-aarch64/crypto/fipsmodule/aesv8-gcm-armv8-unroll8.S
@@ -36,7 +36,7 @@ aesv8_gcm_8x_enc_128:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -72,7 +72,7 @@ aesv8_gcm_8x_enc_128:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				  	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				  	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 6
 	add	v30.4s, v30.4s, v31.4s		//CTR block 6
@@ -98,7 +98,7 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 0
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 1
@@ -141,7 +141,7 @@ aesv8_gcm_8x_enc_128:
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 3
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 3
 	aese	v0.16b, v26.16b
@@ -182,7 +182,7 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 5
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 5
@@ -216,7 +216,7 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 6
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 7
@@ -253,7 +253,7 @@ aesv8_gcm_8x_enc_128:
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 8
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 8
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	aese	v3.16b, v26.16b						//AES block 8k+11 - round 9
 	aese	v4.16b, v28.16b
@@ -321,29 +321,29 @@ aesv8_gcm_8x_enc_128:
 
 L128_enc_main_loop:	//main	loop start
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 	rev64	v8.16b, v8.16b						//GHASH block 8k
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5 (t0, t1, t2 and t3 free)
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
 
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 
@@ -362,9 +362,9 @@ L128_enc_main_loop:	//main	loop start
 	pmull2	v9.1q, v11.2d, v20.2d				//GHASH block 8k+3 - high
 
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h3l | h3h
+	ldr	q25, [x6, #80]				//load h3l | h3h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 0
@@ -404,7 +404,7 @@ L128_enc_main_loop:	//main	loop start
 	trn1	v29.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 1
 	aese	v3.16b, v27.16b
@@ -449,19 +449,19 @@ L128_enc_main_loop:	//main	loop start
 
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 3
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4 (t0, t1, and t2 free)
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 3
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 3
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
@@ -504,7 +504,7 @@ L128_enc_main_loop:	//main	loop start
 
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 4
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	trn1	v13.2d, v15.2d, v14.2d				//GHASH block 8k+6, 8k+7 - mid
 
 	pmull2	v16.1q, v12.2d, v24.2d				//GHASH block 8k+4 - mid
@@ -570,7 +570,7 @@ L128_enc_main_loop:	//main	loop start
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 6
 
 .long	0xce153652	//eor3 v18.16b, v18.16b, v21.16b, v13.16b			//GHASH block 8k+6, 8k+7 - mid
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 .long	0xce0b3231	//eor3 v17.16b, v17.16b, v11.16b, v12.16b			//GHASH block 8k+6, 8k+7 - high
 
 	aese	v2.16b, v27.16b
@@ -611,7 +611,7 @@ L128_enc_main_loop:	//main	loop start
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 8
 .long	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	ext	v29.16b, v17.16b, v17.16b, #8				//MODULO - other top alignment
 	rev32	v23.16b, v30.16b					//CTR block 8k+18
@@ -675,21 +675,21 @@ L128_enc_main_loop:	//main	loop start
 
 L128_enc_prepretail:	//PREPRETAIL
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h6k | h5k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h6k | h5k
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
 
@@ -712,7 +712,7 @@ L128_enc_prepretail:	//PREPRETAIL
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
 
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
 	pmull2	v18.1q, v8.2d, v24.2d				//GHASH block 8k	- mid
@@ -761,12 +761,12 @@ L128_enc_prepretail:	//PREPRETAIL
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 1
 	pmull	v20.1q, v11.1d, v20.1d				//GHASH block 8k+3 - low
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
@@ -780,8 +780,8 @@ L128_enc_prepretail:	//PREPRETAIL
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
 
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 1
 
@@ -813,11 +813,11 @@ L128_enc_prepretail:	//PREPRETAIL
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 2
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 2
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	trn1	v16.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
 	aese	v0.16b, v26.16b
@@ -881,7 +881,7 @@ L128_enc_prepretail:	//PREPRETAIL
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 4
 
 	pmull2	v12.1q, v15.2d, v20.2d				//GHASH block 8k+7 - high
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	pmull	v20.1q, v15.1d, v20.1d				//GHASH block 8k+7 - low
 
 .long	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
@@ -931,7 +931,7 @@ L128_enc_prepretail:	//PREPRETAIL
 
 	pmull	v21.1q, v17.1d, v16.1d		 	//MODULO - top 64b align with mid
 .long	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 7
@@ -977,7 +977,7 @@ L128_enc_prepretail:	//PREPRETAIL
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 8
 
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 	aese	v6.16b, v26.16b						//AES block 8k+14 - round 9
 	aese	v2.16b, v26.16b						//AES block 8k+10 - round 9
 
@@ -995,16 +995,16 @@ L128_enc_tail:	//TAIL
 	ldr	q8, [x0], #16				//AES block 8k+8 - load plaintext
 
 	mov	v29.16b, v27.16b
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 .long	0xce007509	//eor3 v9.16b, v8.16b, v0.16b, v29.16b			//AES block 8k+8 - result
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 	cmp	x5, #112
 	b.gt	L128_enc_blocks_more_than_7
@@ -1059,7 +1059,7 @@ L128_enc_tail:	//TAIL
 	mov	v6.16b, v1.16b
 
 	cmp	x5, #32
-	ldr	q24, [x3, #96]					//load h4k | h3k
+	ldr	q24, [x6, #64]					//load h4k | h3k
 	b.gt	L128_enc_blocks_more_than_2
 
 	cmp	x5, #16
@@ -1068,7 +1068,7 @@ L128_enc_tail:	//TAIL
 	mov	v7.16b, v1.16b
 	b.gt	L128_enc_blocks_more_than_1
 
-	ldr	q21, [x3, #48]					//load h2k | h1k
+	ldr	q21, [x6, #16]					//load h2k | h1k
 	sub	v30.4s, v30.4s, v31.4s
 	b	L128_enc_blocks_less_than_1
 L128_enc_blocks_more_than_7:	//blocks	left >  7
@@ -1171,7 +1171,7 @@ L128_enc_blocks_more_than_3:	//blocks	left >  3
 
 	st1	{ v9.16b}, [x2], #16			  	//AES final-3 block - store result
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
@@ -1180,7 +1180,7 @@ L128_enc_blocks_more_than_3:	//blocks	left >  3
 	movi	v16.8b, #0						//supress further partial tag feed in
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	pmull	v26.1q, v8.1d, v25.1d				//GHASH final-3 block - low
 
 	ldr	q9, [x0], #16				//AES final-2 block - load plaintext
@@ -1208,7 +1208,7 @@ L128_enc_blocks_more_than_2:	//blocks	left >  2
 	ldr	q9, [x0], #16				//AES final-1 block - load plaintext
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-2 block - mid
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 	movi	v16.8b, #0						//supress further partial tag feed in
 
@@ -1228,7 +1228,7 @@ L128_enc_blocks_more_than_1:	//blocks	left >  1
 
 	st1	{ v9.16b}, [x2], #16			  	//AES final-1 block - store result
 
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 	ldr	q9, [x0], #16				//AES final block - load plaintext
@@ -1243,7 +1243,7 @@ L128_enc_blocks_more_than_1:	//blocks	left >  1
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
 
@@ -1264,16 +1264,16 @@ L128_enc_blocks_less_than_1:	//blocks	left <= 1
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
 	and	x1, x1, #127			 	//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 	cmp	x1, #64
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
 
 	mov	v0.d[1], x14
 	mov	v0.d[0], x13					//ctr0b is mask for last block
@@ -1290,7 +1290,7 @@ L128_enc_blocks_less_than_1:	//blocks	left <= 1
 	ins	v16.d[0], v8.d[1]					//GHASH final block - mid
 
 	eor	v16.8b, v16.8b, v8.8b				//GHASH final block - mid
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	pmull	v16.1q, v16.1d, v21.1d				//GHASH final block - mid
@@ -1343,7 +1343,7 @@ aesv8_gcm_8x_dec_128:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -1354,7 +1354,7 @@ aesv8_gcm_8x_dec_128:
 	mov	x5, x9
 	ld1	{ v0.16b}, [x16]					//CTR block 0
 
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	sub	x5, x5, #1		//byte_len - 1
 
 	mov	x15, #0x100000000				//set up counter increment
@@ -1418,7 +1418,7 @@ aesv8_gcm_8x_dec_128:
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 1
@@ -1456,7 +1456,7 @@ aesv8_gcm_8x_dec_128:
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 3
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 3
 
@@ -1491,7 +1491,7 @@ aesv8_gcm_8x_dec_128:
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 4
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 5
 	aese	v3.16b, v28.16b
@@ -1542,7 +1542,7 @@ aesv8_gcm_8x_dec_128:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 7
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 7
@@ -1577,7 +1577,7 @@ aesv8_gcm_8x_dec_128:
 	aese	v1.16b, v26.16b						//AES block 1 - round 9
 	aese	v6.16b, v26.16b						//AES block 6 - round 9
 
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 	aese	v4.16b, v26.16b						//AES block 4 - round 9
 	aese	v3.16b, v26.16b						//AES block 3 - round 9
 
@@ -1630,9 +1630,9 @@ aesv8_gcm_8x_dec_128:
 	b.ge	L128_dec_prepretail					//do prepretail
 
 L128_dec_main_loop:	//main	loop start
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
@@ -1640,9 +1640,9 @@ L128_dec_main_loop:	//main	loop start
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
@@ -1651,12 +1651,12 @@ L128_dec_main_loop:	//main	loop start
 
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	pmull2	v16.1q, v9.2d, v23.2d				//GHASH block 8k+1 - high
 	pmull2	v17.1q, v8.2d, v25.2d				//GHASH block 8k - high
@@ -1699,7 +1699,7 @@ L128_dec_main_loop:	//main	loop start
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
 .long	0xce1d2631	//eor3 v17.16b, v17.16b, v29.16b, v9.16b			//GHASH block 8k+2, 8k+3 - high
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	trn1	v29.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 1
@@ -1708,9 +1708,9 @@ L128_dec_main_loop:	//main	loop start
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	pmull2	v18.1q, v8.2d, v24.2d				//GHASH block 8k	- mid
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	pmull	v24.1q, v8.1d, v24.1d				//GHASH block 8k+1 - mid
 	aese	v6.16b, v27.16b
@@ -1738,9 +1738,9 @@ L128_dec_main_loop:	//main	loop start
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 2
 	eor	v10.16b, v10.16b, v29.16b				//GHASH block 8k+2, 8k+3 - mid
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v18.16b, v18.16b, v24.16b				//GHASH block 8k+1 - mid
@@ -1765,12 +1765,12 @@ L128_dec_main_loop:	//main	loop start
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 3
 	trn2	v12.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
@@ -1817,7 +1817,7 @@ L128_dec_main_loop:	//main	loop start
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 4
 	trn2	v14.2d, v15.2d, v14.2d				//GHASH block 8k+6, 8k+7 - mid
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	pmull2	v16.1q, v12.2d, v24.2d				//GHASH block 8k+4 - mid
@@ -1871,7 +1871,7 @@ L128_dec_main_loop:	//main	loop start
 .long	0xce0b3231	//eor3 v17.16b, v17.16b, v11.16b, v12.16b			//GHASH block 8k+6, 8k+7 - high
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	ldr	d16, [x10]			//MODULO - load modulo constant
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+6, 8k+7 - low
@@ -1935,7 +1935,7 @@ L128_dec_main_loop:	//main	loop start
 
 	aese	v0.16b, v26.16b						//AES block 8k+8 - round 9
 	aese	v1.16b, v26.16b						//AES block 8k+9 - round 9
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	aese	v6.16b, v26.16b						//AES block 8k+14 - round 9
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
@@ -1986,19 +1986,19 @@ L128_dec_prepretail:	//PREPRETAIL
 
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5
 
@@ -2006,8 +2006,8 @@ L128_dec_prepretail:	//PREPRETAIL
 
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
@@ -2067,13 +2067,13 @@ L128_dec_prepretail:	//PREPRETAIL
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 1
@@ -2086,9 +2086,9 @@ L128_dec_prepretail:	//PREPRETAIL
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 
@@ -2116,13 +2116,13 @@ L128_dec_prepretail:	//PREPRETAIL
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
 	trn2	v12.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 3
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	pmull2	v10.1q, v13.2d, v23.2d				//GHASH block 8k+5 - high
 	pmull	v23.1q, v13.1d, v23.1d				//GHASH block 8k+5 - low
 
@@ -2177,7 +2177,7 @@ L128_dec_prepretail:	//PREPRETAIL
 	pmull2	v13.1q, v14.2d, v21.2d				//GHASH block 8k+6 - mid
 	pmull	v21.1q, v14.1d, v21.1d				//GHASH block 8k+7 - mid
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 .long	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 5
@@ -2228,7 +2228,7 @@ L128_dec_prepretail:	//PREPRETAIL
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 7
 .long	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	pmull	v29.1q, v17.1d, v16.1d		 	//MODULO - top 64b align with mid
 	aese	v3.16b, v27.16b
@@ -2250,7 +2250,7 @@ L128_dec_prepretail:	//PREPRETAIL
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
 
 .long	0xce1d5652	//eor3 v18.16b, v18.16b, v29.16b, v21.16b			//MODULO - fold into mid
-	ldr	q27, [x8, #160]					//load rk10
+	ldr	q27, [x11, #160]					//load rk10
 
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 8
@@ -2294,15 +2294,15 @@ L128_dec_tail:	//TAIL
 
 	cmp	x5, #112
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 	ldr	q9, [x0], #16				//AES block 8k+8 - load ciphertext
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
 
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 
@@ -2358,7 +2358,7 @@ L128_dec_tail:	//TAIL
 	mov	v7.16b, v6.16b
 	cmp	x5, #32
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	mov	v6.16b, v1.16b
 	b.gt	L128_dec_blocks_more_than_2
 
@@ -2369,7 +2369,7 @@ L128_dec_tail:	//TAIL
 	b.gt	L128_dec_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	L128_dec_blocks_less_than_1
 L128_dec_blocks_more_than_7:	//blocks	left >  7
 	rev64	v8.16b, v9.16b						//GHASH final-7 block
@@ -2472,9 +2472,9 @@ L128_dec_blocks_more_than_3:	//blocks	left >  3
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-3 block - mid
 
@@ -2499,7 +2499,7 @@ L128_dec_blocks_more_than_2:	//blocks	left >  2
 	st1	{ v12.16b}, [x2], #16			 	//AES final-2 block - store result
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 	movi	v16.8b, #0						//supress further partial tag feed in
 
@@ -2524,7 +2524,7 @@ L128_dec_blocks_more_than_1:	//blocks	left >  1
 	st1	{ v12.16b}, [x2], #16			 	//AES final-1 block - store result
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
@@ -2538,7 +2538,7 @@ L128_dec_blocks_more_than_1:	//blocks	left >  1
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-1 block - high
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
 .long	0xce07752c	//eor3 v12.16b, v9.16b, v7.16b, v29.16b				//AES final block - result
@@ -2558,20 +2558,20 @@ L128_dec_blocks_less_than_1:	//blocks	left <= 1
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
 
 	mov	v0.d[1], x14
 	mov	v0.d[0], x13					//ctr0b is mask for last block
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
 
@@ -2642,7 +2642,7 @@ aesv8_gcm_8x_enc_192:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -2678,7 +2678,7 @@ aesv8_gcm_8x_enc_192:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	add	x5, x5, x0
 
@@ -2705,7 +2705,7 @@ aesv8_gcm_8x_enc_192:
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 0
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 1
@@ -2745,7 +2745,7 @@ aesv8_gcm_8x_enc_192:
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 3
 
@@ -2788,7 +2788,7 @@ aesv8_gcm_8x_enc_192:
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 5
 
@@ -2826,7 +2826,7 @@ aesv8_gcm_8x_enc_192:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 6
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 7
@@ -2875,7 +2875,7 @@ aesv8_gcm_8x_enc_192:
 	ld1	{ v19.16b}, [x3]
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 9
@@ -2918,7 +2918,7 @@ aesv8_gcm_8x_enc_192:
 
 	aese	v4.16b, v28.16b						//AES block 12 - round 11
 	aese	v7.16b, v28.16b						//AES block 15 - round 11
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	aese	v1.16b, v28.16b						//AES block 9 - round 11
 	aese	v5.16b, v28.16b						//AES block 13 - round 11
@@ -2971,21 +2971,21 @@ aesv8_gcm_8x_enc_192:
 
 L192_enc_main_loop:	//main	loop start
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4 (t0, t1, and t2 free)
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 	rev64	v8.16b, v8.16b						//GHASH block 8k
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
@@ -3016,7 +3016,7 @@ L192_enc_main_loop:	//main	loop start
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	pmull2	v17.1q, v8.2d, v25.2d				//GHASH block 8k - high
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
@@ -3029,8 +3029,8 @@ L192_enc_main_loop:	//main	loop start
 	trn1	v18.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 1
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	pmull2	v29.1q, v10.2d, v22.2d				//GHASH block 8k+2 - high
 	pmull	v19.1q, v8.1d, v25.1d				//GHASH block 8k - low
@@ -3082,14 +3082,14 @@ L192_enc_main_loop:	//main	loop start
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 3
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	pmull2	v18.1q, v8.2d, v24.2d				//GHASH block 8k - mid
@@ -3135,12 +3135,12 @@ L192_enc_main_loop:	//main	loop start
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 5
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7 (t0, t1, t2 and t3 free)
@@ -3155,8 +3155,8 @@ L192_enc_main_loop:	//main	loop start
 
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 5
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 5
@@ -3203,7 +3203,7 @@ L192_enc_main_loop:	//main	loop start
 	eor	v14.16b, v14.16b, v13.16b				//GHASH block 8k+6, 8k+7 - mid
 
 	pmull2	v16.1q, v12.2d, v24.2d				//GHASH block 8k+4 - mid
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull	v24.1q, v12.1d, v24.1d				//GHASH block 8k+5 - mid
 
 	aese	v4.16b, v27.16b
@@ -3250,7 +3250,7 @@ L192_enc_main_loop:	//main	loop start
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 8
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 8
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+6, 8k+7 - low
 	rev32	v20.16b, v30.16b					//CTR block 8k+16
@@ -3288,7 +3288,7 @@ L192_enc_main_loop:	//main	loop start
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 10
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 10
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 	ext	v29.16b, v17.16b, v17.16b, #8				//MODULO - other top alignment
 
 	aese	v0.16b, v27.16b
@@ -3361,26 +3361,26 @@ L192_enc_main_loop:	//main	loop start
 
 L192_enc_prepretail:	//PREPRETAIL
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
@@ -3414,7 +3414,7 @@ L192_enc_prepretail:	//PREPRETAIL
 	trn2	v8.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	pmull	v23.1q, v9.1d, v23.1d				//GHASH block 8k+1 - low
 	eor	v17.16b, v17.16b, v16.16b				//GHASH block 8k+1 - high
@@ -3474,7 +3474,7 @@ L192_enc_prepretail:	//PREPRETAIL
 
 	eor	v10.16b, v10.16b, v29.16b				//GHASH block 8k+2, 8k+3 - mid
 	pmull	v24.1q, v8.1d, v24.1d				//GHASH block 8k+1 - mid
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 3
@@ -3488,17 +3488,17 @@ L192_enc_prepretail:	//PREPRETAIL
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 3
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 3
@@ -3532,14 +3532,14 @@ L192_enc_prepretail:	//PREPRETAIL
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7 (t0, t1, t2 and t3 free)
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 5
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	pmull2	v11.1q, v14.2d, v22.2d				//GHASH block 8k+6 - high
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
@@ -3611,7 +3611,7 @@ L192_enc_prepretail:	//PREPRETAIL
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 7
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 7
@@ -3653,7 +3653,7 @@ L192_enc_prepretail:	//PREPRETAIL
 
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 9
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 9
 
@@ -3671,7 +3671,7 @@ L192_enc_prepretail:	//PREPRETAIL
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 9
 
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 10
@@ -3708,18 +3708,18 @@ L192_enc_prepretail:	//PREPRETAIL
 
 L192_enc_tail:	//TAIL
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	sub	x5, x4, x0 	//main_end_input_ptr is number of bytes left to process
 
 	ldr	q8, [x0], #16				//AES block 8k+8 - l3ad plaintext
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	mov	v29.16b, v26.16b
 
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	cmp	x5, #112
@@ -3777,7 +3777,7 @@ L192_enc_tail:	//TAIL
 	mov	v6.16b, v1.16b
 	sub	v30.4s, v30.4s, v31.4s
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	cmp	x5, #32
 	b.gt	L192_enc_blocks_more_than_2
 
@@ -3788,7 +3788,7 @@ L192_enc_tail:	//TAIL
 	b.gt	L192_enc_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	L192_enc_blocks_less_than_1
 L192_enc_blocks_more_than_7:	//blocks	left >  7
 	st1	{ v9.16b}, [x2], #16			 	//AES final-7 block  - store result
@@ -3887,7 +3887,7 @@ L192_enc_blocks_more_than_4:	//blocks	left >  4
 .long	0xce047529	//eor3 v9.16b, v9.16b, v4.16b, v29.16b			//AES final-3 block - result
 L192_enc_blocks_more_than_3:	//blocks	left >  3
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	st1	{ v9.16b}, [x2], #16			 	//AES final-3 block - store result
 
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
@@ -3896,7 +3896,7 @@ L192_enc_blocks_more_than_3:	//blocks	left >  3
 	movi	v16.8b, #0						//supress further partial tag feed in
 
 	ldr	q9, [x0], #16				//AES final-2 block - load plaintext
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
@@ -3919,7 +3919,7 @@ L192_enc_blocks_more_than_2:	//blocks	left >  2
 	st1	{ v9.16b}, [x2], #16			 	//AES final-2 block - store result
 
 	rev64	v8.16b, v9.16b						//GHASH final-2 block
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
@@ -3942,7 +3942,7 @@ L192_enc_blocks_more_than_2:	//blocks	left >  2
 .long	0xce067529	//eor3 v9.16b, v9.16b, v6.16b, v29.16b			//AES final-1 block - result
 L192_enc_blocks_more_than_1:	//blocks	left >  1
 
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	st1	{ v9.16b}, [x2], #16			 	//AES final-1 block - store result
 
@@ -3958,7 +3958,7 @@ L192_enc_blocks_more_than_1:	//blocks	left >  1
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 
 	ldr	q9, [x0], #16				//AES final block - load plaintext
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
 
@@ -3971,7 +3971,7 @@ L192_enc_blocks_more_than_1:	//blocks	left >  1
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-1 block - high
 L192_enc_blocks_less_than_1:	//blocks	left <= 1
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
 	sub	x1, x1, #128				//bit_length -= 128
@@ -3980,15 +3980,15 @@ L192_enc_blocks_less_than_1:	//blocks	left <= 1
 
 	and	x1, x1, #127				//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
 
 	mov	v0.d[1], x14
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
@@ -4060,7 +4060,7 @@ aesv8_gcm_8x_dec_192:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -4094,7 +4094,7 @@ aesv8_gcm_8x_dec_192:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 6
 	add	v30.4s, v30.4s, v31.4s		//CTR block 6
@@ -4119,7 +4119,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 0
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 1
@@ -4163,7 +4163,7 @@ aesv8_gcm_8x_dec_192:
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 3
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 3
 	aese	v5.16b, v26.16b
@@ -4204,7 +4204,7 @@ aesv8_gcm_8x_dec_192:
 
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 5
@@ -4240,7 +4240,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 6
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	add	v30.4s, v30.4s, v31.4s		//CTR block 7
 
@@ -4291,7 +4291,7 @@ aesv8_gcm_8x_dec_192:
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 9
@@ -4331,7 +4331,7 @@ aesv8_gcm_8x_dec_192:
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 10
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 10
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	aese	v0.16b, v28.16b						//AES block 0 - round 11
 	aese	v1.16b, v28.16b						//AES block 1 - round 11
@@ -4388,16 +4388,16 @@ aesv8_gcm_8x_dec_192:
 
 L192_dec_main_loop:	//main	loop start
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
 
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
@@ -4430,14 +4430,14 @@ L192_dec_main_loop:	//main	loop start
 
 	pmull	v19.1q, v8.1d, v25.1d				//GHASH block 8k - low
 	pmull2	v16.1q, v9.2d, v23.2d				//GHASH block 8k+1 - high
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
 	pmull	v23.1q, v9.1d, v23.1d				//GHASH block 8k+1 - low
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	aese	v0.16b, v27.16b
@@ -4460,8 +4460,8 @@ L192_dec_main_loop:	//main	loop start
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	trn2	v8.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 
 	eor	v17.16b, v17.16b, v16.16b				//GHASH block 8k+1 - high
@@ -4492,9 +4492,9 @@ L192_dec_main_loop:	//main	loop start
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 2
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 2
@@ -4514,7 +4514,7 @@ L192_dec_main_loop:	//main	loop start
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 3
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	eor	v10.16b, v10.16b, v29.16b				//GHASH block 8k+2, 8k+3 - mid
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
@@ -4553,16 +4553,16 @@ L192_dec_main_loop:	//main	loop start
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 4
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 5
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 5
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
@@ -4584,8 +4584,8 @@ L192_dec_main_loop:	//main	loop start
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	pmull2	v10.1q, v13.2d, v23.2d				//GHASH block 8k+5 - high
 	pmull	v23.1q, v13.1d, v23.1d				//GHASH block 8k+5 - low
 
@@ -4625,7 +4625,7 @@ L192_dec_main_loop:	//main	loop start
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 7
 
@@ -4683,7 +4683,7 @@ L192_dec_main_loop:	//main	loop start
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 9
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 9
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 .long	0xce114e52	//eor3 v18.16b, v18.16b, v17.16b, v19.16b		 	//MODULO - karatsuba tidy up
 	ldp	q8, q9, [x0], #32			//AES block 8k+8, 8k+9 - load ciphertext
@@ -4718,7 +4718,7 @@ L192_dec_main_loop:	//main	loop start
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 10
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 10
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 
 	ldp	q14, q15, [x0], #32			//AES block 8k+14, 8k+15 - load ciphertext
 	aese	v4.16b, v27.16b
@@ -4776,13 +4776,13 @@ L192_dec_main_loop:	//main	loop start
 	b.lt	L192_dec_main_loop
 
 L192_dec_prepretail:	//PREPRETAIL
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
@@ -4795,9 +4795,9 @@ L192_dec_prepretail:	//PREPRETAIL
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 
@@ -4824,7 +4824,7 @@ L192_dec_prepretail:	//PREPRETAIL
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 1
@@ -4853,8 +4853,8 @@ L192_dec_prepretail:	//PREPRETAIL
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 1
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 2
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
@@ -4900,20 +4900,20 @@ L192_dec_prepretail:	//PREPRETAIL
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 3
 
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 3
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 	pmull	v21.1q, v10.1d, v21.1d				//GHASH block 8k+3 - mid
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	eor	v18.16b, v18.16b, v24.16b				//GHASH block 8k+1 - mid
 	aese	v2.16b, v26.16b
@@ -4946,8 +4946,8 @@ L192_dec_prepretail:	//PREPRETAIL
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 4
 
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	trn1	v16.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
 
 	aese	v7.16b, v28.16b
@@ -4957,7 +4957,7 @@ L192_dec_prepretail:	//PREPRETAIL
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 5
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 5
 	aese	v5.16b, v28.16b
@@ -5017,7 +5017,7 @@ L192_dec_prepretail:	//PREPRETAIL
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull	v21.1q, v14.1d, v21.1d				//GHASH block 8k+7 - mid
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
@@ -5064,7 +5064,7 @@ L192_dec_prepretail:	//PREPRETAIL
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 8
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 8
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 .long	0xce1d5652	//eor3 v18.16b, v18.16b, v29.16b, v21.16b			//MODULO - fold into mid
 	aese	v7.16b, v26.16b
@@ -5087,7 +5087,7 @@ L192_dec_prepretail:	//PREPRETAIL
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 9
 
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
-	ldr	q26, [x8, #192]					//load rk12
+	ldr	q26, [x11, #192]					//load rk12
 	ext	v21.16b, v18.16b, v18.16b, #8			 	//MODULO - other mid alignment
 
 	aese	v2.16b, v27.16b
@@ -5126,16 +5126,16 @@ L192_dec_tail:	//TAIL
 
 	sub	x5, x4, x0 	//main_end_input_ptr is number of bytes left to process
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	ldr	q9, [x0], #16				//AES block 8k+8 - load ciphertext
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	mov	v29.16b, v26.16b
 
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
@@ -5194,7 +5194,7 @@ L192_dec_tail:	//TAIL
 	cmp	x5, #32
 
 	mov	v6.16b, v1.16b
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	b.gt	L192_dec_blocks_more_than_2
 
 	sub	v30.4s, v30.4s, v31.4s
@@ -5204,7 +5204,7 @@ L192_dec_tail:	//TAIL
 	b.gt	L192_dec_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	L192_dec_blocks_less_than_1
 L192_dec_blocks_more_than_7:	//blocks	left >  7
 	rev64	v8.16b, v9.16b						//GHASH final-7 block
@@ -5298,7 +5298,7 @@ L192_dec_blocks_more_than_4:	//blocks	left >  4
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-4 block - high
 L192_dec_blocks_more_than_3:	//blocks	left >  3
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
 	ldr	q9, [x0], #16				//AES final-2 block - load ciphertext
@@ -5317,7 +5317,7 @@ L192_dec_blocks_more_than_3:	//blocks	left >  3
 .long	0xce05752c	//eor3 v12.16b, v9.16b, v5.16b, v29.16b				//AES final-2 block - result
 
 	eor	v19.16b, v19.16b, v26.16b					//GHASH final-3 block - low
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-3 block - mid
 
@@ -5327,7 +5327,7 @@ L192_dec_blocks_more_than_3:	//blocks	left >  3
 L192_dec_blocks_more_than_2:	//blocks	left >  2
 
 	rev64	v8.16b, v9.16b						//GHASH final-2 block
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
@@ -5354,12 +5354,12 @@ L192_dec_blocks_more_than_1:	//blocks	left >  1
 
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 	ldr	q9, [x0], #16				//AES final block - load ciphertext
-	ldr	q22, [x3, #64]				//load h1l | h1h
+	ldr	q22, [x6, #32]				//load h1l | h1h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
 	movi	v16.8b, #0						//supress further partial tag feed in
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	pmull	v26.1q, v8.1d, v22.1d				//GHASH final-1 block - low
 	ins	v27.d[0], v8.d[1]					//GHASH final-1 block - mid
@@ -5388,17 +5388,17 @@ L192_dec_blocks_less_than_1:	//blocks	left <= 1
 	str	q30, [x16]					//store the updated counter
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 
 	and	x1, x1, #127				//bit_length %= 128
 
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
 
-	csel	x13, x7, x6, lt
-	csel	x14, x6, xzr, lt
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	csel	x13, x8, x7, lt
+	csel	x14, x7, xzr, lt
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	mov	v0.d[1], x14
@@ -5468,7 +5468,7 @@ aesv8_gcm_8x_enc_256:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -5507,7 +5507,7 @@ aesv8_gcm_8x_enc_256:
 
 	rev32	v5.16b, v30.16b				//CTR block 5
 	add	v30.4s, v30.4s, v31.4s		//CTR block 5
-	ldp	q26, q27, [x8, #0]				 	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				 	//load rk0, rk1
 
 	rev32	v6.16b, v30.16b				//CTR block 6
 	add	v30.4s, v30.4s, v31.4s		//CTR block 6
@@ -5532,7 +5532,7 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 0
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 1
@@ -5577,7 +5577,7 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 3
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 3
@@ -5617,7 +5617,7 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 5
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 5
@@ -5651,7 +5651,7 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 6
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 7
@@ -5695,7 +5695,7 @@ aesv8_gcm_8x_enc_256:
 	ld1	{ v19.16b}, [x3]
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 9
@@ -5737,7 +5737,7 @@ aesv8_gcm_8x_enc_256:
 
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 11
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 11
 
@@ -5756,7 +5756,7 @@ aesv8_gcm_8x_enc_256:
 	aesmc	v7.16b, v7.16b			//AES block 7 - round 11
 
 	add	v30.4s, v30.4s, v31.4s		//CTR block 7
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 12
@@ -5834,17 +5834,17 @@ aesv8_gcm_8x_enc_256:
 	b.ge	L256_enc_prepretail					//do prepretail
 
 L256_enc_main_loop:	//main	loop start
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev64	v11.16b, v11.16b						//GHASH block 8k+3
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
@@ -5854,9 +5854,9 @@ L256_enc_main_loop:	//main	loop start
 
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	aese	v3.16b, v26.16b
@@ -5879,7 +5879,7 @@ L256_enc_main_loop:	//main	loop start
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	eor	v8.16b, v8.16b, v19.16b				 	//PRE 1
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
@@ -5941,7 +5941,7 @@ L256_enc_main_loop:	//main	loop start
 
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
 
 	aese	v2.16b, v26.16b
@@ -5957,9 +5957,9 @@ L256_enc_main_loop:	//main	loop start
 
 	pmull	v20.1q, v11.1d, v20.1d				//GHASH block 8k+3 - low
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	trn1	v16.2d, v13.2d, v12.2d				//GHASH block 8k+4, 8k+5 - mid
@@ -5988,7 +5988,7 @@ L256_enc_main_loop:	//main	loop start
 
 	trn2	v10.2d, v11.2d, v10.2d				//GHASH block 8k+2, 8k+3 - mid
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
@@ -6038,21 +6038,21 @@ L256_enc_main_loop:	//main	loop start
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 6
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 7
 	aese	v3.16b, v27.16b
@@ -6115,7 +6115,7 @@ L256_enc_main_loop:	//main	loop start
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 9
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 9
 	aese	v3.16b, v26.16b
@@ -6158,7 +6158,7 @@ L256_enc_main_loop:	//main	loop start
 
 .long	0xce0b3231	//eor3 v17.16b, v17.16b, v11.16b, v12.16b			//GHASH block 8k+6, 8k+7 - high
 
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 	rev32	v20.16b, v30.16b					//CTR block 8k+16
 
 	ext	v21.16b, v17.16b, v17.16b, #8			 	//MODULO - other top alignment
@@ -6199,7 +6199,7 @@ L256_enc_main_loop:	//main	loop start
 
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 12
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 	aese	v7.16b, v27.16b						//AES block 8k+15 - round 13
 
 	ldp	q10, q11, [x0], #32			//AES block 8k+10, 8k+11 - load plaintext
@@ -6264,7 +6264,7 @@ L256_enc_main_loop:	//main	loop start
 
 L256_enc_prepretail:	//PREPRETAIL
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
@@ -6273,8 +6273,8 @@ L256_enc_prepretail:	//PREPRETAIL
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 
@@ -6303,20 +6303,20 @@ L256_enc_prepretail:	//PREPRETAIL
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 1
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 1
 
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	aese	v2.16b, v27.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 1
 
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
@@ -6352,7 +6352,7 @@ L256_enc_prepretail:	//PREPRETAIL
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	trn1	v18.2d, v9.2d, v8.2d				//GHASH block 8k, 8k+1 - mid
 	pmull2	v17.1q, v8.2d, v25.2d				//GHASH block 8k - high
 
@@ -6430,11 +6430,11 @@ L256_enc_prepretail:	//PREPRETAIL
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 5
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	pmull2	v29.1q, v10.2d, v21.2d				//GHASH block 8k+2 - mid
 	pmull	v21.1q, v10.1d, v21.1d				//GHASH block 8k+3 - mid
@@ -6460,8 +6460,8 @@ L256_enc_prepretail:	//PREPRETAIL
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 6
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 6
 	aese	v3.16b, v26.16b
@@ -6476,12 +6476,12 @@ L256_enc_prepretail:	//PREPRETAIL
 
 	pmull2	v8.1q, v12.2d, v25.2d				//GHASH block 8k+4 - high
 	pmull	v25.1q, v12.1d, v25.1d				//GHASH block 8k+4 - low
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 7
 	aese	v4.16b, v27.16b
@@ -6544,7 +6544,7 @@ L256_enc_prepretail:	//PREPRETAIL
 .long	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
 .long	0xce082a31	//eor3 v17.16b, v17.16b, v8.16b, v10.16b			//GHASH block 8k+4, 8k+5 - high
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 9
 	aese	v0.16b, v26.16b
@@ -6594,7 +6594,7 @@ L256_enc_prepretail:	//PREPRETAIL
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 11
 
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 	ext	v21.16b, v17.16b, v17.16b, #8			 	//MODULO - other top alignment
 	aese	v2.16b, v28.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 11
@@ -6615,7 +6615,7 @@ L256_enc_prepretail:	//PREPRETAIL
 	pmull	v17.1q, v18.1d, v16.1d			//MODULO - mid 64b align with low
 	aese	v3.16b, v28.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 11
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 12
@@ -6652,17 +6652,17 @@ L256_enc_prepretail:	//PREPRETAIL
 	aese	v6.16b, v27.16b						//AES block 8k+14 - round 13
 L256_enc_tail:	//TAIL
 
-	ldp	q24, q25, [x3, #192]			//load h8l | h8h
+	ldp	q24, q25, [x6, #160]			//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	sub	x5, x4, x0		//main_end_input_ptr is number of bytes left to process
 
 	ldr	q8, [x0], #16				//AES block 8k+8 - load plaintext
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	ext	v16.16b, v19.16b, v19.16b, #8				//prepare final partial tag
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	mov	v29.16b, v28.16b
@@ -6718,7 +6718,7 @@ L256_enc_tail:	//TAIL
 
 	cmp	x5, #32
 	mov	v7.16b, v6.16b
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	mov	v6.16b, v1.16b
 	sub	v30.4s, v30.4s, v31.4s
@@ -6731,7 +6731,7 @@ L256_enc_tail:	//TAIL
 	b.gt	L256_enc_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	L256_enc_blocks_less_than_1
 L256_enc_blocks_more_than_7:	//blocks	left >  7
 	st1	{ v9.16b}, [x2], #16				//AES final-7 block  - store result
@@ -6833,7 +6833,7 @@ L256_enc_blocks_more_than_3:	//blocks	left >  3
 
 	st1	{ v9.16b}, [x2], #16				//AES final-3 block - store result
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
 
@@ -6844,7 +6844,7 @@ L256_enc_blocks_more_than_3:	//blocks	left >  3
 
 	eor	v17.16b, v17.16b, v28.16b					//GHASH final-3 block - high
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-3 block - mid
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-3 block - mid
 	ldr	q9, [x0], #16				//AES final-2 block - load plaintext
@@ -6859,7 +6859,7 @@ L256_enc_blocks_more_than_3:	//blocks	left >  3
 	eor	v19.16b, v19.16b, v26.16b					//GHASH final-3 block - low
 L256_enc_blocks_more_than_2:	//blocks	left >  2
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 
 	st1	{ v9.16b}, [x2], #16			 	//AES final-2 block - store result
@@ -6889,7 +6889,7 @@ L256_enc_blocks_more_than_1:	//blocks	left >  1
 
 	st1	{ v9.16b}, [x2], #16				//AES final-1 block - store result
 
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-1 block
 	ldr	q9, [x0], #16				//AES final block - load plaintext
@@ -6906,7 +6906,7 @@ L256_enc_blocks_more_than_1:	//blocks	left >  1
 	pmull	v26.1q, v8.1d, v22.1d				//GHASH final-1 block - low
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 
 	eor	v19.16b, v19.16b, v26.16b					//GHASH final-1 block - low
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
@@ -6922,18 +6922,18 @@ L256_enc_blocks_less_than_1:	//blocks	left <= 1
 
 	neg	x1, x1				//bit_length = 128 - #bits in input (in range [1,128])
 
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x14, x6, xzr, lt
-	csel	x13, x7, x6, lt
+	csel	x14, x7, xzr, lt
+	csel	x13, x8, x7, lt
 
 	mov	v0.d[0], x13					//ctr0b is mask for last block
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
@@ -7002,7 +7002,7 @@ aesv8_gcm_8x_dec_256:
 	stp	d8, d9, [sp, #-80]!
 	lsr	x9, x1, #3
 	mov	x16, x4
-	mov	x8, x5
+	mov	x11, x5
 	stp	d10, d11, [sp, #16]
 	stp	d12, d13, [sp, #32]
 	stp	d14, d15, [sp, #48]
@@ -7028,7 +7028,7 @@ aesv8_gcm_8x_dec_256:
 
 	rev32	v2.16b, v30.16b				//CTR block 2
 	add	v30.4s, v30.4s, v31.4s		//CTR block 2
-	ldp	q26, q27, [x8, #0]				  	//load rk0, rk1
+	ldp	q26, q27, [x11, #0]				  	//load rk0, rk1
 
 	rev32	v3.16b, v30.16b				//CTR block 3
 	add	v30.4s, v30.4s, v31.4s		//CTR block 3
@@ -7063,7 +7063,7 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 0
 	aese	v7.16b, v26.16b
 	aesmc	v7.16b, v7.16b		        //AES block 7 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b		        //AES block 6 - round 1
@@ -7102,7 +7102,7 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 2
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 2
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 3
@@ -7148,7 +7148,7 @@ aesv8_gcm_8x_dec_256:
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 5
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 4 - round 5
 	aese	v7.16b, v28.16b
@@ -7183,7 +7183,7 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v2.16b, v2.16b			//AES block 2 - round 6
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 6
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 5 - round 7
@@ -7230,7 +7230,7 @@ aesv8_gcm_8x_dec_256:
 	ld1	{ v19.16b}, [x3]
 	ext	v19.16b, v19.16b, v19.16b, #8
 	rev64	v19.16b, v19.16b
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	add	x4, x0, x1, lsr #3 //end_input_ptr
 	add	x5, x5, x0
 
@@ -7270,7 +7270,7 @@ aesv8_gcm_8x_dec_256:
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 10
 	aese	v3.16b, v27.16b
 	aesmc	v3.16b, v3.16b			//AES block 3 - round 10
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 0 - round 11
@@ -7292,7 +7292,7 @@ aesv8_gcm_8x_dec_256:
 
 	aese	v6.16b, v28.16b
 	aesmc	v6.16b, v6.16b			//AES block 6 - round 11
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 1 - round 12
@@ -7370,13 +7370,13 @@ aesv8_gcm_8x_dec_256:
 
 L256_dec_main_loop:	//main	loop start
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
@@ -7408,12 +7408,12 @@ L256_dec_main_loop:	//main	loop start
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 0
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 0
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 
 	eor	v8.16b, v8.16b, v19.16b					//PRE 1
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 1
@@ -7457,7 +7457,7 @@ L256_dec_main_loop:	//main	loop start
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	pmull2	v29.1q, v10.2d, v22.2d				//GHASH block 8k+2 - high
 	aese	v3.16b, v26.16b
 	aesmc	v3.16b, v3.16b			//AES block 8k+11 - round 3
@@ -7505,12 +7505,12 @@ L256_dec_main_loop:	//main	loop start
 	aese	v4.16b, v27.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 4
 
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 	eor	v8.16b, v8.16b, v18.16b			//GHASH block 8k, 8k+1 - mid
 	pmull	v22.1q, v10.1d, v22.1d				//GHASH block 8k+2 - low
 
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
 	eor	v19.16b, v19.16b, v23.16b				//GHASH block 8k+1 - low
@@ -7566,9 +7566,9 @@ L256_dec_main_loop:	//main	loop start
 	pmull	v21.1q, v10.1d, v21.1d				//GHASH block 8k+3 - mid
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v14.16b, v14.16b						//GHASH block 8k+6
 	eor	v18.16b, v18.16b, v24.16b				//GHASH block 8k+1 - mid
@@ -7577,11 +7577,11 @@ L256_dec_main_loop:	//main	loop start
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 7
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 .long	0xce157652	//eor3 v18.16b, v18.16b, v21.16b, v29.16b			//GHASH block 8k+2, 8k+3 - mid
 	aese	v7.16b, v27.16b
@@ -7594,8 +7594,8 @@ L256_dec_main_loop:	//main	loop start
 	aese	v6.16b, v27.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 7
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 7
 	aese	v4.16b, v27.16b
@@ -7629,7 +7629,7 @@ L256_dec_main_loop:	//main	loop start
 	aese	v7.16b, v28.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 8
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 	pmull	v22.1q, v14.1d, v22.1d				//GHASH block 8k+6 - low
 	trn2	v14.2d, v15.2d, v14.2d				//GHASH block 8k+6, 8k+7 - mid
 
@@ -7696,7 +7696,7 @@ L256_dec_main_loop:	//main	loop start
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+16
 	aese	v1.16b, v28.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 11
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 
 	aese	v0.16b, v28.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 11
@@ -7736,7 +7736,7 @@ L256_dec_main_loop:	//main	loop start
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 11
 
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 	aese	v5.16b, v26.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 12
 	aese	v3.16b, v26.16b
@@ -7796,30 +7796,30 @@ L256_dec_main_loop:	//main	loop start
 	b.lt	L256_dec_main_loop
 
 L256_dec_prepretail:	//PREPRETAIL
-	ldp	q26, q27, [x8, #0]					//load rk0, rk1
+	ldp	q26, q27, [x11, #0]					//load rk0, rk1
 	rev32	v5.16b, v30.16b				//CTR block 8k+13
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+13
 
 	rev64	v12.16b, v12.16b						//GHASH block 8k+4
-	ldr	q21, [x3, #144]				//load h6k | h5k
-	ldr	q24, [x3, #192]				//load h8k | h7k
+	ldr	q21, [x6, #112]				//load h6k | h5k
+	ldr	q24, [x6, #160]				//load h8k | h7k
 
 	rev32	v6.16b, v30.16b				//CTR block 8k+14
 	rev64	v8.16b, v8.16b						//GHASH block 8k
 	add	v30.4s, v30.4s, v31.4s		//CTR block 8k+14
 
 	ext	v19.16b, v19.16b, v19.16b, #8				//PRE 0
-	ldr	q23, [x3, #176]				//load h7l | h7h
+	ldr	q23, [x6, #144]				//load h7l | h7h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #208]				//load h8l | h8h
+	ldr	q25, [x6, #176]				//load h8l | h8h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v9.16b, v9.16b						//GHASH block 8k+1
 
 	rev32	v7.16b, v30.16b				//CTR block 8k+15
 	rev64	v10.16b, v10.16b						//GHASH block 8k+2
-	ldr	q20, [x3, #128]				//load h5l | h5h
+	ldr	q20, [x6, #96]				//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #160]				//load h6l | h6h
+	ldr	q22, [x6, #128]				//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	aese	v0.16b, v26.16b
@@ -7843,7 +7843,7 @@ L256_dec_prepretail:	//PREPRETAIL
 	aese	v2.16b, v26.16b
 	aesmc	v2.16b, v2.16b			//AES block 8k+10 - round 0
 
-	ldp	q28, q26, [x8, #32]				//load rk2, rk3
+	ldp	q28, q26, [x11, #32]				//load rk2, rk3
 	aese	v0.16b, v27.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 1
 	eor	v8.16b, v8.16b, v19.16b					//PRE 1
@@ -7903,7 +7903,7 @@ L256_dec_prepretail:	//PREPRETAIL
 	aese	v4.16b, v28.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 2
 
-	ldp	q27, q28, [x8, #64]				//load rk4, rk5
+	ldp	q27, q28, [x11, #64]				//load rk4, rk5
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 3
 	pmull2	v9.1q, v11.2d, v20.2d				//GHASH block 8k+3 - high
@@ -7934,9 +7934,9 @@ L256_dec_prepretail:	//PREPRETAIL
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 4
 
 .long	0xce165273	//eor3 v19.16b, v19.16b, v22.16b, v20.16b			//GHASH block 8k+2, 8k+3 - low
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	aese	v7.16b, v27.16b
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 4
@@ -7976,11 +7976,11 @@ L256_dec_prepretail:	//PREPRETAIL
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 5
 	aese	v5.16b, v28.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 5
-	ldp	q26, q27, [x8, #96]				//load rk6, rk7
+	ldp	q26, q27, [x11, #96]				//load rk6, rk7
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v15.16b, v15.16b						//GHASH block 8k+7
 	rev64	v13.16b, v13.16b						//GHASH block 8k+5
@@ -7991,8 +7991,8 @@ L256_dec_prepretail:	//PREPRETAIL
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 6
-	ldr	q21, [x3, #48]				//load h2k | h1k
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q21, [x6, #16]				//load h2k | h1k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	aese	v6.16b, v26.16b
 	aesmc	v6.16b, v6.16b			//AES block 8k+14 - round 6
 
@@ -8022,7 +8022,7 @@ L256_dec_prepretail:	//PREPRETAIL
 	aese	v4.16b, v26.16b
 	aesmc	v4.16b, v4.16b			//AES block 8k+12 - round 6
 
-	ldp	q28, q26, [x8, #128]				//load rk8, rk9
+	ldp	q28, q26, [x11, #128]				//load rk8, rk9
 	pmull	v22.1q, v14.1d, v22.1d				//GHASH block 8k+6 - low
 	aese	v5.16b, v27.16b
 	aesmc	v5.16b, v5.16b			//AES block 8k+13 - round 7
@@ -8085,7 +8085,7 @@ L256_dec_prepretail:	//PREPRETAIL
 	pmull	v21.1q, v14.1d, v21.1d				//GHASH block 8k+7 - mid
 	pmull	v20.1q, v15.1d, v20.1d				//GHASH block 8k+7 - low
 
-	ldp	q27, q28, [x8, #160]				//load rk10, rk11
+	ldp	q27, q28, [x11, #160]				//load rk10, rk11
 .long	0xce195e73	//eor3 v19.16b, v19.16b, v25.16b, v23.16b			//GHASH block 8k+4, 8k+5 - low
 .long	0xce184252	//eor3 v18.16b, v18.16b, v24.16b, v16.16b			//GHASH block 8k+4, 8k+5 - mid
 
@@ -8122,7 +8122,7 @@ L256_dec_prepretail:	//PREPRETAIL
 	aesmc	v7.16b, v7.16b			//AES block 8k+15 - round 10
 	aese	v1.16b, v27.16b
 	aesmc	v1.16b, v1.16b			//AES block 8k+9 - round 10
-	ldp	q26, q27, [x8, #192]				//load rk12, rk13
+	ldp	q26, q27, [x11, #192]				//load rk12, rk13
 
 	ext	v21.16b, v17.16b, v17.16b, #8				//MODULO - other top alignment
 
@@ -8165,7 +8165,7 @@ L256_dec_prepretail:	//PREPRETAIL
 
 	aese	v0.16b, v26.16b
 	aesmc	v0.16b, v0.16b			//AES block 8k+8 - round 12
-	ldr	q28, [x8, #224]					//load rk14
+	ldr	q28, [x11, #224]					//load rk14
 	aese	v1.16b, v26.16b
 	aesmc	v1.16b, v1.16b	        	//AES block 8k+9 - round 12
 
@@ -8192,15 +8192,15 @@ L256_dec_tail:	//TAIL
 
 	ldr	q9, [x0], #16				//AES block 8k+8 - load ciphertext
 
-	ldp	q24, q25, [x3, #192]			//load h8k | h7k
+	ldp	q24, q25, [x6, #160]			//load h8k | h7k
 	ext	v25.16b, v25.16b, v25.16b, #8
 	mov	v29.16b, v28.16b
 
-	ldp	q20, q21, [x3, #128]			//load h5l | h5h
+	ldp	q20, q21, [x6, #96]			//load h5l | h5h
 	ext	v20.16b, v20.16b, v20.16b, #8
 
 .long	0xce00752c	//eor3 v12.16b, v9.16b, v0.16b, v29.16b				//AES block 8k+8 - result
-	ldp	q22, q23, [x3, #160]			//load h6l | h6h
+	ldp	q22, q23, [x6, #128]			//load h6l | h6h
 	ext	v22.16b, v22.16b, v22.16b, #8
 	ext	v23.16b, v23.16b, v23.16b, #8
 	b.gt	L256_dec_blocks_more_than_7
@@ -8250,7 +8250,7 @@ L256_dec_tail:	//TAIL
 	mov	v5.16b, v1.16b
 	b.gt	L256_dec_blocks_more_than_3
 
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 	sub	v30.4s, v30.4s, v31.4s
 	mov	v7.16b, v6.16b
 
@@ -8265,7 +8265,7 @@ L256_dec_tail:	//TAIL
 	b.gt	L256_dec_blocks_more_than_1
 
 	sub	v30.4s, v30.4s, v31.4s
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	b	L256_dec_blocks_less_than_1
 L256_dec_blocks_more_than_7:	//blocks	left >  7
 	rev64	v8.16b, v9.16b						//GHASH final-7 block
@@ -8360,13 +8360,13 @@ L256_dec_blocks_more_than_4:	//blocks	left >  4
 .long	0xce04752c	//eor3 v12.16b, v9.16b, v4.16b, v29.16b				//AES final-3 block - result
 L256_dec_blocks_more_than_3:	//blocks	left >  3
 
-	ldr	q25, [x3, #112]				//load h4l | h4h
+	ldr	q25, [x6, #80]				//load h4l | h4h
 	ext	v25.16b, v25.16b, v25.16b, #8
 	rev64	v8.16b, v9.16b						//GHASH final-3 block
 
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
 	ldr	q9, [x0], #16				//AES final-2 block - load ciphertext
-	ldr	q24, [x3, #96]				//load h4k | h3k
+	ldr	q24, [x6, #64]				//load h4k | h3k
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-3 block - mid
 	st1	{ v12.16b}, [x2], #16			 	//AES final-3 block - store result
@@ -8390,7 +8390,7 @@ L256_dec_blocks_more_than_2:	//blocks	left >  2
 
 	rev64	v8.16b, v9.16b						//GHASH final-2 block
 
-	ldr	q23, [x3, #80]				//load h3l | h3h
+	ldr	q23, [x6, #48]				//load h3l | h3h
 	ext	v23.16b, v23.16b, v23.16b, #8
 	ldr	q9, [x0], #16				//AES final-1 block - load ciphertext
 
@@ -8418,14 +8418,14 @@ L256_dec_blocks_more_than_1:	//blocks	left >  1
 	eor	v8.16b, v8.16b, v16.16b					//feed in partial tag
 
 	ins	v27.d[0], v8.d[1]					//GHASH final-1 block - mid
-	ldr	q22, [x3, #64]				//load h2l | h2h
+	ldr	q22, [x6, #32]				//load h2l | h2h
 	ext	v22.16b, v22.16b, v22.16b, #8
 
 	eor	v27.8b, v27.8b, v8.8b				//GHASH final-1 block - mid
 	ldr	q9, [x0], #16				//AES final block - load ciphertext
 	st1	{ v12.16b}, [x2], #16			 	//AES final-1 block - store result
 
-	ldr	q21, [x3, #48]				//load h2k | h1k
+	ldr	q21, [x6, #16]				//load h2k | h1k
 	pmull	v26.1q, v8.1d, v22.1d				//GHASH final-1 block - low
 
 	ins	v27.d[1], v27.d[0]					//GHASH final-1 block - mid
@@ -8444,7 +8444,7 @@ L256_dec_blocks_more_than_1:	//blocks	left >  1
 L256_dec_blocks_less_than_1:	//blocks	left <= 1
 
 	ld1	{ v26.16b}, [x2]					//load existing bytes where the possibly partial last block is to be stored
-	mvn	x6, xzr						//temp0_x = 0xffffffffffffffff
+	mvn	x7, xzr						//temp0_x = 0xffffffffffffffff
 	and	x1, x1, #127				//bit_length %= 128
 
 	sub	x1, x1, #128				//bit_length -= 128
@@ -8455,18 +8455,18 @@ L256_dec_blocks_less_than_1:	//blocks	left <= 1
 
 	and	x1, x1, #127			 	//bit_length %= 128
 
-	lsr	x6, x6, x1				//temp0_x is mask for top 64b of last block
+	lsr	x7, x7, x1				//temp0_x is mask for top 64b of last block
 	cmp	x1, #64
-	mvn	x7, xzr						//temp1_x = 0xffffffffffffffff
+	mvn	x8, xzr						//temp1_x = 0xffffffffffffffff
 
-	csel	x14, x6, xzr, lt
-	csel	x13, x7, x6, lt
+	csel	x14, x7, xzr, lt
+	csel	x13, x8, x7, lt
 
 	mov	v0.d[0], x13					//ctr0b is mask for last block
 	mov	v0.d[1], x14
 
 	and	v9.16b, v9.16b, v0.16b					//possibly partial last block has zeroes in highest bits
-	ldr	q20, [x3, #32]				//load h1l | h1h
+	ldr	q20, [x6]				//load h1l | h1h
 	ext	v20.16b, v20.16b, v20.16b, #8
 	bif	v12.16b, v26.16b, v0.16b					//insert existing bytes in top end of result before storing
 
diff --git a/generated-src/win-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S b/generated-src/win-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S
index 12d19d11e..c49b2719c 100644
--- a/generated-src/win-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S
+++ b/generated-src/win-aarch64/crypto/fipsmodule/aesv8-gcm-armv8.S
@@ -92,7 +92,7 @@ aes_gcm_enc_kernel:
 	ldr	q23, [x8, #80]                                 // load rk5
 	aese	v1.16b, v19.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 1
-	ldr	q14, [x3, #80]                         // load h3l | h3h
+	ldr	q14, [x6, #48]                              // load h3l | h3h
 	ext	v14.16b, v14.16b, v14.16b, #8
 	aese	v3.16b, v18.16b
 	aesmc	v3.16b, v3.16b          // AES block 3 - round 0
@@ -101,14 +101,14 @@ aes_gcm_enc_kernel:
 	ldr	q22, [x8, #64]                                 // load rk4
 	aese	v1.16b, v20.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 2
-	ldr	q13, [x3, #64]                         // load h2l | h2h
+	ldr	q13, [x6, #32]                              // load h2l | h2h
 	ext	v13.16b, v13.16b, v13.16b, #8
 	aese	v3.16b, v19.16b
 	aesmc	v3.16b, v3.16b          // AES block 3 - round 1
 	ldr	q30, [x8, #192]                               // load rk12
 	aese	v2.16b, v20.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 2
-	ldr	q15, [x3, #112]                        // load h4l | h4h
+	ldr	q15, [x6, #80]                              // load h4l | h4h
 	ext	v15.16b, v15.16b, v15.16b, #8
 	aese	v1.16b, v21.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 3
@@ -151,7 +151,7 @@ aes_gcm_enc_kernel:
 	ldr	q27, [x8, #144]                                // load rk9
 	aese	v0.16b, v24.16b
 	aesmc	v0.16b, v0.16b          // AES block 0 - round 6
-	ldr	q12, [x3, #32]                         // load h1l | h1h
+	ldr	q12, [x6]                                   // load h1l | h1h
 	ext	v12.16b, v12.16b, v12.16b, #8
 	aese	v2.16b, v24.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 6
@@ -854,15 +854,15 @@ aes_gcm_dec_kernel:
 	ldr	q19, [x8, #16]                                 // load rk1
 	aese	v0.16b, v18.16b
 	aesmc	v0.16b, v0.16b          // AES block 0 - round 0
-	ldr	q14, [x3, #80]                         // load h3l | h3h
+	ldr	q14, [x6, #48]                              // load h3l | h3h
 	ext	v14.16b, v14.16b, v14.16b, #8
 	aese	v3.16b, v18.16b
 	aesmc	v3.16b, v3.16b          // AES block 3 - round 0
-	ldr	q15, [x3, #112]                        // load h4l | h4h
+	ldr	q15, [x6, #80]                              // load h4l | h4h
 	ext	v15.16b, v15.16b, v15.16b, #8
 	aese	v1.16b, v18.16b
 	aesmc	v1.16b, v1.16b          // AES block 1 - round 0
-	ldr	q13, [x3, #64]                         // load h2l | h2h
+	ldr	q13, [x6, #32]                              // load h2l | h2h
 	ext	v13.16b, v13.16b, v13.16b, #8
 	aese	v2.16b, v18.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 0
@@ -882,7 +882,7 @@ aes_gcm_dec_kernel:
 	ldr	q30, [x8, #192]                               // load rk12
 	aese	v0.16b, v20.16b
 	aesmc	v0.16b, v0.16b          // AES block 0 - round 2
-	ldr	q12, [x3, #32]                         // load h1l | h1h
+	ldr	q12, [x6]                                   // load h1l | h1h
 	ext	v12.16b, v12.16b, v12.16b, #8
 	aese	v2.16b, v20.16b
 	aesmc	v2.16b, v2.16b          // AES block 2 - round 2
diff --git a/generated-src/win-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.asm b/generated-src/win-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.asm
index 67c43c940..a344209f4 100644
--- a/generated-src/win-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.asm
+++ b/generated-src/win-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.asm
@@ -411,17 +411,18 @@ $L$SEH_prolog_aesni_gcm_decrypt_20:
 $L$SEH_prolog_aesni_gcm_decrypt_21:
 	vzeroupper
 
+	mov	r12,QWORD[64+rbp]
 	vmovdqu	xmm1,XMMWORD[rdi]
 	add	rsp,-128
 	mov	ebx,DWORD[12+rdi]
 	lea	r11,[$L$bswap_mask]
 	lea	r14,[((-128))+r9]
 	mov	r15,0xf80
-	vmovdqu	xmm8,XMMWORD[rsi]
+	vmovdqu	xmm8,XMMWORD[r12]
 	and	rsp,-128
 	vmovdqu	xmm0,XMMWORD[r11]
 	lea	r9,[128+r9]
-	lea	rsi,[((32+32))+rsi]
+	lea	rsi,[32+rsi]
 	mov	r10d,DWORD[((240-128))+r9]
 	vpshufb	xmm8,xmm8,xmm0
 
@@ -466,6 +467,7 @@ $L$dec_no_key_aliasing:
 
 	call	_aesni_ctr32_ghash_6x
 
+	mov	r12,QWORD[64+rbp]
 	vmovups	XMMWORD[(-96)+rdx],xmm9
 	vmovups	XMMWORD[(-80)+rdx],xmm10
 	vmovups	XMMWORD[(-64)+rdx],xmm11
@@ -474,7 +476,7 @@ $L$dec_no_key_aliasing:
 	vmovups	XMMWORD[(-16)+rdx],xmm14
 
 	vpshufb	xmm8,xmm8,XMMWORD[r11]
-	vmovdqu	XMMWORD[(-64)+rsi],xmm8
+	vmovdqu	XMMWORD[r12],xmm8
 
 	vzeroupper
 	movaps	xmm6,XMMWORD[((-208))+rbp]
@@ -722,8 +724,9 @@ $L$enc_no_key_aliasing:
 
 	call	_aesni_ctr32_6x
 
-	vmovdqu	xmm8,XMMWORD[rsi]
-	lea	rsi,[((32+32))+rsi]
+	mov	r12,QWORD[64+rbp]
+	lea	rsi,[32+rsi]
+	vmovdqu	xmm8,XMMWORD[r12]
 	sub	r8,12
 	mov	rax,0x60*2
 	vpshufb	xmm8,xmm8,xmm0
@@ -901,8 +904,9 @@ $L$enc_no_key_aliasing:
 	vpclmulqdq	xmm8,xmm8,xmm3,0x10
 	vpxor	xmm2,xmm2,xmm7
 	vpxor	xmm8,xmm8,xmm2
+	mov	r12,QWORD[64+rbp]
 	vpshufb	xmm8,xmm8,XMMWORD[r11]
-	vmovdqu	XMMWORD[(-64)+rsi],xmm8
+	vmovdqu	XMMWORD[r12],xmm8
 
 	vzeroupper
 	movaps	xmm6,XMMWORD[((-208))+rbp]
