From 27bf444f55717acc9e2d28e92091f15568ef07f6 Mon Sep 17 00:00:00 2001
From: John Harrison <jargh@amazon.com>
Date: Mon, 19 Jul 2021 12:47:08 -0700
Subject: [PATCH] Standardize numeric literals in ARM code to use # symbol

The code previously had a mix of cases where the "#" prefix was
used before numeric literals and cases where it wasn't. It makes
no difference to gcc or the GNU assembler (and hence this change
does not alter the object files or the proofs) but this makes
things more consistent and may help other tools processing the
ARM asm files.

s2n-bignum original commit: https://github.com/awslabs/s2n-bignum/commit/a1e2fbc3bfb1577aa038a66e2574d600c7b4816b
---
 arm/fastmul/bignum_emontredc_8n.S |  58 ++---
 arm/fastmul/bignum_kmul_16_32.S   | 268 +++++++++++------------
 arm/fastmul/bignum_ksqr_16_32.S   | 198 ++++++++---------
 arm/fastmul/bignum_ksqr_32_64.S   | 340 +++++++++++++++---------------
 arm/generic/bignum_ge.S           |  28 +--
 arm/generic/bignum_mul.S          |  14 +-
 arm/generic/bignum_optsub.S       |   4 +-
 arm/p384/bignum_add_p384.S        |  10 +-
 arm/p384/bignum_bigendian_6.S     |  44 ++--
 arm/p384/bignum_cmul_p384.S       |  12 +-
 arm/p384/bignum_deamont_p384.S    |  24 +--
 arm/p384/bignum_demont_p384.S     |  16 +-
 arm/p384/bignum_double_p384.S     |   6 +-
 arm/p384/bignum_half_p384.S       |  28 +--
 arm/p384/bignum_mod_n384.S        |  48 ++---
 arm/p384/bignum_mod_n384_6.S      |  12 +-
 arm/p384/bignum_mod_p384.S        |  50 ++---
 arm/p384/bignum_mod_p384_6.S      |   6 +-
 arm/p384/bignum_montmul_p384.S    |  80 +++----
 arm/p384/bignum_montsqr_p384.S    |  36 ++--
 arm/p384/bignum_mux_6.S           |   8 +-
 arm/p384/bignum_neg_p384.S        |  16 +-
 arm/p384/bignum_nonzero_6.S       |   4 +-
 arm/p384/bignum_optneg_p384.S     |  20 +-
 arm/p384/bignum_sub_p384.S        |   4 +-
 arm/p384/bignum_tomont_p384.S     |  24 +--
 arm/p384/bignum_triple_p384.S     |  22 +-
 arm/p521/bignum_add_p521.S        |  28 +--
 arm/p521/bignum_double_p521.S     |  12 +-
 arm/p521/bignum_half_p521.S       |  38 ++--
 arm/p521/bignum_sub_p521.S        |  26 +--
 31 files changed, 742 insertions(+), 742 deletions(-)

diff --git a/arm/fastmul/bignum_emontredc_8n.S b/arm/fastmul/bignum_emontredc_8n.S
index 426bd73db..636f6111c 100644
--- a/arm/fastmul/bignum_emontredc_8n.S
+++ b/arm/fastmul/bignum_emontredc_8n.S
@@ -45,7 +45,7 @@
         mul     \l, \t, \h
         umulh   \h, \t, \h
         cinv    \c, \c, cc
-        adds    xzr, \c, 1
+        adds    xzr, \c, #1
         eor     \l, \l, \c
         adcs    \a, \a, \l
         eor     \h, \h, \c
@@ -132,7 +132,7 @@
                 ldp     u2, u3, [z]
                 adds    c0, c0, u2
                 adcs    c1, c1, u3
-                ldp     u2, u3, [z, 16]
+                ldp     u2, u3, [z, #16]
                 adcs    c2, c2, u2
                 adcs    c3, c3, u3
                 adc     c4, xzr, xzr
@@ -198,7 +198,7 @@
                 mov     c0, u4
 
                 stp     u0, u1, [z]
-                stp     u2, u3, [z, 16]
+                stp     u2, u3, [z, #16]
 .endm
 
 // *****************************************************
@@ -207,22 +207,22 @@
 
 bignum_emontredc_8n:
 
-                stp     x19, x20, [sp, -16]!
-                stp     x21, x22, [sp, -16]!
-                stp     x23, x24, [sp, -16]!
-                stp     x25, x26, [sp, -16]!
-                stp     x27, x28, [sp, -16]!
+                stp     x19, x20, [sp, #-16]!
+                stp     x21, x22, [sp, #-16]!
+                stp     x23, x24, [sp, #-16]!
+                stp     x25, x26, [sp, #-16]!
+                stp     x27, x28, [sp, #-16]!
 
 // Set up (k/4 - 1)<<5 which is used as inner count and pointer fixup
 // ns i = k/4 as the outer loop count.
 // At this point skip everything if k/4 = 0, returning our x0 = 0 value
 
-                lsr     k4m1, x0, 2
+                lsr     k4m1, x0, #2
                 mov     i, k4m1
-                subs    c, k4m1, 1
+                subs    c, k4m1, #1
                 bcc     end
                 mov     tc, xzr
-                lsl     k4m1, c, 5
+                lsl     k4m1, c, #5
 
 // Outer loop, one digit of Montgomery reduction adding in word * m.
 // Rather than propagating the carry to the end each time, we
@@ -233,12 +233,12 @@ outerloop:
 // Load [u3;u2;u1;u0] = bottom 4 digits of the input at current window
 
                 ldp     u0, u1, [z]
-                ldp     u2, u3, [z, 16]
+                ldp     u2, u3, [z, #16]
 
 // Load the bottom 4 digits of m
 
                 ldp     b0, b1, [m]
-                ldp     b2, b3, [m, 16]
+                ldp     b2, b3, [m, #16]
 
 // Montgomery step 0
 
@@ -330,36 +330,36 @@ outerloop:
 // We don't use these ourselves again though; they stay in [a3;a2;a1;a0]
 
                 stp     a0, a1, [z]
-                stp     a2, a3, [z, 16]
+                stp     a2, a3, [z, #16]
 
 // Repeated multiply-add block to do the k/4-1 remaining 4-digit chunks
 
                 cbz     k4m1, madddone
                 mov     j, k4m1
 maddloop:
-                add     m, m, 32
-                add     z, z, 32
+                add     m, m, #32
+                add     z, z, #32
 
                 ldp     b0, b1, [m]
-                ldp     b2, b3, [m, 16]
+                ldp     b2, b3, [m, #16]
                 madd4
-                subs    j, j, 32
+                subs    j, j, #32
                 bne     maddloop
 madddone:
 
 // Add the carry out to the existing z contents, propagating the
 // top carry tc up by 32 places as we move "leftwards".
 
-                ldp     u0, u1, [z, 32]
-                ldp     u2, u3, [z, 48]
+                ldp     u0, u1, [z, #32]
+                ldp     u2, u3, [z, #48]
                 adds    xzr, tc, tc
                 adcs    u0, u0, c0
                 adcs    u1, u1, c1
                 adcs    u2, u2, c2
                 adcs    u3, u3, c3
                 csetm   tc, cs
-                stp     u0, u1, [z, 32]
-                stp     u2, u3, [z, 48]
+                stp     u0, u1, [z, #32]
+                stp     u2, u3, [z, #48]
 
 // Compensate for the repeated bumps in m and z in the inner loop
 
@@ -368,8 +368,8 @@ madddone:
 
 // Bump up z only and keep going
 
-                add     z, z, 32
-                subs    i, i, 1
+                add     z, z, #32
+                subs    i, i, #1
                 bne     outerloop
 
 // Return the top carry as 0 or 1 (it's currently a bitmask)
@@ -377,10 +377,10 @@ madddone:
                 neg     x0, tc
 
 end:
-                ldp     x27, x28, [sp], 16
-                ldp     x25, x26, [sp], 16
-                ldp     x23, x24, [sp], 16
-                ldp     x21, x22, [sp], 16
-                ldp     x19, x20, [sp], 16
+                ldp     x27, x28, [sp], #16
+                ldp     x25, x26, [sp], #16
+                ldp     x23, x24, [sp], #16
+                ldp     x21, x22, [sp], #16
+                ldp     x19, x20, [sp], #16
 
                 ret
diff --git a/arm/fastmul/bignum_kmul_16_32.S b/arm/fastmul/bignum_kmul_16_32.S
index 7da0dc558..0b25e3e8b 100644
--- a/arm/fastmul/bignum_kmul_16_32.S
+++ b/arm/fastmul/bignum_kmul_16_32.S
@@ -45,12 +45,12 @@ bignum_kmul_16_32:
 
 // Save registers, including return address
 
-        stp     x19, x20, [sp, -16]!
-        stp     x21, x22, [sp, -16]!
-        stp     x23, x24, [sp, -16]!
-        stp     x25, x26, [sp, -16]!
-        stp     x27, x28, [sp, -16]!
-        stp     x29, x30, [sp, -16]!
+        stp     x19, x20, [sp, #-16]!
+        stp     x21, x22, [sp, #-16]!
+        stp     x23, x24, [sp, #-16]!
+        stp     x25, x26, [sp, #-16]!
+        stp     x27, x28, [sp, #-16]!
+        stp     x29, x30, [sp, #-16]!
 
 // Move parameters into subroutine-safe places
 
@@ -66,20 +66,20 @@ bignum_kmul_16_32:
 // Compute absolute difference [t..] = |x_lo - x_hi|
 // and the sign s = sgn(x_lo - x_hi) as a bitmask (all 1s for negative)
 
-        ldp     x10, x11, [x, 0]
-        ldp     x8, x9, [x, 64]
+        ldp     x10, x11, [x, #0]
+        ldp     x8, x9, [x, #64]
         subs    x10, x10, x8
         sbcs    x11, x11, x9
-        ldp     x12, x13, [x, 16]
-        ldp     x8, x9, [x, 80]
+        ldp     x12, x13, [x, #16]
+        ldp     x8, x9, [x, #80]
         sbcs    x12, x12, x8
         sbcs    x13, x13, x9
-        ldp     x14, x15, [x, 32]
-        ldp     x8, x9, [x, 96]
+        ldp     x14, x15, [x, #32]
+        ldp     x8, x9, [x, #96]
         sbcs    x14, x14, x8
         sbcs    x15, x15, x9
-        ldp     x16, x17, [x, 48]
-        ldp     x8, x9, [x, 112]
+        ldp     x16, x17, [x, #48]
+        ldp     x8, x9, [x, #112]
         sbcs    x16, x16, x8
         sbcs    x17, x17, x9
         csetm   s, cc
@@ -93,42 +93,42 @@ bignum_kmul_16_32:
         adcs    x12, x12, xzr
         eor     x13, x13, s
         adcs    x13, x13, xzr
-        stp     x12, x13, [t, 16]
+        stp     x12, x13, [t, #16]
         eor     x14, x14, s
         adcs    x14, x14, xzr
         eor     x15, x15, s
         adcs    x15, x15, xzr
-        stp     x14, x15, [t, 32]
+        stp     x14, x15, [t, #32]
         eor     x16, x16, s
         adcs    x16, x16, xzr
         eor     x17, x17, s
         adcs    x17, x17, xzr
-        stp     x16, x17, [t, 48]
+        stp     x16, x17, [t, #48]
 
 // Compute H = x_hi * y_hi in top half of buffer (size 8 x 8 -> 16)
 
-        add     x0, z, 128
-        add     x1, x, 64
-        add     x2, y, 64
+        add     x0, z, #128
+        add     x1, x, #64
+        add     x2, y, #64
         bl      local_mul_8_16
 
 // Compute the other absolute difference [t+8..] = |y_hi - y_lo|
 // Collect the combined product sign bitmask (all 1s for negative) in s
 
-        ldp     x10, x11, [y, 0]
-        ldp     x8, x9, [y, 64]
+        ldp     x10, x11, [y, #0]
+        ldp     x8, x9, [y, #64]
         subs    x10, x8, x10
         sbcs    x11, x9, x11
-        ldp     x12, x13, [y, 16]
-        ldp     x8, x9, [y, 80]
+        ldp     x12, x13, [y, #16]
+        ldp     x8, x9, [y, #80]
         sbcs    x12, x8, x12
         sbcs    x13, x9, x13
-        ldp     x14, x15, [y, 32]
-        ldp     x8, x9, [y, 96]
+        ldp     x14, x15, [y, #32]
+        ldp     x8, x9, [y, #96]
         sbcs    x14, x8, x14
         sbcs    x15, x9, x15
-        ldp     x16, x17, [y, 48]
-        ldp     x8, x9, [y, 112]
+        ldp     x16, x17, [y, #48]
+        ldp     x8, x9, [y, #112]
         sbcs    x16, x8, x16
         sbcs    x17, x9, x17
         csetm   m, cc
@@ -137,22 +137,22 @@ bignum_kmul_16_32:
         adcs    x10, x10, xzr
         eor     x11, x11, m
         adcs    x11, x11, xzr
-        stp     x10, x11, [t, 64]
+        stp     x10, x11, [t, #64]
         eor     x12, x12, m
         adcs    x12, x12, xzr
         eor     x13, x13, m
         adcs    x13, x13, xzr
-        stp     x12, x13, [t, 80]
+        stp     x12, x13, [t, #80]
         eor     x14, x14, m
         adcs    x14, x14, xzr
         eor     x15, x15, m
         adcs    x15, x15, xzr
-        stp     x14, x15, [t, 96]
+        stp     x14, x15, [t, #96]
         eor     x16, x16, m
         adcs    x16, x16, xzr
         eor     x17, x17, m
         adcs    x17, x17, xzr
-        stp     x16, x17, [t, 112]
+        stp     x16, x17, [t, #112]
         eor     s, s, m
 
 // Compute H' = H + L_top in place of H (it cannot overflow)
@@ -160,35 +160,35 @@ bignum_kmul_16_32:
 
         .set    i, 0
 
-        ldp     x10, x11, [z, 128+8*i]
-        ldp     x12, x13, [z, 64+8*i]
+        ldp     x10, x11, [z, #128+8*i]
+        ldp     x12, x13, [z, #64+8*i]
         adds    x10, x10, x12
         adcs    x11, x11, x13
-        stp     x10, x11, [z, 128+8*i]
+        stp     x10, x11, [z, #128+8*i]
         .set    i, (i+2)
 
 .rep 3
-        ldp     x10, x11, [z, 128+8*i]
-        ldp     x12, x13, [z, 64+8*i]
+        ldp     x10, x11, [z, #128+8*i]
+        ldp     x12, x13, [z, #64+8*i]
         adcs    x10, x10, x12
         adcs    x11, x11, x13
-        stp     x10, x11, [z, 128+8*i]
+        stp     x10, x11, [z, #128+8*i]
         .set    i, (i+2)
 .endr
 
 .rep 4
-        ldp     x10, x11, [z, 128+8*i]
+        ldp     x10, x11, [z, #128+8*i]
         adcs    x10, x10, xzr
         adcs    x11, x11, xzr
-        stp     x10, x11, [z, 128+8*i]
+        stp     x10, x11, [z, #128+8*i]
         .set    i, (i+2)
 .endr
 
 // Compute M = |x_lo - x_hi| * |y_hi - y_lo| in [t+16...], size 16
 
-        add     x0, t, 128
+        add     x0, t, #128
         mov     x1, t
-        add     x2, t, 64
+        add     x2, t, #64
         bl      local_mul_8_16
 
 // Add the interlocking H' and L_bot terms, storing in registers x15..x0
@@ -196,35 +196,35 @@ bignum_kmul_16_32:
 // (Note that we no longer need the input x was pointing at.)
 
         ldp     x0, x1, [z]
-        ldp     x16, x17, [z, 128]
+        ldp     x16, x17, [z, #128]
         adds    x0, x0, x16
         adcs    x1, x1, x17
-        ldp     x2, x3, [z, 16]
-        ldp     x16, x17, [z, 144]
+        ldp     x2, x3, [z, #16]
+        ldp     x16, x17, [z, #144]
         adcs    x2, x2, x16
         adcs    x3, x3, x17
-        ldp     x4, x5, [z, 32]
-        ldp     x16, x17, [z, 160]
+        ldp     x4, x5, [z, #32]
+        ldp     x16, x17, [z, #160]
         adcs    x4, x4, x16
         adcs    x5, x5, x17
-        ldp     x6, x7, [z, 48]
-        ldp     x16, x17, [z, 176]
+        ldp     x6, x7, [z, #48]
+        ldp     x16, x17, [z, #176]
         adcs    x6, x6, x16
         adcs    x7, x7, x17
-        ldp     x8, x9, [z, 128]
-        ldp     x16, x17, [z, 192]
+        ldp     x8, x9, [z, #128]
+        ldp     x16, x17, [z, #192]
         adcs    x8, x8, x16
         adcs    x9, x9, x17
-        ldp     x10, x11, [z, 144]
-        ldp     x16, x17, [z, 208]
+        ldp     x10, x11, [z, #144]
+        ldp     x16, x17, [z, #208]
         adcs    x10, x10, x16
         adcs    x11, x11, x17
-        ldp     x12, x13, [z, 160]
-        ldp     x16, x17, [z, 224]
+        ldp     x12, x13, [z, #160]
+        ldp     x16, x17, [z, #224]
         adcs    x12, x12, x16
         adcs    x13, x13, x17
-        ldp     x14, x15, [z, 176]
-        ldp     x16, x17, [z, 240]
+        ldp     x14, x15, [z, #176]
+        ldp     x16, x17, [z, #240]
         adcs    x14, x14, x16
         adcs    x15, x15, x17
 
@@ -234,54 +234,54 @@ bignum_kmul_16_32:
 
         cmn     s, s
 
-        ldp     x16, x17, [t, 128]
+        ldp     x16, x17, [t, #128]
         eor     x16, x16, s
         adcs    x0, x0, x16
         eor     x17, x17, s
         adcs    x1, x1, x17
-        stp     x0, x1, [z, 64]
-        ldp     x16, x17, [t, 144]
+        stp     x0, x1, [z, #64]
+        ldp     x16, x17, [t, #144]
         eor     x16, x16, s
         adcs    x2, x2, x16
         eor     x17, x17, s
         adcs    x3, x3, x17
-        stp     x2, x3, [z, 80]
-        ldp     x16, x17, [t, 160]
+        stp     x2, x3, [z, #80]
+        ldp     x16, x17, [t, #160]
         eor     x16, x16, s
         adcs    x4, x4, x16
         eor     x17, x17, s
         adcs    x5, x5, x17
-        stp     x4, x5, [z, 96]
-        ldp     x16, x17, [t, 176]
+        stp     x4, x5, [z, #96]
+        ldp     x16, x17, [t, #176]
         eor     x16, x16, s
         adcs    x6, x6, x16
         eor     x17, x17, s
         adcs    x7, x7, x17
-        stp     x6, x7, [z, 112]
-        ldp     x16, x17, [t, 192]
+        stp     x6, x7, [z, #112]
+        ldp     x16, x17, [t, #192]
         eor     x16, x16, s
         adcs    x8, x8, x16
         eor     x17, x17, s
         adcs    x9, x9, x17
-        stp     x8, x9, [z, 128]
-        ldp     x16, x17, [t, 208]
+        stp     x8, x9, [z, #128]
+        ldp     x16, x17, [t, #208]
         eor     x16, x16, s
         adcs    x10, x10, x16
         eor     x17, x17, s
         adcs    x11, x11, x17
-        stp     x10, x11, [z, 144]
-        ldp     x16, x17, [t, 224]
+        stp     x10, x11, [z, #144]
+        ldp     x16, x17, [t, #224]
         eor     x16, x16, s
         adcs    x12, x12, x16
         eor     x17, x17, s
         adcs    x13, x13, x17
-        stp     x12, x13, [z, 160]
-        ldp     x16, x17, [t, 240]
+        stp     x12, x13, [z, #160]
+        ldp     x16, x17, [t, #240]
         eor     x16, x16, s
         adcs    x14, x14, x16
         eor     x17, x17, s
         adcs    x15, x15, x17
-        stp     x14, x15, [z, 176]
+        stp     x14, x15, [z, #176]
 
 // Get the next digits effectively resulting so far starting at 24
 
@@ -291,31 +291,31 @@ bignum_kmul_16_32:
 // Now the final 8 digits of padding; the first one is special in using y
 // and also in getting the carry chain started
 
-        ldp     x10, x11, [z, 192]
+        ldp     x10, x11, [z, #192]
         adds    x10, x10, y
         adcs    x11, x11, t
-        stp     x10, x11, [z, 192]
-        ldp     x10, x11, [z, 208]
+        stp     x10, x11, [z, #192]
+        ldp     x10, x11, [z, #208]
         adcs    x10, x10, t
         adcs    x11, x11, t
-        stp     x10, x11, [z, 208]
-        ldp     x10, x11, [z, 224]
+        stp     x10, x11, [z, #208]
+        ldp     x10, x11, [z, #224]
         adcs    x10, x10, t
         adcs    x11, x11, t
-        stp     x10, x11, [z, 224]
-        ldp     x10, x11, [z, 240]
+        stp     x10, x11, [z, #224]
+        ldp     x10, x11, [z, #240]
         adcs    x10, x10, t
         adcs    x11, x11, t
-        stp     x10, x11, [z, 240]
+        stp     x10, x11, [z, #240]
 
 // Restore registers and return
 
-        ldp     x29, x30, [sp], 16
-        ldp     x27, x28, [sp], 16
-        ldp     x25, x26, [sp], 16
-        ldp     x23, x24, [sp], 16
-        ldp     x21, x22, [sp], 16
-        ldp     x19, x20, [sp], 16
+        ldp     x29, x30, [sp], #16
+        ldp     x27, x28, [sp], #16
+        ldp     x25, x26, [sp], #16
+        ldp     x23, x24, [sp], #16
+        ldp     x21, x22, [sp], #16
+        ldp     x19, x20, [sp], #16
 
         ret
 
@@ -326,8 +326,8 @@ bignum_kmul_16_32:
 local_mul_8_16:
         ldp     x3, x4, [x1]
         ldp     x7, x8, [x2]
-        ldp     x5, x6, [x1, 16]
-        ldp     x9, x10, [x2, 16]
+        ldp     x5, x6, [x1, #16]
+        ldp     x9, x10, [x2, #16]
         mul     x11, x3, x7
         mul     x15, x4, x8
         mul     x16, x5, x9
@@ -359,7 +359,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x16, x16, x22
         eor     x21, x21, x20
@@ -373,7 +373,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x12, x12, x22
         eor     x21, x21, x20
@@ -391,7 +391,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x15, x15, x22
         eor     x21, x21, x20
@@ -406,7 +406,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x13, x13, x22
         eor     x21, x21, x20
@@ -423,7 +423,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x14, x14, x22
         eor     x21, x21, x20
@@ -439,7 +439,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x14, x14, x22
         eor     x21, x21, x20
@@ -447,14 +447,14 @@ local_mul_8_16:
         adcs    x16, x16, x20
         adcs    x17, x17, x20
         adc     x19, x19, x20
-        ldp     x3, x4, [x1, 32]
+        ldp     x3, x4, [x1, #32]
         stp     x11, x12, [x0]
-        ldp     x7, x8, [x2, 32]
-        stp     x13, x14, [x0, 16]
-        ldp     x5, x6, [x1, 48]
-        stp     x15, x16, [x0, 32]
-        ldp     x9, x10, [x2, 48]
-        stp     x17, x19, [x0, 48]
+        ldp     x7, x8, [x2, #32]
+        stp     x13, x14, [x0, #16]
+        ldp     x5, x6, [x1, #48]
+        stp     x15, x16, [x0, #32]
+        ldp     x9, x10, [x2, #48]
+        stp     x17, x19, [x0, #48]
         mul     x11, x3, x7
         mul     x15, x4, x8
         mul     x16, x5, x9
@@ -478,10 +478,10 @@ local_mul_8_16:
         adcs    x16, x19, x16
         adcs    x17, xzr, x17
         adc     x19, xzr, x19
-        ldp     x22, x21, [x0, 32]
+        ldp     x22, x21, [x0, #32]
         adds    x11, x11, x22
         adcs    x12, x12, x21
-        ldp     x22, x21, [x0, 48]
+        ldp     x22, x21, [x0, #48]
         adcs    x13, x13, x22
         adcs    x14, x14, x21
         adcs    x15, x15, xzr
@@ -496,7 +496,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x16, x16, x22
         eor     x21, x21, x20
@@ -510,7 +510,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x12, x12, x22
         eor     x21, x21, x20
@@ -528,7 +528,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x15, x15, x22
         eor     x21, x21, x20
@@ -543,7 +543,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x13, x13, x22
         eor     x21, x21, x20
@@ -560,7 +560,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x14, x14, x22
         eor     x21, x21, x20
@@ -576,7 +576,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x14, x14, x22
         eor     x21, x21, x20
@@ -587,19 +587,19 @@ local_mul_8_16:
         ldp     x22, x21, [x1]
         subs    x3, x3, x22
         sbcs    x4, x4, x21
-        ldp     x22, x21, [x1, 16]
+        ldp     x22, x21, [x1, #16]
         sbcs    x5, x5, x22
         sbcs    x6, x6, x21
         csetm   x24, cc
-        stp     x11, x12, [x0, 64]
+        stp     x11, x12, [x0, #64]
         ldp     x22, x21, [x2]
         subs    x7, x22, x7
         sbcs    x8, x21, x8
-        ldp     x22, x21, [x2, 16]
+        ldp     x22, x21, [x2, #16]
         sbcs    x9, x22, x9
         sbcs    x10, x21, x10
         csetm   x1, cc
-        stp     x13, x14, [x0, 80]
+        stp     x13, x14, [x0, #80]
         eor     x3, x3, x24
         subs    x3, x3, x24
         eor     x4, x4, x24
@@ -608,7 +608,7 @@ local_mul_8_16:
         sbcs    x5, x5, x24
         eor     x6, x6, x24
         sbc     x6, x6, x24
-        stp     x15, x16, [x0, 96]
+        stp     x15, x16, [x0, #96]
         eor     x7, x7, x1
         subs    x7, x7, x1
         eor     x8, x8, x1
@@ -617,7 +617,7 @@ local_mul_8_16:
         sbcs    x9, x9, x1
         eor     x10, x10, x1
         sbc     x10, x10, x1
-        stp     x17, x19, [x0, 112]
+        stp     x17, x19, [x0, #112]
         eor     x1, x1, x24
         mul     x11, x3, x7
         mul     x15, x4, x8
@@ -650,7 +650,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x16, x16, x22
         eor     x21, x21, x20
@@ -664,7 +664,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x12, x12, x22
         eor     x21, x21, x20
@@ -682,7 +682,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x15, x15, x22
         eor     x21, x21, x20
@@ -697,7 +697,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x13, x13, x22
         eor     x21, x21, x20
@@ -714,7 +714,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x14, x14, x22
         eor     x21, x21, x20
@@ -730,7 +730,7 @@ local_mul_8_16:
         mul     x22, x24, x21
         umulh   x21, x24, x21
         cinv    x20, x20, cc
-        cmn     x20, 0x1
+        cmn     x20, #0x1
         eor     x22, x22, x20
         adcs    x14, x14, x22
         eor     x21, x21, x20
@@ -739,22 +739,22 @@ local_mul_8_16:
         adcs    x17, x17, x20
         adc     x19, x19, x20
         ldp     x3, x4, [x0]
-        ldp     x7, x8, [x0, 64]
+        ldp     x7, x8, [x0, #64]
         adds    x3, x3, x7
         adcs    x4, x4, x8
-        ldp     x5, x6, [x0, 16]
-        ldp     x9, x10, [x0, 80]
+        ldp     x5, x6, [x0, #16]
+        ldp     x9, x10, [x0, #80]
         adcs    x5, x5, x9
         adcs    x6, x6, x10
-        ldp     x20, x21, [x0, 96]
+        ldp     x20, x21, [x0, #96]
         adcs    x7, x7, x20
         adcs    x8, x8, x21
-        ldp     x22, x23, [x0, 112]
+        ldp     x22, x23, [x0, #112]
         adcs    x9, x9, x22
         adcs    x10, x10, x23
         adcs    x24, x1, xzr
         adc     x2, x1, xzr
-        cmn     x1, 0x1
+        cmn     x1, #0x1
         eor     x11, x11, x1
         adcs    x3, x11, x3
         eor     x12, x12, x1
@@ -775,10 +775,10 @@ local_mul_8_16:
         adcs    x21, x21, x2
         adcs    x22, x22, x2
         adc     x23, x23, x2
-        stp     x3, x4, [x0, 32]
-        stp     x5, x6, [x0, 48]
-        stp     x7, x8, [x0, 64]
-        stp     x9, x10, [x0, 80]
-        stp     x20, x21, [x0, 96]
-        stp     x22, x23, [x0, 112]
+        stp     x3, x4, [x0, #32]
+        stp     x5, x6, [x0, #48]
+        stp     x7, x8, [x0, #64]
+        stp     x9, x10, [x0, #80]
+        stp     x20, x21, [x0, #96]
+        stp     x22, x23, [x0, #112]
         ret
diff --git a/arm/fastmul/bignum_ksqr_16_32.S b/arm/fastmul/bignum_ksqr_16_32.S
index 2ea51ee85..61a4adeef 100644
--- a/arm/fastmul/bignum_ksqr_16_32.S
+++ b/arm/fastmul/bignum_ksqr_16_32.S
@@ -42,10 +42,10 @@ bignum_ksqr_16_32:
 
 // Save registers, including return address
 
-        stp     x19, x20, [sp, -16]!
-        stp     x21, x22, [sp, -16]!
-        stp     x23, x24, [sp, -16]!
-        stp     x25, x30, [sp, -16]!
+        stp     x19, x20, [sp, #-16]!
+        stp     x21, x22, [sp, #-16]!
+        stp     x23, x24, [sp, #-16]!
+        stp     x25, x30, [sp, #-16]!
 
 // Move parameters into subroutine-safe places
 
@@ -59,20 +59,20 @@ bignum_ksqr_16_32:
 
 // Compute absolute difference [t..] = |x_lo - x_hi|
 
-        ldp     x10, x11, [x, 0]
-        ldp     x8, x9, [x, 64]
+        ldp     x10, x11, [x, #0]
+        ldp     x8, x9, [x, #64]
         subs    x10, x10, x8
         sbcs    x11, x11, x9
-        ldp     x12, x13, [x, 16]
-        ldp     x8, x9, [x, 80]
+        ldp     x12, x13, [x, #16]
+        ldp     x8, x9, [x, #80]
         sbcs    x12, x12, x8
         sbcs    x13, x13, x9
-        ldp     x14, x15, [x, 32]
-        ldp     x8, x9, [x, 96]
+        ldp     x14, x15, [x, #32]
+        ldp     x8, x9, [x, #96]
         sbcs    x14, x14, x8
         sbcs    x15, x15, x9
-        ldp     x16, x17, [x, 48]
-        ldp     x8, x9, [x, 112]
+        ldp     x16, x17, [x, #48]
+        ldp     x8, x9, [x, #112]
         sbcs    x16, x16, x8
         sbcs    x17, x17, x9
         csetm   s, cc
@@ -86,22 +86,22 @@ bignum_ksqr_16_32:
         adcs    x12, x12, xzr
         eor     x13, x13, s
         adcs    x13, x13, xzr
-        stp     x12, x13, [t, 16]
+        stp     x12, x13, [t, #16]
         eor     x14, x14, s
         adcs    x14, x14, xzr
         eor     x15, x15, s
         adcs    x15, x15, xzr
-        stp     x14, x15, [t, 32]
+        stp     x14, x15, [t, #32]
         eor     x16, x16, s
         adcs    x16, x16, xzr
         eor     x17, x17, s
         adcs    x17, x17, xzr
-        stp     x16, x17, [t, 48]
+        stp     x16, x17, [t, #48]
 
 // Compute H = x_hi * y_hi in top half of buffer (size 8 x 8 -> 16)
 
-        add     x0, z, 128
-        add     x1, x, 64
+        add     x0, z, #128
+        add     x1, x, #64
         bl      local_sqr_8_16
 
 // Compute H' = H + L_top in place of H (it cannot overflow)
@@ -109,33 +109,33 @@ bignum_ksqr_16_32:
 
         .set    i, 0
 
-        ldp     x10, x11, [z, 128+8*i]
-        ldp     x12, x13, [z, 64+8*i]
+        ldp     x10, x11, [z, #128+8*i]
+        ldp     x12, x13, [z, #64+8*i]
         adds    x10, x10, x12
         adcs    x11, x11, x13
-        stp     x10, x11, [z, 128+8*i]
+        stp     x10, x11, [z, #128+8*i]
         .set    i, (i+2)
 
 .rep 3
-        ldp     x10, x11, [z, 128+8*i]
-        ldp     x12, x13, [z, 64+8*i]
+        ldp     x10, x11, [z, #128+8*i]
+        ldp     x12, x13, [z, #64+8*i]
         adcs    x10, x10, x12
         adcs    x11, x11, x13
-        stp     x10, x11, [z, 128+8*i]
+        stp     x10, x11, [z, #128+8*i]
         .set    i, (i+2)
 .endr
 
 .rep 4
-        ldp     x10, x11, [z, 128+8*i]
+        ldp     x10, x11, [z, #128+8*i]
         adcs    x10, x10, xzr
         adcs    x11, x11, xzr
-        stp     x10, x11, [z, 128+8*i]
+        stp     x10, x11, [z, #128+8*i]
         .set    i, (i+2)
 .endr
 
 // Compute M = |x_lo - x_hi| * |y_hi - y_lo| in [t+8...], size 16
 
-        add     x0, t, 64
+        add     x0, t, #64
         mov     x1, t
         bl      local_sqr_8_16
 
@@ -144,73 +144,73 @@ bignum_ksqr_16_32:
 // (Note that we no longer need the input x was pointing at.)
 
         ldp     x0, x1, [z]
-        ldp     x16, x17, [z, 128]
+        ldp     x16, x17, [z, #128]
         adds    x0, x0, x16
         adcs    x1, x1, x17
-        ldp     x2, x3, [z, 16]
-        ldp     x16, x17, [z, 144]
+        ldp     x2, x3, [z, #16]
+        ldp     x16, x17, [z, #144]
         adcs    x2, x2, x16
         adcs    x3, x3, x17
-        ldp     x4, x5, [z, 32]
-        ldp     x16, x17, [z, 160]
+        ldp     x4, x5, [z, #32]
+        ldp     x16, x17, [z, #160]
         adcs    x4, x4, x16
         adcs    x5, x5, x17
-        ldp     x6, x7, [z, 48]
-        ldp     x16, x17, [z, 176]
+        ldp     x6, x7, [z, #48]
+        ldp     x16, x17, [z, #176]
         adcs    x6, x6, x16
         adcs    x7, x7, x17
-        ldp     x8, x9, [z, 128]
-        ldp     x16, x17, [z, 192]
+        ldp     x8, x9, [z, #128]
+        ldp     x16, x17, [z, #192]
         adcs    x8, x8, x16
         adcs    x9, x9, x17
-        ldp     x10, x11, [z, 144]
-        ldp     x16, x17, [z, 208]
+        ldp     x10, x11, [z, #144]
+        ldp     x16, x17, [z, #208]
         adcs    x10, x10, x16
         adcs    x11, x11, x17
-        ldp     x12, x13, [z, 160]
-        ldp     x16, x17, [z, 224]
+        ldp     x12, x13, [z, #160]
+        ldp     x16, x17, [z, #224]
         adcs    x12, x12, x16
         adcs    x13, x13, x17
-        ldp     x14, x15, [z, 176]
-        ldp     x16, x17, [z, 240]
+        ldp     x14, x15, [z, #176]
+        ldp     x16, x17, [z, #240]
         adcs    x14, x14, x16
         adcs    x15, x15, x17
         cset    x, cs
 
 // Subtract the mid-term cross product M
 
-        ldp     x16, x17, [t, 64]
+        ldp     x16, x17, [t, #64]
         subs    x0, x0, x16
         sbcs    x1, x1, x17
-        stp     x0, x1, [z, 64]
-        ldp     x16, x17, [t, 80]
+        stp     x0, x1, [z, #64]
+        ldp     x16, x17, [t, #80]
         sbcs    x2, x2, x16
         sbcs    x3, x3, x17
-        stp     x2, x3, [z, 80]
-        ldp     x16, x17, [t, 96]
+        stp     x2, x3, [z, #80]
+        ldp     x16, x17, [t, #96]
         sbcs    x4, x4, x16
         sbcs    x5, x5, x17
-        stp     x4, x5, [z, 96]
-        ldp     x16, x17, [t, 112]
+        stp     x4, x5, [z, #96]
+        ldp     x16, x17, [t, #112]
         sbcs    x6, x6, x16
         sbcs    x7, x7, x17
-        stp     x6, x7, [z, 112]
-        ldp     x16, x17, [t, 128]
+        stp     x6, x7, [z, #112]
+        ldp     x16, x17, [t, #128]
         sbcs    x8, x8, x16
         sbcs    x9, x9, x17
-        stp     x8, x9, [z, 128]
-        ldp     x16, x17, [t, 144]
+        stp     x8, x9, [z, #128]
+        ldp     x16, x17, [t, #144]
         sbcs    x10, x10, x16
         sbcs    x11, x11, x17
-        stp     x10, x11, [z, 144]
-        ldp     x16, x17, [t, 160]
+        stp     x10, x11, [z, #144]
+        ldp     x16, x17, [t, #160]
         sbcs    x12, x12, x16
         sbcs    x13, x13, x17
-        stp     x12, x13, [z, 160]
-        ldp     x16, x17, [t, 176]
+        stp     x12, x13, [z, #160]
+        ldp     x16, x17, [t, #176]
         sbcs    x14, x14, x16
         sbcs    x15, x15, x17
-        stp     x14, x15, [z, 176]
+        stp     x14, x15, [z, #176]
 
 // Get the next digits effectively resulting so far starting at 24
 
@@ -220,29 +220,29 @@ bignum_ksqr_16_32:
 // Now the final 8 digits of padding; the first one is special in using x
 // and also in getting the carry chain started
 
-        ldp     x10, x11, [z, 192]
+        ldp     x10, x11, [z, #192]
         adds    x10, x10, x
         adcs    x11, x11, t
-        stp     x10, x11, [z, 192]
-        ldp     x10, x11, [z, 208]
+        stp     x10, x11, [z, #192]
+        ldp     x10, x11, [z, #208]
         adcs    x10, x10, t
         adcs    x11, x11, t
-        stp     x10, x11, [z, 208]
-        ldp     x10, x11, [z, 224]
+        stp     x10, x11, [z, #208]
+        ldp     x10, x11, [z, #224]
         adcs    x10, x10, t
         adcs    x11, x11, t
-        stp     x10, x11, [z, 224]
-        ldp     x10, x11, [z, 240]
+        stp     x10, x11, [z, #224]
+        ldp     x10, x11, [z, #240]
         adcs    x10, x10, t
         adcs    x11, x11, t
-        stp     x10, x11, [z, 240]
+        stp     x10, x11, [z, #240]
 
 // Restore registers and return
 
-        ldp     x25, x30, [sp], 16
-        ldp     x23, x24, [sp], 16
-        ldp     x21, x22, [sp], 16
-        ldp     x19, x20, [sp], 16
+        ldp     x25, x30, [sp], #16
+        ldp     x23, x24, [sp], #16
+        ldp     x21, x22, [sp], #16
+        ldp     x19, x20, [sp], #16
 
         ret
 
@@ -253,9 +253,9 @@ bignum_ksqr_16_32:
 
 local_sqr_8_16:
         ldp     x2, x3, [x1]
-        ldp     x4, x5, [x1, 16]
-        ldp     x6, x7, [x1, 32]
-        ldp     x8, x9, [x1, 48]
+        ldp     x4, x5, [x1, #16]
+        ldp     x6, x7, [x1, #32]
+        ldp     x8, x9, [x1, #48]
         mul     x17, x2, x4
         mul     x14, x3, x5
         umulh   x20, x2, x4
@@ -277,7 +277,7 @@ local_sqr_8_16:
         adc     x21, x21, xzr
         adds    x20, x20, x14
         adc     x21, x21, xzr
-        cmn     x11, 0x1
+        cmn     x11, #0x1
         adcs    x19, x19, x13
         adcs    x20, x20, x12
         adc     x21, x21, x11
@@ -304,7 +304,7 @@ local_sqr_8_16:
         adcs    x20, x20, xzr
         adcs    x21, x21, xzr
         adc     x10, x10, xzr
-        stp     x17, x19, [x0, 16]
+        stp     x17, x19, [x0, #16]
         mul     x12, x4, x4
         mul     x13, x5, x5
         mul     x15, x4, x5
@@ -319,10 +319,10 @@ local_sqr_8_16:
         adc     x14, x14, xzr
         adds    x12, x12, x20
         adcs    x11, x11, x21
-        stp     x12, x11, [x0, 32]
+        stp     x12, x11, [x0, #32]
         adcs    x13, x13, x10
         adc     x14, x14, xzr
-        stp     x13, x14, [x0, 48]
+        stp     x13, x14, [x0, #48]
         mul     x17, x6, x8
         mul     x14, x7, x9
         umulh   x20, x6, x8
@@ -344,7 +344,7 @@ local_sqr_8_16:
         adc     x21, x21, xzr
         adds    x20, x20, x14
         adc     x21, x21, xzr
-        cmn     x11, 0x1
+        cmn     x11, #0x1
         adcs    x19, x19, x13
         adcs    x20, x20, x12
         adc     x21, x21, x11
@@ -365,13 +365,13 @@ local_sqr_8_16:
         adds    x11, x11, x15
         adcs    x13, x13, x16
         adc     x14, x14, xzr
-        stp     x12, x11, [x0, 64]
+        stp     x12, x11, [x0, #64]
         adds    x17, x17, x13
         adcs    x19, x19, x14
         adcs    x20, x20, xzr
         adcs    x21, x21, xzr
         adc     x10, x10, xzr
-        stp     x17, x19, [x0, 80]
+        stp     x17, x19, [x0, #80]
         mul     x12, x8, x8
         mul     x13, x9, x9
         mul     x15, x8, x9
@@ -386,10 +386,10 @@ local_sqr_8_16:
         adc     x14, x14, xzr
         adds    x12, x12, x20
         adcs    x11, x11, x21
-        stp     x12, x11, [x0, 96]
+        stp     x12, x11, [x0, #96]
         adcs    x13, x13, x10
         adc     x14, x14, xzr
-        stp     x13, x14, [x0, 112]
+        stp     x13, x14, [x0, #112]
         mul     x10, x2, x6
         mul     x14, x3, x7
         mul     x15, x4, x8
@@ -421,7 +421,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x15, x15, x21
         eor     x20, x20, x19
@@ -435,7 +435,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x11, x11, x21
         eor     x20, x20, x19
@@ -453,7 +453,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x14, x14, x21
         eor     x20, x20, x19
@@ -468,7 +468,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x12, x12, x21
         eor     x20, x20, x19
@@ -485,7 +485,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x13, x13, x21
         eor     x20, x20, x19
@@ -501,7 +501,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x13, x13, x21
         eor     x20, x20, x19
@@ -518,28 +518,28 @@ local_sqr_8_16:
         adcs    x16, x16, x16
         adcs    x17, x17, x17
         adc     x19, xzr, xzr
-        ldp     x2, x3, [x0, 32]
+        ldp     x2, x3, [x0, #32]
         adds    x10, x10, x2
         adcs    x11, x11, x3
-        stp     x10, x11, [x0, 32]
-        ldp     x2, x3, [x0, 48]
+        stp     x10, x11, [x0, #32]
+        ldp     x2, x3, [x0, #48]
         adcs    x12, x12, x2
         adcs    x13, x13, x3
-        stp     x12, x13, [x0, 48]
-        ldp     x2, x3, [x0, 64]
+        stp     x12, x13, [x0, #48]
+        ldp     x2, x3, [x0, #64]
         adcs    x14, x14, x2
         adcs    x15, x15, x3
-        stp     x14, x15, [x0, 64]
-        ldp     x2, x3, [x0, 80]
+        stp     x14, x15, [x0, #64]
+        ldp     x2, x3, [x0, #80]
         adcs    x16, x16, x2
         adcs    x17, x17, x3
-        stp     x16, x17, [x0, 80]
-        ldp     x2, x3, [x0, 96]
+        stp     x16, x17, [x0, #80]
+        ldp     x2, x3, [x0, #96]
         adcs    x2, x2, x19
         adcs    x3, x3, xzr
-        stp     x2, x3, [x0, 96]
-        ldp     x2, x3, [x0, 112]
+        stp     x2, x3, [x0, #96]
+        ldp     x2, x3, [x0, #112]
         adcs    x2, x2, xzr
         adc     x3, x3, xzr
-        stp     x2, x3, [x0, 112]
+        stp     x2, x3, [x0, #112]
         ret
diff --git a/arm/fastmul/bignum_ksqr_32_64.S b/arm/fastmul/bignum_ksqr_32_64.S
index 5b8f49faa..0afa0d345 100644
--- a/arm/fastmul/bignum_ksqr_32_64.S
+++ b/arm/fastmul/bignum_ksqr_32_64.S
@@ -41,8 +41,8 @@ bignum_ksqr_32_64:
 
 // Save extra registers and return address, store parameters safely
 
-        stp     x19, x20, [sp, -16]!
-        stp     x21, x30, [sp, -16]!
+        stp     x19, x20, [sp, #-16]!
+        stp     x21, x30, [sp, #-16]!
 
         mov     z, x0
         mov     x, x1
@@ -54,81 +54,81 @@ bignum_ksqr_32_64:
 
 // Compute H = x_hi * y_hi in top half of buffer (size 16 x 16 -> 32)
 
-        add     x0, z, 8*2*k
-        add     x1, x, 8*k
+        add     x0, z, #8*2*k
+        add     x1, x, #8*k
         mov     x2, t
         bl      local_ksqr_16_32
 
 // Compute H' = H + L_top in place of H (it cannot overflow)
 
-        ldp     x0, x1, [z, 16*k]
-        ldp     x2, x3, [z, 16*l]
+        ldp     x0, x1, [z, #16*k]
+        ldp     x2, x3, [z, #16*l]
         adds    x0, x0, x2
         adcs    x1, x1, x3
-        stp     x0, x1, [z, 16*k]
+        stp     x0, x1, [z, #16*k]
 
         .set    i, 1
         .rep (l-1)
-        ldp     x0, x1, [z, 16*(k+i)]
-        ldp     x2, x3, [z, 16*(l+i)]
+        ldp     x0, x1, [z, #16*(k+i)]
+        ldp     x2, x3, [z, #16*(l+i)]
         adcs    x0, x0, x2
         adcs    x1, x1, x3
-        stp     x0, x1, [z, 16*(k+i)]
+        stp     x0, x1, [z, #16*(k+i)]
         .set    i, (i+1)
         .endr
 
         .rep    (l-1)
-        ldp     x0, x1, [z, 16*(k+i)]
+        ldp     x0, x1, [z, #16*(k+i)]
         adcs    x0, x0, xzr
         adcs    x1, x1, xzr
-        stp     x0, x1, [z, 16*(k+i)]
+        stp     x0, x1, [z, #16*(k+i)]
         .set    i, (i+1)
         .endr
 
-        ldp     x0, x1, [z, 16*(k+i)]
+        ldp     x0, x1, [z, #16*(k+i)]
         adcs    x0, x0, xzr
         adc     x1, x1, xzr
-        stp     x0, x1, [z, 16*(k+i)]
+        stp     x0, x1, [z, #16*(k+i)]
 
 // Compute absolute difference [t..] = |x_lo - x_hi|
 
-        ldp     x0, x1, [x, 128]
+        ldp     x0, x1, [x, #128]
         ldp     x16, x17, [x]
         subs    x0, x0, x16
         sbcs    x1, x1, x17
 
-        ldp     x2, x3, [x, 144]
-        ldp     x16, x17, [x, 16]
+        ldp     x2, x3, [x, #144]
+        ldp     x16, x17, [x, #16]
         sbcs    x2, x2, x16
         sbcs    x3, x3, x17
 
-        ldp     x4, x5, [x, 160]
-        ldp     x16, x17, [x, 32]
+        ldp     x4, x5, [x, #160]
+        ldp     x16, x17, [x, #32]
         sbcs    x4, x4, x16
         sbcs    x5, x5, x17
 
-        ldp     x6, x7, [x, 176]
-        ldp     x16, x17, [x, 48]
+        ldp     x6, x7, [x, #176]
+        ldp     x16, x17, [x, #48]
         sbcs    x6, x6, x16
         sbcs    x7, x7, x17
 
-        ldp     x8, x9, [x, 192]
-        ldp     x16, x17, [x, 64]
+        ldp     x8, x9, [x, #192]
+        ldp     x16, x17, [x, #64]
         sbcs    x8, x8, x16
         sbcs    x9, x9, x17
 
-        ldp     x10, x11, [x, 208]
-        ldp     x16, x17, [x, 80]
+        ldp     x10, x11, [x, #208]
+        ldp     x16, x17, [x, #80]
         sbcs    x10, x10, x16
         sbcs    x11, x11, x17
 
-        ldp     x12, x13, [x, 224]
-        ldp     x16, x17, [x, 96]
+        ldp     x12, x13, [x, #224]
+        ldp     x16, x17, [x, #96]
         sbcs    x12, x12, x16
         sbcs    x13, x13, x17
 
-        ldp     x14, x15, [x, 240]
-        ldp     x16, x17, [x, 112]
+        ldp     x14, x15, [x, #240]
+        ldp     x16, x17, [x, #112]
         sbcs    x14, x14, x16
         sbcs    x15, x15, x17
 
@@ -146,78 +146,78 @@ bignum_ksqr_32_64:
         adcs    x2, x2, xzr
         eor     x3, x3, c
         adcs    x3, x3, xzr
-        stp     x2, x3, [t, 16]
+        stp     x2, x3, [t, #16]
 
         eor     x4, x4, c
         adcs    x4, x4, xzr
         eor     x5, x5, c
         adcs    x5, x5, xzr
-        stp     x4, x5, [t, 32]
+        stp     x4, x5, [t, #32]
 
         eor     x6, x6, c
         adcs    x6, x6, xzr
         eor     x7, x7, c
         adcs    x7, x7, xzr
-        stp     x6, x7, [t, 48]
+        stp     x6, x7, [t, #48]
 
         eor     x8, x8, c
         adcs    x8, x8, xzr
         eor     x9, x9, c
         adcs    x9, x9, xzr
-        stp     x8, x9, [t, 64]
+        stp     x8, x9, [t, #64]
 
         eor     x10, x10, c
         adcs    x10, x10, xzr
         eor     x11, x11, c
         adcs    x11, x11, xzr
-        stp     x10, x11, [t, 80]
+        stp     x10, x11, [t, #80]
 
         eor     x12, x12, c
         adcs    x12, x12, xzr
         eor     x13, x13, c
         adcs    x13, x13, xzr
-        stp     x12, x13, [t, 96]
+        stp     x12, x13, [t, #96]
 
         eor     x14, x14, c
         adcs    x14, x14, xzr
         eor     x15, x15, c
         adc     x15, x15, xzr
-        stp     x14, x15, [t, 112]
+        stp     x14, x15, [t, #112]
 
 // Compute M = |x_lo - x_hi|^2, size 32
 
-        add     x0, t, 8*k
+        add     x0, t, #8*k
         mov     x1, t
-        add     x2, t, 8*3*k
+        add     x2, t, #8*3*k
         bl      local_ksqr_16_32
 
 // Add the interlocking H' and L_bot terms
 // Intercept the carry at the 3k position and store it in x.
 // (Note that we no longer need the input x was pointing at.)
 
-        ldp     x0, x1, [z, 16*k]
+        ldp     x0, x1, [z, #16*k]
         ldp     x2, x3, [z]
         adds    x0, x0, x2
         adcs    x1, x1, x3
-        stp     x0, x1, [z, 16*l]
+        stp     x0, x1, [z, #16*l]
 
         .set    i, 1
         .rep (l-1)
-        ldp     x0, x1, [z, 16*(k+i)]
-        ldp     x2, x3, [z, 16*i]
+        ldp     x0, x1, [z, #16*(k+i)]
+        ldp     x2, x3, [z, #16*i]
         adcs    x0, x0, x2
         adcs    x1, x1, x3
-        stp     x0, x1, [z, 16*(l+i)]
+        stp     x0, x1, [z, #16*(l+i)]
         .set    i, (i+1)
         .endr
 
         .set    i, 0
         .rep    l
-        ldp     x0, x1, [z, 16*(k+i)]
-        ldp     x2, x3, [z, 16*(3*l+i)]
+        ldp     x0, x1, [z, #16*(k+i)]
+        ldp     x2, x3, [z, #16*(3*l+i)]
         adcs    x0, x0, x2
         adcs    x1, x1, x3
-        stp     x0, x1, [z, 16*(k+i)]
+        stp     x0, x1, [z, #16*(k+i)]
         .set    i, (i+1)
         .endr
 
@@ -225,19 +225,19 @@ bignum_ksqr_32_64:
 
 // Subtract the mid-term cross product M
 
-        ldp     x0, x1, [z, 16*l]
-        ldp     x2, x3, [t, 16*l]
+        ldp     x0, x1, [z, #16*l]
+        ldp     x2, x3, [t, #16*l]
         subs    x0, x0, x2
         sbcs    x1, x1, x3
-        stp     x0, x1, [z, 16*l]
+        stp     x0, x1, [z, #16*l]
 
         .set    i, l+1
         .rep (k-1)
-        ldp     x0, x1, [z, 16*i]
-        ldp     x2, x3, [t, 16*i]
+        ldp     x0, x1, [z, #16*i]
+        ldp     x2, x3, [t, #16*i]
         sbcs    x0, x0, x2
         sbcs    x1, x1, x3
-        stp     x0, x1, [z, 16*i]
+        stp     x0, x1, [z, #16*i]
         .set    i, (i+1)
         .endr
 
@@ -249,29 +249,29 @@ bignum_ksqr_32_64:
 
 // Now propagate through the top quarter of the result
 
-        ldp     x0, x1, [z, 16*3*l]
+        ldp     x0, x1, [z, #16*3*l]
         adds    x0, x0, x
         adcs    x1, x1, c
-        stp     x0, x1, [z, 16*3*l]
+        stp     x0, x1, [z, #16*3*l]
 
         .set    i, 3*l+1
         .rep    (l-2)
-        ldp     x0, x1, [z, 16*i]
+        ldp     x0, x1, [z, #16*i]
         adcs    x0, x0, c
         adcs    x1, x1, c
-        stp     x0, x1, [z, 16*i]
+        stp     x0, x1, [z, #16*i]
         .set    i, (i+1)
         .endr
 
-        ldp     x0, x1, [z, 16*i]
+        ldp     x0, x1, [z, #16*i]
         adcs    x0, x0, c
         adc     x1, x1, c
-        stp     x0, x1, [z, 16*i]
+        stp     x0, x1, [z, #16*i]
 
 // Restore
 
-        ldp     x21, x30, [sp], 16
-        ldp     x19, x20, [sp], 16
+        ldp     x21, x30, [sp], #16
+        ldp     x19, x20, [sp], #16
 
         ret
 
@@ -279,28 +279,28 @@ bignum_ksqr_32_64:
 // This includes in turn a copy of bignum_sqr_8_16.
 
 local_ksqr_16_32:
-        stp     x19, x20, [sp, -16]!
-        stp     x21, x22, [sp, -16]!
-        stp     x23, x24, [sp, -16]!
-        stp     x25, x30, [sp, -16]!
+        stp     x19, x20, [sp, #-16]!
+        stp     x21, x22, [sp, #-16]!
+        stp     x23, x24, [sp, #-16]!
+        stp     x25, x30, [sp, #-16]!
         mov     x23, x0
         mov     x24, x1
         mov     x25, x2
         bl      local_sqr_8_16
         ldp     x10, x11, [x24]
-        ldp     x8, x9, [x24, 64]
+        ldp     x8, x9, [x24, #64]
         subs    x10, x10, x8
         sbcs    x11, x11, x9
-        ldp     x12, x13, [x24, 16]
-        ldp     x8, x9, [x24, 80]
+        ldp     x12, x13, [x24, #16]
+        ldp     x8, x9, [x24, #80]
         sbcs    x12, x12, x8
         sbcs    x13, x13, x9
-        ldp     x14, x15, [x24, 32]
-        ldp     x8, x9, [x24, 96]
+        ldp     x14, x15, [x24, #32]
+        ldp     x8, x9, [x24, #96]
         sbcs    x14, x14, x8
         sbcs    x15, x15, x9
-        ldp     x16, x17, [x24, 48]
-        ldp     x8, x9, [x24, 112]
+        ldp     x16, x17, [x24, #48]
+        ldp     x8, x9, [x24, #112]
         sbcs    x16, x16, x8
         sbcs    x17, x17, x9
         csetm   x19, cc
@@ -314,153 +314,153 @@ local_ksqr_16_32:
         adcs    x12, x12, xzr
         eor     x13, x13, x19
         adcs    x13, x13, xzr
-        stp     x12, x13, [x25, 16]
+        stp     x12, x13, [x25, #16]
         eor     x14, x14, x19
         adcs    x14, x14, xzr
         eor     x15, x15, x19
         adcs    x15, x15, xzr
-        stp     x14, x15, [x25, 32]
+        stp     x14, x15, [x25, #32]
         eor     x16, x16, x19
         adcs    x16, x16, xzr
         eor     x17, x17, x19
         adcs    x17, x17, xzr
-        stp     x16, x17, [x25, 48]
-        add     x0, x23, 0x80
-        add     x1, x24, 0x40
+        stp     x16, x17, [x25, #48]
+        add     x0, x23, #0x80
+        add     x1, x24, #0x40
         bl      local_sqr_8_16
-        ldp     x10, x11, [x23, 128]
-        ldp     x12, x13, [x23, 64]
+        ldp     x10, x11, [x23, #128]
+        ldp     x12, x13, [x23, #64]
         adds    x10, x10, x12
         adcs    x11, x11, x13
-        stp     x10, x11, [x23, 128]
-        ldp     x10, x11, [x23, 144]
-        ldp     x12, x13, [x23, 80]
+        stp     x10, x11, [x23, #128]
+        ldp     x10, x11, [x23, #144]
+        ldp     x12, x13, [x23, #80]
         adcs    x10, x10, x12
         adcs    x11, x11, x13
-        stp     x10, x11, [x23, 144]
-        ldp     x10, x11, [x23, 160]
-        ldp     x12, x13, [x23, 96]
+        stp     x10, x11, [x23, #144]
+        ldp     x10, x11, [x23, #160]
+        ldp     x12, x13, [x23, #96]
         adcs    x10, x10, x12
         adcs    x11, x11, x13
-        stp     x10, x11, [x23, 160]
-        ldp     x10, x11, [x23, 176]
-        ldp     x12, x13, [x23, 112]
+        stp     x10, x11, [x23, #160]
+        ldp     x10, x11, [x23, #176]
+        ldp     x12, x13, [x23, #112]
         adcs    x10, x10, x12
         adcs    x11, x11, x13
-        stp     x10, x11, [x23, 176]
-        ldp     x10, x11, [x23, 192]
+        stp     x10, x11, [x23, #176]
+        ldp     x10, x11, [x23, #192]
         adcs    x10, x10, xzr
         adcs    x11, x11, xzr
-        stp     x10, x11, [x23, 192]
-        ldp     x10, x11, [x23, 208]
+        stp     x10, x11, [x23, #192]
+        ldp     x10, x11, [x23, #208]
         adcs    x10, x10, xzr
         adcs    x11, x11, xzr
-        stp     x10, x11, [x23, 208]
-        ldp     x10, x11, [x23, 224]
+        stp     x10, x11, [x23, #208]
+        ldp     x10, x11, [x23, #224]
         adcs    x10, x10, xzr
         adcs    x11, x11, xzr
-        stp     x10, x11, [x23, 224]
-        ldp     x10, x11, [x23, 240]
+        stp     x10, x11, [x23, #224]
+        ldp     x10, x11, [x23, #240]
         adcs    x10, x10, xzr
         adcs    x11, x11, xzr
-        stp     x10, x11, [x23, 240]
-        add     x0, x25, 0x40
+        stp     x10, x11, [x23, #240]
+        add     x0, x25, #0x40
         mov     x1, x25
         bl      local_sqr_8_16
         ldp     x0, x1, [x23]
-        ldp     x16, x17, [x23, 128]
+        ldp     x16, x17, [x23, #128]
         adds    x0, x0, x16
         adcs    x1, x1, x17
-        ldp     x2, x3, [x23, 16]
-        ldp     x16, x17, [x23, 144]
+        ldp     x2, x3, [x23, #16]
+        ldp     x16, x17, [x23, #144]
         adcs    x2, x2, x16
         adcs    x3, x3, x17
-        ldp     x4, x5, [x23, 32]
-        ldp     x16, x17, [x23, 160]
+        ldp     x4, x5, [x23, #32]
+        ldp     x16, x17, [x23, #160]
         adcs    x4, x4, x16
         adcs    x5, x5, x17
-        ldp     x6, x7, [x23, 48]
-        ldp     x16, x17, [x23, 176]
+        ldp     x6, x7, [x23, #48]
+        ldp     x16, x17, [x23, #176]
         adcs    x6, x6, x16
         adcs    x7, x7, x17
-        ldp     x8, x9, [x23, 128]
-        ldp     x16, x17, [x23, 192]
+        ldp     x8, x9, [x23, #128]
+        ldp     x16, x17, [x23, #192]
         adcs    x8, x8, x16
         adcs    x9, x9, x17
-        ldp     x10, x11, [x23, 144]
-        ldp     x16, x17, [x23, 208]
+        ldp     x10, x11, [x23, #144]
+        ldp     x16, x17, [x23, #208]
         adcs    x10, x10, x16
         adcs    x11, x11, x17
-        ldp     x12, x13, [x23, 160]
-        ldp     x16, x17, [x23, 224]
+        ldp     x12, x13, [x23, #160]
+        ldp     x16, x17, [x23, #224]
         adcs    x12, x12, x16
         adcs    x13, x13, x17
-        ldp     x14, x15, [x23, 176]
-        ldp     x16, x17, [x23, 240]
+        ldp     x14, x15, [x23, #176]
+        ldp     x16, x17, [x23, #240]
         adcs    x14, x14, x16
         adcs    x15, x15, x17
         cset    x24, cs
-        ldp     x16, x17, [x25, 64]
+        ldp     x16, x17, [x25, #64]
         subs    x0, x0, x16
         sbcs    x1, x1, x17
-        stp     x0, x1, [x23, 64]
-        ldp     x16, x17, [x25, 80]
+        stp     x0, x1, [x23, #64]
+        ldp     x16, x17, [x25, #80]
         sbcs    x2, x2, x16
         sbcs    x3, x3, x17
-        stp     x2, x3, [x23, 80]
-        ldp     x16, x17, [x25, 96]
+        stp     x2, x3, [x23, #80]
+        ldp     x16, x17, [x25, #96]
         sbcs    x4, x4, x16
         sbcs    x5, x5, x17
-        stp     x4, x5, [x23, 96]
-        ldp     x16, x17, [x25, 112]
+        stp     x4, x5, [x23, #96]
+        ldp     x16, x17, [x25, #112]
         sbcs    x6, x6, x16
         sbcs    x7, x7, x17
-        stp     x6, x7, [x23, 112]
-        ldp     x16, x17, [x25, 128]
+        stp     x6, x7, [x23, #112]
+        ldp     x16, x17, [x25, #128]
         sbcs    x8, x8, x16
         sbcs    x9, x9, x17
-        stp     x8, x9, [x23, 128]
-        ldp     x16, x17, [x25, 144]
+        stp     x8, x9, [x23, #128]
+        ldp     x16, x17, [x25, #144]
         sbcs    x10, x10, x16
         sbcs    x11, x11, x17
-        stp     x10, x11, [x23, 144]
-        ldp     x16, x17, [x25, 160]
+        stp     x10, x11, [x23, #144]
+        ldp     x16, x17, [x25, #160]
         sbcs    x12, x12, x16
         sbcs    x13, x13, x17
-        stp     x12, x13, [x23, 160]
-        ldp     x16, x17, [x25, 176]
+        stp     x12, x13, [x23, #160]
+        ldp     x16, x17, [x25, #176]
         sbcs    x14, x14, x16
         sbcs    x15, x15, x17
-        stp     x14, x15, [x23, 176]
+        stp     x14, x15, [x23, #176]
         sbcs    x24, x24, xzr
         csetm   x25, cc
-        ldp     x10, x11, [x23, 192]
+        ldp     x10, x11, [x23, #192]
         adds    x10, x10, x24
         adcs    x11, x11, x25
-        stp     x10, x11, [x23, 192]
-        ldp     x10, x11, [x23, 208]
+        stp     x10, x11, [x23, #192]
+        ldp     x10, x11, [x23, #208]
         adcs    x10, x10, x25
         adcs    x11, x11, x25
-        stp     x10, x11, [x23, 208]
-        ldp     x10, x11, [x23, 224]
+        stp     x10, x11, [x23, #208]
+        ldp     x10, x11, [x23, #224]
         adcs    x10, x10, x25
         adcs    x11, x11, x25
-        stp     x10, x11, [x23, 224]
-        ldp     x10, x11, [x23, 240]
+        stp     x10, x11, [x23, #224]
+        ldp     x10, x11, [x23, #240]
         adcs    x10, x10, x25
         adcs    x11, x11, x25
-        stp     x10, x11, [x23, 240]
-        ldp     x25, x30, [sp], 16
-        ldp     x23, x24, [sp], 16
-        ldp     x21, x22, [sp], 16
-        ldp     x19, x20, [sp], 16
+        stp     x10, x11, [x23, #240]
+        ldp     x25, x30, [sp], #16
+        ldp     x23, x24, [sp], #16
+        ldp     x21, x22, [sp], #16
+        ldp     x19, x20, [sp], #16
         ret
 
 local_sqr_8_16:
         ldp     x2, x3, [x1]
-        ldp     x4, x5, [x1, 16]
-        ldp     x6, x7, [x1, 32]
-        ldp     x8, x9, [x1, 48]
+        ldp     x4, x5, [x1, #16]
+        ldp     x6, x7, [x1, #32]
+        ldp     x8, x9, [x1, #48]
         mul     x17, x2, x4
         mul     x14, x3, x5
         umulh   x20, x2, x4
@@ -482,7 +482,7 @@ local_sqr_8_16:
         adc     x21, x21, xzr
         adds    x20, x20, x14
         adc     x21, x21, xzr
-        cmn     x11, 0x1
+        cmn     x11, #0x1
         adcs    x19, x19, x13
         adcs    x20, x20, x12
         adc     x21, x21, x11
@@ -509,7 +509,7 @@ local_sqr_8_16:
         adcs    x20, x20, xzr
         adcs    x21, x21, xzr
         adc     x10, x10, xzr
-        stp     x17, x19, [x0, 16]
+        stp     x17, x19, [x0, #16]
         mul     x12, x4, x4
         mul     x13, x5, x5
         mul     x15, x4, x5
@@ -524,10 +524,10 @@ local_sqr_8_16:
         adc     x14, x14, xzr
         adds    x12, x12, x20
         adcs    x11, x11, x21
-        stp     x12, x11, [x0, 32]
+        stp     x12, x11, [x0, #32]
         adcs    x13, x13, x10
         adc     x14, x14, xzr
-        stp     x13, x14, [x0, 48]
+        stp     x13, x14, [x0, #48]
         mul     x17, x6, x8
         mul     x14, x7, x9
         umulh   x20, x6, x8
@@ -549,7 +549,7 @@ local_sqr_8_16:
         adc     x21, x21, xzr
         adds    x20, x20, x14
         adc     x21, x21, xzr
-        cmn     x11, 0x1
+        cmn     x11, #0x1
         adcs    x19, x19, x13
         adcs    x20, x20, x12
         adc     x21, x21, x11
@@ -570,13 +570,13 @@ local_sqr_8_16:
         adds    x11, x11, x15
         adcs    x13, x13, x16
         adc     x14, x14, xzr
-        stp     x12, x11, [x0, 64]
+        stp     x12, x11, [x0, #64]
         adds    x17, x17, x13
         adcs    x19, x19, x14
         adcs    x20, x20, xzr
         adcs    x21, x21, xzr
         adc     x10, x10, xzr
-        stp     x17, x19, [x0, 80]
+        stp     x17, x19, [x0, #80]
         mul     x12, x8, x8
         mul     x13, x9, x9
         mul     x15, x8, x9
@@ -591,10 +591,10 @@ local_sqr_8_16:
         adc     x14, x14, xzr
         adds    x12, x12, x20
         adcs    x11, x11, x21
-        stp     x12, x11, [x0, 96]
+        stp     x12, x11, [x0, #96]
         adcs    x13, x13, x10
         adc     x14, x14, xzr
-        stp     x13, x14, [x0, 112]
+        stp     x13, x14, [x0, #112]
         mul     x10, x2, x6
         mul     x14, x3, x7
         mul     x15, x4, x8
@@ -626,7 +626,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x15, x15, x21
         eor     x20, x20, x19
@@ -640,7 +640,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x11, x11, x21
         eor     x20, x20, x19
@@ -658,7 +658,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x14, x14, x21
         eor     x20, x20, x19
@@ -673,7 +673,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x12, x12, x21
         eor     x20, x20, x19
@@ -690,7 +690,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x13, x13, x21
         eor     x20, x20, x19
@@ -706,7 +706,7 @@ local_sqr_8_16:
         mul     x21, x22, x20
         umulh   x20, x22, x20
         cinv    x19, x19, cc
-        cmn     x19, 0x1
+        cmn     x19, #0x1
         eor     x21, x21, x19
         adcs    x13, x13, x21
         eor     x20, x20, x19
@@ -723,28 +723,28 @@ local_sqr_8_16:
         adcs    x16, x16, x16
         adcs    x17, x17, x17
         adc     x19, xzr, xzr
-        ldp     x2, x3, [x0, 32]
+        ldp     x2, x3, [x0, #32]
         adds    x10, x10, x2
         adcs    x11, x11, x3
-        stp     x10, x11, [x0, 32]
-        ldp     x2, x3, [x0, 48]
+        stp     x10, x11, [x0, #32]
+        ldp     x2, x3, [x0, #48]
         adcs    x12, x12, x2
         adcs    x13, x13, x3
-        stp     x12, x13, [x0, 48]
-        ldp     x2, x3, [x0, 64]
+        stp     x12, x13, [x0, #48]
+        ldp     x2, x3, [x0, #64]
         adcs    x14, x14, x2
         adcs    x15, x15, x3
-        stp     x14, x15, [x0, 64]
-        ldp     x2, x3, [x0, 80]
+        stp     x14, x15, [x0, #64]
+        ldp     x2, x3, [x0, #80]
         adcs    x16, x16, x2
         adcs    x17, x17, x3
-        stp     x16, x17, [x0, 80]
-        ldp     x2, x3, [x0, 96]
+        stp     x16, x17, [x0, #80]
+        ldp     x2, x3, [x0, #96]
         adcs    x2, x2, x19
         adcs    x3, x3, xzr
-        stp     x2, x3, [x0, 96]
-        ldp     x2, x3, [x0, 112]
+        stp     x2, x3, [x0, #96]
+        ldp     x2, x3, [x0, #112]
         adcs    x2, x2, xzr
         adc     x3, x3, xzr
-        stp     x2, x3, [x0, 112]
+        stp     x2, x3, [x0, #112]
         ret
diff --git a/arm/generic/bignum_ge.S b/arm/generic/bignum_ge.S
index 6d44be00e..231f11e5d 100644
--- a/arm/generic/bignum_ge.S
+++ b/arm/generic/bignum_ge.S
@@ -50,19 +50,19 @@ bignum_ge:
 
                 cbz     n, xtest
 xmainloop:
-                ldr     a, [x, i, LSL 3]
-                ldr     d, [y, i, LSL 3]
+                ldr     a, [x, i, lsl #3]
+                ldr     d, [y, i, lsl #3]
                 sbcs    xzr, a, d
-                add     i, i, 1
-                sub     n, n, 1
+                add     i, i, #1
+                sub     n, n, #1
                 cbnz    n, xmainloop
 xtest:
                 cbz     m, xskip
 xtoploop:
-                ldr     a, [x, i, LSL 3]
+                ldr     a, [x, i, lsl #3]
                 sbcs    xzr, a, xzr
-                add     i, i, 1
-                sub     m, m, 1
+                add     i, i, #1
+                sub     m, m, #1
                 cbnz    m, xtoploop
 xskip:
                 cset    x0, cs
@@ -76,17 +76,17 @@ ylonger:
                 cbz     m, ytoploop
                 sub     n, n, m
 ymainloop:
-                ldr     a, [x, i, LSL 3]
-                ldr     d, [y, i, LSL 3]
+                ldr     a, [x, i, lsl #3]
+                ldr     d, [y, i, lsl #3]
                 sbcs    xzr, a, d
-                add     i, i, 1
-                sub     m, m, 1
+                add     i, i, #1
+                sub     m, m, #1
                 cbnz    m, ymainloop
 ytoploop:
-                ldr     a, [y, i, LSL 3]
+                ldr     a, [y, i, lsl #3]
                 sbcs    xzr, xzr, a
-                add     i, i, 1
-                sub     n, n, 1
+                add     i, i, #1
+                sub     n, n, #1
                 cbnz    n, ytoploop
 
                 cset    x0, cs
diff --git a/arm/generic/bignum_mul.S b/arm/generic/bignum_mul.S
index 225066c5a..067079998 100644
--- a/arm/generic/bignum_mul.S
+++ b/arm/generic/bignum_mul.S
@@ -70,7 +70,7 @@ outerloop:
 // First let a = MAX 0 (k + 1 - n) and b = MIN (k + 1) m
 // We want to accumulate all x[i] * y[k - i] for a <= i < b
 
-                add     a, k, 1
+                add     a, k, #1
                 cmp     a, m
                 csel    b, a, m, cc
                 subs    a, a, n
@@ -85,32 +85,32 @@ outerloop:
 // Increment xx per iteration but just use loop counter with yy
 // So we start with [xx] = x[a] and [yy] = y[(k - b) + (b - a)] = y[k - a]
 
-                lsl     xx, a, 3
+                lsl     xx, a, #3
                 add     xx, xx, x
 
                 sub     yy, k, b
-                lsl     yy, yy, 3
+                lsl     yy, yy, #3
                 add     yy, yy, y
 
 // And index using the loop counter i = b - a, ..., i = 1
 
 innerloop:
                 ldr     a, [xx], #8
-                ldr     b, [yy, i, LSL 3]
+                ldr     b, [yy, i, lsl #3]
                 mul     d, a, b
                 umulh   a, a, b
                 adds    l, l, d
                 adcs    h, h, a
                 adc     c, c, xzr
-                subs    i, i, 1
+                subs    i, i, #1
                 bne     innerloop
 
 innerend:
-                str     l, [z, k, LSL 3]
+                str     l, [z, k, lsl #3]
                 mov     l, h
                 mov     h, c
 
-                add     k, k, 1
+                add     k, k, #1
                 cmp     k, p
                 bcc     outerloop                       // Inverted carry flag!
 
diff --git a/arm/generic/bignum_optsub.S b/arm/generic/bignum_optsub.S
index c25cddd6b..db3c2972f 100644
--- a/arm/generic/bignum_optsub.S
+++ b/arm/generic/bignum_optsub.S
@@ -62,8 +62,8 @@ loop:
                 and     b, b, m
                 sbcs    a, a, b
                 str     a, [z, i]
-                add     i, i, 8
-                sub     k, k, 1
+                add     i, i, #8
+                sub     k, k, #1
                 cbnz    k, loop
 
 // Return (non-inverted) carry flag
diff --git a/arm/p384/bignum_add_p384.S b/arm/p384/bignum_add_p384.S
index e3d47b9a0..b6ca20263 100644
--- a/arm/p384/bignum_add_p384.S
+++ b/arm/p384/bignum_add_p384.S
@@ -58,11 +58,11 @@ bignum_add_p384:
 
 // Now compare [d5; d4; d3; d2; d1; d0] with p_384
 
-                mov     l, 0x00000000ffffffff
+                mov     l, #0x00000000ffffffff
                 subs    xzr, d0, l
-                mov     l, 0xffffffff00000000
+                mov     l, #0xffffffff00000000
                 sbcs    xzr, d1, l
-                mov     l, 0xfffffffffffffffe
+                mov     l, #0xfffffffffffffffe
                 sbcs    xzr, d2, l
                 adcs    xzr, d3, xzr
                 adcs    xzr, d4, xzr
@@ -76,12 +76,12 @@ bignum_add_p384:
 
 // Now correct by subtracting masked p_384
 
-                mov     l, 0x00000000ffffffff
+                mov     l, #0x00000000ffffffff
                 and     l, l, c
                 subs    d0, d0, l
                 eor     l, l, c
                 sbcs    d1, d1, l
-                mov     l, 0xfffffffffffffffe
+                mov     l, #0xfffffffffffffffe
                 and     l, l, c
                 sbcs    d2, d2, l
                 sbcs    d3, d3, c
diff --git a/arm/p384/bignum_bigendian_6.S b/arm/p384/bignum_bigendian_6.S
index 652441a0f..f149b4936 100644
--- a/arm/p384/bignum_bigendian_6.S
+++ b/arm/p384/bignum_bigendian_6.S
@@ -50,22 +50,22 @@
 #define c x4
 
 .macro accumdigit dest, i
-        ldrb    dshort, [x, 8 * \i + 7]
-        extr    \dest, d, xzr, 8
-        ldrb    dshort, [x, 8 * \i + 6]
-        extr    \dest, d, \dest, 8
-        ldrb    dshort, [x, 8 * \i + 5]
-        extr    \dest, d, \dest, 8
-        ldrb    dshort, [x, 8 * \i + 4]
-        extr    \dest, d, \dest, 8
-        ldrb    dshort, [x, 8 * \i + 3]
-        extr    \dest, d, \dest, 8
-        ldrb    dshort, [x, 8 * \i + 2]
-        extr    \dest, d, \dest, 8
-        ldrb    dshort, [x, 8 * \i + 1]
-        extr    \dest, d, \dest, 8
-        ldrb    dshort, [x, 8 * \i + 0]
-        extr    \dest, d, \dest, 8
+        ldrb    dshort, [x, #8 * \i + 7]
+        extr    \dest, d, xzr, #8
+        ldrb    dshort, [x, #8 * \i + 6]
+        extr    \dest, d, \dest, #8
+        ldrb    dshort, [x, #8 * \i + 5]
+        extr    \dest, d, \dest, #8
+        ldrb    dshort, [x, #8 * \i + 4]
+        extr    \dest, d, \dest, #8
+        ldrb    dshort, [x, #8 * \i + 3]
+        extr    \dest, d, \dest, #8
+        ldrb    dshort, [x, #8 * \i + 2]
+        extr    \dest, d, \dest, #8
+        ldrb    dshort, [x, #8 * \i + 1]
+        extr    \dest, d, \dest, #8
+        ldrb    dshort, [x, #8 * \i + 0]
+        extr    \dest, d, \dest, #8
 .endm
 
 // The reads and writes are organized in mirror-image pairs (0-5, 1-4, 2-3)
@@ -80,21 +80,21 @@ bignum_tobytes_6:
 
                 accumdigit a, 0
                 accumdigit c, 5
-                str     a, [z, 8*5]
-                str     c, [z, 8*0]
+                str     a, [z, #8*5]
+                str     c, [z, #8*0]
 
 // 1 and 4 words
 
                 accumdigit a, 1
                 accumdigit c, 4
-                str     a, [z, 8*4]
-                str     c, [z, 8*1]
+                str     a, [z, #8*4]
+                str     c, [z, #8*1]
 
 // 2 and 3 words
 
                 accumdigit a, 2
                 accumdigit c, 3
-                str     a, [z, 8*3]
-                str     c, [z, 8*2]
+                str     a, [z, #8*3]
+                str     c, [z, #8*2]
 
                 ret
diff --git a/arm/p384/bignum_cmul_p384.S b/arm/p384/bignum_cmul_p384.S
index e693876a2..26a63cc13 100644
--- a/arm/p384/bignum_cmul_p384.S
+++ b/arm/p384/bignum_cmul_p384.S
@@ -98,11 +98,11 @@ bignum_cmul_p384:
 //       = l + 2^128 * (h + 1) + 2^96 * (h + 1) - 2^32 * (h + 1) + (h + 1)
 //       = l + 2^128 * (h + 1) + 2^96 * h + 2^32 * ~h + (h + 1)
 
-                add     h1, h, 1
+                add     h1, h, #1
                 orn     hn, xzr, h
-                lsl     a0, hn, 32
-                extr    a1, h, hn, 32
-                lsr     a2, h, 32
+                lsl     a0, hn, #32
+                extr    a1, h, hn, #32
+                lsr     a2, h, #32
 
                 adds    a0, a0, h1
                 adcs    a1, a1, xzr
@@ -120,12 +120,12 @@ bignum_cmul_p384:
 
                 csetm   m, cc
 
-                mov     l, 0x00000000ffffffff
+                mov     l, #0x00000000ffffffff
                 and     l, l, m
                 adds    d0, d0, l
                 eor     l, l, m
                 adcs    d1, d1, l
-                mov     l, 0xfffffffffffffffe
+                mov     l, #0xfffffffffffffffe
                 and     l, l, m
                 adcs    d2, d2, l
                 adcs    d3, d3, m
diff --git a/arm/p384/bignum_deamont_p384.S b/arm/p384/bignum_deamont_p384.S
index 0d300a724..ed9de1c04 100644
--- a/arm/p384/bignum_deamont_p384.S
+++ b/arm/p384/bignum_deamont_p384.S
@@ -41,19 +41,19 @@
 .macro          montreds d6,d5,d4,d3,d2,d1,d0, t3,t2,t1
 // Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64
 // Recycle d0 (which we know gets implicitly cancelled) to store it
-                lsl     \t1, \d0, 32
+                lsl     \t1, \d0, #32
                 add     \d0, \t1, \d0
 // Now let [t2;t1] = 2^64 * w - w + w_hi where w_hi = floor(w/2^32)
 // We need to subtract 2^32 * this, and we can ignore its lower 32
 // bits since by design it will cancel anyway; we only need the w_hi
 // part to get the carry propagation going.
-                lsr     \t1, \d0, 32
+                lsr     \t1, \d0, #32
                 subs    \t1, \t1, \d0
                 sbc     \t2, \d0, xzr
 // Now select in t1 the field to subtract from d1
-                extr    \t1, \t2, \t1, 32
+                extr    \t1, \t2, \t1, #32
 // And now get the terms to subtract from d2 and d3
-                lsr     \t2, \t2, 32
+                lsr     \t2, \t2, #32
                 adds    \t2, \t2, \d0
                 adc     \t3, xzr, xzr
 // Do the subtraction of that portion
@@ -91,8 +91,8 @@ bignum_deamont_p384:
 // Set up an initial window with the input x and an extra leading zero
 
                 ldp     d0, d1, [x]
-                ldp     d2, d3, [x, 16]
-                ldp     d4, d5, [x, 32]
+                ldp     d2, d3, [x, #16]
+                ldp     d4, d5, [x, #32]
 
 // Systematically scroll left doing 1-step reductions
 
@@ -112,9 +112,9 @@ bignum_deamont_p384:
 // 2^384 - p_384 = [0;0;0;w;v;u]. This will set CF if
 // dd + (2^384 - p_384) >= 2^384, hence iff dd >= p_384
 
-                mov     u, 0xffffffff00000001
-                mov     v, 0x00000000ffffffff
-                mov     w, 0x0000000000000001
+                mov     u, #0xffffffff00000001
+                mov     v, #0x00000000ffffffff
+                mov     w, #0x0000000000000001
 
                 adds    xzr, d0, u
                 adcs    xzr, d1, v
@@ -131,7 +131,7 @@ bignum_deamont_p384:
                 adds    d0, d0, u
                 and     v, v, w
                 adcs    d1, d1, v
-                and     w, w, 1
+                and     w, w, #1
                 adcs    d2, d2, w
                 adcs    d3, d3, xzr
                 adcs    d4, d4, xzr
@@ -140,7 +140,7 @@ bignum_deamont_p384:
 // Store it back
 
                 stp     d0, d1, [z]
-                stp     d2, d3, [z, 16]
-                stp     d4, d5, [z, 32]
+                stp     d2, d3, [z, #16]
+                stp     d4, d5, [z, #32]
 
                 ret
diff --git a/arm/p384/bignum_demont_p384.S b/arm/p384/bignum_demont_p384.S
index 1f147d45b..d9f520071 100644
--- a/arm/p384/bignum_demont_p384.S
+++ b/arm/p384/bignum_demont_p384.S
@@ -41,19 +41,19 @@
 .macro          montreds d6,d5,d4,d3,d2,d1,d0, t3,t2,t1
 // Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64
 // Recycle d0 (which we know gets implicitly cancelled) to store it
-                lsl     \t1, \d0, 32
+                lsl     \t1, \d0, #32
                 add     \d0, \t1, \d0
 // Now let [t2;t1] = 2^64 * w - w + w_hi where w_hi = floor(w/2^32)
 // We need to subtract 2^32 * this, and we can ignore its lower 32
 // bits since by design it will cancel anyway; we only need the w_hi
 // part to get the carry propagation going.
-                lsr     \t1, \d0, 32
+                lsr     \t1, \d0, #32
                 subs    \t1, \t1, \d0
                 sbc     \t2, \d0, xzr
 // Now select in t1 the field to subtract from d1
-                extr    \t1, \t2, \t1, 32
+                extr    \t1, \t2, \t1, #32
 // And now get the terms to subtract from d2 and d3
-                lsr     \t2, \t2, 32
+                lsr     \t2, \t2, #32
                 adds    \t2, \t2, \d0
                 adc     \t3, xzr, xzr
 // Do the subtraction of that portion
@@ -91,8 +91,8 @@ bignum_demont_p384:
 // Set up an initial window with the input x and an extra leading zero
 
                 ldp     d0, d1, [x]
-                ldp     d2, d3, [x, 16]
-                ldp     d4, d5, [x, 32]
+                ldp     d2, d3, [x, #16]
+                ldp     d4, d5, [x, #32]
 
 // Systematically scroll left doing 1-step reductions
 
@@ -111,7 +111,7 @@ bignum_demont_p384:
 // This is already our answer with no correction needed
 
                 stp     d0, d1, [z]
-                stp     d2, d3, [z, 16]
-                stp     d4, d5, [z, 32]
+                stp     d2, d3, [z, #16]
+                stp     d4, d5, [z, #32]
 
                 ret
diff --git a/arm/p384/bignum_double_p384.S b/arm/p384/bignum_double_p384.S
index 89a5361c7..65bc382c4 100644
--- a/arm/p384/bignum_double_p384.S
+++ b/arm/p384/bignum_double_p384.S
@@ -60,11 +60,11 @@ bignum_double_p384:
 
 // Subtract p_384 to give 2 * x - p_384 = c + [n5; n4; n3; n2; n1; n0]
 
-                mov     n0, 0x00000000ffffffff
+                mov     n0, #0x00000000ffffffff
                 subs    n0, d0, n0
-                mov     n1, 0xffffffff00000000
+                mov     n1, #0xffffffff00000000
                 sbcs    n1, d1, n1
-                mov     n2, 0xfffffffffffffffe
+                mov     n2, #0xfffffffffffffffe
                 sbcs    n2, d2, n2
                 adcs    n3, d3, xzr
                 adcs    n4, d4, xzr
diff --git a/arm/p384/bignum_half_p384.S b/arm/p384/bignum_half_p384.S
index 2e7007eae..b7a1e3850 100644
--- a/arm/p384/bignum_half_p384.S
+++ b/arm/p384/bignum_half_p384.S
@@ -45,23 +45,23 @@ bignum_half_p384:
 // Load the 4 digits of x
 
                 ldp     d0, d1, [x]
-                ldp     d2, d3, [x, 16]
-                ldp     d4, d5, [x, 32]
+                ldp     d2, d3, [x, #16]
+                ldp     d4, d5, [x, #32]
 
 // Get a bitmask corresponding to the lowest bit of the input
 
-                and     m, d0, 1
+                and     m, d0, #1
                 neg     m, m
 
 // Do a masked addition of p_384, catching carry in a 7th word
 
-                mov     n, 0x00000000ffffffff
+                mov     n, #0x00000000ffffffff
                 and     n, n, m
                 adds    d0, d0, n
-                mov     n, 0xffffffff00000000
+                mov     n, #0xffffffff00000000
                 and     n, n, m
                 adcs    d1, d1, n
-                mov     n, 0xfffffffffffffffe
+                mov     n, #0xfffffffffffffffe
                 and     n, n, m
                 adcs    d2, d2, n
                 adcs    d3, d3, m
@@ -71,18 +71,18 @@ bignum_half_p384:
 
 // Now shift that sum right one place
 
-                extr    d0, d1, d0, 1
-                extr    d1, d2, d1, 1
-                extr    d2, d3, d2, 1
-                extr    d3, d4, d3, 1
-                extr    d4, d5, d4, 1
-                extr    d5, d6, d5, 1
+                extr    d0, d1, d0, #1
+                extr    d1, d2, d1, #1
+                extr    d2, d3, d2, #1
+                extr    d3, d4, d3, #1
+                extr    d4, d5, d4, #1
+                extr    d5, d6, d5, #1
 
 // Store back
 
                 stp     d0, d1, [z]
-                stp     d2, d3, [z, 16]
-                stp     d4, d5, [z, 32]
+                stp     d2, d3, [z, #16]
+                stp     d4, d5, [z, #32]
 
 // Return
 
diff --git a/arm/p384/bignum_mod_n384.S b/arm/p384/bignum_mod_n384.S
index 530ae91be..08c780459 100644
--- a/arm/p384/bignum_mod_n384.S
+++ b/arm/p384/bignum_mod_n384.S
@@ -64,9 +64,9 @@
 
 .macro movbig nn, n3, n2, n1, n0
                 movz    \nn, \n0
-                movk    \nn, \n1, lsl 16
-                movk    \nn, \n2, lsl 32
-                movk    \nn, \n3, lsl 48
+                movk    \nn, \n1, lsl #16
+                movk    \nn, \n2, lsl #32
+                movk    \nn, \n3, lsl #48
 .endm
 
 .text
@@ -76,23 +76,23 @@ bignum_mod_n384:
 
 // If the input is already <= 5 words long, go to a trivial "copy" path
 
-                cmp     k, 6
+                cmp     k, #6
                 bcc     short
 
 // Otherwise load the top 6 digits (top-down) and reduce k by 6
 
-                sub     k, k, 6
-                lsl     t0, k, 3
+                sub     k, k, #6
+                lsl     t0, k, #3
                 add     t0, t0, x
-                ldp     m4, m5, [t0, 32]
-                ldp     m2, m3, [t0, 16]
+                ldp     m4, m5, [t0, #32]
+                ldp     m2, m3, [t0, #16]
                 ldp     m0, m1, [t0]
 
 // Load the complicated three words of 2^384 - n_384 = [0; 0; 0; n2; n1; n0]
 
-                movbig  n0, 0x1313, 0xe695, 0x333a, 0xd68d
-                movbig  n1, 0xa7e5, 0xf24d, 0xb74f, 0x5885
-                movbig  n2, 0x389c, 0xb27e, 0x0bc8, 0xd220
+                movbig  n0, #0x1313, #0xe695, #0x333a, #0xd68d
+                movbig  n1, #0xa7e5, #0xf24d, #0xb74f, #0x5885
+                movbig  n2, #0x389c, #0xb27e, #0x0bc8, #0xd220
 
 // Reduce the top 6 digits mod n_384 (a conditional subtraction of n_384)
 
@@ -116,7 +116,7 @@ loop:
 
 // Compute q = min (m5 + 1) (2^64 - 1)
 
-                adds    q, m5, 1
+                adds    q, m5, #1
                 csetm   t0, cs
                 orr     q, q, t0
 
@@ -135,8 +135,8 @@ loop:
 
 // Decrement k and load the next digit
 
-                sub     k, k, 1
-                ldr     d, [x, k, LSL 3]
+                sub     k, k, #1
+                ldr     d, [x, k, lsl #3]
 
 // Compensate for 2^384 * q
 
@@ -173,8 +173,8 @@ loop:
 
 writeback:
                 stp     m0, m1, [z]
-                stp     m2, m3, [z, 16]
-                stp     m4, m5, [z, 32]
+                stp     m2, m3, [z, #16]
+                stp     m4, m5, [z, #32]
 
                 ret
 
@@ -190,16 +190,16 @@ short:
 
                 cbz     k, writeback
                 ldr     m0, [x]
-                subs    k, k, 1
+                subs    k, k, #1
                 beq     writeback
-                ldr     m1, [x, 8]
-                subs    k, k, 1
+                ldr     m1, [x, #8]
+                subs    k, k, #1
                 beq     writeback
-                ldr     m2, [x, 16]
-                subs    k, k, 1
+                ldr     m2, [x, #16]
+                subs    k, k, #1
                 beq     writeback
-                ldr     m3, [x, 24]
-                subs    k, k, 1
+                ldr     m3, [x, #24]
+                subs    k, k, #1
                 beq     writeback
-                ldr     m4, [x, 32]
+                ldr     m4, [x, #32]
                 b       writeback
diff --git a/arm/p384/bignum_mod_n384_6.S b/arm/p384/bignum_mod_n384_6.S
index 60b0fa577..b1e65c9be 100644
--- a/arm/p384/bignum_mod_n384_6.S
+++ b/arm/p384/bignum_mod_n384_6.S
@@ -44,9 +44,9 @@
 
 .macro movbig x, n3, n2, n1, n0
                 movz    \x, \n0
-                movk    \x, \n1, lsl 16
-                movk    \x, \n2, lsl 32
-                movk    \x, \n3, lsl 48
+                movk    \x, \n1, lsl #16
+                movk    \x, \n2, lsl #32
+                movk    \x, \n3, lsl #48
 .endm
 
 .text
@@ -56,9 +56,9 @@ bignum_mod_n384_6:
 
 // Load the complicated lower three words of n_384
 
-                movbig  n0, 0xecec, 0x196a, 0xccc5, 0x2973
-                movbig  n1, 0x581a, 0x0db2, 0x48b0, 0xa77a
-                movbig  n2, 0xc763, 0x4d81, 0xf437, 0x2ddf
+                movbig  n0, #0xecec, #0x196a, #0xccc5, #0x2973
+                movbig  n1, #0x581a, #0x0db2, #0x48b0, #0xa77a
+                movbig  n2, #0xc763, #0x4d81, #0xf437, #0x2ddf
 
 // Load the input number
 
diff --git a/arm/p384/bignum_mod_p384.S b/arm/p384/bignum_mod_p384.S
index 853b14704..24aad7ffb 100644
--- a/arm/p384/bignum_mod_p384.S
+++ b/arm/p384/bignum_mod_p384.S
@@ -52,23 +52,23 @@ bignum_mod_p384:
 
 // If the input is already <= 5 words long, go to a trivial "copy" path
 
-                cmp     k, 6
+                cmp     k, #6
                 bcc     short
 
 // Otherwise load the top 6 digits (top-down) and reduce k by 6
 
-                sub     k, k, 6
-                lsl     t0, k, 3
+                sub     k, k, #6
+                lsl     t0, k, #3
                 add     t0, t0, x
-                ldp     m4, m5, [t0, 32]
-                ldp     m2, m3, [t0, 16]
+                ldp     m4, m5, [t0, #32]
+                ldp     m2, m3, [t0, #16]
                 ldp     m0, m1, [t0]
 
 // Load the complicated lower three words of p_384 = [-1;-1;-1;n2;n1;n0]
 
-                mov     n0, 0x00000000ffffffff
-                mov     n1, 0xffffffff00000000
-                mov     n2, 0xfffffffffffffffe
+                mov     n0, #0x00000000ffffffff
+                mov     n1, #0xffffffff00000000
+                mov     n2, #0xfffffffffffffffe
 
 // Reduce the top 6 digits mod p_384 (a conditional subtraction of p_384)
 
@@ -94,23 +94,23 @@ loop:
 // [m5;m4;m3;m2;m1;m0;t5] |-> [m5;m4;m3;m2;m1;m0]; the shuffling downwards is
 // absorbed into the various ALU operations
 
-                sub     k, k, 1
-                ldr     t5, [x, k, LSL 3]
+                sub     k, k, #1
+                ldr     t5, [x, k, lsl #3]
 
 // Initial quotient approximation q = min (h + 1) (2^64 - 1)
 
-                adds    m5, m5, 1
+                adds    m5, m5, #1
                 csetm   t3, cs
                 add     m5, m5, t3
                 orn     n1, xzr, t3
-                sub     t2, m5, 1
+                sub     t2, m5, #1
                 sub     t1, xzr, m5
 
 // Correction term [m5;t2;t1;t0] = q * (2^384 - p_384), using m5 as a temp
 
-                lsl     t0, t1, 32
-                extr    t1, t2, t1, 32
-                lsr     t2, t2, 32
+                lsl     t0, t1, #32
+                extr    t1, t2, t1, #32
+                lsr     t2, t2, #32
                 adds    t0, t0, m5
                 adcs    t1, t1, xzr
                 adcs    t2, t2, m5
@@ -144,8 +144,8 @@ loop:
 
 writeback:
                 stp     m0, m1, [z]
-                stp     m2, m3, [z, 16]
-                stp     m4, m5, [z, 32]
+                stp     m2, m3, [z, #16]
+                stp     m4, m5, [z, #32]
 
                 ret
 
@@ -161,17 +161,17 @@ short:
 
                 cbz     k, writeback
                 ldr     m0, [x]
-                subs    k, k, 1
+                subs    k, k, #1
                 beq     writeback
-                ldr     m1, [x, 8]
-                subs    k, k, 1
+                ldr     m1, [x, #8]
+                subs    k, k, #1
                 beq     writeback
-                ldr     m2, [x, 16]
-                subs    k, k, 1
+                ldr     m2, [x, #16]
+                subs    k, k, #1
                 beq     writeback
-                ldr     m3, [x, 24]
-                subs    k, k, 1
+                ldr     m3, [x, #24]
+                subs    k, k, #1
                 beq     writeback
-                ldr     m4, [x, 32]
+                ldr     m4, [x, #32]
                 b       writeback
 
diff --git a/arm/p384/bignum_mod_p384_6.S b/arm/p384/bignum_mod_p384_6.S
index e9aed5820..35465d58f 100644
--- a/arm/p384/bignum_mod_p384_6.S
+++ b/arm/p384/bignum_mod_p384_6.S
@@ -47,9 +47,9 @@ bignum_mod_p384_6:
 
 // Load the complicated lower three words of p_384 = [-1;-1;-1;n2;n1;n0]
 
-                mov     n0, 0x00000000ffffffff
-                mov     n1, 0xffffffff00000000
-                mov     n2, 0xfffffffffffffffe
+                mov     n0, #0x00000000ffffffff
+                mov     n1, #0xffffffff00000000
+                mov     n2, #0xfffffffffffffffe
 
 // Load the input number
 
diff --git a/arm/p384/bignum_montmul_p384.S b/arm/p384/bignum_montmul_p384.S
index 8cf49d930..d2a72ca10 100644
--- a/arm/p384/bignum_montmul_p384.S
+++ b/arm/p384/bignum_montmul_p384.S
@@ -61,19 +61,19 @@
 .macro          montreds d6,d5,d4,d3,d2,d1,d0, t3,t2,t1
 // Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64
 // Recycle d0 (which we know gets implicitly cancelled) to store it
-                lsl     \t1, \d0, 32
+                lsl     \t1, \d0, #32
                 add     \d0, \t1, \d0
 // Now let [t2;t1] = 2^64 * w - w + w_hi where w_hi = floor(w/2^32)
 // We need to subtract 2^32 * this, and we can ignore its lower 32
 // bits since by design it will cancel anyway; we only need the w_hi
 // part to get the carry propagation going.
-                lsr     \t1, \d0, 32
+                lsr     \t1, \d0, #32
                 subs    \t1, \t1, \d0
                 sbc     \t2, \d0, xzr
 // Now select in t1 the field to subtract from d1
-                extr    \t1, \t2, \t1, 32
+                extr    \t1, \t2, \t1, #32
 // And now get the terms to subtract from d2 and d3
-                lsr     \t2, \t2, 32
+                lsr     \t2, \t2, #32
                 adds    \t2, \t2, \d0
                 adc     \t3, xzr, xzr
 // Do the subtraction of that portion
@@ -116,18 +116,18 @@ bignum_montmul_p384:
 
 // Save some registers
 
-                stp     x19, x20, [sp, -16]!
-                stp     x21, x22, [sp, -16]!
-                stp     x23, x24, [sp, -16]!
+                stp     x19, x20, [sp, #-16]!
+                stp     x21, x22, [sp, #-16]!
+                stp     x23, x24, [sp, #-16]!
 
 // Load in all words of both inputs
 
                 ldp     a0, a1, [x1]
-                ldp     a2, a3, [x1, 16]
-                ldp     a4, a5, [x1, 32]
+                ldp     a2, a3, [x1, #16]
+                ldp     a4, a5, [x1, #32]
                 ldp     b0, b1, [x2]
-                ldp     b2, b3, [x2, 16]
-                ldp     b4, b5, [x2, 32]
+                ldp     b2, b3, [x2, #16]
+                ldp     b4, b5, [x2, #32]
 
 // Multiply low halves with a 3x3->6 ADK multiplier as [s5;s4;s3;s2;s1;s0]
 
@@ -153,7 +153,7 @@ bignum_montmul_p384:
                 adc     s5, s5, xzr
 
                 muldiffn t3,t2,t1, t4, a0,a1, b1,b0
-                adds    xzr, t3, 1
+                adds    xzr, t3, #1
                 adcs    s1, s1, t1
                 adcs    s2, s2, t2
                 adcs    s3, s3, t3
@@ -161,14 +161,14 @@ bignum_montmul_p384:
                 adc     s5, s5, t3
 
                 muldiffn t3,t2,t1, t4, a0,a2, b2,b0
-                adds    xzr, t3, 1
+                adds    xzr, t3, #1
                 adcs    s2, s2, t1
                 adcs    s3, s3, t2
                 adcs    s4, s4, t3
                 adc     s5, s5, t3
 
                 muldiffn t3,t2,t1, t4, a1,a2, b2,b1
-                adds    xzr, t3, 1
+                adds    xzr, t3, #1
                 adcs    s3, s3, t1
                 adcs    s4, s4, t2
                 adc     s5, s5, t3
@@ -186,8 +186,8 @@ bignum_montmul_p384:
                 montreds s2,s1,s0,s5,s4,s3,s2, t1,t2,t3
 
                 stp     s3, s4, [x0]
-                stp     s5, s0, [x0, 16]
-                stp     s1, s2, [x0, 32]
+                stp     s5, s0, [x0, #16]
+                stp     s1, s2, [x0, #32]
 
 // Multiply high halves with a 3x3->6 ADK multiplier as [s5;s4;s3;s2;s1;s0]
 
@@ -213,7 +213,7 @@ bignum_montmul_p384:
                 adc     s5, s5, xzr
 
                 muldiffn t3,t2,t1, t4, a3,a4, b4,b3
-                adds    xzr, t3, 1
+                adds    xzr, t3, #1
                 adcs    s1, s1, t1
                 adcs    s2, s2, t2
                 adcs    s3, s3, t3
@@ -221,14 +221,14 @@ bignum_montmul_p384:
                 adc     s5, s5, t3
 
                 muldiffn t3,t2,t1, t4, a3,a5, b5,b3
-                adds    xzr, t3, 1
+                adds    xzr, t3, #1
                 adcs    s2, s2, t1
                 adcs    s3, s3, t2
                 adcs    s4, s4, t3
                 adc     s5, s5, t3
 
                 muldiffn t3,t2,t1, t4, a4,a5, b5,b4
-                adds    xzr, t3, 1
+                adds    xzr, t3, #1
                 adcs    s3, s3, t1
                 adcs    s4, s4, t2
                 adc     s5, s5, t3
@@ -239,7 +239,7 @@ bignum_montmul_p384:
                 sbcs    a4, a4, a1
                 sbcs    a5, a5, a2
                 sbc     a0, xzr, xzr
-                adds    xzr, a0, 1
+                adds    xzr, a0, #1
                 eor     a3, a3, a0
                 adcs    a3, a3, xzr
                 eor     a4, a4, a0
@@ -254,7 +254,7 @@ bignum_montmul_p384:
                 sbcs    b2, b2, b5
                 sbc     b5, xzr, xzr
 
-                adds    xzr, b5, 1
+                adds    xzr, b5, #1
                 eor     b0, b0, b5
                 adcs    b0, b0, xzr
                 eor     b1, b1, b5
@@ -272,16 +272,16 @@ bignum_montmul_p384:
                 ldp     t1, t2, [x0]
                 adds    s0, s0, t1
                 adcs    s1, s1, t2
-                ldp     t1, t2, [x0, 16]
+                ldp     t1, t2, [x0, #16]
                 adcs    s2, s2, t1
                 adcs    s3, s3, t2
-                ldp     t1, t2, [x0, 32]
+                ldp     t1, t2, [x0, #32]
                 adcs    s4, s4, t1
                 adcs    s5, s5, t2
                 adc     s6, xzr, xzr
                 stp     s0, s1, [x0]
-                stp     s2, s3, [x0, 16]
-                stp     s4, s5, [x0, 32]
+                stp     s2, s3, [x0, #16]
+                stp     s4, s5, [x0, #32]
 
 // Multiply with yet a third 3x3 ADK for the complex mid-term
 
@@ -307,7 +307,7 @@ bignum_montmul_p384:
                 adc     s5, s5, xzr
 
                 muldiffn t3,t2,t1, t4, a3,a4, b1,b0
-                adds    xzr, t3, 1
+                adds    xzr, t3, #1
                 adcs    s1, s1, t1
                 adcs    s2, s2, t2
                 adcs    s3, s3, t3
@@ -315,14 +315,14 @@ bignum_montmul_p384:
                 adc     s5, s5, t3
 
                 muldiffn t3,t2,t1, t4, a3,a5, b2,b0
-                adds    xzr, t3, 1
+                adds    xzr, t3, #1
                 adcs    s2, s2, t1
                 adcs    s3, s3, t2
                 adcs    s4, s4, t3
                 adc     s5, s5, t3
 
                 muldiffn t3,t2,t1, t4, a4,a5, b2,b1
-                adds    xzr, t3, 1
+                adds    xzr, t3, #1
                 adcs    s3, s3, t1
                 adcs    s4, s4, t2
                 adc     s5, s5, t3
@@ -330,14 +330,14 @@ bignum_montmul_p384:
 // Unstash the H + L' sum to add in twice
 
                 ldp     a0, a1, [x0]
-                ldp     a2, a3, [x0, 16]
-                ldp     a4, a5, [x0, 32]
+                ldp     a2, a3, [x0, #16]
+                ldp     a4, a5, [x0, #32]
 
 // Set up a sign-modified version of the mid-product in a long accumulator
 // as [b3;b2;b1;b0;s5;s4;s3;s2;s1;s0], adding in the H + L' term once with
 // zero offset as this signed value is created
 
-                adds    xzr, b5, 1
+                adds    xzr, b5, #1
                 eor     s0, s0, b5
                 adcs    s0, s0, a0
                 eor     s1, s1, b5
@@ -382,8 +382,8 @@ bignum_montmul_p384:
 // elaborate final correction in the style of bignum_cmul_p384, just
 // a little bit simpler because we know q is small.
 
-                add     t2, b3, 1
-                lsl     t1, t2, 32
+                add     t2, b3, #1
+                lsl     t1, t2, #32
                 subs    t4, t2, t1
                 sbc     t1, t1, xzr
 
@@ -396,12 +396,12 @@ bignum_montmul_p384:
 
                 csetm   t2, cc
 
-                mov     t3, 0x00000000ffffffff
+                mov     t3, #0x00000000ffffffff
                 and     t3, t3, t2
                 adds    s3, s3, t3
                 eor     t3, t3, t2
                 adcs    s4, s4, t3
-                mov     t3, 0xfffffffffffffffe
+                mov     t3, #0xfffffffffffffffe
                 and     t3, t3, t2
                 adcs    s5, s5, t3
                 adcs    b0, b0, t2
@@ -411,13 +411,13 @@ bignum_montmul_p384:
 // Write back the result
 
                 stp     s3, s4, [x0]
-                stp     s5, b0, [x0, 16]
-                stp     b1, b2, [x0, 32]
+                stp     s5, b0, [x0, #16]
+                stp     b1, b2, [x0, #32]
 
 // Restore registers and return
 
-                ldp     x23, x24, [sp], 16
-                ldp     x21, x22, [sp], 16
-                ldp     x19, x20, [sp], 16
+                ldp     x23, x24, [sp], #16
+                ldp     x21, x22, [sp], #16
+                ldp     x19, x20, [sp], #16
 
                 ret
diff --git a/arm/p384/bignum_montsqr_p384.S b/arm/p384/bignum_montsqr_p384.S
index 789c377f1..e0c549b72 100644
--- a/arm/p384/bignum_montsqr_p384.S
+++ b/arm/p384/bignum_montsqr_p384.S
@@ -60,19 +60,19 @@
 .macro          montreds d6,d5,d4,d3,d2,d1,d0, t3,t2,t1
 // Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64
 // Recycle d0 (which we know gets implicitly cancelled) to store it
-                lsl     \t1, \d0, 32
+                lsl     \t1, \d0, #32
                 add     \d0, \t1, \d0
 // Now let [t2;t1] = 2^64 * w - w + w_hi where w_hi = floor(w/2^32)
 // We need to subtract 2^32 * this, and we can ignore its lower 32
 // bits since by design it will cancel anyway; we only need the w_hi
 // part to get the carry propagation going.
-                lsr     \t1, \d0, 32
+                lsr     \t1, \d0, #32
                 subs    \t1, \t1, \d0
                 sbc     \t2, \d0, xzr
 // Now select in t1 the field to subtract from d1
-                extr    \t1, \t2, \t1, 32
+                extr    \t1, \t2, \t1, #32
 // And now get the terms to subtract from d2 and d3
-                lsr     \t2, \t2, 32
+                lsr     \t2, \t2, #32
                 adds    \t2, \t2, \d0
                 adc     \t3, xzr, xzr
 // Do the subtraction of that portion
@@ -108,8 +108,8 @@ bignum_montsqr_p384:
 // Load in all words of the input
 
                 ldp     a0, a1, [x1]
-                ldp     a2, a3, [x1, 16]
-                ldp     a4, a5, [x1, 32]
+                ldp     a2, a3, [x1, #16]
+                ldp     a4, a5, [x1, #32]
 
 // Square the low half getting a result in [c5;c4;c3;c2;c1;c0]
 
@@ -154,8 +154,8 @@ bignum_montsqr_p384:
                 montreds c2,c1,c0,c5,c4,c3,c2, d1,d2,d3
 
                 stp     c3, c4, [x0]
-                stp     c5, c0, [x0, 16]
-                stp     c1, c2, [x0, 32]
+                stp     c5, c0, [x0, #16]
+                stp     c1, c2, [x0, #32]
 
 // Compute product of the cross-term with ADK 3x3->6 multiplier
 
@@ -206,7 +206,7 @@ bignum_montsqr_p384:
                 adc     s5, h2, xzr
 
                 muldiffn c,h,l, t, a0,a1, a4,a3
-                adds    xzr, c, 1
+                adds    xzr, c, #1
                 adcs    s1, s1, l
                 adcs    s2, s2, h
                 adcs    s3, s3, c
@@ -214,14 +214,14 @@ bignum_montsqr_p384:
                 adc     s5, s5, c
 
                 muldiffn c,h,l, t, a0,a2, a5,a3
-                adds    xzr, c, 1
+                adds    xzr, c, #1
                 adcs    s2, s2, l
                 adcs    s3, s3, h
                 adcs    s4, s4, c
                 adc     s5, s5, c
 
                 muldiffn c,h,l, t, a1,a2, a5,a4
-                adds    xzr, c, 1
+                adds    xzr, c, #1
                 adcs    s3, s3, l
                 adcs    s4, s4, h
                 adc     s5, s5, c
@@ -239,10 +239,10 @@ bignum_montsqr_p384:
                 ldp     a0, a1, [x0]
                 adds    s0, s0, a0
                 adcs    s1, s1, a1
-                ldp     a0, a1, [x0, 16]
+                ldp     a0, a1, [x0, #16]
                 adcs    s2, s2, a0
                 adcs    s3, s3, a1
-                ldp     a0, a1, [x0, 32]
+                ldp     a0, a1, [x0, #32]
                 adcs    s4, s4, a0
                 adcs    s5, s5, a1
                 adc     s6, s6, xzr
@@ -336,9 +336,9 @@ bignum_montsqr_p384:
 // comparison to catch cases where the residue is >= p.
 // First set [0;0;0;t3;t2;t1] = 2^384 - p_384
 
-                mov     t1, 0xffffffff00000001
-                mov     t2, 0x00000000ffffffff
-                mov     t3, 0x0000000000000001
+                mov     t1, #0xffffffff00000001
+                mov     t2, #0x00000000ffffffff
+                mov     t3, #0x0000000000000001
 
 // Let dd = [] be the 6-word intermediate result.
 // Set CF if the addition dd + (2^384 - p_384) >= 2^384, hence iff dd >= p_384.
@@ -374,7 +374,7 @@ bignum_montsqr_p384:
 // Store it back
 
                 stp     r0, r1, [x0]
-                stp     r2, r3, [x0, 16]
-                stp     r4, r5, [x0, 32]
+                stp     r2, r3, [x0, #16]
+                stp     r4, r5, [x0, #32]
 
                 ret
diff --git a/arm/p384/bignum_mux_6.S b/arm/p384/bignum_mux_6.S
index 18118eac5..57f0e709b 100644
--- a/arm/p384/bignum_mux_6.S
+++ b/arm/p384/bignum_mux_6.S
@@ -37,14 +37,14 @@
 
 bignum_mux_6:
 
-                cmp     p, 0                    // Set condition codes p = 0
+                cmp     p, #0                    // Set condition codes p = 0
 
                 .set    i, 0
                 .rep    6
-                ldr     a, [x, 8*i]
-                ldr     p, [y, 8*i]
+                ldr     a, [x, #8*i]
+                ldr     p, [y, #8*i]
                 csel    a, a, p, ne
-                str     a, [z, 8*i]
+                str     a, [z, #8*i]
                 .set    i, (i+1)
                 .endr
 
diff --git a/arm/p384/bignum_neg_p384.S b/arm/p384/bignum_neg_p384.S
index 6a5d56b23..73b8ce680 100644
--- a/arm/p384/bignum_neg_p384.S
+++ b/arm/p384/bignum_neg_p384.S
@@ -43,8 +43,8 @@ bignum_neg_p384:
 // Load the 6 digits of x
 
                 ldp     d0, d1, [x]
-                ldp     d2, d3, [x, 16]
-                ldp     d4, d5, [x, 32]
+                ldp     d2, d3, [x, #16]
+                ldp     d4, d5, [x, #32]
 
 // Set a bitmask p for the input being nonzero, so that we avoid doing
 // -0 = p_384 and hence maintain strict modular reduction
@@ -54,21 +54,21 @@ bignum_neg_p384:
                 orr     p, p, t
                 orr     t, d4, d5
                 orr     p, p, t
-                cmp     p, 0
+                cmp     p, #0
                 csetm   p, ne
 
 // Load and mask the complicated lower three words of
 // p_384 = [-1;-1;-1;n2;n1;n0] and subtract, using mask itself for upper digits
 
-                mov     t, 0x00000000ffffffff
+                mov     t, #0x00000000ffffffff
                 and     t, t, p
                 subs    d0, t, d0
 
-                mov     t, 0xffffffff00000000
+                mov     t, #0xffffffff00000000
                 and     t, t, p
                 sbcs    d1, t, d1
 
-                mov     t, 0xfffffffffffffffe
+                mov     t, #0xfffffffffffffffe
                 and     t, t, p
                 sbcs    d2, t, d2
 
@@ -79,8 +79,8 @@ bignum_neg_p384:
 // Write back the result
 
                 stp     d0, d1, [z]
-                stp     d2, d3, [z, 16]
-                stp     d4, d5, [z, 32]
+                stp     d2, d3, [z, #16]
+                stp     d4, d5, [z, #32]
 
 // Return
 
diff --git a/arm/p384/bignum_nonzero_6.S b/arm/p384/bignum_nonzero_6.S
index f6c92c40f..7f255b6ac 100644
--- a/arm/p384/bignum_nonzero_6.S
+++ b/arm/p384/bignum_nonzero_6.S
@@ -36,10 +36,10 @@ bignum_nonzero_6:
 
                 ldp     a, d, [x]
                 orr     a, a, d
-                ldp     c, d, [x, 16]
+                ldp     c, d, [x, #16]
                 orr     c, c, d
                 orr     a, a, c
-                ldp     c, d, [x, 32]
+                ldp     c, d, [x, #32]
                 orr     c, c, d
                 orr     a, a, c
 
diff --git a/arm/p384/bignum_optneg_p384.S b/arm/p384/bignum_optneg_p384.S
index aea904229..f0312c20a 100644
--- a/arm/p384/bignum_optneg_p384.S
+++ b/arm/p384/bignum_optneg_p384.S
@@ -49,8 +49,8 @@ bignum_optneg_p384:
 // Load the 6 digits of x
 
                 ldp     d0, d1, [x]
-                ldp     d2, d3, [x, 16]
-                ldp     d4, d5, [x, 32]
+                ldp     d2, d3, [x, #16]
+                ldp     d4, d5, [x, #32]
 
 // Adjust p by zeroing it if the input is zero (to avoid giving -0 = p, which
 // is not strictly reduced even though it's correct modulo p)
@@ -60,15 +60,15 @@ bignum_optneg_p384:
                 orr     n2, d4, d5
                 orr     n3, n0, n1
                 orr     n4, n2, n3
-                cmp     n4, 0
+                cmp     n4, #0
                 csel    p, xzr, p, eq
 
 // Load the complicated lower three words of p_384 = [-1;-1;-1;n2;n1;n0] and -1
 
-                mov     n0, 0x00000000ffffffff
-                mov     n1, 0xffffffff00000000
-                mov     n2, 0xfffffffffffffffe
-                mov     n5, 0xffffffffffffffff
+                mov     n0, #0x00000000ffffffff
+                mov     n1, #0xffffffff00000000
+                mov     n2, #0xfffffffffffffffe
+                mov     n5, #0xffffffffffffffff
 
 // Do the subtraction, which by hypothesis does not underflow
 
@@ -81,7 +81,7 @@ bignum_optneg_p384:
 
 // Set condition code if original x is nonzero and p was nonzero
 
-                cmp     p, 0
+                cmp     p, #0
 
 // Hence multiplex and write back
 
@@ -93,8 +93,8 @@ bignum_optneg_p384:
                 csel    n5, n5, d5, ne
 
                 stp     n0, n1, [z]
-                stp     n2, n3, [z, 16]
-                stp     n4, n5, [z, 32]
+                stp     n2, n3, [z, #16]
+                stp     n4, n5, [z, #32]
 
 // Return
 
diff --git a/arm/p384/bignum_sub_p384.S b/arm/p384/bignum_sub_p384.S
index 2ef51ef2b..4eb6f47f2 100644
--- a/arm/p384/bignum_sub_p384.S
+++ b/arm/p384/bignum_sub_p384.S
@@ -63,12 +63,12 @@ bignum_sub_p384:
 
 // Now correct by adding masked p_384
 
-                mov     l, 0x00000000ffffffff
+                mov     l, #0x00000000ffffffff
                 and     l, l, c
                 adds    d0, d0, l
                 eor     l, l, c
                 adcs    d1, d1, l
-                mov     l, 0xfffffffffffffffe
+                mov     l, #0xfffffffffffffffe
                 and     l, l, c
                 adcs    d2, d2, l
                 adcs    d3, d3, c
diff --git a/arm/p384/bignum_tomont_p384.S b/arm/p384/bignum_tomont_p384.S
index c3483a56c..4a5bab24d 100644
--- a/arm/p384/bignum_tomont_p384.S
+++ b/arm/p384/bignum_tomont_p384.S
@@ -33,16 +33,16 @@
 
 .macro modstep_p384 d6,d5,d4,d3,d2,d1,d0, t1,t2,t3
 // Initial quotient approximation q = min (h + 1) (2^64 - 1)
-                adds    \d6, \d6, 1
+                adds    \d6, \d6, #1
                 csetm   \t3, cs
                 add     \d6, \d6, \t3
                 orn     \t3, xzr, \t3
-                sub     \t2, \d6, 1
+                sub     \t2, \d6, #1
                 sub     \t1, xzr, \d6
 // Correction term [d6;t2;t1;d0] = q * (2^384 - p_384)
-                lsl     \d0, \t1, 32
-                extr    \t1, \t2, \t1, 32
-                lsr     \t2, \t2, 32
+                lsl     \d0, \t1, #32
+                extr    \t1, \t2, \t1, #32
+                lsr     \t2, \t2, #32
                 adds    \d0, \d0, \d6
                 adcs    \t1, \t1, xzr
                 adcs    \t2, \t2, \d6
@@ -55,12 +55,12 @@
                 adcs    \d5, \d5, xzr
                 adc     \t3, \t3, xzr
 // Use net top of the 7-word answer in t3 for masked correction
-                mov     \t1, 0x00000000ffffffff
+                mov     \t1, #0x00000000ffffffff
                 and     \t1, \t1, \t3
                 adds    \d0, \d0, \t1
                 eor     \t1, \t1, \t3
                 adcs    \d1, \d1, \t1
-                mov     \t1, 0xfffffffffffffffe
+                mov     \t1, #0xfffffffffffffffe
                 and     \t1, \t1, \t3
                 adcs    \d2, \d2, \t1
                 adcs    \d3, \d3, \t3
@@ -92,16 +92,16 @@ bignum_tomont_p384:
 // Load the inputs
 
                 ldp     d0, d1, [x1]
-                ldp     d2, d3, [x1, 16]
-                ldp     d4, d5, [x1, 32]
+                ldp     d2, d3, [x1, #16]
+                ldp     d4, d5, [x1, #32]
 
 // Do an initial reduction to make sure this is < p_384, using just
 // a copy of the bignum_mod_p384 code. This is needed to set up the
 // invariant "input < p_384" for the main modular reduction steps.
 
-                mov     n0, 0x00000000ffffffff
-                mov     n1, 0xffffffff00000000
-                mov     n2, 0xfffffffffffffffe
+                mov     n0, #0x00000000ffffffff
+                mov     n1, #0xffffffff00000000
+                mov     n2, #0xfffffffffffffffe
                 subs    n0, d0, n0
                 sbcs    n1, d1, n1
                 sbcs    n2, d2, n2
diff --git a/arm/p384/bignum_triple_p384.S b/arm/p384/bignum_triple_p384.S
index e13ce79c2..2f449ff21 100644
--- a/arm/p384/bignum_triple_p384.S
+++ b/arm/p384/bignum_triple_p384.S
@@ -69,33 +69,33 @@ bignum_triple_p384:
 
 // First do the multiplication by 3, getting z = [h; d5; ...; d0]
 
-                lsl     d0, a0, 1
+                lsl     d0, a0, #1
                 adds    d0, d0, a0
-                extr    d1, a1, a0, 63
+                extr    d1, a1, a0, #63
                 adcs    d1, d1, a1
-                extr    d2, a2, a1, 63
+                extr    d2, a2, a1, #63
                 adcs    d2, d2, a2
-                extr    d3, a3, a2, 63
+                extr    d3, a3, a2, #63
                 adcs    d3, d3, a3
-                extr    d4, a4, a3, 63
+                extr    d4, a4, a3, #63
                 adcs    d4, d4, a4
-                extr    d5, a5, a4, 63
+                extr    d5, a5, a4, #63
                 adcs    d5, d5, a5
-                lsr     h, a5, 63
+                lsr     h, a5, #63
                 adc     h, h, xzr
 
 // For this limited range a simple quotient estimate of q = h + 1 works, where
 // h = floor(z / 2^384). Then -p_384 <= z - q * p_384 < p_384, so we just need
 // to subtract q * p_384 and then if that's negative, add back p_384.
 
-                add     q, h, 1
+                add     q, h, #1
 
 // Initial subtraction of z - q * p_384, with bitmask c for the carry
 // Actually done as an addition of (z - 2^384 * h) + q * (2^384 - p_384)
 // which, because q = h + 1, is exactly 2^384 + (z - q * p_384), and
 // therefore CF <=> 2^384 + (z - q * p_384) >= 2^384 <=> z >= q * p_384.
 
-                lsl     t1, q, 32
+                lsl     t1, q, #32
                 subs    t0, q, t1
                 sbc     t1, t1, xzr
 
@@ -109,12 +109,12 @@ bignum_triple_p384:
 
 // Use the bitmask c for final masked addition of p_384.
 
-                mov     t0, 0x00000000ffffffff
+                mov     t0, #0x00000000ffffffff
                 and     t0, t0, c
                 adds    d0, d0, t0
                 eor     t0, t0, c
                 adcs    d1, d1, t0
-                mov     t0, 0xfffffffffffffffe
+                mov     t0, #0xfffffffffffffffe
                 and     t0, t0, c
                 adcs    d2, d2, t0
                 adcs    d3, d3, c
diff --git a/arm/p521/bignum_add_p521.S b/arm/p521/bignum_add_p521.S
index 771b2b15a..6e38bc7d1 100644
--- a/arm/p521/bignum_add_p521.S
+++ b/arm/p521/bignum_add_p521.S
@@ -51,33 +51,33 @@ bignum_add_p521:
                 ldp     l, h, [y]
                 adcs    d0, d0, l
                 adcs    d1, d1, h
-                ldp     d2, d3, [x, 16]
-                ldp     l, h, [y, 16]
+                ldp     d2, d3, [x, #16]
+                ldp     l, h, [y, #16]
                 adcs    d2, d2, l
                 adcs    d3, d3, h
-                ldp     d4, d5, [x, 32]
-                ldp     l, h, [y, 32]
+                ldp     d4, d5, [x, #32]
+                ldp     l, h, [y, #32]
                 adcs    d4, d4, l
                 adcs    d5, d5, h
-                ldp     d6, d7, [x, 48]
-                ldp     l, h, [y, 48]
+                ldp     d6, d7, [x, #48]
+                ldp     l, h, [y, #48]
                 adcs    d6, d6, l
                 adcs    d7, d7, h
-                ldr     d8, [x, 64]
-                ldr     l, [y, 64]
+                ldr     d8, [x, #64]
+                ldr     l, [y, #64]
                 adc     d8, d8, l
 
 // Now x + y >= p_521 <=> s = x + y + 1 >= 2^521
 // Set CF <=> s = x + y + 1 >= 2^521 and make it a mask in l as well
 
-                subs    l, d8, 512
+                subs    l, d8, #512
                 csetm   l, cs
 
 // Now if CF is set (and l is all 1s), we want (x + y) - p_521 = s - 2^521
 // while otherwise we want x + y = s - 1 (from existing CF, which is nice)
 
                 sbcs    d0, d0, xzr
-                and     l, l, 512
+                and     l, l, #512
                 sbcs    d1, d1, xzr
                 sbcs    d2, d2, xzr
                 sbcs    d3, d3, xzr
@@ -90,9 +90,9 @@ bignum_add_p521:
 // Store the result
 
                 stp     d0, d1, [z]
-                stp     d2, d3, [z, 16]
-                stp     d4, d5, [z, 32]
-                stp     d6, d7, [z, 48]
-                str     d8, [z, 64]
+                stp     d2, d3, [z, #16]
+                stp     d4, d5, [z, #32]
+                stp     d6, d7, [z, #48]
+                str     d8, [z, #64]
 
                 ret
diff --git a/arm/p521/bignum_double_p521.S b/arm/p521/bignum_double_p521.S
index 807360fce..49ea92fee 100644
--- a/arm/p521/bignum_double_p521.S
+++ b/arm/p521/bignum_double_p521.S
@@ -38,8 +38,8 @@ bignum_double_p521:
 // We can decide whether 2 * x >= p_521 just by 2 * x >= 2^521, which
 // amounts to whether the top word is >= 256
 
-                ldr     c, [x, 64]
-                subs    xzr, c, 256
+                ldr     c, [x, #64]
+                subs    xzr, c, #256
 
 // Now if 2 * x >= p_521 we want 2 * x - p_521 = (2 * x + 1) - 2^521
 // and otherwise just 2 * x. Feed in the condition as the carry bit
@@ -47,16 +47,16 @@ bignum_double_p521:
 
                 .set    i, 0
                 .rep 4
-                ldp     l, h, [x, 8*i]
+                ldp     l, h, [x, #8*i]
                 adcs    l, l, l
                 adcs    h, h, h
-                stp     l, h, [z, 8*i]
+                stp     l, h, [z, #8*i]
                 .set    i, (i+2)
                 .endr
 
                 adc     c, c, c
-                and     c, c, 0x1FF
-                str     c, [z, 64]
+                and     c, c, #0x1FF
+                str     c, [z, #64]
 
                 ret
 
diff --git a/arm/p521/bignum_half_p521.S b/arm/p521/bignum_half_p521.S
index 4499a9883..9e44d79f8 100644
--- a/arm/p521/bignum_half_p521.S
+++ b/arm/p521/bignum_half_p521.S
@@ -47,30 +47,30 @@ bignum_half_p521:
 // We do a 521-bit rotation one bit right, since 2^521 == 1 (mod p_521)
 
                 ldp     d0, d1, [x]
-                and     a, d0, 1
-                extr    d0, d1, d0, 1
+                and     a, d0, #1
+                extr    d0, d1, d0, #1
 
-                ldp     d2, d3, [x, 16]
-                extr    d1, d2, d1, 1
+                ldp     d2, d3, [x, #16]
+                extr    d1, d2, d1, #1
                 stp     d0, d1, [z]
-                extr    d2, d3, d2, 1
+                extr    d2, d3, d2, #1
 
-                ldp     d4, d5, [x, 32]
-                extr    d3, d4, d3, 1
-                stp     d2, d3, [z, 16]
-                extr    d4, d5, d4, 1
+                ldp     d4, d5, [x, #32]
+                extr    d3, d4, d3, #1
+                stp     d2, d3, [z, #16]
+                extr    d4, d5, d4, #1
 
-                ldp     d6, d7, [x, 48]
-                extr    d5, d6, d5, 1
-                stp     d4, d5, [z, 32]
-                extr    d6, d7, d6, 1
+                ldp     d6, d7, [x, #48]
+                extr    d5, d6, d5, #1
+                stp     d4, d5, [z, #32]
+                extr    d6, d7, d6, #1
 
-                ldr     d8, [x, 64]
-                extr    d7, d8, d7, 1
-                stp     d6, d7, [z, 48]
-                lsl     d8, d8, 55
-                extr    d8, a, d8, 56
-                str     d8, [z, 64]
+                ldr     d8, [x, #64]
+                extr    d7, d8, d7, #1
+                stp     d6, d7, [z, #48]
+                lsl     d8, d8, #55
+                extr    d8, a, d8, #56
+                str     d8, [z, #64]
 
 // Return
 
diff --git a/arm/p521/bignum_sub_p521.S b/arm/p521/bignum_sub_p521.S
index c0bdf3438..a9846ce24 100644
--- a/arm/p521/bignum_sub_p521.S
+++ b/arm/p521/bignum_sub_p521.S
@@ -49,20 +49,20 @@ bignum_sub_p521:
                 ldp     l, h, [y]
                 subs    d0, d0, l
                 sbcs    d1, d1, h
-                ldp     d2, d3, [x, 16]
-                ldp     l, h, [y, 16]
+                ldp     d2, d3, [x, #16]
+                ldp     l, h, [y, #16]
                 sbcs    d2, d2, l
                 sbcs    d3, d3, h
-                ldp     d4, d5, [x, 32]
-                ldp     l, h, [y, 32]
+                ldp     d4, d5, [x, #32]
+                ldp     l, h, [y, #32]
                 sbcs    d4, d4, l
                 sbcs    d5, d5, h
-                ldp     d6, d7, [x, 48]
-                ldp     l, h, [y, 48]
+                ldp     d6, d7, [x, #48]
+                ldp     l, h, [y, #48]
                 sbcs    d6, d6, l
                 sbcs    d7, d7, h
-                ldr     d8, [x, 64]
-                ldr     l, [y, 64]
+                ldr     d8, [x, #64]
+                ldr     l, [y, #64]
                 sbcs    d8, d8, l
 
 // Now if x < y we want (x - y) + p_521 == (x - y) - 1 (mod 2^521)
@@ -78,14 +78,14 @@ bignum_sub_p521:
                 sbcs    d6, d6, xzr
                 sbcs    d7, d7, xzr
                 sbcs    d8, d8, xzr
-                and     d8, d8, 0x1FF
+                and     d8, d8, #0x1FF
 
 // Store the result
 
                 stp     d0, d1, [z]
-                stp     d2, d3, [z, 16]
-                stp     d4, d5, [z, 32]
-                stp     d6, d7, [z, 48]
-                str     d8, [z, 64]
+                stp     d2, d3, [z, #16]
+                stp     d4, d5, [z, #32]
+                stp     d6, d7, [z, #48]
+                str     d8, [z, #64]
 
                 ret
