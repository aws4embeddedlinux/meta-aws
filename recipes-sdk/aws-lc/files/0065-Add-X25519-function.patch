From 7ac45698abba4a2786ad0e7c7d22fd13de3a3cd8 Mon Sep 17 00:00:00 2001
From: John Harrison <jargh@amazon.com>
Date: Fri, 30 Sep 2022 14:01:39 -0700
Subject: [PATCH] Add X25519 function

This provides the standard curve25519 "X25519" function including the
specific input mangling from "https://www.rfc-editor.org/rfc/rfc7748"
and proved correct against a spec based on Bernstein's original
description. The inputs and outputs are the usual s2n-bignum numbers
with four 64-bit word digits each. (That is, not arrays of bytes,
though on a little-endian machine with no special alignment
restrictions the difference amounts just to a pointer typecast.)

s2n-bignum original commit: https://github.com/awslabs/s2n-bignum/commit/ca2f87e37b8ede100961d1dadd440f9fad0981e1
---
 arm/curve25519/curve25519_x25519.S         | 1337 ++++++++++++++++++
 arm/curve25519/curve25519_x25519_alt.S     | 1151 ++++++++++++++++
 x86_att/curve25519/curve25519_x25519.S     | 1245 +++++++++++++++++
 x86_att/curve25519/curve25519_x25519_alt.S | 1413 ++++++++++++++++++++
 4 files changed, 5146 insertions(+)
 create mode 100644 arm/curve25519/curve25519_x25519.S
 create mode 100644 arm/curve25519/curve25519_x25519_alt.S
 create mode 100644 x86_att/curve25519/curve25519_x25519.S
 create mode 100644 x86_att/curve25519/curve25519_x25519_alt.S

diff --git a/arm/curve25519/curve25519_x25519.S b/arm/curve25519/curve25519_x25519.S
new file mode 100644
index 000000000..bea6d50ce
--- /dev/null
+++ b/arm/curve25519/curve25519_x25519.S
@@ -0,0 +1,1337 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// The x25519 function for curve25519
+// Inputs scalar[4], point[4]; output res[4]
+//
+// extern void curve25519_x25519
+//   (uint64_t res[static 4],uint64_t scalar[static 4],uint64_t point[static 4])
+//
+// Given a scalar n and the X coordinate of an input point P = (X,Y) on
+// curve25519 (Y can live in any extension field of characteristic 2^255-19),
+// this returns the X coordinate of n * P = (X, Y), or 0 when n * P is the
+// point at infinity. Both n and X inputs are first slightly modified/mangled
+// as specified in the relevant RFC (https://www.rfc-editor.org/rfc/rfc7748);
+// in particular the lower three bits of n are set to zero.
+//
+// Standard ARM ABI: X0 = res, X1 = scalar, X2 = point
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(curve25519_x25519)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(curve25519_x25519)
+
+        .text
+        .balign 4
+
+// Size of individual field elements
+
+#define NUMSIZE 32
+
+// Stable homes for the input result argument during the whole body
+// and other variables that are only needed prior to the modular inverse.
+
+#define res x23
+#define i x20
+#define swap x21
+
+// Pointers to result x coord to be written
+
+#define resx res, #0
+
+// Pointer-offset pairs for temporaries on stack with some aliasing.
+// Both dmsn and dnsm need space for >= 5 digits, and we allocate 8
+
+#define scalar sp, #(0*NUMSIZE)
+
+#define pointx sp, #(1*NUMSIZE)
+
+#define zm sp, #(2*NUMSIZE)
+#define sm sp, #(2*NUMSIZE)
+#define dpro sp, #(2*NUMSIZE)
+
+#define sn sp, #(3*NUMSIZE)
+
+#define dm sp, #(4*NUMSIZE)
+
+#define zn sp, #(5*NUMSIZE)
+#define dn sp, #(5*NUMSIZE)
+#define e sp, #(5*NUMSIZE)
+
+#define dmsn sp, #(6*NUMSIZE)
+#define p sp, #(6*NUMSIZE)
+
+#define xm sp, #(8*NUMSIZE)
+#define dnsm sp, #(8*NUMSIZE)
+#define spro sp, #(8*NUMSIZE)
+
+#define xn sp, #(10*NUMSIZE)
+#define s sp, #(10*NUMSIZE)
+
+#define d sp, #(11*NUMSIZE)
+
+// Total size to reserve on the stack
+
+#define NSPACE (12*NUMSIZE)
+
+// Macros wrapping up the basic field operation calls
+// bignum_mul_p25519 and bignum_sqr_p25519.
+// These two are only trivially different from pure
+// function calls to those subroutines.
+
+#define mul_p25519(p0,p1,p2)                    \
+        ldp     x3, x4, [p1];                   \
+        ldp     x5, x6, [p2];                   \
+        mul     x7, x3, x5;                     \
+        umulh   x8, x3, x5;                     \
+        mul     x9, x4, x6;                     \
+        umulh   x10, x4, x6;                    \
+        subs    x4, x4, x3;                     \
+        cneg    x4, x4, cc;                     \
+        csetm   x16, cc;                        \
+        adds    x9, x9, x8;                     \
+        adc     x10, x10, xzr;                  \
+        subs    x3, x5, x6;                     \
+        cneg    x3, x3, cc;                     \
+        cinv    x16, x16, cc;                   \
+        mul     x15, x4, x3;                    \
+        umulh   x3, x4, x3;                     \
+        adds    x8, x7, x9;                     \
+        adcs    x9, x9, x10;                    \
+        adc     x10, x10, xzr;                  \
+        cmn     x16, #0x1;                      \
+        eor     x15, x15, x16;                  \
+        adcs    x8, x15, x8;                    \
+        eor     x3, x3, x16;                    \
+        adcs    x9, x3, x9;                     \
+        adc     x10, x10, x16;                  \
+        ldp     x3, x4, [p1+16];                \
+        ldp     x5, x6, [p2+16];                \
+        mul     x11, x3, x5;                    \
+        umulh   x12, x3, x5;                    \
+        mul     x13, x4, x6;                    \
+        umulh   x14, x4, x6;                    \
+        subs    x4, x4, x3;                     \
+        cneg    x4, x4, cc;                     \
+        csetm   x16, cc;                        \
+        adds    x13, x13, x12;                  \
+        adc     x14, x14, xzr;                  \
+        subs    x3, x5, x6;                     \
+        cneg    x3, x3, cc;                     \
+        cinv    x16, x16, cc;                   \
+        mul     x15, x4, x3;                    \
+        umulh   x3, x4, x3;                     \
+        adds    x12, x11, x13;                  \
+        adcs    x13, x13, x14;                  \
+        adc     x14, x14, xzr;                  \
+        cmn     x16, #0x1;                      \
+        eor     x15, x15, x16;                  \
+        adcs    x12, x15, x12;                  \
+        eor     x3, x3, x16;                    \
+        adcs    x13, x3, x13;                   \
+        adc     x14, x14, x16;                  \
+        ldp     x3, x4, [p1+16];                \
+        ldp     x15, x16, [p1];                 \
+        subs    x3, x3, x15;                    \
+        sbcs    x4, x4, x16;                    \
+        csetm   x16, cc;                        \
+        ldp     x15, x0, [p2];                  \
+        subs    x5, x15, x5;                    \
+        sbcs    x6, x0, x6;                     \
+        csetm   x0, cc;                         \
+        eor     x3, x3, x16;                    \
+        subs    x3, x3, x16;                    \
+        eor     x4, x4, x16;                    \
+        sbc     x4, x4, x16;                    \
+        eor     x5, x5, x0;                     \
+        subs    x5, x5, x0;                     \
+        eor     x6, x6, x0;                     \
+        sbc     x6, x6, x0;                     \
+        eor     x16, x0, x16;                   \
+        adds    x11, x11, x9;                   \
+        adcs    x12, x12, x10;                  \
+        adcs    x13, x13, xzr;                  \
+        adc     x14, x14, xzr;                  \
+        mul     x2, x3, x5;                     \
+        umulh   x0, x3, x5;                     \
+        mul     x15, x4, x6;                    \
+        umulh   x1, x4, x6;                     \
+        subs    x4, x4, x3;                     \
+        cneg    x4, x4, cc;                     \
+        csetm   x9, cc;                         \
+        adds    x15, x15, x0;                   \
+        adc     x1, x1, xzr;                    \
+        subs    x6, x5, x6;                     \
+        cneg    x6, x6, cc;                     \
+        cinv    x9, x9, cc;                     \
+        mul     x5, x4, x6;                     \
+        umulh   x6, x4, x6;                     \
+        adds    x0, x2, x15;                    \
+        adcs    x15, x15, x1;                   \
+        adc     x1, x1, xzr;                    \
+        cmn     x9, #0x1;                       \
+        eor     x5, x5, x9;                     \
+        adcs    x0, x5, x0;                     \
+        eor     x6, x6, x9;                     \
+        adcs    x15, x6, x15;                   \
+        adc     x1, x1, x9;                     \
+        adds    x9, x11, x7;                    \
+        adcs    x10, x12, x8;                   \
+        adcs    x11, x13, x11;                  \
+        adcs    x12, x14, x12;                  \
+        adcs    x13, x13, xzr;                  \
+        adc     x14, x14, xzr;                  \
+        cmn     x16, #0x1;                      \
+        eor     x2, x2, x16;                    \
+        adcs    x9, x2, x9;                     \
+        eor     x0, x0, x16;                    \
+        adcs    x10, x0, x10;                   \
+        eor     x15, x15, x16;                  \
+        adcs    x11, x15, x11;                  \
+        eor     x1, x1, x16;                    \
+        adcs    x12, x1, x12;                   \
+        adcs    x13, x13, x16;                  \
+        adc     x14, x14, x16;                  \
+        mov     x3, #0x26;                      \
+        and     x5, x11, #0xffffffff;           \
+        lsr     x4, x11, #32;                   \
+        mul     x5, x3, x5;                     \
+        mul     x4, x3, x4;                     \
+        adds    x7, x7, x5;                     \
+        and     x5, x12, #0xffffffff;           \
+        lsr     x12, x12, #32;                  \
+        mul     x5, x3, x5;                     \
+        mul     x12, x3, x12;                   \
+        adcs    x8, x8, x5;                     \
+        and     x5, x13, #0xffffffff;           \
+        lsr     x13, x13, #32;                  \
+        mul     x5, x3, x5;                     \
+        mul     x13, x3, x13;                   \
+        adcs    x9, x9, x5;                     \
+        and     x5, x14, #0xffffffff;           \
+        lsr     x14, x14, #32;                  \
+        mul     x5, x3, x5;                     \
+        mul     x14, x3, x14;                   \
+        adcs    x10, x10, x5;                   \
+        cset    x11, cs;                        \
+        lsl     x5, x4, #32;                    \
+        adds    x7, x7, x5;                     \
+        extr    x5, x12, x4, #32;               \
+        adcs    x8, x8, x5;                     \
+        extr    x5, x13, x12, #32;              \
+        adcs    x9, x9, x5;                     \
+        extr    x5, x14, x13, #32;              \
+        adcs    x10, x10, x5;                   \
+        lsr     x5, x14, #32;                   \
+        adc     x11, x11, x5;                   \
+        cmn     x10, x10;                       \
+        orr     x10, x10, #0x8000000000000000;  \
+        adc     x0, x11, x11;                   \
+        mov     x3, #0x13;                      \
+        madd    x5, x3, x0, x3;                 \
+        adds    x7, x7, x5;                     \
+        adcs    x8, x8, xzr;                    \
+        adcs    x9, x9, xzr;                    \
+        adcs    x10, x10, xzr;                  \
+        csel    x3, x3, xzr, cc;                \
+        subs    x7, x7, x3;                     \
+        sbcs    x8, x8, xzr;                    \
+        sbcs    x9, x9, xzr;                    \
+        sbc     x10, x10, xzr;                  \
+        and     x10, x10, #0x7fffffffffffffff;  \
+        stp     x7, x8, [p0];                   \
+        stp     x9, x10, [p0+16]
+
+#define sqr_p25519(p0,p1)                       \
+        ldp     x6, x7, [p1];                   \
+        ldp     x10, x11, [p1+16];              \
+        mul     x4, x6, x10;                    \
+        mul     x9, x7, x11;                    \
+        umulh   x12, x6, x10;                   \
+        subs    x13, x6, x7;                    \
+        cneg    x13, x13, cc;                   \
+        csetm   x3, cc;                         \
+        subs    x2, x11, x10;                   \
+        cneg    x2, x2, cc;                     \
+        mul     x8, x13, x2;                    \
+        umulh   x2, x13, x2;                    \
+        cinv    x3, x3, cc;                     \
+        eor     x8, x8, x3;                     \
+        eor     x2, x2, x3;                     \
+        adds    x5, x4, x12;                    \
+        adc     x12, x12, xzr;                  \
+        umulh   x13, x7, x11;                   \
+        adds    x5, x5, x9;                     \
+        adcs    x12, x12, x13;                  \
+        adc     x13, x13, xzr;                  \
+        adds    x12, x12, x9;                   \
+        adc     x13, x13, xzr;                  \
+        cmn     x3, #0x1;                       \
+        adcs    x5, x5, x8;                     \
+        adcs    x12, x12, x2;                   \
+        adc     x13, x13, x3;                   \
+        adds    x4, x4, x4;                     \
+        adcs    x5, x5, x5;                     \
+        adcs    x12, x12, x12;                  \
+        adcs    x13, x13, x13;                  \
+        adc     x14, xzr, xzr;                  \
+        mul     x2, x6, x6;                     \
+        mul     x8, x7, x7;                     \
+        mul     x15, x6, x7;                    \
+        umulh   x3, x6, x6;                     \
+        umulh   x9, x7, x7;                     \
+        umulh   x16, x6, x7;                    \
+        adds    x3, x3, x15;                    \
+        adcs    x8, x8, x16;                    \
+        adc     x9, x9, xzr;                    \
+        adds    x3, x3, x15;                    \
+        adcs    x8, x8, x16;                    \
+        adc     x9, x9, xzr;                    \
+        adds    x4, x4, x8;                     \
+        adcs    x5, x5, x9;                     \
+        adcs    x12, x12, xzr;                  \
+        adcs    x13, x13, xzr;                  \
+        adc     x14, x14, xzr;                  \
+        mul     x6, x10, x10;                   \
+        mul     x8, x11, x11;                   \
+        mul     x15, x10, x11;                  \
+        umulh   x7, x10, x10;                   \
+        umulh   x9, x11, x11;                   \
+        umulh   x16, x10, x11;                  \
+        adds    x7, x7, x15;                    \
+        adcs    x8, x8, x16;                    \
+        adc     x9, x9, xzr;                    \
+        adds    x7, x7, x15;                    \
+        adcs    x8, x8, x16;                    \
+        adc     x9, x9, xzr;                    \
+        adds    x6, x6, x12;                    \
+        adcs    x7, x7, x13;                    \
+        adcs    x8, x8, x14;                    \
+        adc     x9, x9, xzr;                    \
+        mov     x10, #0x26;                     \
+        and     x11, x6, #0xffffffff;           \
+        lsr     x12, x6, #32;                   \
+        mul     x11, x10, x11;                  \
+        mul     x12, x10, x12;                  \
+        adds    x2, x2, x11;                    \
+        and     x11, x7, #0xffffffff;           \
+        lsr     x7, x7, #32;                    \
+        mul     x11, x10, x11;                  \
+        mul     x7, x10, x7;                    \
+        adcs    x3, x3, x11;                    \
+        and     x11, x8, #0xffffffff;           \
+        lsr     x8, x8, #32;                    \
+        mul     x11, x10, x11;                  \
+        mul     x8, x10, x8;                    \
+        adcs    x4, x4, x11;                    \
+        and     x11, x9, #0xffffffff;           \
+        lsr     x9, x9, #32;                    \
+        mul     x11, x10, x11;                  \
+        mul     x9, x10, x9;                    \
+        adcs    x5, x5, x11;                    \
+        cset    x6, cs;                         \
+        lsl     x11, x12, #32;                  \
+        adds    x2, x2, x11;                    \
+        extr    x11, x7, x12, #32;              \
+        adcs    x3, x3, x11;                    \
+        extr    x11, x8, x7, #32;               \
+        adcs    x4, x4, x11;                    \
+        extr    x11, x9, x8, #32;               \
+        adcs    x5, x5, x11;                    \
+        lsr     x11, x9, #32;                   \
+        adc     x6, x6, x11;                    \
+        cmn     x5, x5;                         \
+        orr     x5, x5, #0x8000000000000000;    \
+        adc     x13, x6, x6;                    \
+        mov     x10, #0x13;                     \
+        madd    x11, x10, x13, x10;             \
+        adds    x2, x2, x11;                    \
+        adcs    x3, x3, xzr;                    \
+        adcs    x4, x4, xzr;                    \
+        adcs    x5, x5, xzr;                    \
+        csel    x10, x10, xzr, cc;              \
+        subs    x2, x2, x10;                    \
+        sbcs    x3, x3, xzr;                    \
+        sbcs    x4, x4, xzr;                    \
+        sbc     x5, x5, xzr;                    \
+        and     x5, x5, #0x7fffffffffffffff;    \
+        stp     x2, x3, [p0];                   \
+        stp     x4, x5, [p0+16]
+
+// Multiplication just giving a 5-digit result (actually < 39 * 2^256)
+// by not doing anything beyond the first stage of reduction
+
+#define mul_5(p0,p1,p2)                         \
+        ldp     x3, x4, [p1];                   \
+        ldp     x5, x6, [p2];                   \
+        mul     x7, x3, x5;                     \
+        umulh   x8, x3, x5;                     \
+        mul     x9, x4, x6;                     \
+        umulh   x10, x4, x6;                    \
+        subs    x4, x4, x3;                     \
+        cneg    x4, x4, cc;                     \
+        csetm   x16, cc;                        \
+        adds    x9, x9, x8;                     \
+        adc     x10, x10, xzr;                  \
+        subs    x3, x5, x6;                     \
+        cneg    x3, x3, cc;                     \
+        cinv    x16, x16, cc;                   \
+        mul     x15, x4, x3;                    \
+        umulh   x3, x4, x3;                     \
+        adds    x8, x7, x9;                     \
+        adcs    x9, x9, x10;                    \
+        adc     x10, x10, xzr;                  \
+        cmn     x16, #0x1;                      \
+        eor     x15, x15, x16;                  \
+        adcs    x8, x15, x8;                    \
+        eor     x3, x3, x16;                    \
+        adcs    x9, x3, x9;                     \
+        adc     x10, x10, x16;                  \
+        ldp     x3, x4, [p1+16];                \
+        ldp     x5, x6, [p2+16];                \
+        mul     x11, x3, x5;                    \
+        umulh   x12, x3, x5;                    \
+        mul     x13, x4, x6;                    \
+        umulh   x14, x4, x6;                    \
+        subs    x4, x4, x3;                     \
+        cneg    x4, x4, cc;                     \
+        csetm   x16, cc;                        \
+        adds    x13, x13, x12;                  \
+        adc     x14, x14, xzr;                  \
+        subs    x3, x5, x6;                     \
+        cneg    x3, x3, cc;                     \
+        cinv    x16, x16, cc;                   \
+        mul     x15, x4, x3;                    \
+        umulh   x3, x4, x3;                     \
+        adds    x12, x11, x13;                  \
+        adcs    x13, x13, x14;                  \
+        adc     x14, x14, xzr;                  \
+        cmn     x16, #0x1;                      \
+        eor     x15, x15, x16;                  \
+        adcs    x12, x15, x12;                  \
+        eor     x3, x3, x16;                    \
+        adcs    x13, x3, x13;                   \
+        adc     x14, x14, x16;                  \
+        ldp     x3, x4, [p1+16];                \
+        ldp     x15, x16, [p1];                 \
+        subs    x3, x3, x15;                    \
+        sbcs    x4, x4, x16;                    \
+        csetm   x16, cc;                        \
+        ldp     x15, x0, [p2];                  \
+        subs    x5, x15, x5;                    \
+        sbcs    x6, x0, x6;                     \
+        csetm   x0, cc;                         \
+        eor     x3, x3, x16;                    \
+        subs    x3, x3, x16;                    \
+        eor     x4, x4, x16;                    \
+        sbc     x4, x4, x16;                    \
+        eor     x5, x5, x0;                     \
+        subs    x5, x5, x0;                     \
+        eor     x6, x6, x0;                     \
+        sbc     x6, x6, x0;                     \
+        eor     x16, x0, x16;                   \
+        adds    x11, x11, x9;                   \
+        adcs    x12, x12, x10;                  \
+        adcs    x13, x13, xzr;                  \
+        adc     x14, x14, xzr;                  \
+        mul     x2, x3, x5;                     \
+        umulh   x0, x3, x5;                     \
+        mul     x15, x4, x6;                    \
+        umulh   x1, x4, x6;                     \
+        subs    x4, x4, x3;                     \
+        cneg    x4, x4, cc;                     \
+        csetm   x9, cc;                         \
+        adds    x15, x15, x0;                   \
+        adc     x1, x1, xzr;                    \
+        subs    x6, x5, x6;                     \
+        cneg    x6, x6, cc;                     \
+        cinv    x9, x9, cc;                     \
+        mul     x5, x4, x6;                     \
+        umulh   x6, x4, x6;                     \
+        adds    x0, x2, x15;                    \
+        adcs    x15, x15, x1;                   \
+        adc     x1, x1, xzr;                    \
+        cmn     x9, #0x1;                       \
+        eor     x5, x5, x9;                     \
+        adcs    x0, x5, x0;                     \
+        eor     x6, x6, x9;                     \
+        adcs    x15, x6, x15;                   \
+        adc     x1, x1, x9;                     \
+        adds    x9, x11, x7;                    \
+        adcs    x10, x12, x8;                   \
+        adcs    x11, x13, x11;                  \
+        adcs    x12, x14, x12;                  \
+        adcs    x13, x13, xzr;                  \
+        adc     x14, x14, xzr;                  \
+        cmn     x16, #0x1;                      \
+        eor     x2, x2, x16;                    \
+        adcs    x9, x2, x9;                     \
+        eor     x0, x0, x16;                    \
+        adcs    x10, x0, x10;                   \
+        eor     x15, x15, x16;                  \
+        adcs    x11, x15, x11;                  \
+        eor     x1, x1, x16;                    \
+        adcs    x12, x1, x12;                   \
+        adcs    x13, x13, x16;                  \
+        adc     x14, x14, x16;                  \
+        mov     x3, #0x26;                      \
+        and     x5, x11, #0xffffffff;           \
+        lsr     x4, x11, #32;                   \
+        mul     x5, x3, x5;                     \
+        mul     x4, x3, x4;                     \
+        adds    x7, x7, x5;                     \
+        and     x5, x12, #0xffffffff;           \
+        lsr     x12, x12, #32;                  \
+        mul     x5, x3, x5;                     \
+        mul     x12, x3, x12;                   \
+        adcs    x8, x8, x5;                     \
+        and     x5, x13, #0xffffffff;           \
+        lsr     x13, x13, #32;                  \
+        mul     x5, x3, x5;                     \
+        mul     x13, x3, x13;                   \
+        adcs    x9, x9, x5;                     \
+        and     x5, x14, #0xffffffff;           \
+        lsr     x14, x14, #32;                  \
+        mul     x5, x3, x5;                     \
+        mul     x14, x3, x14;                   \
+        adcs    x10, x10, x5;                   \
+        cset    x11, cs;                        \
+        lsl     x5, x4, #32;                    \
+        adds    x7, x7, x5;                     \
+        extr    x5, x12, x4, #32;               \
+        adcs    x8, x8, x5;                     \
+        extr    x5, x13, x12, #32;              \
+        adcs    x9, x9, x5;                     \
+        extr    x5, x14, x13, #32;              \
+        adcs    x10, x10, x5;                   \
+        lsr     x5, x14, #32;                   \
+        adc     x11, x11, x5;                   \
+        stp     x7, x8, [p0];                   \
+        stp     x9, x10, [p0+16];               \
+        str     x11, [p0+32]
+
+// Squaring just giving a result < 2 * p_25519, which is done by
+// basically skipping the +1 in the quotient estimate and the final
+// optional correction.
+
+#define sqr_4(p0,p1)                            \
+        ldp     x6, x7, [p1];                   \
+        ldp     x10, x11, [p1+16];              \
+        mul     x4, x6, x10;                    \
+        mul     x9, x7, x11;                    \
+        umulh   x12, x6, x10;                   \
+        subs    x13, x6, x7;                    \
+        cneg    x13, x13, cc;                   \
+        csetm   x3, cc;                         \
+        subs    x2, x11, x10;                   \
+        cneg    x2, x2, cc;                     \
+        mul     x8, x13, x2;                    \
+        umulh   x2, x13, x2;                    \
+        cinv    x3, x3, cc;                     \
+        eor     x8, x8, x3;                     \
+        eor     x2, x2, x3;                     \
+        adds    x5, x4, x12;                    \
+        adc     x12, x12, xzr;                  \
+        umulh   x13, x7, x11;                   \
+        adds    x5, x5, x9;                     \
+        adcs    x12, x12, x13;                  \
+        adc     x13, x13, xzr;                  \
+        adds    x12, x12, x9;                   \
+        adc     x13, x13, xzr;                  \
+        cmn     x3, #0x1;                       \
+        adcs    x5, x5, x8;                     \
+        adcs    x12, x12, x2;                   \
+        adc     x13, x13, x3;                   \
+        adds    x4, x4, x4;                     \
+        adcs    x5, x5, x5;                     \
+        adcs    x12, x12, x12;                  \
+        adcs    x13, x13, x13;                  \
+        adc     x14, xzr, xzr;                  \
+        mul     x2, x6, x6;                     \
+        mul     x8, x7, x7;                     \
+        mul     x15, x6, x7;                    \
+        umulh   x3, x6, x6;                     \
+        umulh   x9, x7, x7;                     \
+        umulh   x16, x6, x7;                    \
+        adds    x3, x3, x15;                    \
+        adcs    x8, x8, x16;                    \
+        adc     x9, x9, xzr;                    \
+        adds    x3, x3, x15;                    \
+        adcs    x8, x8, x16;                    \
+        adc     x9, x9, xzr;                    \
+        adds    x4, x4, x8;                     \
+        adcs    x5, x5, x9;                     \
+        adcs    x12, x12, xzr;                  \
+        adcs    x13, x13, xzr;                  \
+        adc     x14, x14, xzr;                  \
+        mul     x6, x10, x10;                   \
+        mul     x8, x11, x11;                   \
+        mul     x15, x10, x11;                  \
+        umulh   x7, x10, x10;                   \
+        umulh   x9, x11, x11;                   \
+        umulh   x16, x10, x11;                  \
+        adds    x7, x7, x15;                    \
+        adcs    x8, x8, x16;                    \
+        adc     x9, x9, xzr;                    \
+        adds    x7, x7, x15;                    \
+        adcs    x8, x8, x16;                    \
+        adc     x9, x9, xzr;                    \
+        adds    x6, x6, x12;                    \
+        adcs    x7, x7, x13;                    \
+        adcs    x8, x8, x14;                    \
+        adc     x9, x9, xzr;                    \
+        mov     x10, #0x26;                     \
+        and     x11, x6, #0xffffffff;           \
+        lsr     x12, x6, #32;                   \
+        mul     x11, x10, x11;                  \
+        mul     x12, x10, x12;                  \
+        adds    x2, x2, x11;                    \
+        and     x11, x7, #0xffffffff;           \
+        lsr     x7, x7, #32;                    \
+        mul     x11, x10, x11;                  \
+        mul     x7, x10, x7;                    \
+        adcs    x3, x3, x11;                    \
+        and     x11, x8, #0xffffffff;           \
+        lsr     x8, x8, #32;                    \
+        mul     x11, x10, x11;                  \
+        mul     x8, x10, x8;                    \
+        adcs    x4, x4, x11;                    \
+        and     x11, x9, #0xffffffff;           \
+        lsr     x9, x9, #32;                    \
+        mul     x11, x10, x11;                  \
+        mul     x9, x10, x9;                    \
+        adcs    x5, x5, x11;                    \
+        cset    x6, cs;                         \
+        lsl     x11, x12, #32;                  \
+        adds    x2, x2, x11;                    \
+        extr    x11, x7, x12, #32;              \
+        adcs    x3, x3, x11;                    \
+        extr    x11, x8, x7, #32;               \
+        adcs    x4, x4, x11;                    \
+        extr    x11, x9, x8, #32;               \
+        adcs    x5, x5, x11;                    \
+        lsr     x11, x9, #32;                   \
+        adc     x6, x6, x11;                    \
+        cmn     x5, x5;                         \
+        bic     x5, x5, #0x8000000000000000;    \
+        adc     x13, x6, x6;                    \
+        mov     x10, #0x13;                     \
+        mul     x11, x13, x10;                  \
+        adds    x2, x2, x11;                    \
+        adcs    x3, x3, xzr;                    \
+        adcs    x4, x4, xzr;                    \
+        adc     x5, x5, xzr;                    \
+        stp     x2, x3, [p0];                   \
+        stp     x4, x5, [p0+16]
+
+// Plain 4-digit add without any normalization
+// With inputs < p_25519 (indeed < 2^255) it still gives a 4-digit result
+
+#define add_4(p0,p1,p2)                         \
+        ldp     x0, x1, [p1];                   \
+        ldp     x4, x5, [p2];                   \
+        adds    x0, x0, x4;                     \
+        adcs    x1, x1, x5;                     \
+        ldp     x2, x3, [p1+16];                \
+        ldp     x6, x7, [p2+16];                \
+        adcs    x2, x2, x6;                     \
+        adcs    x3, x3, x7;                     \
+        stp     x0, x1, [p0];                   \
+        stp     x2, x3, [p0+16]
+
+// Add 5-digit inputs and normalize to 4 digits
+
+#define add5_4(p0,p1,p2)                        \
+        ldp     x0, x1, [p1];                   \
+        ldp     x4, x5, [p2];                   \
+        adds    x0, x0, x4;                     \
+        adcs    x1, x1, x5;                     \
+        ldp     x2, x3, [p1+16];                \
+        ldp     x6, x7, [p2+16];                \
+        adcs    x2, x2, x6;                     \
+        adcs    x3, x3, x7;                     \
+        ldr     x4, [p1+32];                    \
+        ldr     x5, [p2+32];                    \
+        adc     x4, x4, x5;                     \
+        cmn     x3, x3;                         \
+        bic     x3, x3, #0x8000000000000000;    \
+        adc     x8, x4, x4;                     \
+        mov     x7, #19;                        \
+        mul     x11, x7, x8;                    \
+        adds    x0, x0, x11;                    \
+        adcs    x1, x1, xzr;                    \
+        adcs    x2, x2, xzr;                    \
+        adc     x3, x3, xzr;                    \
+        stp     x0, x1, [p0];                   \
+        stp     x2, x3, [p0+16]
+
+// Subtraction of a pair of numbers < p_25519 just sufficient
+// to give a 4-digit result. It actually always does (x - z) + (2^255-19)
+// which in turn is done by (x - z) - (2^255+19) discarding the 2^256
+// implicitly
+
+#define sub_4(p0,p1,p2)                         \
+        ldp     x5, x6, [p1];                   \
+        ldp     x4, x3, [p2];                   \
+        subs    x5, x5, x4;                     \
+        sbcs    x6, x6, x3;                     \
+        ldp     x7, x8, [p1+16];                \
+        ldp     x4, x3, [p2+16];                \
+        sbcs    x7, x7, x4;                     \
+        sbcs    x8, x8, x3;                     \
+        mov     x3, #19;                        \
+        subs    x5, x5, x3;                     \
+        sbcs    x6, x6, xzr;                    \
+        sbcs    x7, x7, xzr;                    \
+        mov     x4, #0x8000000000000000;        \
+        sbc     x8, x8, x4;                     \
+        stp     x5, x6, [p0];                   \
+        stp     x7, x8, [p0+16]
+
+// Modular subtraction with double modulus 2 * p_25519 = 2^256 - 38
+
+#define sub_twice4(p0,p1,p2)                    \
+        ldp     x5, x6, [p1];                   \
+        ldp     x4, x3, [p2];                   \
+        subs    x5, x5, x4;                     \
+        sbcs    x6, x6, x3;                     \
+        ldp     x7, x8, [p1+16];                \
+        ldp     x4, x3, [p2+16];                \
+        sbcs    x7, x7, x4;                     \
+        sbcs    x8, x8, x3;                     \
+        mov     x4, #38;                        \
+        csel    x3, x4, xzr, lo;                \
+        subs    x5, x5, x3;                     \
+        sbcs    x6, x6, xzr;                    \
+        sbcs    x7, x7, xzr;                    \
+        sbc     x8, x8, xzr;                    \
+        stp     x5, x6, [p0];                   \
+        stp     x7, x8, [p0+16]
+
+// 5-digit subtraction with upward bias to make it positive, adding
+// 1000 * (2^255 - 19) = 2^256 * 500 - 19000, then normalizing to 4 digits
+
+#define sub5_4(p0,p1,p2)                        \
+        ldp     x0, x1, [p1];                   \
+        ldp     x4, x5, [p2];                   \
+        subs    x0, x0, x4;                     \
+        sbcs    x1, x1, x5;                     \
+        ldp     x2, x3, [p1+16];                \
+        ldp     x6, x7, [p2+16];                \
+        sbcs    x2, x2, x6;                     \
+        sbcs    x3, x3, x7;                     \
+        ldr     x4, [p1+32];                    \
+        ldr     x5, [p2+32];                    \
+        sbc     x4, x4, x5;                     \
+        mov     x7, -19000;                     \
+        adds x0, x0, x7;                        \
+        sbcs    x1, x1, xzr;                    \
+        sbcs    x2, x2, xzr;                    \
+        sbcs    x3, x3, xzr;                    \
+        mov     x7, 499;                        \
+        adc     x4, x4, x7;                     \
+        cmn     x3, x3;                         \
+        bic     x3, x3, #0x8000000000000000;    \
+        adc     x8, x4, x4;                     \
+        mov     x7, #19;                        \
+        mul     x11, x7, x8;                    \
+        adds    x0, x0, x11;                    \
+        adcs    x1, x1, xzr;                    \
+        adcs    x2, x2, xzr;                    \
+        adc     x3, x3, xzr;                    \
+        stp     x0, x1, [p0];                   \
+        stp     x2, x3, [p0+16]
+
+// Combined z = c * x + y with reduction only < 2 * p_25519
+// where c is initially in the X1 register. It is assumed
+// that 19 * (c * x + y) < 2^60 * 2^256 so we don't need a
+// high mul in the final part.
+
+#define cmadd_4(p0,p2,p3)                       \
+        ldp     x7, x8, [p2];                   \
+        ldp     x9, x10, [p2+16];               \
+        mul     x3, x1, x7;                     \
+        mul     x4, x1, x8;                     \
+        mul     x5, x1, x9;                     \
+        mul     x6, x1, x10;                    \
+        umulh   x7, x1, x7;                     \
+        umulh   x8, x1, x8;                     \
+        umulh   x9, x1, x9;                     \
+        umulh   x10, x1, x10;                   \
+        adds    x4, x4, x7;                     \
+        adcs    x5, x5, x8;                     \
+        adcs    x6, x6, x9;                     \
+        adc     x10, x10, xzr;                  \
+        ldp     x7, x8, [p3];                   \
+        adds    x3, x3, x7;                     \
+        adcs    x4, x4, x8;                     \
+        ldp     x7, x8, [p3+16];                \
+        adcs    x5, x5, x7;                     \
+        adcs    x6, x6, x8;                     \
+        adc     x10, x10, xzr;                  \
+        cmn     x6, x6;                         \
+        bic     x6, x6, #0x8000000000000000;    \
+        adc     x8, x10, x10;                   \
+        mov     x9, #19;                        \
+        mul     x7, x8, x9;                     \
+        adds    x3, x3, x7;                     \
+        adcs    x4, x4, xzr;                    \
+        adcs    x5, x5, xzr;                    \
+        adc     x6, x6, xzr;                    \
+        stp     x3, x4, [p0];                   \
+        stp     x5, x6, [p0+16]
+
+// Multiplex: z := if NZ then x else y
+
+#define mux_4(p0,p1,p2)                         \
+        ldp     x0, x1, [p1];                   \
+        ldp     x2, x3, [p2];                   \
+        csel    x0, x0, x2, ne;                 \
+        csel    x1, x1, x3, ne;                 \
+        stp     x0, x1, [p0];                   \
+        ldp     x0, x1, [p1+16];                \
+        ldp     x2, x3, [p2+16];                \
+        csel    x0, x0, x2, ne;                 \
+        csel    x1, x1, x3, ne;                 \
+        stp     x0, x1, [p0+16]
+
+S2N_BN_SYMBOL(curve25519_x25519):
+
+// Save regs and make room for temporaries
+
+        stp     x19, x20, [sp, -16]!
+        stp     x21, x22, [sp, -16]!
+        stp     x23, x24, [sp, -16]!
+        sub     sp, sp, #NSPACE
+
+// Move the output pointer to a stable place
+
+        mov     res, x0
+
+// Copy the inputs to the local variables while mangling them:
+//
+//  - The scalar gets turned into 01xxx...xxx000 by tweaking bits.
+//    Actually the top zero doesn't matter since the loop below
+//    never looks at it, so we don't literally modify that.
+//
+//  - The point x coord is reduced mod 2^255 *then* mod 2^255-19
+
+        ldp     x10, x11, [x1]
+        bic     x10, x10, #7
+        stp     x10, x11, [scalar]
+        ldp     x12, x13, [x1, #16]
+        orr     x13, x13, #0x4000000000000000
+        stp     x12, x13, [scalar+16]
+
+        ldp     x10, x11, [x2]
+        subs    x6, x10, #-19
+        adcs    x7, x11, xzr
+        ldp     x12, x13, [x2, #16]
+        and     x13, x13, #0x7fffffffffffffff
+        adcs    x8, x12, xzr
+        mov     x9, #0x7fffffffffffffff
+        sbcs    x9, x13, x9
+
+        csel    x10, x6, x10, cs
+        csel    x11, x7, x11, cs
+        csel    x12, x8, x12, cs
+        csel    x13, x9, x13, cs
+
+        stp     x10, x11, [pointx]
+        stp     x12, x13, [pointx+16]
+
+// Initialize (xn,zn) = (1,0) and (xm,zm) = (x,1) with swap = 0
+// We use the fact that the point x coordinate is still in registers
+
+        mov     x2, #1
+        stp     x2, xzr, [xn]
+        stp     xzr, xzr, [xn+16]
+        stp     xzr, xzr, [zn]
+        stp     xzr, xzr, [zn+16]
+        stp     x10, x11, [xm]
+        stp     x12, x13, [xm+16]
+        stp     x2, xzr, [zm]
+        stp     xzr, xzr, [zm+16]
+        mov     swap, xzr
+
+// The outer loop over scalar bits from i = 254, ..., i = 0 (inclusive)
+// This starts at 254, and so implicitly masks bit 255 of the scalar.
+
+        mov     i, #254
+
+scalarloop:
+
+// sm = xm + zm; sn = xn + zn; dm = xm - zm; dn = xn - zn
+// The adds don't need any normalization as they're fed to muls
+// Just make sure the subs fit in 4 digits
+
+        sub_4(dm, xm, zm)
+        add_4(sn, xn, zn)
+        sub_4(dn, xn, zn)
+        add_4(sm, xm, zm)
+
+// ADDING: dmsn = dm * sn; dnsm = sm * dn
+// DOUBLING: mux d = xt - zt and s = xt + zt for appropriate choice of (xt,zt)
+
+        mul_5(dmsn,sn,dm)
+
+        lsr     x0, i, #6
+        ldr     x2, [sp, x0, lsl #3]    // Exploiting scalar = sp exactly
+        lsr     x2, x2, i
+        and     x2, x2, #1
+
+        cmp     swap, x2
+        mov     swap, x2
+
+        mux_4(d,dm,dn)
+        mux_4(s,sm,sn)
+
+        mul_5(dnsm,sm,dn)
+
+// DOUBLING: d = (xt - zt)^2 normalized only to 4 digits
+
+        sqr_4(d,d)
+
+// ADDING: dpro = (dmsn - dnsm)^2, spro = (dmsn + dnsm)^2
+// DOUBLING: s = (xt + zt)^2, normalized only to 4 digits
+
+        sub5_4(dpro,dmsn,dnsm)
+        sqr_4(s,s)
+        add5_4(spro,dmsn,dnsm)
+        sqr_4(dpro,dpro)
+
+// DOUBLING: p = 4 * xt * zt = s - d
+
+        sub_twice4(p,s,d)
+
+// ADDING: xm' = (dmsn + dnsm)^2
+
+        sqr_p25519(xm,spro)
+
+// DOUBLING: e = 121666 * p + d
+
+        mov     x1, 0xdb42
+        orr     x1, x1, 0x10000
+        cmadd_4(e,p,d)
+
+// DOUBLING: xn' = (xt + zt)^2 * (xt - zt)^2 = s * d
+
+        mul_p25519(xn,s,d)
+
+// ADDING: zm' = x * (dmsn - dnsm)^2
+
+        mul_p25519(zm,dpro,pointx)
+
+// DOUBLING: zn' = (4 * xt * zt) * ((xt - zt)^2 + 121666 * (4 * xt * zt))
+//               = p * (d + 121666 * p)
+
+        mul_p25519(zn,p,e)
+
+// Loop down as far as 0 (inclusive)
+
+        subs    i, i, #1
+        bcs     scalarloop
+
+// Since the scalar was forced to be a multiple of 8, we know it's even.
+// Hence there is no need to multiplex: the projective answer is (xn,zn)
+// and we can ignore (xm,zm); indeed we could have avoided the last three
+// differential additions and just done the doublings.
+// First set up the constant sn = 2^255 - 19 for the modular inverse.
+
+        mov     x0, #-19
+        mov     x1, #-1
+        mov     x2, #0x7fffffffffffffff
+        stp     x0, x1, [sn]
+        stp     x1, x2, [sn+16]
+
+// Prepare to call the modular inverse function to get zm = 1/zn
+
+        mov     x0, #4
+        add     x1, zm
+        add     x2, zn
+        add     x3, sn
+        add     x4, p
+
+// Inline copy of bignum_modinv, identical except for stripping out the
+// prologue and epilogue saving and restoring registers and the initial
+// test for k = 0 (which is trivially false here since k = 4). For more
+// details and explanations see "arm/generic/bignum_modinv.S".
+
+        lsl     x10, x0, #3
+        add     x21, x4, x10
+        add     x22, x21, x10
+        mov     x10, xzr
+copyloop:
+        ldr     x11, [x2, x10, lsl #3]
+        ldr     x12, [x3, x10, lsl #3]
+        str     x11, [x21, x10, lsl #3]
+        str     x12, [x22, x10, lsl #3]
+        str     x12, [x4, x10, lsl #3]
+        str     xzr, [x1, x10, lsl #3]
+        add     x10, x10, #0x1
+        cmp     x10, x0
+        b.cc    copyloop
+        ldr     x11, [x4]
+        sub     x12, x11, #0x1
+        str     x12, [x4]
+        lsl     x20, x11, #2
+        sub     x20, x11, x20
+        eor     x20, x20, #0x2
+        mov     x12, #0x1
+        madd    x12, x11, x20, x12
+        mul     x11, x12, x12
+        madd    x20, x12, x20, x20
+        mul     x12, x11, x11
+        madd    x20, x11, x20, x20
+        mul     x11, x12, x12
+        madd    x20, x12, x20, x20
+        madd    x20, x11, x20, x20
+        lsl     x2, x0, #7
+outerloop:
+        add     x10, x2, #0x3f
+        lsr     x5, x10, #6
+        cmp     x5, x0
+        csel    x5, x0, x5, cs
+        mov     x13, xzr
+        mov     x15, xzr
+        mov     x14, xzr
+        mov     x16, xzr
+        mov     x19, xzr
+        mov     x10, xzr
+toploop:
+        ldr     x11, [x21, x10, lsl #3]
+        ldr     x12, [x22, x10, lsl #3]
+        orr     x17, x11, x12
+        cmp     x17, xzr
+        and     x17, x19, x13
+        csel    x15, x17, x15, ne
+        and     x17, x19, x14
+        csel    x16, x17, x16, ne
+        csel    x13, x11, x13, ne
+        csel    x14, x12, x14, ne
+        csetm   x19, ne
+        add     x10, x10, #0x1
+        cmp     x10, x5
+        b.cc    toploop
+        orr     x11, x13, x14
+        clz     x12, x11
+        negs    x17, x12
+        lsl     x13, x13, x12
+        csel    x15, x15, xzr, ne
+        lsl     x14, x14, x12
+        csel    x16, x16, xzr, ne
+        lsr     x15, x15, x17
+        lsr     x16, x16, x17
+        orr     x13, x13, x15
+        orr     x14, x14, x16
+        ldr     x15, [x21]
+        ldr     x16, [x22]
+        mov     x6, #0x1
+        mov     x7, xzr
+        mov     x8, xzr
+        mov     x9, #0x1
+        mov     x10, #0x3a
+        tst     x15, #0x1
+innerloop:
+        csel    x11, x14, xzr, ne
+        csel    x12, x16, xzr, ne
+        csel    x17, x8, xzr, ne
+        csel    x19, x9, xzr, ne
+        ccmp    x13, x14, #0x2, ne
+        sub     x11, x13, x11
+        sub     x12, x15, x12
+        csel    x14, x14, x13, cs
+        cneg    x11, x11, cc
+        csel    x16, x16, x15, cs
+        cneg    x15, x12, cc
+        csel    x8, x8, x6, cs
+        csel    x9, x9, x7, cs
+        tst     x12, #0x2
+        add     x6, x6, x17
+        add     x7, x7, x19
+        lsr     x13, x11, #1
+        lsr     x15, x15, #1
+        add     x8, x8, x8
+        add     x9, x9, x9
+        sub     x10, x10, #0x1
+        cbnz    x10, innerloop
+        mov     x13, xzr
+        mov     x14, xzr
+        mov     x17, xzr
+        mov     x19, xzr
+        mov     x10, xzr
+congloop:
+        ldr     x11, [x4, x10, lsl #3]
+        ldr     x12, [x1, x10, lsl #3]
+        mul     x15, x6, x11
+        mul     x16, x7, x12
+        adds    x15, x15, x13
+        umulh   x13, x6, x11
+        adc     x13, x13, xzr
+        adds    x15, x15, x16
+        extr    x17, x15, x17, #58
+        str     x17, [x4, x10, lsl #3]
+        mov     x17, x15
+        umulh   x15, x7, x12
+        adc     x13, x13, x15
+        mul     x15, x8, x11
+        mul     x16, x9, x12
+        adds    x15, x15, x14
+        umulh   x14, x8, x11
+        adc     x14, x14, xzr
+        adds    x15, x15, x16
+        extr    x19, x15, x19, #58
+        str     x19, [x1, x10, lsl #3]
+        mov     x19, x15
+        umulh   x15, x9, x12
+        adc     x14, x14, x15
+        add     x10, x10, #0x1
+        cmp     x10, x0
+        b.cc    congloop
+        extr    x13, x13, x17, #58
+        extr    x14, x14, x19, #58
+        ldr     x11, [x4]
+        mul     x17, x11, x20
+        ldr     x12, [x3]
+        mul     x15, x17, x12
+        umulh   x16, x17, x12
+        adds    x11, x11, x15
+        mov     x10, #0x1
+        sub     x11, x0, #0x1
+        cbz     x11, wmontend
+wmontloop:
+        ldr     x11, [x3, x10, lsl #3]
+        ldr     x12, [x4, x10, lsl #3]
+        mul     x15, x17, x11
+        adcs    x12, x12, x16
+        umulh   x16, x17, x11
+        adc     x16, x16, xzr
+        adds    x12, x12, x15
+        sub     x15, x10, #0x1
+        str     x12, [x4, x15, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, wmontloop
+wmontend:
+        adcs    x16, x16, x13
+        adc     x13, xzr, xzr
+        sub     x15, x10, #0x1
+        str     x16, [x4, x15, lsl #3]
+        negs    x10, xzr
+wcmploop:
+        ldr     x11, [x4, x10, lsl #3]
+        ldr     x12, [x3, x10, lsl #3]
+        sbcs    xzr, x11, x12
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, wcmploop
+        sbcs    xzr, x13, xzr
+        csetm   x13, cs
+        negs    x10, xzr
+wcorrloop:
+        ldr     x11, [x4, x10, lsl #3]
+        ldr     x12, [x3, x10, lsl #3]
+        and     x12, x12, x13
+        sbcs    x11, x11, x12
+        str     x11, [x4, x10, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, wcorrloop
+        ldr     x11, [x1]
+        mul     x17, x11, x20
+        ldr     x12, [x3]
+        mul     x15, x17, x12
+        umulh   x16, x17, x12
+        adds    x11, x11, x15
+        mov     x10, #0x1
+        sub     x11, x0, #0x1
+        cbz     x11, zmontend
+zmontloop:
+        ldr     x11, [x3, x10, lsl #3]
+        ldr     x12, [x1, x10, lsl #3]
+        mul     x15, x17, x11
+        adcs    x12, x12, x16
+        umulh   x16, x17, x11
+        adc     x16, x16, xzr
+        adds    x12, x12, x15
+        sub     x15, x10, #0x1
+        str     x12, [x1, x15, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, zmontloop
+zmontend:
+        adcs    x16, x16, x14
+        adc     x14, xzr, xzr
+        sub     x15, x10, #0x1
+        str     x16, [x1, x15, lsl #3]
+        negs    x10, xzr
+zcmploop:
+        ldr     x11, [x1, x10, lsl #3]
+        ldr     x12, [x3, x10, lsl #3]
+        sbcs    xzr, x11, x12
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, zcmploop
+        sbcs    xzr, x14, xzr
+        csetm   x14, cs
+        negs    x10, xzr
+zcorrloop:
+        ldr     x11, [x1, x10, lsl #3]
+        ldr     x12, [x3, x10, lsl #3]
+        and     x12, x12, x14
+        sbcs    x11, x11, x12
+        str     x11, [x1, x10, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, zcorrloop
+        mov     x13, xzr
+        mov     x14, xzr
+        mov     x17, xzr
+        mov     x19, xzr
+        mov     x10, xzr
+crossloop:
+        ldr     x11, [x21, x10, lsl #3]
+        ldr     x12, [x22, x10, lsl #3]
+        mul     x15, x6, x11
+        mul     x16, x7, x12
+        adds    x15, x15, x13
+        umulh   x13, x6, x11
+        adc     x13, x13, xzr
+        subs    x15, x15, x16
+        str     x15, [x21, x10, lsl #3]
+        umulh   x15, x7, x12
+        sub     x17, x15, x17
+        sbcs    x13, x13, x17
+        csetm   x17, cc
+        mul     x15, x8, x11
+        mul     x16, x9, x12
+        adds    x15, x15, x14
+        umulh   x14, x8, x11
+        adc     x14, x14, xzr
+        subs    x15, x15, x16
+        str     x15, [x22, x10, lsl #3]
+        umulh   x15, x9, x12
+        sub     x19, x15, x19
+        sbcs    x14, x14, x19
+        csetm   x19, cc
+        add     x10, x10, #0x1
+        cmp     x10, x5
+        b.cc    crossloop
+        cmn     x17, x17
+        ldr     x15, [x21]
+        mov     x10, xzr
+        sub     x6, x5, #0x1
+        cbz     x6, negskip1
+negloop1:
+        add     x11, x10, #0x8
+        ldr     x12, [x21, x11]
+        extr    x15, x12, x15, #58
+        eor     x15, x15, x17
+        adcs    x15, x15, xzr
+        str     x15, [x21, x10]
+        mov     x15, x12
+        add     x10, x10, #0x8
+        sub     x6, x6, #0x1
+        cbnz    x6, negloop1
+negskip1:
+        extr    x15, x13, x15, #58
+        eor     x15, x15, x17
+        adcs    x15, x15, xzr
+        str     x15, [x21, x10]
+        cmn     x19, x19
+        ldr     x15, [x22]
+        mov     x10, xzr
+        sub     x6, x5, #0x1
+        cbz     x6, negskip2
+negloop2:
+        add     x11, x10, #0x8
+        ldr     x12, [x22, x11]
+        extr    x15, x12, x15, #58
+        eor     x15, x15, x19
+        adcs    x15, x15, xzr
+        str     x15, [x22, x10]
+        mov     x15, x12
+        add     x10, x10, #0x8
+        sub     x6, x6, #0x1
+        cbnz    x6, negloop2
+negskip2:
+        extr    x15, x14, x15, #58
+        eor     x15, x15, x19
+        adcs    x15, x15, xzr
+        str     x15, [x22, x10]
+        mov     x10, xzr
+        cmn     x17, x17
+wfliploop:
+        ldr     x11, [x3, x10, lsl #3]
+        ldr     x12, [x4, x10, lsl #3]
+        and     x11, x11, x17
+        eor     x12, x12, x17
+        adcs    x11, x11, x12
+        str     x11, [x4, x10, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, wfliploop
+        mvn     x19, x19
+        mov     x10, xzr
+        cmn     x19, x19
+zfliploop:
+        ldr     x11, [x3, x10, lsl #3]
+        ldr     x12, [x1, x10, lsl #3]
+        and     x11, x11, x19
+        eor     x12, x12, x19
+        adcs    x11, x11, x12
+        str     x11, [x1, x10, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, zfliploop
+        subs    x2, x2, #0x3a
+        b.hi    outerloop
+
+// Since we eventually want to return 0 when the result is the point at
+// infinity, we force xn = 0 whenever zn = 0. This avoids building in a
+// dependency on the behavior of modular inverse in out-of-scope cases.
+
+        ldp     x0, x1, [zn]
+        ldp     x2, x3, [zn+16]
+        orr     x0, x0, x1
+        orr     x2, x2, x3
+        orr     x4, x0, x2
+        cmp     x4, xzr
+        ldp     x0, x1, [xn]
+        csel    x0, x0, xzr, ne
+        csel    x1, x1, xzr, ne
+        ldp     x2, x3, [xn+16]
+        stp     x0, x1, [xn]
+        csel    x2, x2, xzr, ne
+        csel    x3, x3, xzr, ne
+        stp     x2, x3, [xn+16]
+
+// Now the result is xn * (1/zn).
+
+        mul_p25519(resx,xn,zm)
+
+// Restore stack and registers
+
+        add     sp, sp, #NSPACE
+        ldp     x23, x24, [sp], 16
+        ldp     x21, x22, [sp], 16
+        ldp     x19, x20, [sp], 16
+
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack, "", %progbits
+#endif
diff --git a/arm/curve25519/curve25519_x25519_alt.S b/arm/curve25519/curve25519_x25519_alt.S
new file mode 100644
index 000000000..8a0495f97
--- /dev/null
+++ b/arm/curve25519/curve25519_x25519_alt.S
@@ -0,0 +1,1151 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// The x25519 function for curve25519
+// Inputs scalar[4], point[4]; output res[4]
+//
+// extern void curve25519_x25519_alt
+//   (uint64_t res[static 4],uint64_t scalar[static 4],uint64_t point[static 4])
+//
+// Given a scalar n and the X coordinate of an input point P = (X,Y) on
+// curve25519 (Y can live in any extension field of characteristic 2^255-19),
+// this returns the X coordinate of n * P = (X, Y), or 0 when n * P is the
+// point at infinity. Both n and X inputs are first slightly modified/mangled
+// as specified in the relevant RFC (https://www.rfc-editor.org/rfc/rfc7748);
+// in particular the lower three bits of n are set to zero.
+//
+// Standard ARM ABI: X0 = res, X1 = scalar, X2 = point
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(curve25519_x25519_alt)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(curve25519_x25519_alt)
+
+        .text
+        .balign 4
+
+// Size of individual field elements
+
+#define NUMSIZE 32
+
+// Stable homes for the input result argument during the whole body
+// and other variables that are only needed prior to the modular inverse.
+
+#define res x23
+#define i x20
+#define swap x21
+
+// Pointers to result x coord to be written
+
+#define resx res, #0
+
+// Pointer-offset pairs for temporaries on stack with some aliasing.
+// Both dmsn and dnsm need space for >= 5 digits, and we allocate 8
+
+#define scalar sp, #(0*NUMSIZE)
+
+#define pointx sp, #(1*NUMSIZE)
+
+#define zm sp, #(2*NUMSIZE)
+#define sm sp, #(2*NUMSIZE)
+#define dpro sp, #(2*NUMSIZE)
+
+#define sn sp, #(3*NUMSIZE)
+
+#define dm sp, #(4*NUMSIZE)
+
+#define zn sp, #(5*NUMSIZE)
+#define dn sp, #(5*NUMSIZE)
+#define e sp, #(5*NUMSIZE)
+
+#define dmsn sp, #(6*NUMSIZE)
+#define p sp, #(6*NUMSIZE)
+
+#define xm sp, #(8*NUMSIZE)
+#define dnsm sp, #(8*NUMSIZE)
+#define spro sp, #(8*NUMSIZE)
+
+#define xn sp, #(10*NUMSIZE)
+#define s sp, #(10*NUMSIZE)
+
+#define d sp, #(11*NUMSIZE)
+
+// Total size to reserve on the stack
+
+#define NSPACE (12*NUMSIZE)
+
+// Macros wrapping up the basic field operation calls
+// bignum_mul_p25519_alt and bignum_sqr_p25519_alt.
+// These two are only trivially different from pure
+// function calls to those subroutines.
+
+#define mul_p25519(p0,p1,p2)                    \
+        ldp     x3, x4, [p1];                   \
+        ldp     x7, x8, [p2];                   \
+        mul     x12, x3, x7;                    \
+        umulh   x13, x3, x7;                    \
+        mul     x11, x3, x8;                    \
+        umulh   x14, x3, x8;                    \
+        adds    x13, x13, x11;                  \
+        ldp     x9, x10, [p2+16];               \
+        mul     x11, x3, x9;                    \
+        umulh   x15, x3, x9;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x3, x10;                   \
+        umulh   x16, x3, x10;                   \
+        adcs    x15, x15, x11;                  \
+        adc     x16, x16, xzr;                  \
+        ldp     x5, x6, [p1+16];                \
+        mul     x11, x4, x7;                    \
+        adds    x13, x13, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x16, x16, x11;                  \
+        umulh   x3, x4, x10;                    \
+        adc     x3, x3, xzr;                    \
+        umulh   x11, x4, x7;                    \
+        adds    x14, x14, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x15, x15, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x16, x16, x11;                  \
+        adc     x3, x3, xzr;                    \
+        mul     x11, x5, x7;                    \
+        adds    x14, x14, x11;                  \
+        mul     x11, x5, x8;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x5, x9;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x5, x10;                   \
+        adcs    x3, x3, x11;                    \
+        umulh   x4, x5, x10;                    \
+        adc     x4, x4, xzr;                    \
+        umulh   x11, x5, x7;                    \
+        adds    x15, x15, x11;                  \
+        umulh   x11, x5, x8;                    \
+        adcs    x16, x16, x11;                  \
+        umulh   x11, x5, x9;                    \
+        adcs    x3, x3, x11;                    \
+        adc     x4, x4, xzr;                    \
+        mul     x11, x6, x7;                    \
+        adds    x15, x15, x11;                  \
+        mul     x11, x6, x8;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x6, x9;                    \
+        adcs    x3, x3, x11;                    \
+        mul     x11, x6, x10;                   \
+        adcs    x4, x4, x11;                    \
+        umulh   x5, x6, x10;                    \
+        adc     x5, x5, xzr;                    \
+        umulh   x11, x6, x7;                    \
+        adds    x16, x16, x11;                  \
+        umulh   x11, x6, x8;                    \
+        adcs    x3, x3, x11;                    \
+        umulh   x11, x6, x9;                    \
+        adcs    x4, x4, x11;                    \
+        adc     x5, x5, xzr;                    \
+        mov     x7, #38;                        \
+        mul     x11, x7, x16;                   \
+        umulh   x9, x7, x16;                    \
+        adds    x12, x12, x11;                  \
+        mul     x11, x7, x3;                    \
+        umulh   x3, x7, x3;                     \
+        adcs    x13, x13, x11;                  \
+        mul     x11, x7, x4;                    \
+        umulh   x4, x7, x4;                     \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x7, x5;                    \
+        umulh   x5, x7, x5;                     \
+        adcs    x15, x15, x11;                  \
+        cset    x16, hs;                        \
+        adds    x13, x13, x9;                   \
+        adcs    x14, x14, x3;                   \
+        adcs    x15, x15, x4;                   \
+        adc     x16, x16, x5;                   \
+        cmn     x15, x15;                       \
+        orr     x15, x15, #0x8000000000000000;  \
+        adc     x8, x16, x16;                   \
+        mov     x7, #19;                        \
+        madd    x11, x7, x8, x7;                \
+        adds    x12, x12, x11;                  \
+        adcs    x13, x13, xzr;                  \
+        adcs    x14, x14, xzr;                  \
+        adcs    x15, x15, xzr;                  \
+        csel    x7, x7, xzr, lo;                \
+        subs    x12, x12, x7;                   \
+        sbcs    x13, x13, xzr;                  \
+        sbcs    x14, x14, xzr;                  \
+        sbc     x15, x15, xzr;                  \
+        and     x15, x15, #0x7fffffffffffffff;  \
+        stp     x12, x13, [p0];                 \
+        stp     x14, x15, [p0+16]
+
+#define sqr_p25519(p0,p1)                       \
+        ldp     x2, x3, [p1];                   \
+        mul     x9, x2, x3;                     \
+        umulh   x10, x2, x3;                    \
+        ldp     x4, x5, [p1+16];                \
+        mul     x11, x2, x5;                    \
+        umulh   x12, x2, x5;                    \
+        mul     x7, x2, x4;                     \
+        umulh   x6, x2, x4;                     \
+        adds    x10, x10, x7;                   \
+        adcs    x11, x11, x6;                   \
+        mul     x7, x3, x4;                     \
+        umulh   x6, x3, x4;                     \
+        adc     x6, x6, xzr;                    \
+        adds    x11, x11, x7;                   \
+        mul     x13, x4, x5;                    \
+        umulh   x14, x4, x5;                    \
+        adcs    x12, x12, x6;                   \
+        mul     x7, x3, x5;                     \
+        umulh   x6, x3, x5;                     \
+        adc     x6, x6, xzr;                    \
+        adds    x12, x12, x7;                   \
+        adcs    x13, x13, x6;                   \
+        adc     x14, x14, xzr;                  \
+        adds    x9, x9, x9;                     \
+        adcs    x10, x10, x10;                  \
+        adcs    x11, x11, x11;                  \
+        adcs    x12, x12, x12;                  \
+        adcs    x13, x13, x13;                  \
+        adcs    x14, x14, x14;                  \
+        cset    x6, hs;                         \
+        umulh   x7, x2, x2;                     \
+        mul     x8, x2, x2;                     \
+        adds    x9, x9, x7;                     \
+        mul     x7, x3, x3;                     \
+        adcs    x10, x10, x7;                   \
+        umulh   x7, x3, x3;                     \
+        adcs    x11, x11, x7;                   \
+        mul     x7, x4, x4;                     \
+        adcs    x12, x12, x7;                   \
+        umulh   x7, x4, x4;                     \
+        adcs    x13, x13, x7;                   \
+        mul     x7, x5, x5;                     \
+        adcs    x14, x14, x7;                   \
+        umulh   x7, x5, x5;                     \
+        adc     x6, x6, x7;                     \
+        mov     x3, #38;                        \
+        mul     x7, x3, x12;                    \
+        umulh   x4, x3, x12;                    \
+        adds    x8, x8, x7;                     \
+        mul     x7, x3, x13;                    \
+        umulh   x13, x3, x13;                   \
+        adcs    x9, x9, x7;                     \
+        mul     x7, x3, x14;                    \
+        umulh   x14, x3, x14;                   \
+        adcs    x10, x10, x7;                   \
+        mul     x7, x3, x6;                     \
+        umulh   x6, x3, x6;                     \
+        adcs    x11, x11, x7;                   \
+        cset    x12, hs;                        \
+        adds    x9, x9, x4;                     \
+        adcs    x10, x10, x13;                  \
+        adcs    x11, x11, x14;                  \
+        adc     x12, x12, x6;                   \
+        cmn     x11, x11;                       \
+        orr     x11, x11, #0x8000000000000000;  \
+        adc     x2, x12, x12;                   \
+        mov     x3, #19;                        \
+        madd    x7, x3, x2, x3;                 \
+        adds    x8, x8, x7;                     \
+        adcs    x9, x9, xzr;                    \
+        adcs    x10, x10, xzr;                  \
+        adcs    x11, x11, xzr;                  \
+        csel    x3, x3, xzr, lo;                \
+        subs    x8, x8, x3;                     \
+        sbcs    x9, x9, xzr;                    \
+        sbcs    x10, x10, xzr;                  \
+        sbc     x11, x11, xzr;                  \
+        and     x11, x11, #0x7fffffffffffffff;  \
+        stp     x8, x9, [p0];                   \
+        stp     x10, x11, [p0+16]
+
+// Multiplication just giving a 5-digit result (actually < 39 * 2^256)
+// by not doing anything beyond the first stage of reduction
+
+#define mul_5(p0,p1,p2)                         \
+        ldp     x3, x4, [p1];                   \
+        ldp     x7, x8, [p2];                   \
+        mul     x12, x3, x7;                    \
+        umulh   x13, x3, x7;                    \
+        mul     x11, x3, x8;                    \
+        umulh   x14, x3, x8;                    \
+        adds    x13, x13, x11;                  \
+        ldp     x9, x10, [p2+16];               \
+        mul     x11, x3, x9;                    \
+        umulh   x15, x3, x9;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x3, x10;                   \
+        umulh   x16, x3, x10;                   \
+        adcs    x15, x15, x11;                  \
+        adc     x16, x16, xzr;                  \
+        ldp     x5, x6, [p1+16];                \
+        mul     x11, x4, x7;                    \
+        adds    x13, x13, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x16, x16, x11;                  \
+        umulh   x3, x4, x10;                    \
+        adc     x3, x3, xzr;                    \
+        umulh   x11, x4, x7;                    \
+        adds    x14, x14, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x15, x15, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x16, x16, x11;                  \
+        adc     x3, x3, xzr;                    \
+        mul     x11, x5, x7;                    \
+        adds    x14, x14, x11;                  \
+        mul     x11, x5, x8;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x5, x9;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x5, x10;                   \
+        adcs    x3, x3, x11;                    \
+        umulh   x4, x5, x10;                    \
+        adc     x4, x4, xzr;                    \
+        umulh   x11, x5, x7;                    \
+        adds    x15, x15, x11;                  \
+        umulh   x11, x5, x8;                    \
+        adcs    x16, x16, x11;                  \
+        umulh   x11, x5, x9;                    \
+        adcs    x3, x3, x11;                    \
+        adc     x4, x4, xzr;                    \
+        mul     x11, x6, x7;                    \
+        adds    x15, x15, x11;                  \
+        mul     x11, x6, x8;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x6, x9;                    \
+        adcs    x3, x3, x11;                    \
+        mul     x11, x6, x10;                   \
+        adcs    x4, x4, x11;                    \
+        umulh   x5, x6, x10;                    \
+        adc     x5, x5, xzr;                    \
+        umulh   x11, x6, x7;                    \
+        adds    x16, x16, x11;                  \
+        umulh   x11, x6, x8;                    \
+        adcs    x3, x3, x11;                    \
+        umulh   x11, x6, x9;                    \
+        adcs    x4, x4, x11;                    \
+        adc     x5, x5, xzr;                    \
+        mov     x7, #38;                        \
+        mul     x11, x7, x16;                   \
+        umulh   x9, x7, x16;                    \
+        adds    x12, x12, x11;                  \
+        mul     x11, x7, x3;                    \
+        umulh   x3, x7, x3;                     \
+        adcs    x13, x13, x11;                  \
+        mul     x11, x7, x4;                    \
+        umulh   x4, x7, x4;                     \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x7, x5;                    \
+        umulh   x5, x7, x5;                     \
+        adcs    x15, x15, x11;                  \
+        cset    x16, hs;                        \
+        adds    x13, x13, x9;                   \
+        adcs    x14, x14, x3;                   \
+        adcs    x15, x15, x4;                   \
+        adc     x16, x16, x5;                   \
+        stp     x12, x13, [p0];                 \
+        stp     x14, x15, [p0+16];              \
+        str     x16, [p0+32]
+
+// Squaring just giving a result < 2 * p_25519, which is done by
+// basically skipping the +1 in the quotient estimate and the final
+// optional correction.
+
+#define sqr_4(p0,p1)                            \
+        ldp     x2, x3, [p1];                   \
+        mul     x9, x2, x3;                     \
+        umulh   x10, x2, x3;                    \
+        ldp     x4, x5, [p1+16];                \
+        mul     x11, x2, x5;                    \
+        umulh   x12, x2, x5;                    \
+        mul     x7, x2, x4;                     \
+        umulh   x6, x2, x4;                     \
+        adds    x10, x10, x7;                   \
+        adcs    x11, x11, x6;                   \
+        mul     x7, x3, x4;                     \
+        umulh   x6, x3, x4;                     \
+        adc     x6, x6, xzr;                    \
+        adds    x11, x11, x7;                   \
+        mul     x13, x4, x5;                    \
+        umulh   x14, x4, x5;                    \
+        adcs    x12, x12, x6;                   \
+        mul     x7, x3, x5;                     \
+        umulh   x6, x3, x5;                     \
+        adc     x6, x6, xzr;                    \
+        adds    x12, x12, x7;                   \
+        adcs    x13, x13, x6;                   \
+        adc     x14, x14, xzr;                  \
+        adds    x9, x9, x9;                     \
+        adcs    x10, x10, x10;                  \
+        adcs    x11, x11, x11;                  \
+        adcs    x12, x12, x12;                  \
+        adcs    x13, x13, x13;                  \
+        adcs    x14, x14, x14;                  \
+        cset    x6, hs;                         \
+        umulh   x7, x2, x2;                     \
+        mul     x8, x2, x2;                     \
+        adds    x9, x9, x7;                     \
+        mul     x7, x3, x3;                     \
+        adcs    x10, x10, x7;                   \
+        umulh   x7, x3, x3;                     \
+        adcs    x11, x11, x7;                   \
+        mul     x7, x4, x4;                     \
+        adcs    x12, x12, x7;                   \
+        umulh   x7, x4, x4;                     \
+        adcs    x13, x13, x7;                   \
+        mul     x7, x5, x5;                     \
+        adcs    x14, x14, x7;                   \
+        umulh   x7, x5, x5;                     \
+        adc     x6, x6, x7;                     \
+        mov     x3, #38;                        \
+        mul     x7, x3, x12;                    \
+        umulh   x4, x3, x12;                    \
+        adds    x8, x8, x7;                     \
+        mul     x7, x3, x13;                    \
+        umulh   x13, x3, x13;                   \
+        adcs    x9, x9, x7;                     \
+        mul     x7, x3, x14;                    \
+        umulh   x14, x3, x14;                   \
+        adcs    x10, x10, x7;                   \
+        mul     x7, x3, x6;                     \
+        umulh   x6, x3, x6;                     \
+        adcs    x11, x11, x7;                   \
+        cset    x12, hs;                        \
+        adds    x9, x9, x4;                     \
+        adcs    x10, x10, x13;                  \
+        adcs    x11, x11, x14;                  \
+        adc     x12, x12, x6;                   \
+        cmn     x11, x11;                       \
+        bic     x11, x11, #0x8000000000000000;  \
+        adc     x2, x12, x12;                   \
+        mov     x3, #19;                        \
+        mul     x7, x3, x2;                     \
+        adds    x8, x8, x7;                     \
+        adcs    x9, x9, xzr;                    \
+        adcs    x10, x10, xzr;                  \
+        adc     x11, x11, xzr;                  \
+        stp     x8, x9, [p0];                   \
+        stp     x10, x11, [p0+16]
+
+// Plain 4-digit add without any normalization
+// With inputs < p_25519 (indeed < 2^255) it still gives a 4-digit result
+
+#define add_4(p0,p1,p2)                         \
+        ldp     x0, x1, [p1];                   \
+        ldp     x4, x5, [p2];                   \
+        adds    x0, x0, x4;                     \
+        adcs    x1, x1, x5;                     \
+        ldp     x2, x3, [p1+16];                \
+        ldp     x6, x7, [p2+16];                \
+        adcs    x2, x2, x6;                     \
+        adcs    x3, x3, x7;                     \
+        stp     x0, x1, [p0];                   \
+        stp     x2, x3, [p0+16]
+
+// Add 5-digit inputs and normalize to 4 digits
+
+#define add5_4(p0,p1,p2)                        \
+        ldp     x0, x1, [p1];                   \
+        ldp     x4, x5, [p2];                   \
+        adds    x0, x0, x4;                     \
+        adcs    x1, x1, x5;                     \
+        ldp     x2, x3, [p1+16];                \
+        ldp     x6, x7, [p2+16];                \
+        adcs    x2, x2, x6;                     \
+        adcs    x3, x3, x7;                     \
+        ldr     x4, [p1+32];                    \
+        ldr     x5, [p2+32];                    \
+        adc     x4, x4, x5;                     \
+        cmn     x3, x3;                         \
+        bic     x3, x3, #0x8000000000000000;    \
+        adc     x8, x4, x4;                     \
+        mov     x7, #19;                        \
+        mul     x11, x7, x8;                    \
+        adds    x0, x0, x11;                    \
+        adcs    x1, x1, xzr;                    \
+        adcs    x2, x2, xzr;                    \
+        adc     x3, x3, xzr;                    \
+        stp     x0, x1, [p0];                   \
+        stp     x2, x3, [p0+16]
+
+// Subtraction of a pair of numbers < p_25519 just sufficient
+// to give a 4-digit result. It actually always does (x - z) + (2^255-19)
+// which in turn is done by (x - z) - (2^255+19) discarding the 2^256
+// implicitly
+
+#define sub_4(p0,p1,p2)                         \
+        ldp     x5, x6, [p1];                   \
+        ldp     x4, x3, [p2];                   \
+        subs    x5, x5, x4;                     \
+        sbcs    x6, x6, x3;                     \
+        ldp     x7, x8, [p1+16];                \
+        ldp     x4, x3, [p2+16];                \
+        sbcs    x7, x7, x4;                     \
+        sbcs    x8, x8, x3;                     \
+        mov     x3, #19;                        \
+        subs    x5, x5, x3;                     \
+        sbcs    x6, x6, xzr;                    \
+        sbcs    x7, x7, xzr;                    \
+        mov     x4, #0x8000000000000000;        \
+        sbc     x8, x8, x4;                     \
+        stp     x5, x6, [p0];                   \
+        stp     x7, x8, [p0+16]
+
+// Modular subtraction with double modulus 2 * p_25519 = 2^256 - 38
+
+#define sub_twice4(p0,p1,p2)                    \
+        ldp     x5, x6, [p1];                   \
+        ldp     x4, x3, [p2];                   \
+        subs    x5, x5, x4;                     \
+        sbcs    x6, x6, x3;                     \
+        ldp     x7, x8, [p1+16];                \
+        ldp     x4, x3, [p2+16];                \
+        sbcs    x7, x7, x4;                     \
+        sbcs    x8, x8, x3;                     \
+        mov     x4, #38;                        \
+        csel    x3, x4, xzr, lo;                \
+        subs    x5, x5, x3;                     \
+        sbcs    x6, x6, xzr;                    \
+        sbcs    x7, x7, xzr;                    \
+        sbc     x8, x8, xzr;                    \
+        stp     x5, x6, [p0];                   \
+        stp     x7, x8, [p0+16]
+
+// 5-digit subtraction with upward bias to make it positive, adding
+// 1000 * (2^255 - 19) = 2^256 * 500 - 19000, then normalizing to 4 digits
+
+#define sub5_4(p0,p1,p2)                        \
+        ldp     x0, x1, [p1];                   \
+        ldp     x4, x5, [p2];                   \
+        subs    x0, x0, x4;                     \
+        sbcs    x1, x1, x5;                     \
+        ldp     x2, x3, [p1+16];                \
+        ldp     x6, x7, [p2+16];                \
+        sbcs    x2, x2, x6;                     \
+        sbcs    x3, x3, x7;                     \
+        ldr     x4, [p1+32];                    \
+        ldr     x5, [p2+32];                    \
+        sbc     x4, x4, x5;                     \
+        mov     x7, -19000;                     \
+        adds x0, x0, x7;                        \
+        sbcs    x1, x1, xzr;                    \
+        sbcs    x2, x2, xzr;                    \
+        sbcs    x3, x3, xzr;                    \
+        mov     x7, 499;                        \
+        adc     x4, x4, x7;                     \
+        cmn     x3, x3;                         \
+        bic     x3, x3, #0x8000000000000000;    \
+        adc     x8, x4, x4;                     \
+        mov     x7, #19;                        \
+        mul     x11, x7, x8;                    \
+        adds    x0, x0, x11;                    \
+        adcs    x1, x1, xzr;                    \
+        adcs    x2, x2, xzr;                    \
+        adc     x3, x3, xzr;                    \
+        stp     x0, x1, [p0];                   \
+        stp     x2, x3, [p0+16]
+
+// Combined z = c * x + y with reduction only < 2 * p_25519
+// where c is initially in the X1 register. It is assumed
+// that 19 * (c * x + y) < 2^60 * 2^256 so we don't need a
+// high mul in the final part.
+
+#define cmadd_4(p0,p2,p3)                       \
+        ldp     x7, x8, [p2];                   \
+        ldp     x9, x10, [p2+16];               \
+        mul     x3, x1, x7;                     \
+        mul     x4, x1, x8;                     \
+        mul     x5, x1, x9;                     \
+        mul     x6, x1, x10;                    \
+        umulh   x7, x1, x7;                     \
+        umulh   x8, x1, x8;                     \
+        umulh   x9, x1, x9;                     \
+        umulh   x10, x1, x10;                   \
+        adds    x4, x4, x7;                     \
+        adcs    x5, x5, x8;                     \
+        adcs    x6, x6, x9;                     \
+        adc     x10, x10, xzr;                  \
+        ldp     x7, x8, [p3];                   \
+        adds    x3, x3, x7;                     \
+        adcs    x4, x4, x8;                     \
+        ldp     x7, x8, [p3+16];                \
+        adcs    x5, x5, x7;                     \
+        adcs    x6, x6, x8;                     \
+        adc     x10, x10, xzr;                  \
+        cmn     x6, x6;                         \
+        bic     x6, x6, #0x8000000000000000;    \
+        adc     x8, x10, x10;                   \
+        mov     x9, #19;                        \
+        mul     x7, x8, x9;                     \
+        adds    x3, x3, x7;                     \
+        adcs    x4, x4, xzr;                    \
+        adcs    x5, x5, xzr;                    \
+        adc     x6, x6, xzr;                    \
+        stp     x3, x4, [p0];                   \
+        stp     x5, x6, [p0+16]
+
+// Multiplex: z := if NZ then x else y
+
+#define mux_4(p0,p1,p2)                         \
+        ldp     x0, x1, [p1];                   \
+        ldp     x2, x3, [p2];                   \
+        csel    x0, x0, x2, ne;                 \
+        csel    x1, x1, x3, ne;                 \
+        stp     x0, x1, [p0];                   \
+        ldp     x0, x1, [p1+16];                \
+        ldp     x2, x3, [p2+16];                \
+        csel    x0, x0, x2, ne;                 \
+        csel    x1, x1, x3, ne;                 \
+        stp     x0, x1, [p0+16]
+
+S2N_BN_SYMBOL(curve25519_x25519_alt):
+
+// Save regs and make room for temporaries
+
+        stp     x19, x20, [sp, -16]!
+        stp     x21, x22, [sp, -16]!
+        stp     x23, x24, [sp, -16]!
+        sub     sp, sp, #NSPACE
+
+// Move the output pointer to a stable place
+
+        mov     res, x0
+
+// Copy the inputs to the local variables while mangling them:
+//
+//  - The scalar gets turned into 01xxx...xxx000 by tweaking bits.
+//    Actually the top zero doesn't matter since the loop below
+//    never looks at it, so we don't literally modify that.
+//
+//  - The point x coord is reduced mod 2^255 *then* mod 2^255-19
+
+        ldp     x10, x11, [x1]
+        bic     x10, x10, #7
+        stp     x10, x11, [scalar]
+        ldp     x12, x13, [x1, #16]
+        orr     x13, x13, #0x4000000000000000
+        stp     x12, x13, [scalar+16]
+
+        ldp     x10, x11, [x2]
+        subs    x6, x10, #-19
+        adcs    x7, x11, xzr
+        ldp     x12, x13, [x2, #16]
+        and     x13, x13, #0x7fffffffffffffff
+        adcs    x8, x12, xzr
+        mov     x9, #0x7fffffffffffffff
+        sbcs    x9, x13, x9
+
+        csel    x10, x6, x10, cs
+        csel    x11, x7, x11, cs
+        csel    x12, x8, x12, cs
+        csel    x13, x9, x13, cs
+
+        stp     x10, x11, [pointx]
+        stp     x12, x13, [pointx+16]
+
+// Initialize (xn,zn) = (1,0) and (xm,zm) = (x,1) with swap = 0
+// We use the fact that the point x coordinate is still in registers
+
+        mov     x2, #1
+        stp     x2, xzr, [xn]
+        stp     xzr, xzr, [xn+16]
+        stp     xzr, xzr, [zn]
+        stp     xzr, xzr, [zn+16]
+        stp     x10, x11, [xm]
+        stp     x12, x13, [xm+16]
+        stp     x2, xzr, [zm]
+        stp     xzr, xzr, [zm+16]
+        mov     swap, xzr
+
+// The outer loop over scalar bits from i = 254, ..., i = 0 (inclusive)
+// This starts at 254, and so implicitly masks bit 255 of the scalar.
+
+        mov     i, #254
+
+scalarloop:
+
+// sm = xm + zm; sn = xn + zn; dm = xm - zm; dn = xn - zn
+// The adds don't need any normalization as they're fed to muls
+// Just make sure the subs fit in 4 digits
+
+        sub_4(dm, xm, zm)
+        add_4(sn, xn, zn)
+        sub_4(dn, xn, zn)
+        add_4(sm, xm, zm)
+
+// ADDING: dmsn = dm * sn; dnsm = sm * dn
+// DOUBLING: mux d = xt - zt and s = xt + zt for appropriate choice of (xt,zt)
+
+        mul_5(dmsn,sn,dm)
+
+        lsr     x0, i, #6
+        ldr     x2, [sp, x0, lsl #3]    // Exploiting scalar = sp exactly
+        lsr     x2, x2, i
+        and     x2, x2, #1
+
+        cmp     swap, x2
+        mov     swap, x2
+
+        mux_4(d,dm,dn)
+        mux_4(s,sm,sn)
+
+        mul_5(dnsm,sm,dn)
+
+// DOUBLING: d = (xt - zt)^2 normalized only to 4 digits
+
+        sqr_4(d,d)
+
+// ADDING: dpro = (dmsn - dnsm)^2, spro = (dmsn + dnsm)^2
+// DOUBLING: s = (xt + zt)^2, normalized only to 4 digits
+
+        sub5_4(dpro,dmsn,dnsm)
+        sqr_4(s,s)
+        add5_4(spro,dmsn,dnsm)
+        sqr_4(dpro,dpro)
+
+// DOUBLING: p = 4 * xt * zt = s - d
+
+        sub_twice4(p,s,d)
+
+// ADDING: xm' = (dmsn + dnsm)^2
+
+        sqr_p25519(xm,spro)
+
+// DOUBLING: e = 121666 * p + d
+
+        mov     x1, 0xdb42
+        orr     x1, x1, 0x10000
+        cmadd_4(e,p,d)
+
+// DOUBLING: xn' = (xt + zt)^2 * (xt - zt)^2 = s * d
+
+        mul_p25519(xn,s,d)
+
+// ADDING: zm' = x * (dmsn - dnsm)^2
+
+        mul_p25519(zm,dpro,pointx)
+
+// DOUBLING: zn' = (4 * xt * zt) * ((xt - zt)^2 + 121666 * (4 * xt * zt))
+//               = p * (d + 121666 * p)
+
+        mul_p25519(zn,p,e)
+
+// Loop down as far as 0 (inclusive)
+
+        subs    i, i, #1
+        bcs     scalarloop
+
+// Since the scalar was forced to be a multiple of 8, we know it's even.
+// Hence there is no need to multiplex: the projective answer is (xn,zn)
+// and we can ignore (xm,zm); indeed we could have avoided the last three
+// differential additions and just done the doublings.
+// First set up the constant sn = 2^255 - 19 for the modular inverse.
+
+        mov     x0, #-19
+        mov     x1, #-1
+        mov     x2, #0x7fffffffffffffff
+        stp     x0, x1, [sn]
+        stp     x1, x2, [sn+16]
+
+// Prepare to call the modular inverse function to get zm = 1/zn
+
+        mov     x0, #4
+        add     x1, zm
+        add     x2, zn
+        add     x3, sn
+        add     x4, p
+
+// Inline copy of bignum_modinv, identical except for stripping out the
+// prologue and epilogue saving and restoring registers and the initial
+// test for k = 0 (which is trivially false here since k = 4). For more
+// details and explanations see "arm/generic/bignum_modinv.S".
+
+        lsl     x10, x0, #3
+        add     x21, x4, x10
+        add     x22, x21, x10
+        mov     x10, xzr
+copyloop:
+        ldr     x11, [x2, x10, lsl #3]
+        ldr     x12, [x3, x10, lsl #3]
+        str     x11, [x21, x10, lsl #3]
+        str     x12, [x22, x10, lsl #3]
+        str     x12, [x4, x10, lsl #3]
+        str     xzr, [x1, x10, lsl #3]
+        add     x10, x10, #0x1
+        cmp     x10, x0
+        b.cc    copyloop
+        ldr     x11, [x4]
+        sub     x12, x11, #0x1
+        str     x12, [x4]
+        lsl     x20, x11, #2
+        sub     x20, x11, x20
+        eor     x20, x20, #0x2
+        mov     x12, #0x1
+        madd    x12, x11, x20, x12
+        mul     x11, x12, x12
+        madd    x20, x12, x20, x20
+        mul     x12, x11, x11
+        madd    x20, x11, x20, x20
+        mul     x11, x12, x12
+        madd    x20, x12, x20, x20
+        madd    x20, x11, x20, x20
+        lsl     x2, x0, #7
+outerloop:
+        add     x10, x2, #0x3f
+        lsr     x5, x10, #6
+        cmp     x5, x0
+        csel    x5, x0, x5, cs
+        mov     x13, xzr
+        mov     x15, xzr
+        mov     x14, xzr
+        mov     x16, xzr
+        mov     x19, xzr
+        mov     x10, xzr
+toploop:
+        ldr     x11, [x21, x10, lsl #3]
+        ldr     x12, [x22, x10, lsl #3]
+        orr     x17, x11, x12
+        cmp     x17, xzr
+        and     x17, x19, x13
+        csel    x15, x17, x15, ne
+        and     x17, x19, x14
+        csel    x16, x17, x16, ne
+        csel    x13, x11, x13, ne
+        csel    x14, x12, x14, ne
+        csetm   x19, ne
+        add     x10, x10, #0x1
+        cmp     x10, x5
+        b.cc    toploop
+        orr     x11, x13, x14
+        clz     x12, x11
+        negs    x17, x12
+        lsl     x13, x13, x12
+        csel    x15, x15, xzr, ne
+        lsl     x14, x14, x12
+        csel    x16, x16, xzr, ne
+        lsr     x15, x15, x17
+        lsr     x16, x16, x17
+        orr     x13, x13, x15
+        orr     x14, x14, x16
+        ldr     x15, [x21]
+        ldr     x16, [x22]
+        mov     x6, #0x1
+        mov     x7, xzr
+        mov     x8, xzr
+        mov     x9, #0x1
+        mov     x10, #0x3a
+        tst     x15, #0x1
+innerloop:
+        csel    x11, x14, xzr, ne
+        csel    x12, x16, xzr, ne
+        csel    x17, x8, xzr, ne
+        csel    x19, x9, xzr, ne
+        ccmp    x13, x14, #0x2, ne
+        sub     x11, x13, x11
+        sub     x12, x15, x12
+        csel    x14, x14, x13, cs
+        cneg    x11, x11, cc
+        csel    x16, x16, x15, cs
+        cneg    x15, x12, cc
+        csel    x8, x8, x6, cs
+        csel    x9, x9, x7, cs
+        tst     x12, #0x2
+        add     x6, x6, x17
+        add     x7, x7, x19
+        lsr     x13, x11, #1
+        lsr     x15, x15, #1
+        add     x8, x8, x8
+        add     x9, x9, x9
+        sub     x10, x10, #0x1
+        cbnz    x10, innerloop
+        mov     x13, xzr
+        mov     x14, xzr
+        mov     x17, xzr
+        mov     x19, xzr
+        mov     x10, xzr
+congloop:
+        ldr     x11, [x4, x10, lsl #3]
+        ldr     x12, [x1, x10, lsl #3]
+        mul     x15, x6, x11
+        mul     x16, x7, x12
+        adds    x15, x15, x13
+        umulh   x13, x6, x11
+        adc     x13, x13, xzr
+        adds    x15, x15, x16
+        extr    x17, x15, x17, #58
+        str     x17, [x4, x10, lsl #3]
+        mov     x17, x15
+        umulh   x15, x7, x12
+        adc     x13, x13, x15
+        mul     x15, x8, x11
+        mul     x16, x9, x12
+        adds    x15, x15, x14
+        umulh   x14, x8, x11
+        adc     x14, x14, xzr
+        adds    x15, x15, x16
+        extr    x19, x15, x19, #58
+        str     x19, [x1, x10, lsl #3]
+        mov     x19, x15
+        umulh   x15, x9, x12
+        adc     x14, x14, x15
+        add     x10, x10, #0x1
+        cmp     x10, x0
+        b.cc    congloop
+        extr    x13, x13, x17, #58
+        extr    x14, x14, x19, #58
+        ldr     x11, [x4]
+        mul     x17, x11, x20
+        ldr     x12, [x3]
+        mul     x15, x17, x12
+        umulh   x16, x17, x12
+        adds    x11, x11, x15
+        mov     x10, #0x1
+        sub     x11, x0, #0x1
+        cbz     x11, wmontend
+wmontloop:
+        ldr     x11, [x3, x10, lsl #3]
+        ldr     x12, [x4, x10, lsl #3]
+        mul     x15, x17, x11
+        adcs    x12, x12, x16
+        umulh   x16, x17, x11
+        adc     x16, x16, xzr
+        adds    x12, x12, x15
+        sub     x15, x10, #0x1
+        str     x12, [x4, x15, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, wmontloop
+wmontend:
+        adcs    x16, x16, x13
+        adc     x13, xzr, xzr
+        sub     x15, x10, #0x1
+        str     x16, [x4, x15, lsl #3]
+        negs    x10, xzr
+wcmploop:
+        ldr     x11, [x4, x10, lsl #3]
+        ldr     x12, [x3, x10, lsl #3]
+        sbcs    xzr, x11, x12
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, wcmploop
+        sbcs    xzr, x13, xzr
+        csetm   x13, cs
+        negs    x10, xzr
+wcorrloop:
+        ldr     x11, [x4, x10, lsl #3]
+        ldr     x12, [x3, x10, lsl #3]
+        and     x12, x12, x13
+        sbcs    x11, x11, x12
+        str     x11, [x4, x10, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, wcorrloop
+        ldr     x11, [x1]
+        mul     x17, x11, x20
+        ldr     x12, [x3]
+        mul     x15, x17, x12
+        umulh   x16, x17, x12
+        adds    x11, x11, x15
+        mov     x10, #0x1
+        sub     x11, x0, #0x1
+        cbz     x11, zmontend
+zmontloop:
+        ldr     x11, [x3, x10, lsl #3]
+        ldr     x12, [x1, x10, lsl #3]
+        mul     x15, x17, x11
+        adcs    x12, x12, x16
+        umulh   x16, x17, x11
+        adc     x16, x16, xzr
+        adds    x12, x12, x15
+        sub     x15, x10, #0x1
+        str     x12, [x1, x15, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, zmontloop
+zmontend:
+        adcs    x16, x16, x14
+        adc     x14, xzr, xzr
+        sub     x15, x10, #0x1
+        str     x16, [x1, x15, lsl #3]
+        negs    x10, xzr
+zcmploop:
+        ldr     x11, [x1, x10, lsl #3]
+        ldr     x12, [x3, x10, lsl #3]
+        sbcs    xzr, x11, x12
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, zcmploop
+        sbcs    xzr, x14, xzr
+        csetm   x14, cs
+        negs    x10, xzr
+zcorrloop:
+        ldr     x11, [x1, x10, lsl #3]
+        ldr     x12, [x3, x10, lsl #3]
+        and     x12, x12, x14
+        sbcs    x11, x11, x12
+        str     x11, [x1, x10, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, zcorrloop
+        mov     x13, xzr
+        mov     x14, xzr
+        mov     x17, xzr
+        mov     x19, xzr
+        mov     x10, xzr
+crossloop:
+        ldr     x11, [x21, x10, lsl #3]
+        ldr     x12, [x22, x10, lsl #3]
+        mul     x15, x6, x11
+        mul     x16, x7, x12
+        adds    x15, x15, x13
+        umulh   x13, x6, x11
+        adc     x13, x13, xzr
+        subs    x15, x15, x16
+        str     x15, [x21, x10, lsl #3]
+        umulh   x15, x7, x12
+        sub     x17, x15, x17
+        sbcs    x13, x13, x17
+        csetm   x17, cc
+        mul     x15, x8, x11
+        mul     x16, x9, x12
+        adds    x15, x15, x14
+        umulh   x14, x8, x11
+        adc     x14, x14, xzr
+        subs    x15, x15, x16
+        str     x15, [x22, x10, lsl #3]
+        umulh   x15, x9, x12
+        sub     x19, x15, x19
+        sbcs    x14, x14, x19
+        csetm   x19, cc
+        add     x10, x10, #0x1
+        cmp     x10, x5
+        b.cc    crossloop
+        cmn     x17, x17
+        ldr     x15, [x21]
+        mov     x10, xzr
+        sub     x6, x5, #0x1
+        cbz     x6, negskip1
+negloop1:
+        add     x11, x10, #0x8
+        ldr     x12, [x21, x11]
+        extr    x15, x12, x15, #58
+        eor     x15, x15, x17
+        adcs    x15, x15, xzr
+        str     x15, [x21, x10]
+        mov     x15, x12
+        add     x10, x10, #0x8
+        sub     x6, x6, #0x1
+        cbnz    x6, negloop1
+negskip1:
+        extr    x15, x13, x15, #58
+        eor     x15, x15, x17
+        adcs    x15, x15, xzr
+        str     x15, [x21, x10]
+        cmn     x19, x19
+        ldr     x15, [x22]
+        mov     x10, xzr
+        sub     x6, x5, #0x1
+        cbz     x6, negskip2
+negloop2:
+        add     x11, x10, #0x8
+        ldr     x12, [x22, x11]
+        extr    x15, x12, x15, #58
+        eor     x15, x15, x19
+        adcs    x15, x15, xzr
+        str     x15, [x22, x10]
+        mov     x15, x12
+        add     x10, x10, #0x8
+        sub     x6, x6, #0x1
+        cbnz    x6, negloop2
+negskip2:
+        extr    x15, x14, x15, #58
+        eor     x15, x15, x19
+        adcs    x15, x15, xzr
+        str     x15, [x22, x10]
+        mov     x10, xzr
+        cmn     x17, x17
+wfliploop:
+        ldr     x11, [x3, x10, lsl #3]
+        ldr     x12, [x4, x10, lsl #3]
+        and     x11, x11, x17
+        eor     x12, x12, x17
+        adcs    x11, x11, x12
+        str     x11, [x4, x10, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, wfliploop
+        mvn     x19, x19
+        mov     x10, xzr
+        cmn     x19, x19
+zfliploop:
+        ldr     x11, [x3, x10, lsl #3]
+        ldr     x12, [x1, x10, lsl #3]
+        and     x11, x11, x19
+        eor     x12, x12, x19
+        adcs    x11, x11, x12
+        str     x11, [x1, x10, lsl #3]
+        add     x10, x10, #0x1
+        sub     x11, x10, x0
+        cbnz    x11, zfliploop
+        subs    x2, x2, #0x3a
+        b.hi    outerloop
+
+// Since we eventually want to return 0 when the result is the point at
+// infinity, we force xn = 0 whenever zn = 0. This avoids building in a
+// dependency on the behavior of modular inverse in out-of-scope cases.
+
+        ldp     x0, x1, [zn]
+        ldp     x2, x3, [zn+16]
+        orr     x0, x0, x1
+        orr     x2, x2, x3
+        orr     x4, x0, x2
+        cmp     x4, xzr
+        ldp     x0, x1, [xn]
+        csel    x0, x0, xzr, ne
+        csel    x1, x1, xzr, ne
+        ldp     x2, x3, [xn+16]
+        stp     x0, x1, [xn]
+        csel    x2, x2, xzr, ne
+        csel    x3, x3, xzr, ne
+        stp     x2, x3, [xn+16]
+
+// Now the result is xn * (1/zn).
+
+        mul_p25519(resx,xn,zm)
+
+// Restore stack and registers
+
+        add     sp, sp, #NSPACE
+        ldp     x23, x24, [sp], 16
+        ldp     x21, x22, [sp], 16
+        ldp     x19, x20, [sp], 16
+
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack, "", %progbits
+#endif
diff --git a/x86_att/curve25519/curve25519_x25519.S b/x86_att/curve25519/curve25519_x25519.S
new file mode 100644
index 000000000..09d62751b
--- /dev/null
+++ b/x86_att/curve25519/curve25519_x25519.S
@@ -0,0 +1,1245 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// The x25519 function for curve25519
+// Inputs scalar[4], point[4]; output res[4]
+//
+// extern void curve25519_x25519
+//   (uint64_t res[static 4],uint64_t scalar[static 4],uint64_t point[static 4])
+//
+// Given a scalar n and the X coordinate of an input point P = (X,Y) on
+// curve25519 (Y can live in any extension field of characteristic 2^255-19),
+// this returns the X coordinate of n * P = (X, Y), or 0 when n * P is the
+// point at infinity. Both n and X inputs are first slightly modified/mangled
+// as specified in the relevant RFC (https://www.rfc-editor.org/rfc/rfc7748);
+// in particular the lower three bits of n are set to zero.
+//
+// Standard x86-64 ABI: RDI = res, RSI = scalar, RDX = point
+// Microsoft x64 ABI:   RCX = res, RDX = scalar, R8 = point
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(curve25519_x25519)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(curve25519_x25519)
+        .text
+
+// Size of individual field elements
+
+#define NUMSIZE 32
+
+// Stable homes for the input result argument during the whole body
+// and other variables that are only needed prior to the modular inverse.
+
+#define res  12*NUMSIZE(%rsp)
+#define i  12*NUMSIZE+8(%rsp)
+#define swap  12*NUMSIZE+16(%rsp)
+
+// Pointers to result x coord to be written, assuming the base "res"
+// has been loaded into %rbp
+
+#define resx 0(%rbp)
+
+// Pointer-offset pairs for temporaries on stack with some aliasing.
+// Both dmsn and dnsm need space for >= 5 digits, and we allocate 8
+
+#define scalar (0*NUMSIZE)(%rsp)
+
+#define pointx (1*NUMSIZE)(%rsp)
+
+#define dm (2*NUMSIZE)(%rsp)
+
+#define zm (3*NUMSIZE)(%rsp)
+#define sm (3*NUMSIZE)(%rsp)
+#define dpro (3*NUMSIZE)(%rsp)
+
+#define sn (4*NUMSIZE)(%rsp)
+
+#define zn (5*NUMSIZE)(%rsp)
+#define dn (5*NUMSIZE)(%rsp)
+#define e (5*NUMSIZE)(%rsp)
+
+#define dmsn (6*NUMSIZE)(%rsp)
+#define p (6*NUMSIZE)(%rsp)
+
+#define xm (8*NUMSIZE)(%rsp)
+#define dnsm (8*NUMSIZE)(%rsp)
+#define spro (8*NUMSIZE)(%rsp)
+
+#define xn (10*NUMSIZE)(%rsp)
+#define s (10*NUMSIZE)(%rsp)
+
+#define d (11*NUMSIZE)(%rsp)
+
+// Total size to reserve on the stack
+// This includes space for the 3 other variables above
+// and rounds up to a multiple of 32
+
+#define NSPACE (13*NUMSIZE)
+
+// Macros wrapping up the basic field operation calls
+// bignum_mul_p25519 and bignum_sqr_p25519.
+// These two are only trivially different from pure
+// function calls to those subroutines.
+
+#define mul_p25519(P0,P1,P2)                    \
+        xorl   %edi, %edi ;                        \
+        movq   P2, %rdx ;                       \
+        mulxq  P1, %r8, %r9 ;                    \
+        mulxq  0x8+P1, %rax, %r10 ;              \
+        addq   %rax, %r9 ;                         \
+        mulxq  0x10+P1, %rax, %r11 ;             \
+        adcq   %rax, %r10 ;                        \
+        mulxq  0x18+P1, %rax, %r12 ;             \
+        adcq   %rax, %r11 ;                        \
+        adcq   %rdi, %r12 ;                        \
+        xorl   %edi, %edi ;                        \
+        movq   0x8+P2, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                  \
+        adcxq  %rax, %r9 ;                         \
+        adoxq  %rbx, %r10 ;                        \
+        mulxq  0x8+P1, %rax, %rbx ;              \
+        adcxq  %rax, %r10 ;                        \
+        adoxq  %rbx, %r11 ;                        \
+        mulxq  0x10+P1, %rax, %rbx ;             \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rbx, %r12 ;                        \
+        mulxq  0x18+P1, %rax, %r13 ;             \
+        adcxq  %rax, %r12 ;                        \
+        adoxq  %rdi, %r13 ;                        \
+        adcxq  %rdi, %r13 ;                        \
+        xorl   %edi, %edi ;                        \
+        movq   0x10+P2, %rdx ;                  \
+        mulxq  P1, %rax, %rbx ;                  \
+        adcxq  %rax, %r10 ;                        \
+        adoxq  %rbx, %r11 ;                        \
+        mulxq  0x8+P1, %rax, %rbx ;              \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rbx, %r12 ;                        \
+        mulxq  0x10+P1, %rax, %rbx ;             \
+        adcxq  %rax, %r12 ;                        \
+        adoxq  %rbx, %r13 ;                        \
+        mulxq  0x18+P1, %rax, %r14 ;             \
+        adcxq  %rax, %r13 ;                        \
+        adoxq  %rdi, %r14 ;                        \
+        adcxq  %rdi, %r14 ;                        \
+        xorl   %edi, %edi ;                        \
+        movq   0x18+P2, %rdx ;                  \
+        mulxq  P1, %rax, %rbx ;                  \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rbx, %r12 ;                        \
+        mulxq  0x8+P1, %rax, %rbx ;              \
+        adcxq  %rax, %r12 ;                        \
+        adoxq  %rbx, %r13 ;                        \
+        mulxq  0x10+P1, %rax, %rbx ;             \
+        adcxq  %rax, %r13 ;                        \
+        adoxq  %rbx, %r14 ;                        \
+        mulxq  0x18+P1, %rax, %r15 ;             \
+        adcxq  %rax, %r14 ;                        \
+        adoxq  %rdi, %r15 ;                        \
+        adcxq  %rdi, %r15 ;                        \
+        movl   $0x26, %edx ;                       \
+        xorl   %edi, %edi ;                        \
+        mulxq  %r12, %rax, %rbx ;                   \
+        adcxq  %rax, %r8 ;                         \
+        adoxq  %rbx, %r9 ;                         \
+        mulxq  %r13, %rax, %rbx ;                   \
+        adcxq  %rax, %r9 ;                         \
+        adoxq  %rbx, %r10 ;                        \
+        mulxq  %r14, %rax, %rbx ;                   \
+        adcxq  %rax, %r10 ;                        \
+        adoxq  %rbx, %r11 ;                        \
+        mulxq  %r15, %rax, %r12 ;                   \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rdi, %r12 ;                        \
+        adcxq  %rdi, %r12 ;                        \
+        shldq  $0x1, %r11, %r12 ;                   \
+        movabs $0x8000000000000000, %rcx ;         \
+        movl   $0x13, %edx ;                       \
+        incq   %r12;                             \
+        orq    %rcx, %r11 ;                        \
+        mulxq  %r12, %rax, %rbx ;                   \
+        addq   %rax, %r8 ;                         \
+        adcq   %rbx, %r9 ;                         \
+        adcq   %rdi, %r10 ;                        \
+        adcq   %rdi, %r11 ;                        \
+        sbbq   %rax, %rax ;                        \
+        notq   %rax;                             \
+        andq   %rdx, %rax ;                        \
+        subq   %rax, %r8 ;                         \
+        sbbq   %rdi, %r9 ;                         \
+        sbbq   %rdi, %r10 ;                        \
+        sbbq   %rdi, %r11 ;                        \
+        notq   %rcx;                             \
+        andq   %rcx, %r11 ;                        \
+        movq   %r8, P0 ;                        \
+        movq   %r9, 0x8+P0 ;                    \
+        movq   %r10, 0x10+P0 ;                  \
+        movq   %r11, 0x18+P0
+
+#define sqr_p25519(P0,P1)                       \
+        movq   P1, %rdx ;                       \
+        mulxq  %rdx, %r8, %r15 ;                    \
+        mulxq  0x8+P1, %r9, %r10 ;               \
+        mulxq  0x18+P1, %r11, %r12 ;             \
+        movq   0x10+P1, %rdx ;                  \
+        mulxq  0x18+P1, %r13, %r14 ;             \
+        xorl   %ebx, %ebx ;                        \
+        mulxq  P1, %rax, %rcx ;                  \
+        adcxq  %rax, %r10 ;                        \
+        adoxq  %rcx, %r11 ;                        \
+        mulxq  0x8+P1, %rax, %rcx ;              \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rcx, %r12 ;                        \
+        movq   0x18+P1, %rdx ;                  \
+        mulxq  0x8+P1, %rax, %rcx ;              \
+        adcxq  %rax, %r12 ;                        \
+        adoxq  %rcx, %r13 ;                        \
+        adcxq  %rbx, %r13 ;                        \
+        adoxq  %rbx, %r14 ;                        \
+        adcq   %rbx, %r14 ;                        \
+        xorl   %ebx, %ebx ;                        \
+        adcxq  %r9, %r9 ;                          \
+        adoxq  %r15, %r9 ;                         \
+        movq   0x8+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                   \
+        adcxq  %r10, %r10 ;                        \
+        adoxq  %rax, %r10 ;                        \
+        adcxq  %r11, %r11 ;                        \
+        adoxq  %rdx, %r11 ;                        \
+        movq   0x10+P1, %rdx ;                  \
+        mulxq  %rdx, %rax, %rdx ;                   \
+        adcxq  %r12, %r12 ;                        \
+        adoxq  %rax, %r12 ;                        \
+        adcxq  %r13, %r13 ;                        \
+        adoxq  %rdx, %r13 ;                        \
+        movq   0x18+P1, %rdx ;                  \
+        mulxq  %rdx, %rax, %r15 ;                   \
+        adcxq  %r14, %r14 ;                        \
+        adoxq  %rax, %r14 ;                        \
+        adcxq  %rbx, %r15 ;                        \
+        adoxq  %rbx, %r15 ;                        \
+        movl   $0x26, %edx ;                       \
+        xorl   %ebx, %ebx ;                        \
+        mulxq  %r12, %rax, %rcx ;                   \
+        adcxq  %rax, %r8 ;                         \
+        adoxq  %rcx, %r9 ;                         \
+        mulxq  %r13, %rax, %rcx ;                   \
+        adcxq  %rax, %r9 ;                         \
+        adoxq  %rcx, %r10 ;                        \
+        mulxq  %r14, %rax, %rcx ;                   \
+        adcxq  %rax, %r10 ;                        \
+        adoxq  %rcx, %r11 ;                        \
+        mulxq  %r15, %rax, %r12 ;                   \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rbx, %r12 ;                        \
+        adcxq  %rbx, %r12 ;                        \
+        shldq  $0x1, %r11, %r12 ;                   \
+        movl   $0x13, %edx ;                       \
+        leaq   0x1(%r12), %rax ;                  \
+        bts    $0x3f, %r11 ;                       \
+        imulq  %rdx, %rax ;                        \
+        addq   %rax, %r8 ;                         \
+        adcq   %rbx, %r9 ;                         \
+        adcq   %rbx, %r10 ;                        \
+        adcq   %rbx, %r11 ;                        \
+        cmovbq %rbx, %rdx ;                        \
+        subq   %rdx, %r8 ;                         \
+        sbbq   %rbx, %r9 ;                         \
+        sbbq   %rbx, %r10 ;                        \
+        sbbq   %rbx, %r11 ;                        \
+        btr    $0x3f, %r11 ;                       \
+        movq   %r8, P0 ;                        \
+        movq   %r9, 0x8+P0 ;                    \
+        movq   %r10, 0x10+P0 ;                  \
+        movq   %r11, 0x18+P0
+
+// Multiplication just giving a 5-digit result (actually < 39 * p_25519)
+// by not doing anything beyond the first stage of reduction
+
+#define mul_5(P0,P1,P2)                         \
+        xorl   %edi, %edi ;                        \
+        movq   P2, %rdx ;                       \
+        mulxq  P1, %r8, %r9 ;                    \
+        mulxq  0x8+P1, %rax, %r10 ;              \
+        addq   %rax, %r9 ;                         \
+        mulxq  0x10+P1, %rax, %r11 ;             \
+        adcq   %rax, %r10 ;                        \
+        mulxq  0x18+P1, %rax, %r12 ;             \
+        adcq   %rax, %r11 ;                        \
+        adcq   %rdi, %r12 ;                        \
+        xorl   %edi, %edi ;                        \
+        movq   0x8+P2, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                  \
+        adcxq  %rax, %r9 ;                         \
+        adoxq  %rbx, %r10 ;                        \
+        mulxq  0x8+P1, %rax, %rbx ;              \
+        adcxq  %rax, %r10 ;                        \
+        adoxq  %rbx, %r11 ;                        \
+        mulxq  0x10+P1, %rax, %rbx ;             \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rbx, %r12 ;                        \
+        mulxq  0x18+P1, %rax, %r13 ;             \
+        adcxq  %rax, %r12 ;                        \
+        adoxq  %rdi, %r13 ;                        \
+        adcxq  %rdi, %r13 ;                        \
+        xorl   %edi, %edi ;                        \
+        movq   0x10+P2, %rdx ;                  \
+        mulxq  P1, %rax, %rbx ;                  \
+        adcxq  %rax, %r10 ;                        \
+        adoxq  %rbx, %r11 ;                        \
+        mulxq  0x8+P1, %rax, %rbx ;              \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rbx, %r12 ;                        \
+        mulxq  0x10+P1, %rax, %rbx ;             \
+        adcxq  %rax, %r12 ;                        \
+        adoxq  %rbx, %r13 ;                        \
+        mulxq  0x18+P1, %rax, %r14 ;             \
+        adcxq  %rax, %r13 ;                        \
+        adoxq  %rdi, %r14 ;                        \
+        adcxq  %rdi, %r14 ;                        \
+        xorl   %edi, %edi ;                        \
+        movq   0x18+P2, %rdx ;                  \
+        mulxq  P1, %rax, %rbx ;                  \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rbx, %r12 ;                        \
+        mulxq  0x8+P1, %rax, %rbx ;              \
+        adcxq  %rax, %r12 ;                        \
+        adoxq  %rbx, %r13 ;                        \
+        mulxq  0x10+P1, %rax, %rbx ;             \
+        adcxq  %rax, %r13 ;                        \
+        adoxq  %rbx, %r14 ;                        \
+        mulxq  0x18+P1, %rax, %r15 ;             \
+        adcxq  %rax, %r14 ;                        \
+        adoxq  %rdi, %r15 ;                        \
+        adcxq  %rdi, %r15 ;                        \
+        movl   $0x26, %edx ;                       \
+        xorl   %edi, %edi ;                        \
+        mulxq  %r12, %rax, %rbx ;                   \
+        adcxq  %rax, %r8 ;                         \
+        adoxq  %rbx, %r9 ;                         \
+        mulxq  %r13, %rax, %rbx ;                   \
+        adcxq  %rax, %r9 ;                         \
+        adoxq  %rbx, %r10 ;                        \
+        mulxq  %r14, %rax, %rbx ;                   \
+        adcxq  %rax, %r10 ;                        \
+        adoxq  %rbx, %r11 ;                        \
+        mulxq  %r15, %rax, %r12 ;                   \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rdi, %r12 ;                        \
+        adcxq  %rdi, %r12 ;                        \
+        movq   %r8, P0 ;                        \
+        movq   %r9, 0x8+P0 ;                    \
+        movq   %r10, 0x10+P0 ;                  \
+        movq   %r11, 0x18+P0 ;                  \
+        movq   %r12, 0x20+P0
+
+// Squaring just giving a result < 2 * p_25519, which is done by
+// basically skipping the +1 in the quotient estimate and the final
+// optional correction.
+
+#define sqr_4(P0,P1)                            \
+        movq   P1, %rdx ;                       \
+        mulxq  %rdx, %r8, %r15 ;                    \
+        mulxq  0x8+P1, %r9, %r10 ;               \
+        mulxq  0x18+P1, %r11, %r12 ;             \
+        movq   0x10+P1, %rdx ;                  \
+        mulxq  0x18+P1, %r13, %r14 ;             \
+        xorl   %ebx, %ebx ;                        \
+        mulxq  P1, %rax, %rcx ;                  \
+        adcxq  %rax, %r10 ;                        \
+        adoxq  %rcx, %r11 ;                        \
+        mulxq  0x8+P1, %rax, %rcx ;              \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rcx, %r12 ;                        \
+        movq   0x18+P1, %rdx ;                  \
+        mulxq  0x8+P1, %rax, %rcx ;              \
+        adcxq  %rax, %r12 ;                        \
+        adoxq  %rcx, %r13 ;                        \
+        adcxq  %rbx, %r13 ;                        \
+        adoxq  %rbx, %r14 ;                        \
+        adcq   %rbx, %r14 ;                        \
+        xorl   %ebx, %ebx ;                        \
+        adcxq  %r9, %r9 ;                          \
+        adoxq  %r15, %r9 ;                         \
+        movq   0x8+P1, %rdx ;                   \
+        mulxq  %rdx, %rax, %rdx ;                   \
+        adcxq  %r10, %r10 ;                        \
+        adoxq  %rax, %r10 ;                        \
+        adcxq  %r11, %r11 ;                        \
+        adoxq  %rdx, %r11 ;                        \
+        movq   0x10+P1, %rdx ;                  \
+        mulxq  %rdx, %rax, %rdx ;                   \
+        adcxq  %r12, %r12 ;                        \
+        adoxq  %rax, %r12 ;                        \
+        adcxq  %r13, %r13 ;                        \
+        adoxq  %rdx, %r13 ;                        \
+        movq   0x18+P1, %rdx ;                  \
+        mulxq  %rdx, %rax, %r15 ;                   \
+        adcxq  %r14, %r14 ;                        \
+        adoxq  %rax, %r14 ;                        \
+        adcxq  %rbx, %r15 ;                        \
+        adoxq  %rbx, %r15 ;                        \
+        movl   $0x26, %edx ;                       \
+        xorl   %ebx, %ebx ;                        \
+        mulxq  %r12, %rax, %rcx ;                   \
+        adcxq  %rax, %r8 ;                         \
+        adoxq  %rcx, %r9 ;                         \
+        mulxq  %r13, %rax, %rcx ;                   \
+        adcxq  %rax, %r9 ;                         \
+        adoxq  %rcx, %r10 ;                        \
+        mulxq  %r14, %rax, %rcx ;                   \
+        adcxq  %rax, %r10 ;                        \
+        adoxq  %rcx, %r11 ;                        \
+        mulxq  %r15, %rax, %r12 ;                   \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rbx, %r12 ;                        \
+        adcxq  %rbx, %r12 ;                        \
+        shldq  $0x1, %r11, %r12 ;                   \
+        btr    $0x3f, %r11 ;                       \
+        movl   $0x13, %edx ;                       \
+        imulq  %r12, %rdx ;                        \
+        addq   %rdx, %r8 ;                         \
+        adcq   %rbx, %r9 ;                         \
+        adcq   %rbx, %r10 ;                        \
+        adcq   %rbx, %r11 ;                        \
+        movq   %r8, P0 ;                        \
+        movq   %r9, 0x8+P0 ;                    \
+        movq   %r10, 0x10+P0 ;                  \
+        movq   %r11, 0x18+P0
+
+// Plain 4-digit add without any normalization
+// With inputs < p_25519 (indeed < 2^255) it still gives a 4-digit result
+
+#define add_4(P0,P1,P2)                         \
+        movq    P1, %rax ;                      \
+        addq    P2, %rax ;                      \
+        movq    %rax, P0 ;                      \
+        movq    8+P1, %rax ;                    \
+        adcq    8+P2, %rax ;                    \
+        movq    %rax, 8+P0 ;                    \
+        movq    16+P1, %rax ;                   \
+        adcq    16+P2, %rax ;                   \
+        movq    %rax, 16+P0 ;                   \
+        movq    24+P1, %rax ;                   \
+        adcq    24+P2, %rax ;                   \
+        movq    %rax, 24+P0
+
+// Add 5-digit inputs and normalize to 4 digits
+
+#define add5_4(P0,P1,P2)                        \
+        movq    P1, %r8 ;                       \
+        addq    P2, %r8 ;                       \
+        movq    8+P1, %r9 ;                     \
+        adcq    8+P2, %r9 ;                     \
+        movq    16+P1, %r10 ;                   \
+        adcq    16+P2, %r10 ;                   \
+        movq    24+P1, %r11 ;                   \
+        adcq    24+P2, %r11 ;                   \
+        movq    32+P1, %r12 ;                   \
+        adcq    32+P2, %r12 ;                   \
+        xorl    %ebx, %ebx ;                       \
+        shldq  $0x1, %r11, %r12 ;                   \
+        btr    $0x3f, %r11 ;                       \
+        movl   $0x13, %edx ;                       \
+        imulq  %r12, %rdx ;                        \
+        addq   %rdx, %r8 ;                         \
+        adcq   %rbx, %r9 ;                         \
+        adcq   %rbx, %r10 ;                        \
+        adcq   %rbx, %r11 ;                        \
+        movq   %r8, P0 ;                        \
+        movq   %r9, 0x8+P0 ;                    \
+        movq   %r10, 0x10+P0 ;                  \
+        movq   %r11, 0x18+P0
+
+// Subtraction of a pair of numbers < p_25519 just sufficient
+// to give a 4-digit result. It actually always does (x - z) + (2^255-19)
+// which in turn is done by (x - z) - (2^255+19) discarding the 2^256
+// implicitly
+
+#define sub_4(P0,P1,P2)                         \
+        movq    P1, %r8 ;                       \
+        subq    P2, %r8 ;                       \
+        movq    8+P1, %r9 ;                     \
+        sbbq    8+P2, %r9 ;                     \
+        movq    16+P1, %r10 ;                   \
+        sbbq    16+P2, %r10 ;                   \
+        movq    24+P1, %rax ;                   \
+        sbbq    24+P2, %rax ;                   \
+        subq    $19, %r8 ;                         \
+        movq    %r8, P0 ;                       \
+        sbbq    $0, %r9 ;                          \
+        movq    %r9, 8+P0 ;                     \
+        sbbq    $0, %r10 ;                         \
+        movq    %r10, 16+P0 ;                   \
+        sbbq    $0, %rax ;                         \
+        btc     $63, %rax ;                        \
+        movq    %rax, 24+P0
+
+// Modular subtraction with double modulus 2 * p_25519 = 2^256 - 38
+
+#define sub_twice4(P0,P1,P2)                    \
+        movq    P1, %r8 ;                       \
+        xorl    %ebx, %ebx ;                       \
+        subq    P2, %r8 ;                       \
+        movq    8+P1, %r9 ;                     \
+        sbbq    8+P2, %r9 ;                     \
+        movl    $38, %ecx ;                        \
+        movq    16+P1, %r10 ;                   \
+        sbbq    16+P2, %r10 ;                   \
+        movq    24+P1, %rax ;                   \
+        sbbq    24+P2, %rax ;                   \
+        cmovncq %rbx, %rcx ;                       \
+        subq    %rcx, %r8 ;                        \
+        sbbq    %rbx, %r9 ;                        \
+        sbbq    %rbx, %r10 ;                       \
+        sbbq    %rbx, %rax ;                       \
+        movq    %r8, P0 ;                       \
+        movq    %r9, 8+P0 ;                     \
+        movq    %r10, 16+P0 ;                   \
+        movq    %rax, 24+P0
+
+// 5-digit subtraction with upward bias to make it positive, adding
+// 1000 * (2^255 - 19) = 2^256 * 500 - 19000, then normalizing to 4 digits
+
+#define sub5_4(P0,P1,P2)                        \
+        movq    P1, %r8 ;                       \
+        subq    P2, %r8 ;                       \
+        movq    8+P1, %r9 ;                     \
+        sbbq    8+P2, %r9 ;                     \
+        movq    16+P1, %r10 ;                   \
+        sbbq    16+P2, %r10 ;                   \
+        movq    24+P1, %r11 ;                   \
+        sbbq    24+P2, %r11 ;                   \
+        movq    32+P1, %r12 ;                   \
+        sbbq    32+P2, %r12 ;                   \
+        xorl    %ebx, %ebx ;                       \
+        subq    $19000, %r8 ;                      \
+        sbbq    %rbx, %r9 ;                        \
+        sbbq    %rbx, %r10 ;                       \
+        sbbq    %rbx, %r11 ;                       \
+        sbbq    %rbx, %r12 ;                       \
+        addq    $500, %r12 ;                       \
+        shldq  $0x1, %r11, %r12 ;                   \
+        btr    $0x3f, %r11 ;                       \
+        movl   $0x13, %edx ;                       \
+        imulq  %r12, %rdx ;                        \
+        addq   %rdx, %r8 ;                         \
+        adcq   %rbx, %r9 ;                         \
+        adcq   %rbx, %r10 ;                        \
+        adcq   %rbx, %r11 ;                        \
+        movq   %r8, P0 ;                        \
+        movq   %r9, 0x8+P0 ;                    \
+        movq   %r10, 0x10+P0 ;                  \
+        movq   %r11, 0x18+P0
+
+// Combined z = c * x + y with reduction only < 2 * p_25519
+// It is assumed that 19 * (c * x + y) < 2^60 * 2^256 so we
+// don't need a high mul in the final part.
+
+#define cmadd_4(P0,C1,P2,P3)                    \
+        movq    P3, %r8 ;                       \
+        movq    8+P3, %r9 ;                     \
+        movq    16+P3, %r10 ;                   \
+        movq    24+P3, %r11 ;                   \
+        xorl    %edi, %edi ;                       \
+        movq    $C1, %rdx ;                        \
+        mulxq   P2, %rax, %rbx ;                 \
+        adcxq   %rax, %r8 ;                        \
+        adoxq   %rbx, %r9 ;                        \
+        mulxq   8+P2, %rax, %rbx ;               \
+        adcxq   %rax, %r9 ;                        \
+        adoxq   %rbx, %r10 ;                       \
+        mulxq   16+P2, %rax, %rbx ;              \
+        adcxq   %rax, %r10 ;                       \
+        adoxq   %rbx, %r11 ;                       \
+        mulxq   24+P2, %rax, %rbx ;              \
+        adcxq   %rax, %r11 ;                       \
+        adoxq   %rdi, %rbx ;                       \
+        adcxq   %rdi, %rbx ;                       \
+        shldq   $0x1, %r11, %rbx ;                  \
+        btr     $63, %r11 ;                        \
+        movl    $0x13, %edx ;                      \
+        imulq   %rdx, %rbx ;                       \
+        addq    %rbx, %r8 ;                        \
+        adcq    %rdi, %r9 ;                        \
+        adcq    %rdi, %r10 ;                       \
+        adcq    %rdi, %r11 ;                       \
+        movq    %r8, P0 ;                       \
+        movq    %r9, 0x8+P0 ;                   \
+        movq    %r10, 0x10+P0 ;                 \
+        movq    %r11, 0x18+P0
+
+// Multiplex: z := if NZ then x else y
+
+#define mux_4(P0,P1,P2)                         \
+        movq    P1, %rax ;                      \
+        movq    P2, %rcx ;                      \
+        cmovzq  %rcx, %rax ;                       \
+        movq    %rax, P0 ;                      \
+        movq    8+P1, %rax ;                    \
+        movq    8+P2, %rcx ;                    \
+        cmovzq  %rcx, %rax ;                       \
+        movq    %rax, 8+P0 ;                    \
+        movq    16+P1, %rax ;                   \
+        movq    16+P2, %rcx ;                   \
+        cmovzq  %rcx, %rax ;                       \
+        movq    %rax, 16+P0 ;                   \
+        movq    24+P1, %rax ;                   \
+        movq    24+P2, %rcx ;                   \
+        cmovzq  %rcx, %rax ;                       \
+        movq    %rax, 24+P0
+
+S2N_BN_SYMBOL(curve25519_x25519):
+
+#if WINDOWS_ABI
+        pushq   %rdi
+        pushq   %rsi
+        movq    %rcx, %rdi
+        movq    %rdx, %rsi
+        movq    %r8, %rdx
+#endif
+
+// Save registers, make room for temps, preserve input arguments.
+
+        pushq   %rbx
+        pushq   %rbp
+        pushq   %r12
+        pushq   %r13
+        pushq   %r14
+        pushq   %r15
+        subq    $NSPACE, %rsp
+
+// Move the output pointer to a stable place
+
+        movq    %rdi, res
+
+// Copy the inputs to the local variables while mangling them:
+//
+//  - The scalar gets turned into 01xxx...xxx000 by tweaking bits.
+//    Actually the top zero doesn't matter since the loop below
+//    never looks at it, so we don't literally modify that.
+//
+//  - The point x coord is reduced mod 2^255 *then* mod 2^255-19
+
+        movq    (%rsi), %rax
+        andq    $~7, %rax
+        movq    %rax, (%rsp)
+        movq    8(%rsi), %rax
+        movq    %rax, 8(%rsp)
+        movq    16(%rsi), %rax
+        movq    %rax, 16(%rsp)
+        movq    24(%rsi), %rax
+        bts     $62, %rax
+        movq    %rax, 24(%rsp)
+
+        movq    (%rdx), %r8
+        movq    8(%rdx), %r9
+        movq    16(%rdx), %r10
+        movq    24(%rdx), %r11
+        btr     $63, %r11
+        movq    $19, %r12
+        xorq    %r13, %r13
+        xorq    %r14, %r14
+        xorq    %r15, %r15
+        addq    %r8, %r12
+        adcq    %r9, %r13
+        adcq    %r10, %r14
+        adcq    %r11, %r15
+        btr     $63, %r15 // x >= 2^255 - 19 <=> x + 19 >= 2^255
+        cmovcq  %r12, %r8
+        movq    %r8, 32(%rsp)
+        cmovcq  %r13, %r9
+        movq    %r9, 40(%rsp)
+        cmovcq  %r14, %r10
+        movq    %r10, 48(%rsp)
+        cmovcq  %r15, %r11
+        movq    %r11, 56(%rsp)
+
+// Initialize (xn,zn) = (1,0) and (xm,zm) = (x,1) with swap = 0
+// We use the fact that the point x coordinate is still in registers
+
+        movq    $1, %rax
+        movq    %rax, 320(%rsp)
+        movq    %rax, 96(%rsp)
+        xorl    %eax, %eax
+        movq    %rax, swap
+        movq    %rax, 160(%rsp)
+        movq    %rax, 328(%rsp)
+        movq    %rax, 104(%rsp)
+        movq    %rax, 168(%rsp)
+        movq    %rax, 336(%rsp)
+        movq    %rax, 112(%rsp)
+        movq    %rax, 176(%rsp)
+        movq    %rax, 344(%rsp)
+        movq    %rax, 120(%rsp)
+        movq    %rax, 184(%rsp)
+        movq    32(%rsp), %rax
+        movq    %r8, 256(%rsp)
+        movq    %r9, 264(%rsp)
+        movq    %r10, 272(%rsp)
+        movq    %r11, 280(%rsp)
+
+// The outer loop over scalar bits from i = 254, ..., i = 0 (inclusive)
+// This starts at 254, and so implicitly masks bit 255 of the scalar.
+
+        movl    $254, %eax
+        movq    %rax, i
+
+scalarloop:
+
+// sm = xm + zm; sn = xn + zn; dm = xm - zm; dn = xn - zn
+// The adds don't need any normalization as they're fed to muls
+// Just make sure the subs fit in 4 digits.
+
+        sub_4(dm,xm,zm)
+        add_4(sn,xn,zn)
+        sub_4(dn,xn,zn)
+        add_4(sm,xm,zm)
+
+// ADDING: dmsn = dm * sn; dnsm = sm * dn
+// DOUBLING: mux d = xt - zt and s = xt + zt for appropriate choice of (xt,zt)
+
+        mul_5(dmsn,sn,dm)
+
+        movq    i, %rdx
+        movq    %rdx, %rcx
+        shrq    $6, %rdx
+        movq    (%rsp,%rdx,8), %rdx
+        shrq    %cl, %rdx
+        andq    $1, %rdx
+        cmpq    swap, %rdx
+        movq    %rdx, swap
+
+        mux_4(d,dm,dn)
+        mux_4(s,sm,sn)
+
+        mul_5(dnsm,sm,dn)
+
+// DOUBLING: d = (xt - zt)^2 normalized only to 4 digits
+
+        sqr_4(d,d)
+
+// ADDING: dpro = (dmsn - dnsm)^2, spro = (dmsn + dnsm)^2
+// DOUBLING: s = (xt + zt)^2, normalized only to 4 digits
+
+        sub5_4(dpro,dmsn,dnsm)
+        sqr_4(s,s)
+        add5_4(spro,dmsn,dnsm)
+        sqr_4(dpro,dpro)
+
+// DOUBLING: p = 4 * xt * zt = s - d
+
+        sub_twice4(p,s,d)
+
+// ADDING: xm' = (dmsn + dnsm)^2
+
+        sqr_p25519(xm,spro)
+
+// DOUBLING: e = 121666 * p + d
+
+        cmadd_4(e,0x1db42,p,d)
+
+// DOUBLING: xn' = (xt + zt)^2 * (xt - zt)^2 = s * d
+
+        mul_p25519(xn,s,d)
+
+// ADDING: zm' = x * (dmsn - dnsm)^2
+
+        mul_p25519(zm,dpro,pointx)
+
+// DOUBLING: zn' = (4 * xt * zt) * ((xt - zt)^2 + 121666 * (4 * xt * zt))
+//               = p * (d + 121666 * p)
+
+        mul_p25519(zn,p,e)
+
+// Loop down as far as 0 (inclusive)
+
+        movq    i, %rax
+        subq    $1, %rax
+        movq    %rax, i
+        jnc     scalarloop
+
+// Since the scalar was forced to be a multiple of 8, we know it's even.
+// Hence there is no need to multiplex: the projective answer is (xn,zn)
+// and we can ignore (xm,zm); indeed we could have avoided the last three
+// differential additions and just done the doublings.
+// First set up the constant sn = 2^255 - 19 for the modular inverse.
+
+        movq    $-19, %rax
+        movq    $-1, %rcx
+        movq    $0x7fffffffffffffff, %rdx
+        movq    %rax, 128(%rsp)
+        movq    %rcx, 136(%rsp)
+        movq    %rcx, 144(%rsp)
+        movq    %rdx, 152(%rsp)
+
+// Prepare to call the modular inverse function to get zm = 1/zn
+
+        movq    $4, %rdi
+        leaq    96(%rsp), %rsi
+        leaq    160(%rsp), %rdx
+        leaq    128(%rsp), %rcx
+        leaq    192(%rsp), %r8
+
+// Inline copy of bignum_modinv, identical except for stripping out the
+// prologue and epilogue saving and restoring registers and the initial
+// test for k = 0 (which is trivially false here since k = 4). For more
+// details and explanations see "x86/generic/bignum_modinv.S". Note
+// that the stack it uses for its own temporaries is 80 bytes so it
+// only overwrites pointx, scalar and dm, which are no longer needed.
+
+        movq    %rsi, 0x40(%rsp)
+        movq    %r8, 0x38(%rsp)
+        movq    %rcx, 0x48(%rsp)
+        leaq    (%r8,%rdi,8), %r10
+        movq    %r10, 0x30(%rsp)
+        leaq    (%r10,%rdi,8), %r15
+        xorq    %r11, %r11
+        xorq    %r9, %r9
+copyloop:
+        movq    (%rdx,%r9,8), %rax
+        movq    (%rcx,%r9,8), %rbx
+        movq    %rax, (%r10,%r9,8)
+        movq    %rbx, (%r15,%r9,8)
+        movq    %rbx, (%r8,%r9,8)
+        movq    %r11, (%rsi,%r9,8)
+        incq    %r9
+        cmpq    %rdi, %r9
+        jb      copyloop
+        movq    (%r8), %rax
+        movq    %rax, %rbx
+        decq    %rbx
+        movq    %rbx, (%r8)
+        movq    %rax, %rbp
+        movq    %rax, %r12
+        shlq    $0x2, %rbp
+        subq    %rbp, %r12
+        xorq    $0x2, %r12
+        movq    %r12, %rbp
+        imulq   %rax, %rbp
+        movl    $0x2, %eax
+        addq    %rbp, %rax
+        addq    $0x1, %rbp
+        imulq   %rax, %r12
+        imulq   %rbp, %rbp
+        movl    $0x1, %eax
+        addq    %rbp, %rax
+        imulq   %rax, %r12
+        imulq   %rbp, %rbp
+        movl    $0x1, %eax
+        addq    %rbp, %rax
+        imulq   %rax, %r12
+        imulq   %rbp, %rbp
+        movl    $0x1, %eax
+        addq    %rbp, %rax
+        imulq   %rax, %r12
+        movq    %r12, 0x28(%rsp)
+        movq    %rdi, %rax
+        shlq    $0x7, %rax
+        movq    %rax, 0x20(%rsp)
+outerloop:
+        movq    0x20(%rsp), %r13
+        addq    $0x3f, %r13
+        shrq    $0x6, %r13
+        cmpq    %rdi, %r13
+        cmovaeq %rdi, %r13
+        xorq    %r12, %r12
+        xorq    %r14, %r14
+        xorq    %rbp, %rbp
+        xorq    %rsi, %rsi
+        xorq    %r11, %r11
+        movq    0x30(%rsp), %r8
+        leaq    (%r8,%rdi,8), %r15
+        xorq    %r9, %r9
+toploop:
+        movq    (%r8,%r9,8), %rbx
+        movq    (%r15,%r9,8), %rcx
+        movq    %r11, %r10
+        andq    %r12, %r10
+        andq    %rbp, %r11
+        movq    %rbx, %rax
+        orq     %rcx, %rax
+        negq    %rax
+        cmovbq  %r10, %r14
+        cmovbq  %r11, %rsi
+        cmovbq  %rbx, %r12
+        cmovbq  %rcx, %rbp
+        sbbq    %r11, %r11
+        incq    %r9
+        cmpq    %r13, %r9
+        jb      toploop
+        movq    %r12, %rax
+        orq     %rbp, %rax
+        bsrq    %rax, %rcx
+        xorq    $0x3f, %rcx
+        shldq   %cl, %r14, %r12
+        shldq   %cl, %rsi, %rbp
+        movq    (%r8), %rax
+        movq    %rax, %r14
+        movq    (%r15), %rax
+        movq    %rax, %rsi
+        movl    $0x1, %r10d
+        movl    $0x0, %r11d
+        movl    $0x0, %ecx
+        movl    $0x1, %edx
+        movl    $0x3a, %r9d
+        movq    %rdi, 0x8(%rsp)
+        movq    %r13, 0x10(%rsp)
+        movq    %r8, (%rsp)
+        movq    %r15, 0x18(%rsp)
+innerloop:
+        movq    %rbp, %rax
+        movq    %rsi, %rdi
+        movq    %rcx, %r13
+        movq    %rdx, %r15
+        movq    $0x1, %rbx
+        negq    %rdi
+        andq    %r14, %rbx
+        cmoveq  %rbx, %rax
+        cmoveq  %rbx, %rdi
+        cmoveq  %rbx, %r13
+        cmoveq  %rbx, %r15
+        movq    %r12, %rbx
+        addq    %r14, %rdi
+        movq    %rdi, %r8
+        negq    %rdi
+        subq    %rax, %rbx
+        cmovbq  %r12, %rbp
+        cmovbq  %r14, %rsi
+        cmovbq  %r10, %rcx
+        cmovbq  %r11, %rdx
+        cmovaeq %r8, %rdi
+        movq    %rbx, %r12
+        notq    %rbx
+        incq    %rbx
+        cmovbq  %rbx, %r12
+        movq    %rdi, %r14
+        addq    %r13, %r10
+        addq    %r15, %r11
+        shrq    $1, %r12
+        shrq    $1, %r14
+        leaq    (%rcx,%rcx), %rcx
+        leaq    (%rdx,%rdx), %rdx
+        decq    %r9
+        jne     innerloop
+        movq    0x8(%rsp), %rdi
+        movq    0x10(%rsp), %r13
+        movq    (%rsp), %r8
+        movq    0x18(%rsp), %r15
+        movq    %r10, (%rsp)
+        movq    %r11, 0x8(%rsp)
+        movq    %rcx, 0x10(%rsp)
+        movq    %rdx, 0x18(%rsp)
+        movq    0x38(%rsp), %r8
+        movq    0x40(%rsp), %r15
+        xorq    %r14, %r14
+        xorq    %rsi, %rsi
+        xorq    %r10, %r10
+        xorq    %r11, %r11
+        xorq    %r9, %r9
+congloop:
+        movq    (%r8,%r9,8), %rcx
+        movq    (%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %r14
+        adcq    $0x0, %rdx
+        movq    %rdx, %r12
+        movq    0x10(%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %rsi
+        adcq    $0x0, %rdx
+        movq    %rdx, %rbp
+        movq    (%r15,%r9,8), %rcx
+        movq    0x8(%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %r14
+        adcq    %rdx, %r12
+        shrdq   $0x3a, %r14, %r10
+        movq    %r10, (%r8,%r9,8)
+        movq    %r14, %r10
+        movq    %r12, %r14
+        movq    0x18(%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %rsi
+        adcq    %rdx, %rbp
+        shrdq   $0x3a, %rsi, %r11
+        movq    %r11, (%r15,%r9,8)
+        movq    %rsi, %r11
+        movq    %rbp, %rsi
+        incq    %r9
+        cmpq    %rdi, %r9
+        jb      congloop
+        shldq   $0x6, %r10, %r14
+        shldq   $0x6, %r11, %rsi
+        movq    0x48(%rsp), %r15
+        movq    (%r8), %rbx
+        movq    0x28(%rsp), %r12
+        imulq   %rbx, %r12
+        movq    (%r15), %rax
+        mulq    %r12
+        addq    %rbx, %rax
+        movq    %rdx, %r10
+        movl    $0x1, %r9d
+        movq    %rdi, %rcx
+        decq    %rcx
+        je      wmontend
+wmontloop:
+        adcq    (%r8,%r9,8), %r10
+        sbbq    %rbx, %rbx
+        movq    (%r15,%r9,8), %rax
+        mulq    %r12
+        subq    %rbx, %rdx
+        addq    %r10, %rax
+        movq    %rax, -0x8(%r8,%r9,8)
+        movq    %rdx, %r10
+        incq    %r9
+        decq    %rcx
+        jne     wmontloop
+wmontend:
+        adcq    %r14, %r10
+        movq    %r10, -0x8(%r8,%rdi,8)
+        sbbq    %r10, %r10
+        negq    %r10
+        movq    %rdi, %rcx
+        xorq    %r9, %r9
+wcmploop:
+        movq    (%r8,%r9,8), %rax
+        sbbq    (%r15,%r9,8), %rax
+        incq    %r9
+        decq    %rcx
+        jne     wcmploop
+        sbbq    $0x0, %r10
+        sbbq    %r10, %r10
+        notq    %r10
+        xorq    %rcx, %rcx
+        xorq    %r9, %r9
+wcorrloop:
+        movq    (%r8,%r9,8), %rax
+        movq    (%r15,%r9,8), %rbx
+        andq    %r10, %rbx
+        negq    %rcx
+        sbbq    %rbx, %rax
+        sbbq    %rcx, %rcx
+        movq    %rax, (%r8,%r9,8)
+        incq    %r9
+        cmpq    %rdi, %r9
+        jb      wcorrloop
+        movq    0x40(%rsp), %r8
+        movq    (%r8), %rbx
+        movq    0x28(%rsp), %rbp
+        imulq   %rbx, %rbp
+        movq    (%r15), %rax
+        mulq    %rbp
+        addq    %rbx, %rax
+        movq    %rdx, %r11
+        movl    $0x1, %r9d
+        movq    %rdi, %rcx
+        decq    %rcx
+        je      zmontend
+zmontloop:
+        adcq    (%r8,%r9,8), %r11
+        sbbq    %rbx, %rbx
+        movq    (%r15,%r9,8), %rax
+        mulq    %rbp
+        subq    %rbx, %rdx
+        addq    %r11, %rax
+        movq    %rax, -0x8(%r8,%r9,8)
+        movq    %rdx, %r11
+        incq    %r9
+        decq    %rcx
+        jne     zmontloop
+zmontend:
+        adcq    %rsi, %r11
+        movq    %r11, -0x8(%r8,%rdi,8)
+        sbbq    %r11, %r11
+        negq    %r11
+        movq    %rdi, %rcx
+        xorq    %r9, %r9
+zcmploop:
+        movq    (%r8,%r9,8), %rax
+        sbbq    (%r15,%r9,8), %rax
+        incq    %r9
+        decq    %rcx
+        jne     zcmploop
+        sbbq    $0x0, %r11
+        sbbq    %r11, %r11
+        notq    %r11
+        xorq    %rcx, %rcx
+        xorq    %r9, %r9
+zcorrloop:
+        movq    (%r8,%r9,8), %rax
+        movq    (%r15,%r9,8), %rbx
+        andq    %r11, %rbx
+        negq    %rcx
+        sbbq    %rbx, %rax
+        sbbq    %rcx, %rcx
+        movq    %rax, (%r8,%r9,8)
+        incq    %r9
+        cmpq    %rdi, %r9
+        jb      zcorrloop
+        movq    0x30(%rsp), %r8
+        leaq    (%r8,%rdi,8), %r15
+        xorq    %r9, %r9
+        xorq    %r12, %r12
+        xorq    %r14, %r14
+        xorq    %rbp, %rbp
+        xorq    %rsi, %rsi
+crossloop:
+        movq    (%r8,%r9,8), %rcx
+        movq    (%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %r14
+        adcq    $0x0, %rdx
+        movq    %rdx, %r10
+        movq    0x10(%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %rsi
+        adcq    $0x0, %rdx
+        movq    %rdx, %r11
+        movq    (%r15,%r9,8), %rcx
+        movq    0x8(%rsp), %rax
+        mulq    %rcx
+        subq    %r12, %rdx
+        subq    %rax, %r14
+        sbbq    %rdx, %r10
+        sbbq    %r12, %r12
+        movq    %r14, (%r8,%r9,8)
+        movq    %r10, %r14
+        movq    0x18(%rsp), %rax
+        mulq    %rcx
+        subq    %rbp, %rdx
+        subq    %rax, %rsi
+        sbbq    %rdx, %r11
+        sbbq    %rbp, %rbp
+        movq    %rsi, (%r15,%r9,8)
+        movq    %r11, %rsi
+        incq    %r9
+        cmpq    %r13, %r9
+        jb      crossloop
+        xorq    %r9, %r9
+        movq    %r12, %r10
+        movq    %rbp, %r11
+        xorq    %r12, %r14
+        xorq    %rbp, %rsi
+optnegloop:
+        movq    (%r8,%r9,8), %rax
+        xorq    %r12, %rax
+        negq    %r10
+        adcq    $0x0, %rax
+        sbbq    %r10, %r10
+        movq    %rax, (%r8,%r9,8)
+        movq    (%r15,%r9,8), %rax
+        xorq    %rbp, %rax
+        negq    %r11
+        adcq    $0x0, %rax
+        sbbq    %r11, %r11
+        movq    %rax, (%r15,%r9,8)
+        incq    %r9
+        cmpq    %r13, %r9
+        jb      optnegloop
+        subq    %r10, %r14
+        subq    %r11, %rsi
+        movq    %r13, %r9
+shiftloop:
+        movq    -0x8(%r8,%r9,8), %rax
+        movq    %rax, %r10
+        shrdq   $0x3a, %r14, %rax
+        movq    %rax, -0x8(%r8,%r9,8)
+        movq    %r10, %r14
+        movq    -0x8(%r15,%r9,8), %rax
+        movq    %rax, %r11
+        shrdq   $0x3a, %rsi, %rax
+        movq    %rax, -0x8(%r15,%r9,8)
+        movq    %r11, %rsi
+        decq    %r9
+        jne     shiftloop
+        notq    %rbp
+        movq    0x48(%rsp), %rcx
+        movq    0x38(%rsp), %r8
+        movq    0x40(%rsp), %r15
+        movq    %r12, %r10
+        movq    %rbp, %r11
+        xorq    %r9, %r9
+fliploop:
+        movq    %rbp, %rdx
+        movq    (%rcx,%r9,8), %rax
+        andq    %rax, %rdx
+        andq    %r12, %rax
+        movq    (%r8,%r9,8), %rbx
+        xorq    %r12, %rbx
+        negq    %r10
+        adcq    %rbx, %rax
+        sbbq    %r10, %r10
+        movq    %rax, (%r8,%r9,8)
+        movq    (%r15,%r9,8), %rbx
+        xorq    %rbp, %rbx
+        negq    %r11
+        adcq    %rbx, %rdx
+        sbbq    %r11, %r11
+        movq    %rdx, (%r15,%r9,8)
+        incq    %r9
+        cmpq    %rdi, %r9
+        jb      fliploop
+        subq    $0x3a,  0x20(%rsp)
+        ja      outerloop
+
+// Since we eventually want to return 0 when the result is the point at
+// infinity, we force xn = 0 whenever zn = 0. This avoids building in a
+// dependency on the behavior of modular inverse in out-of-scope cases.
+
+        movq    160(%rsp), %rax
+        orq     168(%rsp), %rax
+        orq     176(%rsp), %rax
+        orq     184(%rsp), %rax
+        movq    320(%rsp), %rcx
+        cmovzq  %rax, %rcx
+        movq    %rcx, 320(%rsp)
+        movq    328(%rsp), %rcx
+        cmovzq  %rax, %rcx
+        movq    %rcx, 328(%rsp)
+        movq    336(%rsp), %rcx
+        cmovzq  %rax, %rcx
+        movq    %rcx, 336(%rsp)
+        movq    344(%rsp), %rcx
+        cmovzq  %rax, %rcx
+        movq    %rcx, 344(%rsp)
+
+// Now the result is xn * (1/zn).
+
+        movq    res, %rbp
+        mul_p25519(resx,xn,zm)
+
+// Restore stack and registers
+
+        addq    $NSPACE, %rsp
+
+        popq    %r15
+        popq    %r14
+        popq    %r13
+        popq    %r12
+        popq    %rbp
+        popq    %rbx
+
+#if WINDOWS_ABI
+        popq   %rsi
+        popq   %rdi
+#endif
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack, "", %progbits
+#endif
diff --git a/x86_att/curve25519/curve25519_x25519_alt.S b/x86_att/curve25519/curve25519_x25519_alt.S
new file mode 100644
index 000000000..52600bede
--- /dev/null
+++ b/x86_att/curve25519/curve25519_x25519_alt.S
@@ -0,0 +1,1413 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// The x25519 function for curve25519
+// Inputs scalar[4], point[4]; output res[4]
+//
+// extern void curve25519_x25519_alt
+//   (uint64_t res[static 4],uint64_t scalar[static 4],uint64_t point[static 4])
+//
+// Given a scalar n and the X coordinate of an input point P = (X,Y) on
+// curve25519 (Y can live in any extension field of characteristic 2^255-19),
+// this returns the X coordinate of n * P = (X, Y), or 0 when n * P is the
+// point at infinity. Both n and X inputs are first slightly modified/mangled
+// as specified in the relevant RFC (https://www.rfc-editor.org/rfc/rfc7748);
+// in particular the lower three bits of n are set to zero.
+//
+// Standard x86-64 ABI: RDI = res, RSI = scalar, RDX = point
+// Microsoft x64 ABI:   RCX = res, RDX = scalar, R8 = point
+// ----------------------------------------------------------------------------
+#include "_internal_s2n_bignum.h"
+
+
+        S2N_BN_SYM_VISIBILITY_DIRECTIVE(curve25519_x25519_alt)
+        S2N_BN_SYM_PRIVACY_DIRECTIVE(curve25519_x25519_alt)
+        .text
+
+// Size of individual field elements
+
+#define NUMSIZE 32
+
+// Stable homes for the input result argument during the whole body
+// and other variables that are only needed prior to the modular inverse.
+
+#define res  12*NUMSIZE(%rsp)
+#define i  12*NUMSIZE+8(%rsp)
+#define swap  12*NUMSIZE+16(%rsp)
+
+// Pointers to result x coord to be written, assuming the base "res"
+// has been loaded into %rbp
+
+#define resx 0(%rbp)
+
+// Pointer-offset pairs for temporaries on stack with some aliasing.
+// Both dmsn and dnsm need space for >= 5 digits, and we allocate 8
+
+#define scalar (0*NUMSIZE)(%rsp)
+
+#define pointx (1*NUMSIZE)(%rsp)
+
+#define dm (2*NUMSIZE)(%rsp)
+
+#define zm (3*NUMSIZE)(%rsp)
+#define sm (3*NUMSIZE)(%rsp)
+#define dpro (3*NUMSIZE)(%rsp)
+
+#define sn (4*NUMSIZE)(%rsp)
+
+#define zn (5*NUMSIZE)(%rsp)
+#define dn (5*NUMSIZE)(%rsp)
+#define e (5*NUMSIZE)(%rsp)
+
+#define dmsn (6*NUMSIZE)(%rsp)
+#define p (6*NUMSIZE)(%rsp)
+
+#define xm (8*NUMSIZE)(%rsp)
+#define dnsm (8*NUMSIZE)(%rsp)
+#define spro (8*NUMSIZE)(%rsp)
+
+#define xn (10*NUMSIZE)(%rsp)
+#define s (10*NUMSIZE)(%rsp)
+
+#define d (11*NUMSIZE)(%rsp)
+
+// Total size to reserve on the stack
+// This includes space for the 3 other variables above
+// and rounds up to a multiple of 32
+
+#define NSPACE (13*NUMSIZE)
+
+// Macros wrapping up the basic field operation calls
+// bignum_mul_p25519_alt and bignum_sqr_p25519_alt.
+// These two are only trivially different from pure
+// function calls to those subroutines.
+
+#define mul_p25519(P0,P1,P2)                    \
+        movq    P1, %rax ;                      \
+        mulq     P2;                 \
+        movq    %rax, %r8 ;                         \
+        movq    %rdx, %r9 ;                         \
+        xorq    %r10, %r10 ;                        \
+        xorq    %r11, %r11 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x8+P2;             \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     P2;                 \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        adcq    $0x0, %r11 ;                        \
+        xorq    %r12, %r12 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x10+P2;            \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        adcq    %r12, %r12 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     0x8+P2;             \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        adcq    $0x0, %r12 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     P2;                 \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        adcq    $0x0, %r12 ;                        \
+        xorq    %r13, %r13 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x18+P2;            \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    %r13, %r13 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     0x10+P2;            \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    $0x0, %r13 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     0x8+P2;             \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    $0x0, %r13 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq     P2;                 \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    $0x0, %r13 ;                        \
+        xorq    %r14, %r14 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     0x18+P2;            \
+        addq    %rax, %r12 ;                        \
+        adcq    %rdx, %r13 ;                        \
+        adcq    %r14, %r14 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     0x10+P2;            \
+        addq    %rax, %r12 ;                        \
+        adcq    %rdx, %r13 ;                        \
+        adcq    $0x0, %r14 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq     0x8+P2;             \
+        addq    %rax, %r12 ;                        \
+        adcq    %rdx, %r13 ;                        \
+        adcq    $0x0, %r14 ;                        \
+        xorq    %r15, %r15 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     0x18+P2;            \
+        addq    %rax, %r13 ;                        \
+        adcq    %rdx, %r14 ;                        \
+        adcq    %r15, %r15 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq     0x10+P2;            \
+        addq    %rax, %r13 ;                        \
+        adcq    %rdx, %r14 ;                        \
+        adcq    $0x0, %r15 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq     0x18+P2;            \
+        addq    %rax, %r14 ;                        \
+        adcq    %rdx, %r15 ;                        \
+        movl    $0x26, %esi ;                       \
+        movq    %r12, %rax ;                        \
+        mulq    %rsi;                            \
+        addq    %rax, %r8 ;                         \
+        adcq    %rdx, %r9 ;                         \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r13, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r14, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r15, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        xorq    %rcx, %rcx ;                        \
+        addq    %rax, %r11 ;                        \
+        movq    %rdx, %r12 ;                        \
+        adcq    %rcx, %r12 ;                        \
+        shldq   $0x1, %r11, %r12 ;                    \
+        leaq    0x1(%r12), %rax ;                  \
+        movl    $0x13, %esi ;                       \
+        movabsq $0x8000000000000000, %r12 ;         \
+        orq     %r12, %r11 ;                        \
+        imulq   %rsi, %rax ;                        \
+        addq    %rax, %r8 ;                         \
+        adcq    %rcx, %r9 ;                         \
+        adcq    %rcx, %r10 ;                        \
+        adcq    %rcx, %r11 ;                        \
+        sbbq    %rax, %rax ;                        \
+        notq    %rax;                            \
+        andq    %rsi, %rax ;                        \
+        subq    %rax, %r8 ;                         \
+        sbbq    %rcx, %r9 ;                         \
+        sbbq    %rcx, %r10 ;                        \
+        sbbq    %rcx, %r11 ;                        \
+        notq    %r12;                            \
+        andq    %r12, %r11 ;                        \
+        movq    %r8, P0 ;                        \
+        movq    %r9, 0x8+P0 ;                    \
+        movq    %r10, 0x10+P0 ;                  \
+        movq    %r11, 0x18+P0
+
+#define sqr_p25519(P0,P1)                       \
+        movq    P1, %rax ;                      \
+        mulq    %rax;                            \
+        movq    %rax, %r8 ;                         \
+        movq    %rdx, %r9 ;                         \
+        xorq    %r10, %r10 ;                        \
+        xorq    %r11, %r11 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x8+P1;             \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r11 ;                        \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        adcq    $0x0, %r11 ;                        \
+        xorq    %r12, %r12 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq    %rax;                            \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        adcq    $0x0, %r12 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x10+P1;            \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r12 ;                        \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        adcq    $0x0, %r12 ;                        \
+        xorq    %r13, %r13 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x18+P1;            \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r13 ;                        \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    $0x0, %r13 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     0x10+P1;            \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r13 ;                        \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    $0x0, %r13 ;                        \
+        xorq    %r14, %r14 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     0x18+P1;            \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r14 ;                        \
+        addq    %rax, %r12 ;                        \
+        adcq    %rdx, %r13 ;                        \
+        adcq    $0x0, %r14 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq    %rax;                            \
+        addq    %rax, %r12 ;                        \
+        adcq    %rdx, %r13 ;                        \
+        adcq    $0x0, %r14 ;                        \
+        xorq    %r15, %r15 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     0x18+P1;            \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r15 ;                        \
+        addq    %rax, %r13 ;                        \
+        adcq    %rdx, %r14 ;                        \
+        adcq    $0x0, %r15 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq    %rax;                            \
+        addq    %rax, %r14 ;                        \
+        adcq    %rdx, %r15 ;                        \
+        movl    $0x26, %esi ;                       \
+        movq    %r12, %rax ;                        \
+        mulq    %rsi;                            \
+        addq    %rax, %r8 ;                         \
+        adcq    %rdx, %r9 ;                         \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r13, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r14, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r15, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        xorq    %rcx, %rcx ;                        \
+        addq    %rax, %r11 ;                        \
+        movq    %rdx, %r12 ;                        \
+        adcq    %rcx, %r12 ;                        \
+        shldq   $0x1, %r11, %r12 ;                    \
+        leaq    0x1(%r12), %rax ;                  \
+        movl    $0x13, %esi ;                       \
+        movabsq $0x8000000000000000, %r12 ;         \
+        orq     %r12, %r11 ;                        \
+        imulq   %rsi, %rax ;                        \
+        addq    %rax, %r8 ;                         \
+        adcq    %rcx, %r9 ;                         \
+        adcq    %rcx, %r10 ;                        \
+        adcq    %rcx, %r11 ;                        \
+        sbbq    %rax, %rax ;                        \
+        notq    %rax;                            \
+        andq    %rsi, %rax ;                        \
+        subq    %rax, %r8 ;                         \
+        sbbq    %rcx, %r9 ;                         \
+        sbbq    %rcx, %r10 ;                        \
+        sbbq    %rcx, %r11 ;                        \
+        notq    %r12;                            \
+        andq    %r12, %r11 ;                        \
+        movq    %r8, P0 ;                        \
+        movq    %r9, 0x8+P0 ;                    \
+        movq    %r10, 0x10+P0 ;                  \
+        movq    %r11, 0x18+P0
+
+// Multiplication just giving a 5-digit result (actually < 39 * p_25519)
+// by not doing anything beyond the first stage of reduction
+
+#define mul_5(P0,P1,P2)                         \
+        movq    P1, %rax ;                      \
+        mulq     P2;                 \
+        movq    %rax, %r8 ;                         \
+        movq    %rdx, %r9 ;                         \
+        xorq    %r10, %r10 ;                        \
+        xorq    %r11, %r11 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x8+P2;             \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     P2;                 \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        adcq    $0x0, %r11 ;                        \
+        xorq    %r12, %r12 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x10+P2;            \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        adcq    %r12, %r12 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     0x8+P2;             \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        adcq    $0x0, %r12 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     P2;                 \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        adcq    $0x0, %r12 ;                        \
+        xorq    %r13, %r13 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x18+P2;            \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    %r13, %r13 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     0x10+P2;            \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    $0x0, %r13 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     0x8+P2;             \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    $0x0, %r13 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq     P2;                 \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    $0x0, %r13 ;                        \
+        xorq    %r14, %r14 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     0x18+P2;            \
+        addq    %rax, %r12 ;                        \
+        adcq    %rdx, %r13 ;                        \
+        adcq    %r14, %r14 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     0x10+P2;            \
+        addq    %rax, %r12 ;                        \
+        adcq    %rdx, %r13 ;                        \
+        adcq    $0x0, %r14 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq     0x8+P2;             \
+        addq    %rax, %r12 ;                        \
+        adcq    %rdx, %r13 ;                        \
+        adcq    $0x0, %r14 ;                        \
+        xorq    %r15, %r15 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     0x18+P2;            \
+        addq    %rax, %r13 ;                        \
+        adcq    %rdx, %r14 ;                        \
+        adcq    %r15, %r15 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq     0x10+P2;            \
+        addq    %rax, %r13 ;                        \
+        adcq    %rdx, %r14 ;                        \
+        adcq    $0x0, %r15 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq     0x18+P2;            \
+        addq    %rax, %r14 ;                        \
+        adcq    %rdx, %r15 ;                        \
+        movl    $0x26, %esi ;                       \
+        movq    %r12, %rax ;                        \
+        mulq    %rsi;                            \
+        addq    %rax, %r8 ;                         \
+        adcq    %rdx, %r9 ;                         \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r13, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r14, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r15, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        xorq    %rcx, %rcx ;                        \
+        addq    %rax, %r11 ;                        \
+        movq    %rdx, %r12 ;                        \
+        adcq    %rcx, %r12 ;                        \
+        movq    %r8, P0 ;                       \
+        movq    %r9, 0x8+P0 ;                   \
+        movq    %r10, 0x10+P0 ;                 \
+        movq    %r11, 0x18+P0 ;                 \
+        movq    %r12, 0x20+P0
+
+// Squaring just giving a result < 2 * p_25519, which is done by
+// basically skipping the +1 in the quotient estimate and the final
+// optional correction.
+
+#define sqr_4(P0,P1)                            \
+        movq    P1, %rax ;                      \
+        mulq    %rax;                            \
+        movq    %rax, %r8 ;                         \
+        movq    %rdx, %r9 ;                         \
+        xorq    %r10, %r10 ;                        \
+        xorq    %r11, %r11 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x8+P1;             \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r11 ;                        \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        adcq    $0x0, %r11 ;                        \
+        xorq    %r12, %r12 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq    %rax;                            \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        adcq    $0x0, %r12 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x10+P1;            \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r12 ;                        \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        adcq    $0x0, %r12 ;                        \
+        xorq    %r13, %r13 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x18+P1;            \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r13 ;                        \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    $0x0, %r13 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     0x10+P1;            \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r13 ;                        \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    $0x0, %r13 ;                        \
+        xorq    %r14, %r14 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     0x18+P1;            \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r14 ;                        \
+        addq    %rax, %r12 ;                        \
+        adcq    %rdx, %r13 ;                        \
+        adcq    $0x0, %r14 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq    %rax;                            \
+        addq    %rax, %r12 ;                        \
+        adcq    %rdx, %r13 ;                        \
+        adcq    $0x0, %r14 ;                        \
+        xorq    %r15, %r15 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     0x18+P1;            \
+        addq    %rax, %rax ;                        \
+        adcq    %rdx, %rdx ;                        \
+        adcq    $0x0, %r15 ;                        \
+        addq    %rax, %r13 ;                        \
+        adcq    %rdx, %r14 ;                        \
+        adcq    $0x0, %r15 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq    %rax;                            \
+        addq    %rax, %r14 ;                        \
+        adcq    %rdx, %r15 ;                        \
+        movl    $0x26, %esi ;                       \
+        movq    %r12, %rax ;                        \
+        mulq    %rsi;                            \
+        addq    %rax, %r8 ;                         \
+        adcq    %rdx, %r9 ;                         \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r13, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r14, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        sbbq    %rcx, %rcx ;                        \
+        movq    %r15, %rax ;                        \
+        mulq    %rsi;                            \
+        subq    %rcx, %rdx ;                        \
+        xorq    %rcx, %rcx ;                        \
+        addq    %rax, %r11 ;                        \
+        movq    %rdx, %r12 ;                        \
+        adcq    %rcx, %r12 ;                        \
+        shldq   $0x1, %r11, %r12 ;                  \
+        btr     $0x3f, %r11 ;                      \
+        movl    $0x13, %edx ;                      \
+        imulq   %r12, %rdx ;                       \
+        addq    %rdx, %r8 ;                        \
+        adcq    %rcx, %r9 ;                        \
+        adcq    %rcx, %r10 ;                       \
+        adcq    %rcx, %r11 ;                       \
+        movq    %r8, P0 ;                       \
+        movq    %r9, 0x8+P0 ;                   \
+        movq    %r10, 0x10+P0 ;                 \
+        movq    %r11, 0x18+P0
+
+// Plain 4-digit add without any normalization
+// With inputs < p_25519 (indeed < 2^255) it still gives a 4-digit result
+
+#define add_4(P0,P1,P2)                         \
+        movq    P1, %rax ;                      \
+        addq    P2, %rax ;                      \
+        movq    %rax, P0 ;                      \
+        movq    8+P1, %rax ;                    \
+        adcq    8+P2, %rax ;                    \
+        movq    %rax, 8+P0 ;                    \
+        movq    16+P1, %rax ;                   \
+        adcq    16+P2, %rax ;                   \
+        movq    %rax, 16+P0 ;                   \
+        movq    24+P1, %rax ;                   \
+        adcq    24+P2, %rax ;                   \
+        movq    %rax, 24+P0
+
+// Add 5-digit inputs and normalize to 4 digits
+
+#define add5_4(P0,P1,P2)                        \
+        movq    P1, %r8 ;                       \
+        addq    P2, %r8 ;                       \
+        movq    8+P1, %r9 ;                     \
+        adcq    8+P2, %r9 ;                     \
+        movq    16+P1, %r10 ;                   \
+        adcq    16+P2, %r10 ;                   \
+        movq    24+P1, %r11 ;                   \
+        adcq    24+P2, %r11 ;                   \
+        movq    32+P1, %r12 ;                   \
+        adcq    32+P2, %r12 ;                   \
+        xorl    %ebx, %ebx ;                       \
+        shldq  $0x1, %r11, %r12 ;                   \
+        btr    $0x3f, %r11 ;                       \
+        movl   $0x13, %edx ;                       \
+        imulq  %r12, %rdx ;                        \
+        addq   %rdx, %r8 ;                         \
+        adcq   %rbx, %r9 ;                         \
+        adcq   %rbx, %r10 ;                        \
+        adcq   %rbx, %r11 ;                        \
+        movq   %r8, P0 ;                        \
+        movq   %r9, 0x8+P0 ;                    \
+        movq   %r10, 0x10+P0 ;                  \
+        movq   %r11, 0x18+P0
+
+// Subtraction of a pair of numbers < p_25519 just sufficient
+// to give a 4-digit result. It actually always does (x - z) + (2^255-19)
+// which in turn is done by (x - z) - (2^255+19) discarding the 2^256
+// implicitly
+
+#define sub_4(P0,P1,P2)                         \
+        movq    P1, %r8 ;                       \
+        subq    P2, %r8 ;                       \
+        movq    8+P1, %r9 ;                     \
+        sbbq    8+P2, %r9 ;                     \
+        movq    16+P1, %r10 ;                   \
+        sbbq    16+P2, %r10 ;                   \
+        movq    24+P1, %rax ;                   \
+        sbbq    24+P2, %rax ;                   \
+        subq    $19, %r8 ;                         \
+        movq    %r8, P0 ;                       \
+        sbbq    $0, %r9 ;                          \
+        movq    %r9, 8+P0 ;                     \
+        sbbq    $0, %r10 ;                         \
+        movq    %r10, 16+P0 ;                   \
+        sbbq    $0, %rax ;                         \
+        btc     $63, %rax ;                        \
+        movq    %rax, 24+P0
+
+// Modular subtraction with double modulus 2 * p_25519 = 2^256 - 38
+
+#define sub_twice4(P0,P1,P2)                    \
+        movq    P1, %r8 ;                       \
+        xorl    %ebx, %ebx ;                       \
+        subq    P2, %r8 ;                       \
+        movq    8+P1, %r9 ;                     \
+        sbbq    8+P2, %r9 ;                     \
+        movl    $38, %ecx ;                        \
+        movq    16+P1, %r10 ;                   \
+        sbbq    16+P2, %r10 ;                   \
+        movq    24+P1, %rax ;                   \
+        sbbq    24+P2, %rax ;                   \
+        cmovncq %rbx, %rcx ;                       \
+        subq    %rcx, %r8 ;                        \
+        sbbq    %rbx, %r9 ;                        \
+        sbbq    %rbx, %r10 ;                       \
+        sbbq    %rbx, %rax ;                       \
+        movq    %r8, P0 ;                       \
+        movq    %r9, 8+P0 ;                     \
+        movq    %r10, 16+P0 ;                   \
+        movq    %rax, 24+P0
+
+// 5-digit subtraction with upward bias to make it positive, adding
+// 1000 * (2^255 - 19) = 2^256 * 500 - 19000, then normalizing to 4 digits
+
+#define sub5_4(P0,P1,P2)                        \
+        movq    P1, %r8 ;                       \
+        subq    P2, %r8 ;                       \
+        movq    8+P1, %r9 ;                     \
+        sbbq    8+P2, %r9 ;                     \
+        movq    16+P1, %r10 ;                   \
+        sbbq    16+P2, %r10 ;                   \
+        movq    24+P1, %r11 ;                   \
+        sbbq    24+P2, %r11 ;                   \
+        movq    32+P1, %r12 ;                   \
+        sbbq    32+P2, %r12 ;                   \
+        xorl    %ebx, %ebx ;                       \
+        subq    $19000, %r8 ;                      \
+        sbbq    %rbx, %r9 ;                        \
+        sbbq    %rbx, %r10 ;                       \
+        sbbq    %rbx, %r11 ;                       \
+        sbbq    %rbx, %r12 ;                       \
+        addq    $500, %r12 ;                       \
+        shldq   $0x1, %r11, %r12 ;                  \
+        btr     $0x3f, %r11 ;                      \
+        movl    $0x13, %edx ;                      \
+        imulq   %r12, %rdx ;                       \
+        addq    %rdx, %r8 ;                        \
+        adcq    %rbx, %r9 ;                        \
+        adcq    %rbx, %r10 ;                       \
+        adcq    %rbx, %r11 ;                       \
+        movq    %r8, P0 ;                       \
+        movq    %r9, 0x8+P0 ;                   \
+        movq    %r10, 0x10+P0 ;                 \
+        movq    %r11, 0x18+P0
+
+// Combined z = c * x + y with reduction only < 2 * p_25519
+// It is assumed that 19 * (c * x + y) < 2^60 * 2^256 so we
+// don't need a high mul in the final part.
+
+#define cmadd_4(P0,C1,P2,P3)                    \
+        movq    $C1, %rsi ;                         \
+        movq    P2, %rax ;                       \
+        mulq    %rsi;                            \
+        movq    %rax, %r8 ;                         \
+        movq    %rdx, %r9 ;                         \
+        movq    0x8+P2, %rax ;                   \
+        xorq    %r10, %r10 ;                        \
+        mulq    %rsi;                            \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        movq    0x10+P2, %rax ;                  \
+        mulq    %rsi;                            \
+        addq    %rax, %r10 ;                        \
+        adcq    $0x0, %rdx ;                        \
+        movq    0x18+P2, %rax ;                  \
+        movq    %rdx, %r11 ;                        \
+        mulq    %rsi;                            \
+        xorl    %esi, %esi ;                        \
+        addq    %rax, %r11 ;                        \
+        adcq    %rsi, %rdx ;                        \
+        addq    P3, %r8 ;                        \
+        adcq    0x8+P3, %r9 ;                    \
+        adcq    0x10+P3, %r10 ;                  \
+        adcq    0x18+P3, %r11 ;                  \
+        adcq    %rsi, %rdx ;                        \
+        shldq   $0x1, %r11, %rdx ;                  \
+        btr     $63, %r11 ;                        \
+        movl    $0x13, %ebx ;                      \
+        imulq   %rbx, %rdx ;                       \
+        addq    %rdx, %r8 ;                        \
+        adcq    %rsi, %r9 ;                        \
+        adcq    %rsi, %r10 ;                       \
+        adcq    %rsi, %r11 ;                       \
+        movq    %r8, P0 ;                       \
+        movq    %r9, 0x8+P0 ;                   \
+        movq    %r10, 0x10+P0 ;                 \
+        movq    %r11, 0x18+P0
+
+// Multiplex: z := if NZ then x else y
+
+#define mux_4(P0,P1,P2)                         \
+        movq    P1, %rax ;                      \
+        movq    P2, %rcx ;                      \
+        cmovzq  %rcx, %rax ;                       \
+        movq    %rax, P0 ;                      \
+        movq    8+P1, %rax ;                    \
+        movq    8+P2, %rcx ;                    \
+        cmovzq  %rcx, %rax ;                       \
+        movq    %rax, 8+P0 ;                    \
+        movq    16+P1, %rax ;                   \
+        movq    16+P2, %rcx ;                   \
+        cmovzq  %rcx, %rax ;                       \
+        movq    %rax, 16+P0 ;                   \
+        movq    24+P1, %rax ;                   \
+        movq    24+P2, %rcx ;                   \
+        cmovzq  %rcx, %rax ;                       \
+        movq    %rax, 24+P0
+
+S2N_BN_SYMBOL(curve25519_x25519_alt):
+
+#if WINDOWS_ABI
+        pushq   %rdi
+        pushq   %rsi
+        movq    %rcx, %rdi
+        movq    %rdx, %rsi
+        movq    %r8, %rdx
+#endif
+
+// Save registers, make room for temps, preserve input arguments.
+
+        pushq   %rbx
+        pushq   %rbp
+        pushq   %r12
+        pushq   %r13
+        pushq   %r14
+        pushq   %r15
+        subq    $NSPACE, %rsp
+
+// Move the output pointer to a stable place
+
+        movq    %rdi, res
+
+// Copy the inputs to the local variables while mangling them:
+//
+//  - The scalar gets turned into 01xxx...xxx000 by tweaking bits.
+//    Actually the top zero doesn't matter since the loop below
+//    never looks at it, so we don't literally modify that.
+//
+//  - The point x coord is reduced mod 2^255 *then* mod 2^255-19
+
+        movq    (%rsi), %rax
+        andq    $~7, %rax
+        movq    %rax, (%rsp)
+        movq    8(%rsi), %rax
+        movq    %rax, 8(%rsp)
+        movq    16(%rsi), %rax
+        movq    %rax, 16(%rsp)
+        movq    24(%rsi), %rax
+        bts     $62, %rax
+        movq    %rax, 24(%rsp)
+
+        movq    (%rdx), %r8
+        movq    8(%rdx), %r9
+        movq    16(%rdx), %r10
+        movq    24(%rdx), %r11
+        btr     $63, %r11
+        movq    $19, %r12
+        xorq    %r13, %r13
+        xorq    %r14, %r14
+        xorq    %r15, %r15
+        addq    %r8, %r12
+        adcq    %r9, %r13
+        adcq    %r10, %r14
+        adcq    %r11, %r15
+        btr     $63, %r15 // x >= 2^255 - 19 <=> x + 19 >= 2^255
+        cmovcq  %r12, %r8
+        movq    %r8, 32(%rsp)
+        cmovcq  %r13, %r9
+        movq    %r9, 40(%rsp)
+        cmovcq  %r14, %r10
+        movq    %r10, 48(%rsp)
+        cmovcq  %r15, %r11
+        movq    %r11, 56(%rsp)
+
+// Initialize (xn,zn) = (1,0) and (xm,zm) = (x,1) with swap = 0
+// We use the fact that the point x coordinate is still in registers
+
+        movq    $1, %rax
+        movq    %rax, 320(%rsp)
+        movq    %rax, 96(%rsp)
+        xorl    %eax, %eax
+        movq    %rax, swap
+        movq    %rax, 160(%rsp)
+        movq    %rax, 328(%rsp)
+        movq    %rax, 104(%rsp)
+        movq    %rax, 168(%rsp)
+        movq    %rax, 336(%rsp)
+        movq    %rax, 112(%rsp)
+        movq    %rax, 176(%rsp)
+        movq    %rax, 344(%rsp)
+        movq    %rax, 120(%rsp)
+        movq    %rax, 184(%rsp)
+        movq    32(%rsp), %rax
+        movq    %r8, 256(%rsp)
+        movq    %r9, 264(%rsp)
+        movq    %r10, 272(%rsp)
+        movq    %r11, 280(%rsp)
+
+// The outer loop over scalar bits from i = 254, ..., i = 0 (inclusive)
+// This starts at 254, and so implicitly masks bit 255 of the scalar.
+
+        movl    $254, %eax
+        movq    %rax, i
+
+scalarloop:
+
+// sm = xm + zm; sn = xn + zn; dm = xm - zm; dn = xn - zn
+// The adds don't need any normalization as they're fed to muls
+// Just make sure the subs fit in 4 digits.
+
+        sub_4(dm,xm,zm)
+        add_4(sn,xn,zn)
+        sub_4(dn,xn,zn)
+        add_4(sm,xm,zm)
+
+// ADDING: dmsn = dm * sn; dnsm = sm * dn
+// DOUBLING: mux d = xt - zt and s = xt + zt for appropriate choice of (xt,zt)
+
+        mul_5(dmsn,sn,dm)
+
+        movq    i, %rdx
+        movq    %rdx, %rcx
+        shrq    $6, %rdx
+        movq    (%rsp,%rdx,8), %rdx
+        shrq    %cl, %rdx
+        andq    $1, %rdx
+        cmpq    swap, %rdx
+        movq    %rdx, swap
+
+        mux_4(d,dm,dn)
+        mux_4(s,sm,sn)
+
+        mul_5(dnsm,sm,dn)
+
+// DOUBLING: d = (xt - zt)^2 normalized only to 4 digits
+
+        sqr_4(d,d)
+
+// ADDING: dpro = (dmsn - dnsm)^2, spro = (dmsn + dnsm)^2
+// DOUBLING: s = (xt + zt)^2, normalized only to 4 digits
+
+        sub5_4(dpro,dmsn,dnsm)
+        sqr_4(s,s)
+        add5_4(spro,dmsn,dnsm)
+        sqr_4(dpro,dpro)
+
+// DOUBLING: p = 4 * xt * zt = s - d
+
+        sub_twice4(p,s,d)
+
+// ADDING: xm' = (dmsn + dnsm)^2
+
+        sqr_p25519(xm,spro)
+
+// DOUBLING: e = 121666 * p + d
+
+        cmadd_4(e,0x1db42,p,d)
+
+// DOUBLING: xn' = (xt + zt)^2 * (xt - zt)^2 = s * d
+
+        mul_p25519(xn,s,d)
+
+// ADDING: zm' = x * (dmsn - dnsm)^2
+
+        mul_p25519(zm,dpro,pointx)
+
+// DOUBLING: zn' = (4 * xt * zt) * ((xt - zt)^2 + 121666 * (4 * xt * zt))
+//               = p * (d + 121666 * p)
+
+        mul_p25519(zn,p,e)
+
+// Loop down as far as 0 (inclusive)
+
+        movq    i, %rax
+        subq    $1, %rax
+        movq    %rax, i
+        jnc     scalarloop
+
+// Since the scalar was forced to be a multiple of 8, we know it's even.
+// Hence there is no need to multiplex: the projective answer is (xn,zn)
+// and we can ignore (xm,zm); indeed we could have avoided the last three
+// differential additions and just done the doublings.
+// First set up the constant sn = 2^255 - 19 for the modular inverse.
+
+        movq    $-19, %rax
+        movq    $-1, %rcx
+        movq    $0x7fffffffffffffff, %rdx
+        movq    %rax, 128(%rsp)
+        movq    %rcx, 136(%rsp)
+        movq    %rcx, 144(%rsp)
+        movq    %rdx, 152(%rsp)
+
+// Prepare to call the modular inverse function to get zm = 1/zn
+
+        movq    $4, %rdi
+        leaq    96(%rsp), %rsi
+        leaq    160(%rsp), %rdx
+        leaq    128(%rsp), %rcx
+        leaq    192(%rsp), %r8
+
+// Inline copy of bignum_modinv, identical except for stripping out the
+// prologue and epilogue saving and restoring registers and the initial
+// test for k = 0 (which is trivially false here since k = 4). For more
+// details and explanations see "x86/generic/bignum_modinv.S". Note
+// that the stack it uses for its own temporaries is 80 bytes so it
+// only overwrites pointx, scalar and dm, which are no longer needed.
+
+        movq    %rsi, 0x40(%rsp)
+        movq    %r8, 0x38(%rsp)
+        movq    %rcx, 0x48(%rsp)
+        leaq    (%r8,%rdi,8), %r10
+        movq    %r10, 0x30(%rsp)
+        leaq    (%r10,%rdi,8), %r15
+        xorq    %r11, %r11
+        xorq    %r9, %r9
+copyloop:
+        movq    (%rdx,%r9,8), %rax
+        movq    (%rcx,%r9,8), %rbx
+        movq    %rax, (%r10,%r9,8)
+        movq    %rbx, (%r15,%r9,8)
+        movq    %rbx, (%r8,%r9,8)
+        movq    %r11, (%rsi,%r9,8)
+        incq    %r9
+        cmpq    %rdi, %r9
+        jb      copyloop
+        movq    (%r8), %rax
+        movq    %rax, %rbx
+        decq    %rbx
+        movq    %rbx, (%r8)
+        movq    %rax, %rbp
+        movq    %rax, %r12
+        shlq    $0x2, %rbp
+        subq    %rbp, %r12
+        xorq    $0x2, %r12
+        movq    %r12, %rbp
+        imulq   %rax, %rbp
+        movl    $0x2, %eax
+        addq    %rbp, %rax
+        addq    $0x1, %rbp
+        imulq   %rax, %r12
+        imulq   %rbp, %rbp
+        movl    $0x1, %eax
+        addq    %rbp, %rax
+        imulq   %rax, %r12
+        imulq   %rbp, %rbp
+        movl    $0x1, %eax
+        addq    %rbp, %rax
+        imulq   %rax, %r12
+        imulq   %rbp, %rbp
+        movl    $0x1, %eax
+        addq    %rbp, %rax
+        imulq   %rax, %r12
+        movq    %r12, 0x28(%rsp)
+        movq    %rdi, %rax
+        shlq    $0x7, %rax
+        movq    %rax, 0x20(%rsp)
+outerloop:
+        movq    0x20(%rsp), %r13
+        addq    $0x3f, %r13
+        shrq    $0x6, %r13
+        cmpq    %rdi, %r13
+        cmovaeq %rdi, %r13
+        xorq    %r12, %r12
+        xorq    %r14, %r14
+        xorq    %rbp, %rbp
+        xorq    %rsi, %rsi
+        xorq    %r11, %r11
+        movq    0x30(%rsp), %r8
+        leaq    (%r8,%rdi,8), %r15
+        xorq    %r9, %r9
+toploop:
+        movq    (%r8,%r9,8), %rbx
+        movq    (%r15,%r9,8), %rcx
+        movq    %r11, %r10
+        andq    %r12, %r10
+        andq    %rbp, %r11
+        movq    %rbx, %rax
+        orq     %rcx, %rax
+        negq    %rax
+        cmovbq  %r10, %r14
+        cmovbq  %r11, %rsi
+        cmovbq  %rbx, %r12
+        cmovbq  %rcx, %rbp
+        sbbq    %r11, %r11
+        incq    %r9
+        cmpq    %r13, %r9
+        jb      toploop
+        movq    %r12, %rax
+        orq     %rbp, %rax
+        bsrq    %rax, %rcx
+        xorq    $0x3f, %rcx
+        shldq   %cl, %r14, %r12
+        shldq   %cl, %rsi, %rbp
+        movq    (%r8), %rax
+        movq    %rax, %r14
+        movq    (%r15), %rax
+        movq    %rax, %rsi
+        movl    $0x1, %r10d
+        movl    $0x0, %r11d
+        movl    $0x0, %ecx
+        movl    $0x1, %edx
+        movl    $0x3a, %r9d
+        movq    %rdi, 0x8(%rsp)
+        movq    %r13, 0x10(%rsp)
+        movq    %r8, (%rsp)
+        movq    %r15, 0x18(%rsp)
+innerloop:
+        movq    %rbp, %rax
+        movq    %rsi, %rdi
+        movq    %rcx, %r13
+        movq    %rdx, %r15
+        movq    $0x1, %rbx
+        negq    %rdi
+        andq    %r14, %rbx
+        cmoveq  %rbx, %rax
+        cmoveq  %rbx, %rdi
+        cmoveq  %rbx, %r13
+        cmoveq  %rbx, %r15
+        movq    %r12, %rbx
+        addq    %r14, %rdi
+        movq    %rdi, %r8
+        negq    %rdi
+        subq    %rax, %rbx
+        cmovbq  %r12, %rbp
+        cmovbq  %r14, %rsi
+        cmovbq  %r10, %rcx
+        cmovbq  %r11, %rdx
+        cmovaeq %r8, %rdi
+        movq    %rbx, %r12
+        notq    %rbx
+        incq    %rbx
+        cmovbq  %rbx, %r12
+        movq    %rdi, %r14
+        addq    %r13, %r10
+        addq    %r15, %r11
+        shrq    $1, %r12
+        shrq    $1, %r14
+        leaq    (%rcx,%rcx), %rcx
+        leaq    (%rdx,%rdx), %rdx
+        decq    %r9
+        jne     innerloop
+        movq    0x8(%rsp), %rdi
+        movq    0x10(%rsp), %r13
+        movq    (%rsp), %r8
+        movq    0x18(%rsp), %r15
+        movq    %r10, (%rsp)
+        movq    %r11, 0x8(%rsp)
+        movq    %rcx, 0x10(%rsp)
+        movq    %rdx, 0x18(%rsp)
+        movq    0x38(%rsp), %r8
+        movq    0x40(%rsp), %r15
+        xorq    %r14, %r14
+        xorq    %rsi, %rsi
+        xorq    %r10, %r10
+        xorq    %r11, %r11
+        xorq    %r9, %r9
+congloop:
+        movq    (%r8,%r9,8), %rcx
+        movq    (%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %r14
+        adcq    $0x0, %rdx
+        movq    %rdx, %r12
+        movq    0x10(%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %rsi
+        adcq    $0x0, %rdx
+        movq    %rdx, %rbp
+        movq    (%r15,%r9,8), %rcx
+        movq    0x8(%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %r14
+        adcq    %rdx, %r12
+        shrdq   $0x3a, %r14, %r10
+        movq    %r10, (%r8,%r9,8)
+        movq    %r14, %r10
+        movq    %r12, %r14
+        movq    0x18(%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %rsi
+        adcq    %rdx, %rbp
+        shrdq   $0x3a, %rsi, %r11
+        movq    %r11, (%r15,%r9,8)
+        movq    %rsi, %r11
+        movq    %rbp, %rsi
+        incq    %r9
+        cmpq    %rdi, %r9
+        jb      congloop
+        shldq   $0x6, %r10, %r14
+        shldq   $0x6, %r11, %rsi
+        movq    0x48(%rsp), %r15
+        movq    (%r8), %rbx
+        movq    0x28(%rsp), %r12
+        imulq   %rbx, %r12
+        movq    (%r15), %rax
+        mulq    %r12
+        addq    %rbx, %rax
+        movq    %rdx, %r10
+        movl    $0x1, %r9d
+        movq    %rdi, %rcx
+        decq    %rcx
+        je      wmontend
+wmontloop:
+        adcq    (%r8,%r9,8), %r10
+        sbbq    %rbx, %rbx
+        movq    (%r15,%r9,8), %rax
+        mulq    %r12
+        subq    %rbx, %rdx
+        addq    %r10, %rax
+        movq    %rax, -0x8(%r8,%r9,8)
+        movq    %rdx, %r10
+        incq    %r9
+        decq    %rcx
+        jne     wmontloop
+wmontend:
+        adcq    %r14, %r10
+        movq    %r10, -0x8(%r8,%rdi,8)
+        sbbq    %r10, %r10
+        negq    %r10
+        movq    %rdi, %rcx
+        xorq    %r9, %r9
+wcmploop:
+        movq    (%r8,%r9,8), %rax
+        sbbq    (%r15,%r9,8), %rax
+        incq    %r9
+        decq    %rcx
+        jne     wcmploop
+        sbbq    $0x0, %r10
+        sbbq    %r10, %r10
+        notq    %r10
+        xorq    %rcx, %rcx
+        xorq    %r9, %r9
+wcorrloop:
+        movq    (%r8,%r9,8), %rax
+        movq    (%r15,%r9,8), %rbx
+        andq    %r10, %rbx
+        negq    %rcx
+        sbbq    %rbx, %rax
+        sbbq    %rcx, %rcx
+        movq    %rax, (%r8,%r9,8)
+        incq    %r9
+        cmpq    %rdi, %r9
+        jb      wcorrloop
+        movq    0x40(%rsp), %r8
+        movq    (%r8), %rbx
+        movq    0x28(%rsp), %rbp
+        imulq   %rbx, %rbp
+        movq    (%r15), %rax
+        mulq    %rbp
+        addq    %rbx, %rax
+        movq    %rdx, %r11
+        movl    $0x1, %r9d
+        movq    %rdi, %rcx
+        decq    %rcx
+        je      zmontend
+zmontloop:
+        adcq    (%r8,%r9,8), %r11
+        sbbq    %rbx, %rbx
+        movq    (%r15,%r9,8), %rax
+        mulq    %rbp
+        subq    %rbx, %rdx
+        addq    %r11, %rax
+        movq    %rax, -0x8(%r8,%r9,8)
+        movq    %rdx, %r11
+        incq    %r9
+        decq    %rcx
+        jne     zmontloop
+zmontend:
+        adcq    %rsi, %r11
+        movq    %r11, -0x8(%r8,%rdi,8)
+        sbbq    %r11, %r11
+        negq    %r11
+        movq    %rdi, %rcx
+        xorq    %r9, %r9
+zcmploop:
+        movq    (%r8,%r9,8), %rax
+        sbbq    (%r15,%r9,8), %rax
+        incq    %r9
+        decq    %rcx
+        jne     zcmploop
+        sbbq    $0x0, %r11
+        sbbq    %r11, %r11
+        notq    %r11
+        xorq    %rcx, %rcx
+        xorq    %r9, %r9
+zcorrloop:
+        movq    (%r8,%r9,8), %rax
+        movq    (%r15,%r9,8), %rbx
+        andq    %r11, %rbx
+        negq    %rcx
+        sbbq    %rbx, %rax
+        sbbq    %rcx, %rcx
+        movq    %rax, (%r8,%r9,8)
+        incq    %r9
+        cmpq    %rdi, %r9
+        jb      zcorrloop
+        movq    0x30(%rsp), %r8
+        leaq    (%r8,%rdi,8), %r15
+        xorq    %r9, %r9
+        xorq    %r12, %r12
+        xorq    %r14, %r14
+        xorq    %rbp, %rbp
+        xorq    %rsi, %rsi
+crossloop:
+        movq    (%r8,%r9,8), %rcx
+        movq    (%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %r14
+        adcq    $0x0, %rdx
+        movq    %rdx, %r10
+        movq    0x10(%rsp), %rax
+        mulq    %rcx
+        addq    %rax, %rsi
+        adcq    $0x0, %rdx
+        movq    %rdx, %r11
+        movq    (%r15,%r9,8), %rcx
+        movq    0x8(%rsp), %rax
+        mulq    %rcx
+        subq    %r12, %rdx
+        subq    %rax, %r14
+        sbbq    %rdx, %r10
+        sbbq    %r12, %r12
+        movq    %r14, (%r8,%r9,8)
+        movq    %r10, %r14
+        movq    0x18(%rsp), %rax
+        mulq    %rcx
+        subq    %rbp, %rdx
+        subq    %rax, %rsi
+        sbbq    %rdx, %r11
+        sbbq    %rbp, %rbp
+        movq    %rsi, (%r15,%r9,8)
+        movq    %r11, %rsi
+        incq    %r9
+        cmpq    %r13, %r9
+        jb      crossloop
+        xorq    %r9, %r9
+        movq    %r12, %r10
+        movq    %rbp, %r11
+        xorq    %r12, %r14
+        xorq    %rbp, %rsi
+optnegloop:
+        movq    (%r8,%r9,8), %rax
+        xorq    %r12, %rax
+        negq    %r10
+        adcq    $0x0, %rax
+        sbbq    %r10, %r10
+        movq    %rax, (%r8,%r9,8)
+        movq    (%r15,%r9,8), %rax
+        xorq    %rbp, %rax
+        negq    %r11
+        adcq    $0x0, %rax
+        sbbq    %r11, %r11
+        movq    %rax, (%r15,%r9,8)
+        incq    %r9
+        cmpq    %r13, %r9
+        jb      optnegloop
+        subq    %r10, %r14
+        subq    %r11, %rsi
+        movq    %r13, %r9
+shiftloop:
+        movq    -0x8(%r8,%r9,8), %rax
+        movq    %rax, %r10
+        shrdq   $0x3a, %r14, %rax
+        movq    %rax, -0x8(%r8,%r9,8)
+        movq    %r10, %r14
+        movq    -0x8(%r15,%r9,8), %rax
+        movq    %rax, %r11
+        shrdq   $0x3a, %rsi, %rax
+        movq    %rax, -0x8(%r15,%r9,8)
+        movq    %r11, %rsi
+        decq    %r9
+        jne     shiftloop
+        notq    %rbp
+        movq    0x48(%rsp), %rcx
+        movq    0x38(%rsp), %r8
+        movq    0x40(%rsp), %r15
+        movq    %r12, %r10
+        movq    %rbp, %r11
+        xorq    %r9, %r9
+fliploop:
+        movq    %rbp, %rdx
+        movq    (%rcx,%r9,8), %rax
+        andq    %rax, %rdx
+        andq    %r12, %rax
+        movq    (%r8,%r9,8), %rbx
+        xorq    %r12, %rbx
+        negq    %r10
+        adcq    %rbx, %rax
+        sbbq    %r10, %r10
+        movq    %rax, (%r8,%r9,8)
+        movq    (%r15,%r9,8), %rbx
+        xorq    %rbp, %rbx
+        negq    %r11
+        adcq    %rbx, %rdx
+        sbbq    %r11, %r11
+        movq    %rdx, (%r15,%r9,8)
+        incq    %r9
+        cmpq    %rdi, %r9
+        jb      fliploop
+        subq    $0x3a,  0x20(%rsp)
+        ja      outerloop
+
+// Since we eventually want to return 0 when the result is the point at
+// infinity, we force xn = 0 whenever zn = 0. This avoids building in a
+// dependency on the behavior of modular inverse in out-of-scope cases.
+
+        movq    160(%rsp), %rax
+        orq     168(%rsp), %rax
+        orq     176(%rsp), %rax
+        orq     184(%rsp), %rax
+        movq    320(%rsp), %rcx
+        cmovzq  %rax, %rcx
+        movq    %rcx, 320(%rsp)
+        movq    328(%rsp), %rcx
+        cmovzq  %rax, %rcx
+        movq    %rcx, 328(%rsp)
+        movq    336(%rsp), %rcx
+        cmovzq  %rax, %rcx
+        movq    %rcx, 336(%rsp)
+        movq    344(%rsp), %rcx
+        cmovzq  %rax, %rcx
+        movq    %rcx, 344(%rsp)
+
+// Now the result is xn * (1/zn).
+
+        movq    res, %rbp
+        mul_p25519(resx,xn,zm)
+
+// Restore stack and registers
+
+        addq    $NSPACE, %rsp
+
+        popq    %r15
+        popq    %r14
+        popq    %r13
+        popq    %r12
+        popq    %rbp
+        popq    %rbx
+
+#if WINDOWS_ABI
+        popq   %rsi
+        popq   %rdi
+#endif
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack, "", %progbits
+#endif
