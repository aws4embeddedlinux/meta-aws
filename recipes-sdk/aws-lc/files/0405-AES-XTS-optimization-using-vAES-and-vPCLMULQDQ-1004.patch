From 2240ea707a1861c26f9a098a4f76299be9417f44 Mon Sep 17 00:00:00 2001
From: shirsing0712 <106692654+shirsing0712@users.noreply.github.com>
Date: Wed, 7 Jun 2023 18:08:56 -0500
Subject: [PATCH] AES-XTS optimization using vAES and vPCLMULQDQ (#1004)

Add AES-XTS implementation utilizing the enhanced crypto ISA's
AVX512 vAES and vPCLMULQDQ. Performance numbers (MB/s) measured on
an EC2 m6i instance with Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz:

```
           Operation              |  Before  |   After   | Speedup |
AES-256-XTS encrypt (16 bytes)    |   478.4  |    581.2  |  1.21   |
AES-256-XTS encrypt (256 bytes)   |  3765.1  |   5222.0  |  1.39   |
AES-256-XTS encrypt (512 bytes)   |  4787.9  |   8195.2  |  1.71   |
AES-256-XTS encrypt (1350 bytes)  |  5807.1  |  10695.0  |  1.84   |
AES-256-XTS encrypt (8192 bytes)  |  6697.4  |  14692.4  |  2.19   |
AES-256-XTS encrypt (16384 bytes) |  6799.3  |  15081.2  |  2.22   |
AES-256-XTS decrypt (16 bytes)    |   425.9  |    498.7  |  1.17   |
AES-256-XTS decrypt (256 bytes)   |  3612.2  |   4962.5  |  1.37   |
AES-256-XTS decrypt (512 bytes)   |  4643.7  |   7802.9  |  1.68   |
AES-256-XTS decrypt (1350 bytes)  |  5450.4  |  10383.3  |  1.91   |
AES-256-XTS decrypt (8192 bytes)  |  6632.6  |  14622.2  |  2.20   |
AES-256-XTS decrypt (16384 bytes) |  6821.2  |  15041.5  |  2.21   |
```
---
 crypto/fipsmodule/CMakeLists.txt              |    3 +
 crypto/fipsmodule/aes/asm/aesni-xts-avx512.pl | 3174 ++++++++++
 crypto/fipsmodule/aes/internal.h              |   23 +
 crypto/fipsmodule/aes/mode_wrappers.c         |   12 +
 crypto/fipsmodule/cpucap/internal.h           |    5 +
 crypto/fipsmodule/modes/xts_test.cc           |  380 ++
 .../crypto/fipsmodule/aesni-xts-avx512.S      | 5211 ++++++++++++++++
 .../crypto/fipsmodule/aesni-xts-avx512.S      | 5211 ++++++++++++++++
 .../crypto/fipsmodule/aesni-xts-avx512.asm    | 5261 +++++++++++++++++
 9 files changed, 19280 insertions(+)
 create mode 100644 crypto/fipsmodule/aes/asm/aesni-xts-avx512.pl
 create mode 100644 generated-src/linux-x86_64/crypto/fipsmodule/aesni-xts-avx512.S
 create mode 100644 generated-src/mac-x86_64/crypto/fipsmodule/aesni-xts-avx512.S
 create mode 100644 generated-src/win-x86_64/crypto/fipsmodule/aesni-xts-avx512.asm

diff --git a/crypto/fipsmodule/CMakeLists.txt b/crypto/fipsmodule/CMakeLists.txt
index 62b49875e..84b3a3cc3 100644
--- a/crypto/fipsmodule/CMakeLists.txt
+++ b/crypto/fipsmodule/CMakeLists.txt
@@ -17,6 +17,7 @@ if(ARCH STREQUAL "x86_64")
 
     aesni-gcm-avx512.${ASM_EXT}
     aesni-gcm-x86_64.${ASM_EXT}
+    aesni-xts-avx512.${ASM_EXT}
     aesni-x86_64.${ASM_EXT}
     ghash-ssse3-x86_64.${ASM_EXT}
     ghash-x86_64.${ASM_EXT}
@@ -103,6 +104,7 @@ endif()
 if(PERL_EXECUTABLE)
   perlasm(aesni-gcm-x86_64.${ASM_EXT} modes/asm/aesni-gcm-x86_64.pl)
   perlasm(aesni-gcm-avx512.${ASM_EXT} modes/asm/aesni-gcm-avx512.pl)
+  perlasm(aesni-xts-avx512.${ASM_EXT} aes/asm/aesni-xts-avx512.pl)
   perlasm(aesni-x86_64.${ASM_EXT} aes/asm/aesni-x86_64.pl)
   perlasm(aesni-x86.${ASM_EXT} aes/asm/aesni-x86.pl)
   perlasm(aesp8-ppc.${ASM_EXT} aes/asm/aesp8-ppc.pl)
@@ -160,6 +162,7 @@ endif()
 if ((CMAKE_ASM_COMPILER_ID MATCHES "Clang" OR CMAKE_ASM_COMPILER MATCHES "clang") AND
     (CMAKE_ASM_COMPILER_VERSION VERSION_LESS "7.0.0") AND (ARCH STREQUAL "x86_64"))
   set_source_files_properties(${CMAKE_CURRENT_BINARY_DIR}/aesni-gcm-avx512.${ASM_EXT} PROPERTIES COMPILE_FLAGS "-mavx512f -mavx512bw -mavx512dq -mavx512vl")
+  set_source_files_properties(${CMAKE_CURRENT_BINARY_DIR}/aesni-xts-avx512.${ASM_EXT} PROPERTIES COMPILE_FLAGS "-mavx512f -mavx512bw -mavx512dq -mavx512vl -mavx512vbmi2")
 endif()
 
 # s2n-bignum files can be compiled on Unix platforms only (except Apple),
diff --git a/crypto/fipsmodule/aes/asm/aesni-xts-avx512.pl b/crypto/fipsmodule/aes/asm/aesni-xts-avx512.pl
new file mode 100644
index 000000000..72ccf7223
--- /dev/null
+++ b/crypto/fipsmodule/aes/asm/aesni-xts-avx512.pl
@@ -0,0 +1,3174 @@
+#! /usr/bin/env perl
+# Copyright (C) 2023 Intel Corporation
+#
+# Licensed under the OpenSSL license (the "License").  You may not use
+# this file except in compliance with the License.  You can obtain a copy
+# in the file LICENSE in the source distribution or at
+# https://www.openssl.org/source/license.html
+
+# This implementation is based on the AES-XTS code (AVX512VAES + VPCLMULQDQ)
+# from Intel(R) Intelligent Storage Acceleration Library Crypto Version
+# (https://github.com/intel/isa-l_crypto).
+#
+######################################################################
+# The main building block of the loop is code that encrypts/decrypts
+# 8/16 blocks of data stitching with generation of tweak for the next
+# 8/16 blocks, utilizing VAES and VPCLMULQDQ instructions with full width
+# of ZMM registers. The main loop is selected based on the input length.
+# main_loop_run_16 encrypts/decrypts 16 blocks in parallel and it's selected
+# when input length >= 256 bytes (16 blocks)
+# main_loop_run_8 encrypts/decrypts 8 blocks in parallel and it's selected
+# when 128 bytes <= input length < 256 bytes (8-15 blocks)
+# Input length < 128 bytes (8 blocks) is handled by do_n_blocks.
+#
+# This implementation mainly uses vpshrdq from AVX-512-VBMI2 family and vaesenc,
+# vaesdec, vpclmulqdq from AVX-512F family.
+
+# The first two arguments should always be the flavour and output file path.
+if ($#ARGV < 1) { die "Not enough arguments provided.
+  Two arguments are necessary: the flavour and the output file path."; }
+
+$flavour = shift;
+$output  = shift;
+
+$win64=0; $win64=1 if ($flavour =~ /[nm]asm|mingw64/ || $output =~ /\.asm$/);
+
+$avx512vaes = 1;
+for (@ARGV) { $avx512vaes = 0 if (/-DMY_ASSEMBLER_IS_TOO_OLD_FOR_512AVX/); }
+
+$0 =~ m/(.*[\/\\])[^\/\\]+$/; $dir=$1;
+( $xlate="${dir}x86_64-xlate.pl" and -f $xlate ) or
+( $xlate="${dir}../../../perlasm/x86_64-xlate.pl" and -f $xlate) or
+die "can't locate x86_64-xlate.pl";
+
+open OUT,"| \"$^X\" \"$xlate\" $flavour \"$output\"";
+*STDOUT=*OUT;
+
+#======================================================================
+
+if ($avx512vaes) {
+
+  my $GP_STORAGE  = $win64 ? (16 * 33)  : (16 * 23);    # store rbx
+  my $XMM_STORAGE = $win64 ? (16 * 23) : 0;     # store xmm6:xmm15
+  my $VARIABLE_OFFSET = $win64 ? (16 *8 + 16* 15 + 16 * 10 + 8*3) :
+                                 (16*8 + 16 * 15 + 8 * 1);
+
+  my $TW = "%rsp";
+  my $TWTEMPH = "%rbx";
+  my $TWTEMPL = "%rax";
+  my $ZPOLY = "%zmm25";
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;;; Function arguments abstraction
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  my ($key2, $key1, $tweak, $length, $input, $output);
+
+  if ($win64) {
+    $input    = "%rcx";
+    $output   = "%rdx";
+    $length   = "%r8";
+    $key1     = "%r9";
+    $key2     = "%r10";
+    $tweak    = "%r11";
+  } else {
+    $input    = "%rdi";
+    $output   = "%rsi";
+    $length   = "%rdx";
+    $key1     = "%rcx";
+    $key2     = "%r8";
+    $tweak    = "%r9";
+  }
+
+  # arguments for temp parameters
+  my ($tmp1, $gf_poly_8b, $gf_poly_8b_temp);
+  if ($win64) {
+    $tmp1                = "%r10";
+    $gf_poly_8b       = "%rdi";
+    $gf_poly_8b_temp  = "%rsi";
+  } else {
+    $tmp1                = "%r8";
+    $gf_poly_8b       = "%r10";
+    $gf_poly_8b_temp  = "%r11";
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;;; Helper functions
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+  # Generates "random" local labels
+  sub random_string() {
+    my @chars  = ('a' .. 'z', 'A' .. 'Z', '0' .. '9', '_');
+    my $length = 15;
+    my $str;
+    map { $str .= $chars[rand(33)] } 1 .. $length;
+    return $str;
+  }
+
+  # ; Seed the RNG so the labels are generated deterministically
+  srand(12345);
+
+  sub encrypt_tweak_for_encryption {
+    my $key2               = $_[0];
+    my $state_tweak        = $_[1];
+    my $key1               = $_[2];
+    my $raw_key            = $_[3];
+    my $tmp                = $_[4];
+    my $ptr_key2           = $_[5];
+    my $ptr_key1           = $_[6];
+    my $ptr_expanded_keys  = $_[7];
+
+    $code.=<<___;
+    vmovdqu  ($ptr_key2), $key2
+    vpxor    $key2, $state_tweak, $state_tweak  # AddRoundKey(ARK) for tweak encryption
+
+    vmovdqu  ($ptr_key1), $key1
+    vmovdqa  $key1, 0x80($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x10($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 1 for tweak encryption
+
+    vmovdqu  0x10($ptr_key1), $key1
+    vmovdqa  $key1, 0x90($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x20($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 2 for tweak encryption
+
+    vmovdqu  0x20($ptr_key1), $key1
+    vmovdqa  $key1, 0xa0($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x30($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 3 for tweak encryption
+
+    vmovdqu  0x30($ptr_key1), $key1
+    vmovdqa  $key1, 0xb0($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x40($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 4 for tweak encryption
+
+    vmovdqu  0x40($ptr_key1), $key1
+    vmovdqa  $key1, 0xc0($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x50($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 5 for tweak encryption
+
+    vmovdqu  0x50($ptr_key1), $key1
+    vmovdqa  $key1, 0xd0($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x60($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 6 for tweak encryption
+
+    vmovdqu  0x60($ptr_key1), $key1
+    vmovdqa  $key1, 0xe0($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x70($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 7 for tweak encryption
+
+    vmovdqu  0x70($ptr_key1), $key1
+    vmovdqa  $key1, 0xf0($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x80($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 8 for tweak encryption
+
+    vmovdqu  0x80($ptr_key1), $key1
+    vmovdqa  $key1, 0x100($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0x90($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 9 for tweak encryption
+
+    vmovdqu  0x90($ptr_key1), $key1
+    vmovdqa  $key1, 0x110($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0xa0($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 10 for tweak encryption
+
+    vmovdqu  0xa0($ptr_key1), $key1
+    vmovdqa  $key1, 0x120($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0xb0($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 11 for tweak encryption
+
+    vmovdqu  0xb0($ptr_key1), $key1
+    vmovdqa  $key1, 0x130($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0xc0($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 12 for tweak encryption
+
+    vmovdqu  0xc0($ptr_key1), $key1
+    vmovdqa  $key1, 0x140($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0xd0($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 13 for tweak encryption
+
+    vmovdqu  0xd0($ptr_key1), $key1
+    vmovdqa  $key1, 0x150($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0xe0($ptr_key2), $key2
+    vaesenclast  $key2, $state_tweak, $state_tweak # round 14 for tweak encryption
+
+    vmovdqu  0xe0($ptr_key1), $key1
+    vmovdqa  $key1, 0x160($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqa  $state_tweak, ($ptr_expanded_keys)  # Store the encrypted Tweak value
+___
+  }
+
+  sub initialize {
+    my @st;
+    $st[0] = $_[0];
+    $st[1] = $_[1];
+    $st[2] = $_[2];
+    $st[3] = $_[3];
+    $st[4] = $_[4];
+    $st[5] = $_[5];
+    $st[6] = $_[6];
+    $st[7] = $_[7];
+
+    my @tw;
+    $tw[0] = $_[8];
+    $tw[1] = $_[9];
+    $tw[2] = $_[10];
+    $tw[3] = $_[11];
+    $tw[4] = $_[12];
+    $tw[5] = $_[13];
+    $tw[6] = $_[14];
+    my $num_initial_blocks = $_[15];
+
+    $code .= <<___;
+    vmovdqa  0x0($TW), $tw[0]
+    mov      0x0($TW), $TWTEMPL
+    mov      0x08($TW), $TWTEMPH
+    vmovdqu  0x0($input), $st[0]
+___
+
+    if ($num_initial_blocks >= 2) {
+      for (my $i = 1; $i < $num_initial_blocks; $i++) {
+        $code .= "xor      $gf_poly_8b_temp, $gf_poly_8b_temp\n";
+        $code .= "shl      \$1, $TWTEMPL\n";
+        $code .= "adc      $TWTEMPH, $TWTEMPH\n";
+        $code .= "cmovc    $gf_poly_8b, $gf_poly_8b_temp\n";
+        $code .= "xor      $gf_poly_8b_temp, $TWTEMPL\n";
+
+        my $offset = $i * 16;
+        $code .= "mov      $TWTEMPL, $offset($TW)\n";
+        $code .= "mov      $TWTEMPH, `$offset + 8`($TW)\n";
+        $code .= "vmovdqa  $offset($TW), $tw[$i]\n";
+        $code .= "vmovdqu  $offset($input), $st[$i]\n";
+      }
+    }
+  }
+
+  # encrypt initial blocks of AES
+  # 1, 2, 3, 4, 5, 6 or 7 blocks are encrypted
+  # next 8 Tweak values are generated
+  sub encrypt_initial {
+    my @st;
+    $st[0] = $_[0];
+    $st[1] = $_[1];
+    $st[2] = $_[2];
+    $st[3] = $_[3];
+    $st[4] = $_[4];
+    $st[5] = $_[5];
+    $st[6] = $_[6];
+    $st[7] = $_[7];
+
+    my @tw;
+    $tw[0] = $_[8];
+    $tw[1] = $_[9];
+    $tw[2] = $_[10];
+    $tw[3] = $_[11];
+    $tw[4] = $_[12];
+    $tw[5] = $_[13];
+    $tw[6] = $_[14];
+    my $t0 = $_[15];
+    my $num_blocks = $_[16];
+    my $lt128 = $_[17];
+
+    # num_blocks blocks encrypted
+    # num_blocks can be 1, 2, 3, 4, 5, 6, 7
+
+    # xor Tweak value
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vpxor $tw[$i], $st[$i], $st[$i]\n";
+    }
+    $code .= "vmovdqa  0x80($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vpxor $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+___
+    }
+    # round 1
+    $code .= "vmovdqa 0x90($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+    $code .= <<___;
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x0($TW)     # next Tweak1 generated
+      mov     $TWTEMPL, 0x08($TW)
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+___
+    }
+
+    # round 2
+    $code .= "vmovdqa 0xa0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x10($TW) # next Tweak2 generated
+___
+    }
+
+    # round 3
+    $code .= "vmovdqa 0xb0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      mov     $TWTEMPH, 0x18($TW)
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+___
+    }
+
+    # round 4
+    $code .= "vmovdqa 0xc0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+    $code .= <<___;
+    xor     $gf_poly_8b_temp, $TWTEMPL
+    mov     $TWTEMPL, 0x20($TW) # next Tweak3 generated
+    mov     $TWTEMPH, 0x28($TW)
+    xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+    shl     \$1, $TWTEMPL
+___
+    }
+
+    # round 5
+    $code .= "vmovdqa 0xd0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+    $code .= <<___;
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x30($TW) # next Tweak4 generated
+      mov     $TWTEMPH, 0x38($TW)
+___
+    }
+
+    # round 6
+    $code .= "vmovdqa 0xe0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x40($TW) # next Tweak5 generated
+      mov     $TWTEMPH, 0x48($TW)
+___
+    }
+
+    # round 7
+    $code .= "vmovdqa 0xf0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x50($TW) # next Tweak6 generated
+      mov     $TWTEMPH, 0x58($TW)
+___
+    }
+
+    # round 8
+    $code .= "vmovdqa 0x100($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x60($TW) # next Tweak7 generated
+      mov     $TWTEMPH, 0x68($TW)
+___
+    }
+
+    # round 9
+    $code .= "vmovdqa 0x110($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x70($TW) # next Tweak8 generated
+      mov     $TWTEMPH, 0x78($TW)
+___
+    }
+
+    # round 10
+    $code .= "vmovdqa 0x120($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 11
+    $code .= "vmovdqa 0x130($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 12
+    $code .= "vmovdqa 0x140($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 13
+    $code .= "vmovdqa 0x150($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 14
+    $code .= "vmovdqa 0x160($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesenclast $t0, $st[$i], $st[$i]\n";
+    }
+
+    # xor Tweak values
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vpxor $tw[$i], $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      # load next Tweak values
+      $code .= <<___;
+      vmovdqa  0x0($TW), $tw[0]
+      vmovdqa  0x10($TW), $tw[1]
+      vmovdqa  0x20($TW), $tw[2]
+      vmovdqa  0x30($TW), $tw[3]
+      vmovdqa  0x40($TW), $tw[4]
+      vmovdqa  0x50($TW), $tw[5]
+      vmovdqa  0x60($TW), $tw[6]
+___
+    }
+  }
+
+  sub encrypt_tweak_for_decryption {
+    my $key2               = $_[0];
+    my $state_tweak        = $_[1];
+    my $key1               = $_[2];
+    my $raw_key            = $_[3];
+    my $tmp                = $_[4];
+    my $ptr_key2           = $_[5];
+    my $ptr_key1           = $_[6];
+    my $ptr_expanded_keys  = $_[7];
+
+    $code.=<<___;
+    vmovdqu  ($ptr_key2), $key2
+    vpxor    $key2, $state_tweak, $state_tweak  # ARK for tweak encryption
+
+    vmovdqu  0xe0($ptr_key1), $key1
+    vmovdqa  $key1, 0x160($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x10($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 1 for tweak encryption
+
+    vmovdqu  0xd0($ptr_key1), $key1
+    vmovdqa  $key1, 0x150($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x20($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 2 for tweak encryption
+
+    vmovdqu  0xc0($ptr_key1), $key1
+    vmovdqa  $key1, 0x140($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x30($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 3 for tweak encryption
+
+    vmovdqu  0xb0($ptr_key1), $key1
+    vmovdqa  $key1, 0x130($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x40($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 4 for tweak encryption
+
+    vmovdqu  0xa0($ptr_key1), $key1
+    vmovdqa  $key1, 0x120($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x50($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 5 for tweak encryption
+
+    vmovdqu  0x90($ptr_key1), $key1
+    vmovdqa  $key1, 0x110($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x60($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 6 for tweak encryption
+
+    vmovdqu  0x80($ptr_key1), $key1
+    vmovdqa  $key1, 0x100($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x70($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 7 for tweak encryption
+
+    vmovdqu  0x70($ptr_key1), $key1
+    vmovdqa  $key1, 0xf0($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqu  0x80($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 8 for tweak encryption
+
+    vmovdqu  0x60($ptr_key1), $key1
+    vmovdqa  $key1, 0xe0($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0x90($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 9 for tweak encryption
+
+    vmovdqu  0x50($ptr_key1), $key1
+    vmovdqa  $key1, 0xd0($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0xa0($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 10 for tweak encryption
+
+    vmovdqu  0x40($ptr_key1), $key1
+    vmovdqa  $key1, 0xc0($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0xb0($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 11 for tweak encryption
+
+    vmovdqu  0x30($ptr_key1), $key1
+    vmovdqa  $key1, 0xb0($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0xc0($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 12 for tweak encryption
+
+    vmovdqu  0x20($ptr_key1), $key1
+    vmovdqa  $key1, 0xa0($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0xd0($ptr_key2), $key2
+    vaesenc  $key2, $state_tweak, $state_tweak  # round 13 for tweak encryption
+
+    vmovdqu  0x10($ptr_key1), $key1
+    vmovdqa  $key1, 0x90($ptr_expanded_keys)   # store round keys in stack
+
+    vmovdqu  0xe0($ptr_key2), $key2
+    vaesenclast  $key2, $state_tweak, $state_tweak # round 14 for tweak encryption
+
+    vmovdqu  ($ptr_key1), $key1
+    vmovdqa  $key1, 0x80($ptr_expanded_keys)    # store round keys in stack
+
+    vmovdqa  $state_tweak, ($ptr_expanded_keys)  # Store the encrypted Tweak value
+___
+  }
+
+  # decrypt initial blocks of AES
+  # 1, 2, 3, 4, 5, 6 or 7 blocks are encrypted
+  # next 8 Tweak values are generated
+  sub decrypt_initial {
+    my @st;
+    $st[0] = $_[0];
+    $st[1] = $_[1];
+    $st[2] = $_[2];
+    $st[3] = $_[3];
+    $st[4] = $_[4];
+    $st[5] = $_[5];
+    $st[6] = $_[6];
+    $st[7] = $_[7];
+
+    my @tw;
+    $tw[0] = $_[8];
+    $tw[1] = $_[9];
+    $tw[2] = $_[10];
+    $tw[3] = $_[11];
+    $tw[4] = $_[12];
+    $tw[5] = $_[13];
+    $tw[6] = $_[14];
+    my $t0 = $_[15];
+    my $num_blocks = $_[16];
+    my $lt128 = $_[17];
+
+    # num_blocks blocks encrypted
+    # num_blocks can be 1, 2, 3, 4, 5, 6, 7
+
+    #  xor Tweak value
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vpxor $tw[$i], $st[$i], $st[$i]\n";
+    }
+
+    $code .= "vmovdqa  0x80($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vpxor $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+___
+    }
+    # round 1
+    $code .= "vmovdqa 0x90($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+    $code .= <<___;
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, ($TW)     # next Tweak1 generated
+      mov     $TWTEMPL, 0x08($TW)
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+___
+    }
+
+    # round 2
+    $code .= "vmovdqa 0xa0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x10($TW) # next Tweak2 generated
+___
+    }
+
+    # round 3
+    $code .= "vmovdqa 0xb0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      mov     $TWTEMPH, 0x18($TW)
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+___
+    }
+
+    # round 4
+    $code .= "vmovdqa 0xc0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+    $code .= <<___;
+    xor     $gf_poly_8b_temp, $TWTEMPL
+    mov     $TWTEMPL, 0x20($TW) # next Tweak3 generated
+    mov     $TWTEMPH, 0x28($TW)
+    xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+    shl     \$1, $TWTEMPL
+___
+    }
+
+    # round 5
+    $code .= "vmovdqa 0xd0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+    $code .= <<___;
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x30($TW) # next Tweak4 generated
+      mov     $TWTEMPH, 0x38($TW)
+___
+    }
+
+    # round 6
+    $code .= "vmovdqa 0xe0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x40($TW) # next Tweak5 generated
+      mov     $TWTEMPH, 0x48($TW)
+___
+    }
+
+    # round 7
+    $code .= "vmovdqa 0xf0($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x50($TW) # next Tweak6 generated
+      mov     $TWTEMPH, 0x58($TW)
+___
+    }
+
+    # round 8
+    $code .= "vmovdqa 0x100($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x60($TW) # next Tweak7 generated
+      mov     $TWTEMPH, 0x68($TW)
+___
+    }
+
+    # round 9
+    $code .= "vmovdqa 0x110($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      $code .= <<___;
+      xor     $gf_poly_8b_temp, $gf_poly_8b_temp
+      shl     \$1, $TWTEMPL
+      adc     $TWTEMPH, $TWTEMPH
+      cmovc   $gf_poly_8b, $gf_poly_8b_temp
+      xor     $gf_poly_8b_temp, $TWTEMPL
+      mov     $TWTEMPL, 0x70($TW) # next Tweak8 generated
+      mov     $TWTEMPH, 0x78($TW)
+___
+    }
+
+    # round 10
+    $code .= "vmovdqa 0x120($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 11
+    $code .= "vmovdqa 0x130($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 12
+    $code .= "vmovdqa 0x140($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 13
+    $code .= "vmovdqa 0x150($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 14
+    $code .= "vmovdqa 0x160($TW), $t0\n";
+
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vaesdeclast $t0, $st[$i], $st[$i]\n";
+    }
+
+    # xor Tweak values
+    for (my $i = 0; $i < $num_blocks; $i++) {
+      $code .= "vpxor $tw[$i], $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $lt128) {
+      # load next Tweak values
+      $code .= <<___;
+      vmovdqa  ($TW), $tw1
+      vmovdqa  0x10($TW), $tw2
+      vmovdqa  0x20($TW), $tw3
+      vmovdqa  0x30($TW), $tw4
+      vmovdqa  0x40($TW), $tw5
+      vmovdqa  0x50($TW), $tw6
+      vmovdqa  0x60($TW), $tw7
+___
+    }
+  }
+
+  # Encrypt 8 blocks in parallel
+  # generate next 8 tweak values
+  sub encrypt_by_eight_zmm {
+    my $st1 = $_[0];
+    my $st2 = $_[1];
+    my $tw1 = $_[2];
+    my $tw2 = $_[3];
+    my $t0 = $_[4];
+    my $last_eight = $_[5];
+
+    $code .= <<___;
+    # xor Tweak values
+    vpxorq    $tw1, $st1, $st1
+    vpxorq    $tw2, $st2, $st2
+
+    # ARK
+    vbroadcasti32x4 0x80($TW), $t0
+    vpxorq    $t0, $st1, $st1
+    vpxorq    $t0, $st2, $st2
+___
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, $tw1, %zmm13
+      vpclmulqdq	\$0x0, $ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, $tw1, %zmm15
+      vpxord		%zmm14, %zmm15, %zmm15
+___
+    }
+    # round 1
+    $code .= <<___;
+    vbroadcasti32x4 0x90($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 2
+    vbroadcasti32x4 0xa0($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 3
+    vbroadcasti32x4 0xb0($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+___
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, $tw2, %zmm13
+      vpclmulqdq	\$0x0, $ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, $tw2, %zmm16
+      vpxord		%zmm14, %zmm16, %zmm16
+___
+    }
+
+    $code .= <<___;
+    # round 4
+    vbroadcasti32x4 0xc0($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 5
+    vbroadcasti32x4 0xd0($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 6
+    vbroadcasti32x4 0xe0($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 7
+    vbroadcasti32x4 0xf0($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 8
+    vbroadcasti32x4 0x100($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 9
+    vbroadcasti32x4 0x110($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 10
+    vbroadcasti32x4 0x120($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 11
+    vbroadcasti32x4 0x130($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 12
+    vbroadcasti32x4 0x140($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 13
+    vbroadcasti32x4 0x150($TW), $t0
+    vaesenc  $t0, $st1, $st1
+    vaesenc  $t0, $st2, $st2
+
+    # round 14
+    vbroadcasti32x4 0x160($TW), $t0
+    vaesenclast  $t0, $st1, $st1
+    vaesenclast  $t0, $st2, $st2
+
+    # xor Tweak values
+    vpxorq    $tw1, $st1, $st1
+    vpxorq    $tw2, $st2, $st2
+
+    # load next Tweak values
+    vmovdqa32  %zmm15, $tw1
+    vmovdqa32  %zmm16, $tw2
+___
+  }
+
+  # Decrypt 8 blocks in parallel
+  # generate next 8 tweak values
+  sub decrypt_by_eight_zmm {
+    my $st1 = $_[0];
+    my $st2 = $_[1];
+    my $tw1 = $_[2];
+    my $tw2 = $_[3];
+    my $t0 = $_[4];
+    my $last_eight = $_[5];
+
+    $code .= <<___;
+    # xor Tweak values
+    vpxorq    $tw1, $st1, $st1
+    vpxorq    $tw2, $st2, $st2
+
+    # ARK
+    vbroadcasti32x4 0x80($TW), $t0
+    vpxorq    $t0, $st1, $st1
+    vpxorq    $t0, $st2, $st2
+___
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, $tw1, %zmm13
+      vpclmulqdq	\$0x0,$ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, $tw1, %zmm15
+      vpxord		%zmm14, %zmm15, %zmm15
+___
+    }
+    # round 1
+    $code .= <<___;
+    vbroadcasti32x4 0x90($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 2
+    vbroadcasti32x4 0xa0($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 3
+    vbroadcasti32x4 0xb0($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+___
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, $tw2, %zmm13
+      vpclmulqdq	\$0x0,$ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, $tw2, %zmm16
+      vpxord		%zmm14, %zmm16, %zmm16
+___
+    }
+
+    $code .= <<___;
+    # round 4
+    vbroadcasti32x4 0xc0($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 5
+    vbroadcasti32x4 0xd0($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 6
+    vbroadcasti32x4 0xe0($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 7
+    vbroadcasti32x4 0xf0($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 8
+    vbroadcasti32x4 0x100($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 9
+    vbroadcasti32x4 0x110($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 10
+    vbroadcasti32x4 0x120($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 11
+    vbroadcasti32x4 0x130($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 12
+    vbroadcasti32x4 0x140($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 13
+    vbroadcasti32x4 0x150($TW), $t0
+    vaesdec  $t0, $st1, $st1
+    vaesdec  $t0, $st2, $st2
+
+    # round 14
+    vbroadcasti32x4 0x160($TW), $t0
+    vaesdeclast  $t0, $st1, $st1
+    vaesdeclast  $t0, $st2, $st2
+
+    # xor Tweak values
+    vpxorq    $tw1, $st1, $st1
+    vpxorq    $tw2, $st2, $st2
+
+    # load next Tweak values
+    vmovdqa32  %zmm15, $tw1
+    vmovdqa32  %zmm16, $tw2
+___
+  }
+
+  # Encrypt 16 blocks in parallel
+  # generate next 16 tweak values
+  sub encrypt_by_16_zmm {
+    my @st;
+    $st[0] = $_[0];
+    $st[1] = $_[1];
+    $st[2] = $_[2];
+    $st[3] = $_[3];
+
+    my @tw;
+    $tw[0] = $_[4];
+    $tw[1] = $_[5];
+    $tw[2] = $_[6];
+    $tw[3] = $_[7];
+
+    my $t0 = $_[8];
+    my $last_eight = $_[9];
+
+    # xor Tweak values
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vpxorq    $tw[$i], $st[$i], $st[$i]\n";
+    }
+
+    # ARK
+    $code .= "vbroadcasti32x4 0x80($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vpxorq $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, $tw[2], %zmm13
+      vpclmulqdq	\$0x0,$ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, $tw[2], %zmm15
+      vpxord		%zmm14, %zmm15, %zmm15
+___
+    }
+
+    # round 1
+    $code .= "vbroadcasti32x4 0x90($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 2
+    $code .= "vbroadcasti32x4 0xa0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 3
+    $code .= "vbroadcasti32x4 0xb0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, $tw[3], %zmm13
+      vpclmulqdq	\$0x0,$ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, $tw[3], %zmm16
+      vpxord		%zmm14, %zmm16, %zmm16
+___
+    }
+    # round 4
+    $code .= "vbroadcasti32x4 0xc0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 5
+    $code .= "vbroadcasti32x4 0xd0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 6
+    $code .= "vbroadcasti32x4 0xe0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, %zmm15, %zmm13
+      vpclmulqdq	\$0x0,$ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, %zmm15, %zmm17
+      vpxord		%zmm14, %zmm17, %zmm17
+___
+    }
+    # round 7
+    $code .= "vbroadcasti32x4 0xf0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 8
+    $code .= "vbroadcasti32x4 0x100($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 9
+    $code .= "vbroadcasti32x4 0x110($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, %zmm16, %zmm13
+      vpclmulqdq	\$0x0,$ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, %zmm16, %zmm18
+      vpxord		%zmm14, %zmm18, %zmm18
+___
+    }
+    # round 10
+    $code .= "vbroadcasti32x4 0x120($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 11
+    $code .= "vbroadcasti32x4 0x130($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 12
+    $code .= "vbroadcasti32x4 0x140($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 13
+    $code .= "vbroadcasti32x4 0x150($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenc $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 14
+    $code .= "vbroadcasti32x4 0x160($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesenclast $t0, $st[$i], $st[$i]\n";
+    }
+
+
+    # xor Tweak values
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vpxorq    $tw[$i], $st[$i], $st[$i]\n";
+    }
+
+    $code .= <<___;
+    # load next Tweak values
+    vmovdqa32  %zmm15, $tw[0]
+    vmovdqa32  %zmm16, $tw[1]
+    vmovdqa32  %zmm17, $tw[2]
+    vmovdqa32  %zmm18, $tw[3]
+___
+  }
+
+  # Decrypt 16 blocks in parallel
+  # generate next 8 tweak values
+  sub decrypt_by_16_zmm {
+    my @st;
+    $st[0] = $_[0];
+    $st[1] = $_[1];
+    $st[2] = $_[2];
+    $st[3] = $_[3];
+
+    my @tw;
+    $tw[0] = $_[4];
+    $tw[1] = $_[5];
+    $tw[2] = $_[6];
+    $tw[3] = $_[7];
+
+    my $t0 = $_[8];
+    my $last_eight = $_[9];
+
+    # xor Tweak values
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vpxorq    $tw[$i], $st[$i], $st[$i]\n";
+    }
+
+    # ARK
+    $code .= "vbroadcasti32x4 0x80($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vpxorq $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, $tw[2], %zmm13
+      vpclmulqdq	\$0x0,$ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, $tw[2], %zmm15
+      vpxord		%zmm14, %zmm15, %zmm15
+___
+    }
+
+    # round 1
+    $code .= "vbroadcasti32x4 0x90($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 2
+    $code .= "vbroadcasti32x4 0xa0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 3
+    $code .= "vbroadcasti32x4 0xb0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, $tw[3], %zmm13
+      vpclmulqdq	\$0x0,$ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, $tw[3], %zmm16
+      vpxord		%zmm14, %zmm16, %zmm16
+___
+    }
+    # round 4
+    $code .= "vbroadcasti32x4 0xc0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 5
+    $code .= "vbroadcasti32x4 0xd0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 6
+    $code .= "vbroadcasti32x4 0xe0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, %zmm15, %zmm13
+      vpclmulqdq	\$0x0,$ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, %zmm15, %zmm17
+      vpxord		%zmm14, %zmm17, %zmm17
+___
+    }
+    # round 7
+    $code .= "vbroadcasti32x4 0xf0($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 8
+    $code .= "vbroadcasti32x4 0x100($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 9
+    $code .= "vbroadcasti32x4 0x110($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    if (0 == $last_eight) {
+      $code .= <<___;
+      vpsrldq		\$0xf, %zmm16, %zmm13
+      vpclmulqdq	\$0x0,$ZPOLY, %zmm13, %zmm14
+      vpslldq		\$0x1, %zmm16, %zmm18
+      vpxord		%zmm14, %zmm18, %zmm18
+___
+    }
+    # round 10
+    $code .= "vbroadcasti32x4 0x120($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 11
+    $code .= "vbroadcasti32x4 0x130($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 12
+    $code .= "vbroadcasti32x4 0x140($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 13
+    $code .= "vbroadcasti32x4 0x150($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdec $t0, $st[$i], $st[$i]\n";
+    }
+
+    # round 14
+    $code .= "vbroadcasti32x4 0x160($TW), $t0\n";
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vaesdeclast $t0, $st[$i], $st[$i]\n";
+    }
+
+    # xor Tweak values
+    for (my $i = 0; $i < 4; $i++) {
+      $code .= "vpxorq    $tw[$i], $st[$i], $st[$i]\n";
+    }
+
+    $code .= <<___;
+    # load next Tweak values
+    vmovdqa32  %zmm15, $tw[0]
+    vmovdqa32  %zmm16, $tw[1]
+    vmovdqa32  %zmm17, $tw[2]
+    vmovdqa32  %zmm18, $tw[3]
+___
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;void aes_hw_xts_encrypt_avx512(
+  # ;               const uint8_t *in,        // input data
+  # ;               uint8_t *out,             // output data
+  # ;               size_t length,            // sector size, in bytes
+  # ;               const AES_KEY *key1,      // key used for "ECB" encryption, 16*2 bytes
+  # ;               const AES_KEY *key2,      // key used for tweaking, 16*2 bytes
+  # ;               const uint8_t iv[16])      // initial tweak value, 16 bytes
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+  my $rndsuffix = &random_string();
+
+  $code .= ".text\n";
+
+  {
+  $code.=<<___;
+  .globl	aes_hw_xts_encrypt_avx512
+  .hidden	aes_hw_xts_encrypt_avx512
+  .type	aes_hw_xts_encrypt_avx512,\@abi-omnipotent
+  .align	32
+  aes_hw_xts_encrypt_avx512:
+  .cfi_startproc
+          endbranch
+___
+  }
+  $code .= "push 	 %rbp\n";
+  $code .= "mov 	 %rsp,%rbp\n";
+  $code .= "sub 	 \$$VARIABLE_OFFSET,%rsp\n";
+  $code .= "and 	 \$0xffffffffffffffc0,%rsp\n";
+  $code .= "mov 	 %rbx,$GP_STORAGE($TW)\n";
+
+  if ($win64) {
+    $code .= "mov 	 %rdi,$GP_STORAGE + 8*1($TW)\n";
+    $code .= "mov 	 %rsi,$GP_STORAGE + 8*2($TW)\n";
+    $code .= "vmovdqa      %xmm6, $XMM_STORAGE + 16*0($TW)\n";
+    $code .= "vmovdqa      %xmm7, $XMM_STORAGE + 16*1($TW)\n";
+    $code .= "vmovdqa      %xmm8, $XMM_STORAGE + 16*2($TW)\n";
+    $code .= "vmovdqa      %xmm9, $XMM_STORAGE + 16*3($TW)\n";
+    $code .= "vmovdqa      %xmm10, $XMM_STORAGE + 16*4($TW)\n";
+    $code .= "vmovdqa      %xmm11, $XMM_STORAGE + 16*5($TW)\n";
+    $code .= "vmovdqa      %xmm12, $XMM_STORAGE + 16*6($TW)\n";
+    $code .= "vmovdqa      %xmm13, $XMM_STORAGE + 16*7($TW)\n";
+    $code .= "vmovdqa      %xmm14, $XMM_STORAGE + 16*8($TW)\n";
+    $code .= "vmovdqa      %xmm15, $XMM_STORAGE + 16*9($TW)\n";
+  }
+
+  $code .= "mov 	 \$0x87, $gf_poly_8b\n";
+  $code .= "vmovdqu 	 ($tweak),%xmm1\n";      # read initial tweak values
+  $code .= "vpxor 	 %xmm4,%xmm4,%xmm4\n";   # for key expansion
+
+  encrypt_tweak_for_encryption("%xmm0", "%xmm1", "%xmm2", "%xmm3", "%xmm4",
+                               $key2, $key1, $TW);
+
+  if ($win64) {
+    $code .= "mov	 $input, 8 + 8*5(%rbp)\n";  # ciphertext pointer
+    $code .= "mov        $output, 8 + 8*6(%rbp)\n"; # plaintext pointer
+  }
+
+  {
+  $code.=<<___;
+
+  cmp 	 \$0x80,$length
+  jl 	 .L_less_than_128_bytes_${rndsuffix}
+  vpbroadcastq 	 $gf_poly_8b,$ZPOLY
+  cmp 	 \$0x100,$length
+  jge 	 .L_start_by16_${rndsuffix}
+  cmp 	 \$0x80,$length
+  jge 	 .L_start_by8_${rndsuffix}
+
+  .L_do_n_blocks_${rndsuffix}:
+  cmp 	 \$0x0,$length
+  je 	 .L_ret_${rndsuffix}
+  cmp 	 \$0x70,$length
+  jge 	 .L_remaining_num_blocks_is_7_${rndsuffix}
+  cmp 	 \$0x60,$length
+  jge 	 .L_remaining_num_blocks_is_6_${rndsuffix}
+  cmp 	 \$0x50,$length
+  jge 	 .L_remaining_num_blocks_is_5_${rndsuffix}
+  cmp 	 \$0x40,$length
+  jge 	 .L_remaining_num_blocks_is_4_${rndsuffix}
+  cmp 	 \$0x30,$length
+  jge 	 .L_remaining_num_blocks_is_3_${rndsuffix}
+  cmp 	 \$0x20,$length
+  jge 	 .L_remaining_num_blocks_is_2_${rndsuffix}
+  cmp 	 \$0x10,$length
+  jge 	 .L_remaining_num_blocks_is_1_${rndsuffix}
+  vmovdqa 	 %xmm0,%xmm8
+  vmovdqa 	 %xmm9,%xmm0
+  jmp 	 .L_steal_cipher_${rndsuffix}
+
+  .L_remaining_num_blocks_is_7_${rndsuffix}:
+  mov 	 \$0xffffffffffffffff,$tmp1
+  shr 	 \$0x10,$tmp1
+  kmovq 	 $tmp1,%k1
+  vmovdqu8 	 ($input),%zmm1
+  vmovdqu8 	 0x40($input),%zmm2{%k1}
+  add 	 \$0x70,$input
+___
+  }
+
+  encrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8 	 %zmm1,($output)
+  vmovdqu8 	 %zmm2,0x40($output){%k1}
+  add 	 \$0x70,$output
+  vextracti32x4 	 \$0x2,%zmm2,%xmm8
+  vextracti32x4 	 \$0x3,%zmm10,%xmm0
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_${rndsuffix}
+
+  .L_remaining_num_blocks_is_6_${rndsuffix}:
+  vmovdqu8 	 ($input),%zmm1
+  vmovdqu8 	 0x40($input),%ymm2
+  add 	 \$0x60,$input
+___
+  }
+
+  encrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8 	 %zmm1,($output)
+  vmovdqu8 	 %ymm2,0x40($output)
+  add 	 \$0x60,$output
+  vextracti32x4 	 \$0x1,%zmm2,%xmm8
+  vextracti32x4 	 \$0x2,%zmm10,%xmm0
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_${rndsuffix}
+
+  .L_remaining_num_blocks_is_5_${rndsuffix}:
+  vmovdqu8 	 ($input),%zmm1
+  vmovdqu 	 0x40($input),%xmm2
+  add 	 \$0x50,$input
+___
+  }
+
+  encrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8 	 %zmm1,($output)
+  vmovdqu 	 %xmm2,0x40($output)
+  add 	 \$0x50,$output
+  movdqa 	 %xmm2,%xmm8
+  vextracti32x4 	 \$0x1,%zmm10,%xmm0
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_${rndsuffix}
+
+  .L_remaining_num_blocks_is_4_${rndsuffix}:
+  vmovdqu8 	 ($input),%zmm1
+  add 	 \$0x40,$input
+___
+  }
+
+  encrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8 	 %zmm1,($output)
+  add 	 \$0x40,$output
+  vextracti32x4 	 \$0x3,%zmm1,%xmm8
+  vextracti32x4 	 \$0x0,%zmm10,%xmm0
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  {
+  $code .= <<___;
+  .L_remaining_num_blocks_is_3_${rndsuffix}:
+  vextracti32x4 	 \$0x1,%zmm9,%xmm10
+  vextracti32x4 	 \$0x2,%zmm9,%xmm11
+  vmovdqu 	 ($input),%xmm1
+  vmovdqu 	 0x10($input),%xmm2
+  vmovdqu 	 0x20($input),%xmm3
+  add 	 \$0x30,$input
+___
+  }
+
+  encrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 3, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  add 	 \$0x30,$output
+  vmovdqa 	 %xmm3,%xmm8
+  vextracti32x4 	 \$0x3,%zmm9,%xmm0
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  {
+  $code .= <<___;
+  .L_remaining_num_blocks_is_2_${rndsuffix}:
+  vextracti32x4 	 \$0x1,%zmm9,%xmm10
+  vmovdqu 	 ($input),%xmm1
+  vmovdqu 	 0x10($input),%xmm2
+  add 	 \$0x20,$input
+___
+  }
+
+  encrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 2, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  add 	 \$0x20,$output
+  vmovdqa 	 %xmm2,%xmm8
+  vextracti32x4 	 \$0x2,%zmm9,%xmm0
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  {
+  $code .= <<___;
+  .L_remaining_num_blocks_is_1_${rndsuffix}:
+  vmovdqu 	 ($input),%xmm1
+  add 	 \$0x10,$input
+___
+  }
+
+  encrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 1, 1);
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  add 	 \$0x10,$output
+  vmovdqa 	 %xmm1,%xmm8
+  vextracti32x4 	 \$0x1,%zmm9,%xmm0
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_${rndsuffix}
+
+  .L_start_by16_${rndsuffix}:
+  vbroadcasti32x4 	 (%rsp),%zmm0
+  vbroadcasti32x4 shufb_15_7(%rip),%zmm8
+  mov 	 \$0xaa,$tmp1
+  kmovq 	 $tmp1,%k2
+  vpshufb 	 %zmm8,%zmm0,%zmm1
+  vpsllvq const_dq3210(%rip),%zmm0,%zmm4
+  vpsrlvq const_dq5678(%rip),%zmm1,%zmm2
+  vpclmulqdq 	 \$0x0,%zmm25,%zmm2,%zmm3
+  vpxorq 	 %zmm2,%zmm4,%zmm4{%k2}
+  vpxord 	 %zmm4,%zmm3,%zmm9
+  vpsllvq const_dq7654(%rip),%zmm0,%zmm5
+  vpsrlvq const_dq1234(%rip),%zmm1,%zmm6
+  vpclmulqdq 	 \$0x0,%zmm25,%zmm6,%zmm7
+  vpxorq 	 %zmm6,%zmm5,%zmm5{%k2}
+  vpxord 	 %zmm5,%zmm7,%zmm10
+  vpsrldq 	 \$0xf,%zmm9,%zmm13
+  vpclmulqdq 	 \$0x0,%zmm25,%zmm13,%zmm14
+  vpslldq 	 \$0x1,%zmm9,%zmm11
+  vpxord 	 %zmm14,%zmm11,%zmm11
+  vpsrldq 	 \$0xf,%zmm10,%zmm15
+  vpclmulqdq 	 \$0x0,%zmm25,%zmm15,%zmm16
+  vpslldq 	 \$0x1,%zmm10,%zmm12
+  vpxord 	 %zmm16,%zmm12,%zmm12
+
+  .L_main_loop_run_16_${rndsuffix}:
+  vmovdqu8 	 ($input),%zmm1
+  vmovdqu8 	 0x40($input),%zmm2
+  vmovdqu8 	 0x80($input),%zmm3
+  vmovdqu8 	 0xc0($input),%zmm4
+  add 	 \$0x100,$input
+___
+  }
+
+  encrypt_by_16_zmm("%zmm1", "%zmm2", "%zmm3", "%zmm4", "%zmm9",
+                    "%zmm10", "%zmm11", "%zmm12", "%zmm0", 0);
+
+  {
+  $code .= <<___;
+  vmovdqu8 	 %zmm1,($output)
+  vmovdqu8 	 %zmm2,0x40($output)
+  vmovdqu8 	 %zmm3,0x80($output)
+  vmovdqu8 	 %zmm4,0xc0($output)
+  add 	 \$0x100,$output
+  sub 	 \$0x100,$length
+  cmp 	 \$0x100,$length
+  jge 	 .L_main_loop_run_16_${rndsuffix}
+  cmp 	 \$0x80,$length
+  jge 	 .L_main_loop_run_8_${rndsuffix}
+  vextracti32x4 	 \$0x3,%zmm4,%xmm0
+  jmp 	 .L_do_n_blocks_${rndsuffix}
+
+  .L_start_by8_${rndsuffix}:
+  vbroadcasti32x4 	 (%rsp),%zmm0
+  vbroadcasti32x4 shufb_15_7(%rip),%zmm8
+  mov 	 \$0xaa,$tmp1
+  kmovq 	 $tmp1,%k2
+  vpshufb 	 %zmm8,%zmm0,%zmm1
+  vpsllvq const_dq3210(%rip),%zmm0,%zmm4
+  vpsrlvq const_dq5678(%rip),%zmm1,%zmm2
+  vpclmulqdq 	 \$0x0,%zmm25,%zmm2,%zmm3
+  vpxorq 	 %zmm2,%zmm4,%zmm4{%k2}
+  vpxord 	 %zmm4,%zmm3,%zmm9
+  vpsllvq const_dq7654(%rip),%zmm0,%zmm5
+  vpsrlvq const_dq1234(%rip),%zmm1,%zmm6
+  vpclmulqdq 	 \$0x0,%zmm25,%zmm6,%zmm7
+  vpxorq 	 %zmm6,%zmm5,%zmm5{%k2}
+  vpxord 	 %zmm5,%zmm7,%zmm10
+
+  .L_main_loop_run_8_${rndsuffix}:
+  vmovdqu8 	 ($input),%zmm1
+  vmovdqu8 	 0x40($input),%zmm2
+  add 	 \$0x80,$input
+___
+  }
+
+  encrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 0);
+
+  {
+  $code .= <<___;
+  vmovdqu8 	 %zmm1,($output)
+  vmovdqu8 	 %zmm2,0x40($output)
+  add 	 \$0x80,$output
+  sub 	 \$0x80,$length
+  cmp 	 \$0x80,$length
+  jge 	 .L_main_loop_run_8_${rndsuffix}
+  vextracti32x4 	 \$0x3,%zmm2,%xmm0
+  jmp 	 .L_do_n_blocks_${rndsuffix}
+
+  .L_steal_cipher_next_${rndsuffix}:
+  xor 	 $gf_poly_8b_temp,$gf_poly_8b_temp
+  shl 	 \$1, $TWTEMPL
+  adc 	 $TWTEMPH,$TWTEMPH
+  cmovc  $gf_poly_8b,$gf_poly_8b_temp
+  xor 	 $gf_poly_8b_temp,$TWTEMPL
+  mov 	 $TWTEMPL,($TW)
+  mov 	 $TWTEMPH,0x8($TW)
+  vmovdqa 	 ($TW),%xmm0
+
+  .L_steal_cipher_${rndsuffix}:
+  vmovdqa 	 %xmm8,%xmm2
+  lea vpshufb_shf_table(%rip),$TWTEMPL
+  vmovdqu 	 ($TWTEMPL,$length,1),%xmm10
+  vpshufb 	 %xmm10,%xmm8,%xmm8
+  vmovdqu 	 -0x10($input,$length,1),%xmm3
+  vmovdqu 	 %xmm8,-0x10($output,$length,1)
+  lea vpshufb_shf_table(%rip),$TWTEMPL
+  add \$16, $TWTEMPL
+  sub 	 $length,$TWTEMPL
+  vmovdqu 	 ($TWTEMPL),%xmm10
+  vpxor mask1(%rip),%xmm10,%xmm10
+  vpshufb 	 %xmm10,%xmm3,%xmm3
+  vpblendvb 	 %xmm10,%xmm2,%xmm3,%xmm3
+  vpxor 	 %xmm0,%xmm3,%xmm8
+  vpxor 	 0x80(%rsp),%xmm8,%xmm8
+  vaesenc 	 0x90(%rsp),%xmm8,%xmm8
+  vaesenc 	 0xa0(%rsp),%xmm8,%xmm8
+  vaesenc 	 0xb0(%rsp),%xmm8,%xmm8
+  vaesenc 	 0xc0(%rsp),%xmm8,%xmm8
+  vaesenc 	 0xd0(%rsp),%xmm8,%xmm8
+  vaesenc 	 0xe0(%rsp),%xmm8,%xmm8
+  vaesenc 	 0xf0(%rsp),%xmm8,%xmm8
+  vaesenc 	 0x100(%rsp),%xmm8,%xmm8
+  vaesenc 	 0x110(%rsp),%xmm8,%xmm8
+  vaesenc 	 0x120(%rsp),%xmm8,%xmm8
+  vaesenc 	 0x130(%rsp),%xmm8,%xmm8
+  vaesenc 	 0x140(%rsp),%xmm8,%xmm8
+  vaesenc 	 0x150(%rsp),%xmm8,%xmm8
+  vaesenclast 	 0x160(%rsp),%xmm8,%xmm8
+  vpxor 	 %xmm0,%xmm8,%xmm8
+  vmovdqu 	 %xmm8,-0x10($output)
+___
+  }
+  $code .= "\n.L_ret_${rndsuffix}:\n";
+  $code .= "mov 	 $GP_STORAGE($TW),%rbx\n";
+
+  if ($win64) {
+    $code .= "mov 	 $GP_STORAGE + 8*1($TW),%rdi\n";
+    $code .= "mov 	 $GP_STORAGE + 8*2($TW),%rsi\n";
+
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 0($TW), %xmm6\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 1($TW), %xmm7\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 2($TW), %xmm8\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 3($TW), %xmm9\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 4($TW), %xmm10\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 5($TW), %xmm11\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 6($TW), %xmm12\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 7($TW), %xmm13\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 8($TW), %xmm14\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 9($TW), %xmm15\n";
+  }
+
+  {
+  $code .= <<___;
+  mov 	 %rbp,%rsp
+  pop 	 %rbp
+  ret
+
+  .L_less_than_128_bytes_${rndsuffix}:
+  cmp 	 \$0x10,$length
+  jb 	 .L_ret_${rndsuffix}
+  mov 	 $length,$tmp1
+  and 	 \$0x70,$tmp1
+  cmp 	 \$0x60,$tmp1
+  je 	 .L_num_blocks_is_6_${rndsuffix}
+  cmp 	 \$0x50,$tmp1
+  je 	 .L_num_blocks_is_5_${rndsuffix}
+  cmp 	 \$0x40,$tmp1
+  je 	 .L_num_blocks_is_4_${rndsuffix}
+  cmp 	 \$0x30,$tmp1
+  je 	 .L_num_blocks_is_3_${rndsuffix}
+  cmp 	 \$0x20,$tmp1
+  je 	 .L_num_blocks_is_2_${rndsuffix}
+  cmp 	 \$0x10,$tmp1
+  je 	 .L_num_blocks_is_1_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_7_${rndsuffix}:\n";
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 7);
+
+  $code .= "add      \$0x70,$input\n";
+
+  encrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 7, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  vmovdqu 	 %xmm4,0x30($output)
+  vmovdqu 	 %xmm5,0x40($output)
+  vmovdqu 	 %xmm6,0x50($output)
+  vmovdqu 	 %xmm7,0x60($output)
+  add 	 \$0x70,$output
+  vmovdqa 	 %xmm7,%xmm8
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_next_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_6_${rndsuffix}:\n";
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 6);
+
+  $code .= "add      \$0x60,$input\n";
+
+  encrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 6, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  vmovdqu 	 %xmm4,0x30($output)
+  vmovdqu 	 %xmm5,0x40($output)
+  vmovdqu 	 %xmm6,0x50($output)
+  add 	 \$0x60,$output
+  vmovdqa 	 %xmm6,%xmm8
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_next_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_5_${rndsuffix}:\n";
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 5);
+
+  $code .= "add      \$0x50,$input\n";
+
+  encrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 5, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  vmovdqu 	 %xmm4,0x30($output)
+  vmovdqu 	 %xmm5,0x40($output)
+  add 	 \$0x50,$output
+  vmovdqa 	 %xmm5,%xmm8
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_next_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_4_${rndsuffix}:\n";
+
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 4);
+
+  $code .= "add      \$0x40, $input\n";
+
+  encrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 4, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  vmovdqu 	 %xmm4,0x30($output)
+  add 	 \$0x40,$output
+  vmovdqa 	 %xmm4,%xmm8
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_next_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_3_${rndsuffix}:\n";
+
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 3);
+
+  $code .= "add      \$0x30,$input\n";
+
+  encrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 3, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  add 	 \$0x30,$output
+  vmovdqa 	 %xmm3,%xmm8
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_next_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_2_${rndsuffix}:\n";
+
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 2);
+
+  $code .= "add      \$0x20,$input\n";
+
+  encrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 2, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  add 	 \$0x20,$output
+  vmovdqa 	 %xmm2,%xmm8
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_next_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_1_${rndsuffix}:\n";
+
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 1);
+
+  $code .= "add      \$0x10,$input\n";
+
+  encrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 1, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  add 	 \$0x10,$output
+  vmovdqa 	 %xmm1,%xmm8
+  and 	 \$0xf,$length
+  je 	 .L_ret_${rndsuffix}
+  jmp 	 .L_steal_cipher_next_${rndsuffix}
+  ret
+  .cfi_endproc
+___
+  }
+
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+  # ;void aes_hw_xts_decrypt_avx512(
+  # ;               const uint8_t *in,        // input data
+  # ;               uint8_t *out,             // output data
+  # ;               size_t length,            // sector size, in bytes
+  # ;               const AES_KEY *key1,      // key used for "ECB" encryption, 16*2 bytes
+  # ;               const AES_KEY *key2,      // key used for tweaking, 16*2 bytes
+  # ;               const uint8_t iv[16])      // initial tweak value, 16 bytes
+  # ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
+
+  my $rndsuffix = &random_string();
+
+  {
+  $code.=<<___;
+  .globl	aes_hw_xts_decrypt_avx512
+  .hidden	aes_hw_xts_decrypt_avx512
+  .type	aes_hw_xts_decrypt_avx512,\@abi-omnipotent
+  .align	32
+  aes_hw_xts_decrypt_avx512:
+  .cfi_startproc
+          endbranch
+___
+  }
+  $code .= "push 	 %rbp\n";
+  $code .= "mov 	 %rsp,%rbp\n";
+  $code .= "sub 	 \$$VARIABLE_OFFSET,%rsp\n";
+  $code .= "and 	 \$0xffffffffffffffc0,%rsp\n";
+  $code .= "mov 	 %rbx,$GP_STORAGE($TW)\n";
+
+  if ($win64) {
+    $code .= "mov 	 %rdi,$GP_STORAGE + 8*1($TW)\n";
+    $code .= "mov 	 %rsi,$GP_STORAGE + 8*2($TW)\n";
+    $code .= "vmovdqa      %xmm6, $XMM_STORAGE + 16*0($TW)\n";
+    $code .= "vmovdqa      %xmm7, $XMM_STORAGE + 16*1($TW)\n";
+    $code .= "vmovdqa      %xmm8, $XMM_STORAGE + 16*2($TW)\n";
+    $code .= "vmovdqa      %xmm9, $XMM_STORAGE + 16*3($TW)\n";
+    $code .= "vmovdqa      %xmm10, $XMM_STORAGE + 16*4($TW)\n";
+    $code .= "vmovdqa      %xmm11, $XMM_STORAGE + 16*5($TW)\n";
+    $code .= "vmovdqa      %xmm12, $XMM_STORAGE + 16*6($TW)\n";
+    $code .= "vmovdqa      %xmm13, $XMM_STORAGE + 16*7($TW)\n";
+    $code .= "vmovdqa      %xmm14, $XMM_STORAGE + 16*8($TW)\n";
+    $code .= "vmovdqa      %xmm15, $XMM_STORAGE + 16*9($TW)\n";
+  }
+
+  $code .= "mov 	 \$0x87, $gf_poly_8b\n";
+  $code .= "vmovdqu 	 ($tweak),%xmm1\n";      # read initial tweak values
+  $code .= "vpxor 	 %xmm4,%xmm4,%xmm4\n"; # for key expansion
+
+  encrypt_tweak_for_decryption("%xmm0", "%xmm1", "%xmm2", "%xmm3", "%xmm4",
+                               $key2, $key1, $TW);
+
+  if ($win64) {
+    $code .= "mov	 $input, 8 + 8*5(%rbp)\n"; # ciphertext pointer
+    $code .= "mov        $output, 8 + 8*6(%rbp)\n"; # plaintext pointer
+  }
+
+  {
+  $code.=<<___;
+
+  cmp 	 \$0x80,$length
+  jb 	 .L_less_than_128_bytes_${rndsuffix}
+  vpbroadcastq 	 $gf_poly_8b,$ZPOLY
+  cmp 	 \$0x100,$length
+  jge 	 .L_start_by16_${rndsuffix}
+  jmp 	 .L_start_by8_${rndsuffix}
+
+  .L_do_n_blocks_${rndsuffix}:
+  cmp 	 \$0x0,$length
+  je 	 .L_ret_${rndsuffix}
+  cmp 	 \$0x70,$length
+  jge 	 .L_remaining_num_blocks_is_7_${rndsuffix}
+  cmp 	 \$0x60,$length
+  jge 	 .L_remaining_num_blocks_is_6_${rndsuffix}
+  cmp 	 \$0x50,$length
+  jge 	 .L_remaining_num_blocks_is_5_${rndsuffix}
+  cmp 	 \$0x40,$length
+  jge 	 .L_remaining_num_blocks_is_4_${rndsuffix}
+  cmp 	 \$0x30,$length
+  jge 	 .L_remaining_num_blocks_is_3_${rndsuffix}
+  cmp 	 \$0x20,$length
+  jge 	 .L_remaining_num_blocks_is_2_${rndsuffix}
+  cmp 	 \$0x10,$length
+  jge 	 .L_remaining_num_blocks_is_1_${rndsuffix}
+
+  # _remaining_num_blocks_is_0:
+  vmovdqu		%xmm5, %xmm1
+  # xmm5 contains last full block to decrypt with next teawk
+___
+  }
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 1, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu %xmm1, -0x10($output)
+  vmovdqa %xmm1, %xmm8
+
+  # Calc previous tweak
+  mov		\$0x1,$tmp1
+  kmovq		$tmp1, %k1
+  vpsllq	\$0x3f,%xmm9,%xmm13
+  vpsraq	\$0x3f,%xmm13,%xmm14
+  vpandq	%xmm25,%xmm14,%xmm5
+  vpxorq        %xmm5,%xmm9,%xmm9{%k1}
+  vpsrldq       \$0x8,%xmm9,%xmm10
+  .byte 98, 211, 181, 8, 115, 194, 1 #vpshrdq \$0x1,%xmm10,%xmm9,%xmm0
+  vpslldq       \$0x8,%xmm13,%xmm13
+  vpxorq        %xmm13,%xmm0,%xmm0
+  jmp           .L_steal_cipher_${rndsuffix}
+
+  .L_remaining_num_blocks_is_7_${rndsuffix}:
+  mov 	 \$0xffffffffffffffff,$tmp1
+  shr 	 \$0x10,$tmp1
+  kmovq 	 $tmp1,%k1
+  vmovdqu8 	 ($input),%zmm1
+  vmovdqu8 	 0x40($input),%zmm2{%k1}
+  add 	         \$0x70,$input
+  and            \$0xf,$length
+  je             .L_done_7_remain_${rndsuffix}
+  vextracti32x4   \$0x2,%zmm10,%xmm12
+  vextracti32x4   \$0x3,%zmm10,%xmm13
+  vinserti32x4    \$0x2,%xmm13,%zmm10,%zmm10
+___
+  }
+
+  decrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8 	 %zmm1, ($output)
+  vmovdqu8 	 %zmm2, 0x40($output){%k1}
+  add 	         \$0x70, $output
+  vextracti32x4  \$0x2,%zmm2,%xmm8
+  vmovdqa        %xmm12,%xmm0
+  jmp            .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_done_7_remain_${rndsuffix}:\n";
+  decrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8        %zmm1, ($output)
+  vmovdqu8        %zmm2, 0x40($output){%k1}
+  jmp     .L_ret_${rndsuffix}
+
+  .L_remaining_num_blocks_is_6_${rndsuffix}:
+  vmovdqu8 	 ($input),%zmm1
+  vmovdqu8 	 0x40($input),%ymm2
+  add 	         \$0x60,$input
+  and            \$0xf, $length
+  je             .L_done_6_remain_${rndsuffix}
+  vextracti32x4   \$0x1,%zmm10,%xmm12
+  vextracti32x4   \$0x2,%zmm10,%xmm13
+  vinserti32x4    \$0x1,%xmm13,%zmm10,%zmm10
+___
+  }
+
+  decrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8 	 %zmm1, ($output)
+  vmovdqu8 	 %ymm2, 0x40($output)
+  add 	         \$0x60,$output
+  vextracti32x4  \$0x1,%zmm2,%xmm8
+  vmovdqa        %xmm12,%xmm0
+  jmp            .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_done_6_remain_${rndsuffix}:\n";
+  decrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8        %zmm1, ($output)
+  vmovdqu8        %ymm2,0x40($output)
+  jmp             .L_ret_${rndsuffix}
+
+  .L_remaining_num_blocks_is_5_${rndsuffix}:
+  vmovdqu8 	 ($input),%zmm1
+  vmovdqu 	 0x40($input),%xmm2
+  add 	         \$0x50,$input
+  and            \$0xf,$length
+  je             .L_done_5_remain_${rndsuffix}
+  vmovdqa        %xmm10,%xmm12
+  vextracti32x4  \$0x1,%zmm10,%xmm10
+___
+  }
+
+  decrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8         %zmm1, ($output)
+  vmovdqu          %xmm2, 0x40($output)
+  add              \$0x50, $output
+  vmovdqa          %xmm2,%xmm8
+  vmovdqa          %xmm12,%xmm0
+  jmp              .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_done_5_remain_${rndsuffix}:\n";
+  decrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8        %zmm1, ($output)
+  vmovdqu8        %xmm2, 0x40($output)
+  jmp             .L_ret_${rndsuffix}
+
+  .L_remaining_num_blocks_is_4_${rndsuffix}:
+  vmovdqu8 	 ($input),%zmm1
+  add 	         \$0x40,$input
+  and            \$0xf, $length
+  je             .L_done_4_remain_${rndsuffix}
+  vextracti32x4   \$0x3,%zmm9,%xmm12
+  vinserti32x4    \$0x3,%xmm10,%zmm9,%zmm9
+___
+  }
+
+  decrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8        %zmm1,($output)
+  add             \$0x40,$output
+  vextracti32x4   \$0x3,%zmm1,%xmm8
+  vmovdqa         %xmm12,%xmm0
+  jmp             .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_done_4_remain_${rndsuffix}:\n";
+  decrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 1);
+
+  {
+  $code .= <<___;
+  vmovdqu8        %zmm1, ($output)
+  jmp             .L_ret_${rndsuffix}
+
+  .L_remaining_num_blocks_is_3_${rndsuffix}:
+  vmovdqu         ($input),%xmm1
+  vmovdqu         0x10($input),%xmm2
+  vmovdqu         0x20($input),%xmm3
+  add             \$0x30,$input
+  and             \$0xf,$length
+  je              .L_done_3_remain_${rndsuffix}
+  vextracti32x4   \$0x2,%zmm9,%xmm13
+  vextracti32x4   \$0x1,%zmm9,%xmm10
+  vextracti32x4   \$0x3,%zmm9,%xmm11
+___
+  }
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 3, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  add 	         \$0x30,$output
+  vmovdqa 	 %xmm3,%xmm8
+  vmovdqa        %xmm13,%xmm0
+  jmp 	         .L_steal_cipher_${rndsuffix}
+___
+  }
+  $code .= "\n.L_done_3_remain_${rndsuffix}:\n";
+  $code .= "vextracti32x4   \$0x1,%zmm9,%xmm10\n";
+  $code .= "vextracti32x4   \$0x2,%zmm9,%xmm11\n";
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 3, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu %xmm1,($output)
+  vmovdqu %xmm2,0x10($output)
+  vmovdqu %xmm3,0x20($output)
+  jmp     .L_ret_${rndsuffix}
+
+  .L_remaining_num_blocks_is_2_${rndsuffix}:
+  vmovdqu         ($input),%xmm1
+  vmovdqu         0x10($input),%xmm2
+  add             \$0x20,$input
+  and             \$0xf,$length
+  je              .L_done_2_remain_${rndsuffix}
+  vextracti32x4   \$0x2,%zmm9,%xmm10
+  vextracti32x4   \$0x1,%zmm9,%xmm12
+___
+  }
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 2, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  add 	         \$0x20,$output
+  vmovdqa 	 %xmm2,%xmm8
+  vmovdqa 	 %xmm12,%xmm0
+  jmp 	         .L_steal_cipher_${rndsuffix}
+___
+  }
+  $code .= "\n.L_done_2_remain_${rndsuffix}:\n";
+  $code .= "vextracti32x4   \$0x1,%zmm9,%xmm10\n";
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 2, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu   %xmm1,($output)
+  vmovdqu   %xmm2,0x10($output)
+  jmp       .L_ret_${rndsuffix}
+
+  .L_remaining_num_blocks_is_1_${rndsuffix}:
+  vmovdqu 	 ($input),%xmm1
+  add 	         \$0x10,$input
+  and            \$0xf,$length
+  je             .L_done_1_remain_${rndsuffix}
+  vextracti32x4  \$0x1,%zmm9,%xmm11
+___
+  }
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm11", "%xmm10", "%xmm9", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 1, 1);
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  add 	         \$0x10,$output
+  vmovdqa 	 %xmm1,%xmm8
+  vmovdqa 	 %xmm9,%xmm0
+  jmp 	         .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_done_1_remain_${rndsuffix}:\n";
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 1, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu   %xmm1, ($output)
+  jmp       .L_ret_${rndsuffix}
+
+  .L_start_by16_${rndsuffix}:
+  vbroadcasti32x4 	 ($TW),%zmm0
+  vbroadcasti32x4 shufb_15_7(%rip),%zmm8
+  mov 	 \$0xaa,$tmp1
+  kmovq 	 $tmp1,%k2
+
+  # Mult tweak by 2^{3, 2, 1, 0}
+  vpshufb 	 %zmm8,%zmm0,%zmm1
+  vpsllvq const_dq3210(%rip),%zmm0,%zmm4
+  vpsrlvq const_dq5678(%rip),%zmm1,%zmm2
+  vpclmulqdq 	 \$0x0,$ZPOLY,%zmm2,%zmm3
+  vpxorq 	 %zmm2,%zmm4,%zmm4{%k2}
+  vpxord 	 %zmm4,%zmm3,%zmm9
+
+  # Mult tweak by 2^{7, 6, 5, 4}
+  vpsllvq const_dq7654(%rip),%zmm0,%zmm5
+  vpsrlvq const_dq1234(%rip),%zmm1,%zmm6
+  vpclmulqdq 	 \$0x0,%zmm25,%zmm6,%zmm7
+  vpxorq 	 %zmm6,%zmm5,%zmm5{%k2}
+  vpxord 	 %zmm5,%zmm7,%zmm10
+
+  # Make next 8 tweek values by all x 2^8
+  vpsrldq 	 \$0xf,%zmm9,%zmm13
+  vpclmulqdq 	 \$0x0,%zmm25,%zmm13,%zmm14
+  vpslldq 	 \$0x1,%zmm9,%zmm11
+  vpxord 	 %zmm14,%zmm11,%zmm11
+
+  vpsrldq 	 \$0xf,%zmm10,%zmm15
+  vpclmulqdq 	 \$0x0,%zmm25,%zmm15,%zmm16
+  vpslldq 	 \$0x1,%zmm10,%zmm12
+  vpxord 	 %zmm16,%zmm12,%zmm12
+
+  .L_main_loop_run_16_${rndsuffix}:
+  vmovdqu8 	 ($input),%zmm1
+  vmovdqu8 	 0x40($input),%zmm2
+  vmovdqu8 	 0x80($input),%zmm3
+  vmovdqu8 	 0xc0($input),%zmm4
+  vmovdqu8 	 0xf0($input),%zmm5
+  add 	 \$0x100,$input
+___
+  }
+
+  decrypt_by_16_zmm("%zmm1", "%zmm2", "%zmm3", "%zmm4", "%zmm9",
+                    "%zmm10", "%zmm11", "%zmm12", "%zmm0", 0);
+
+  {
+  $code .= <<___;
+  vmovdqu8 	 %zmm1,($output)
+  vmovdqu8 	 %zmm2,0x40($output)
+  vmovdqu8 	 %zmm3,0x80($output)
+  vmovdqu8 	 %zmm4,0xc0($output)
+  add 	 \$0x100,$output
+  sub 	 \$0x100,$length
+  cmp 	 \$0x100,$length
+  jge 	 .L_main_loop_run_16_${rndsuffix}
+
+  cmp 	 \$0x80,$length
+  jge 	 .L_main_loop_run_8_${rndsuffix}
+  jmp 	 .L_do_n_blocks_${rndsuffix}
+
+  .L_start_by8_${rndsuffix}:
+  # Make first 7 tweek values
+  vbroadcasti32x4 	 ($TW),%zmm0
+  vbroadcasti32x4 shufb_15_7(%rip),%zmm8
+  mov 	 \$0xaa,$tmp1
+  kmovq 	 $tmp1,%k2
+
+  # Mult tweak by 2^{3, 2, 1, 0}
+  vpshufb 	 %zmm8,%zmm0,%zmm1
+  vpsllvq const_dq3210(%rip),%zmm0,%zmm4
+  vpsrlvq const_dq5678(%rip),%zmm1,%zmm2
+  vpclmulqdq 	 \$0x0,%zmm25,%zmm2,%zmm3
+  vpxorq 	 %zmm2,%zmm4,%zmm4{%k2}
+  vpxord 	 %zmm4,%zmm3,%zmm9
+
+  # Mult tweak by 2^{7, 6, 5, 4}
+  vpsllvq const_dq7654(%rip),%zmm0,%zmm5
+  vpsrlvq const_dq1234(%rip),%zmm1,%zmm6
+  vpclmulqdq 	 \$0x0,%zmm25,%zmm6,%zmm7
+  vpxorq 	 %zmm6,%zmm5,%zmm5{%k2}
+  vpxord 	 %zmm5,%zmm7,%zmm10
+
+  .L_main_loop_run_8_${rndsuffix}:
+  vmovdqu8 	 ($input),%zmm1
+  vmovdqu8 	 0x40($input),%zmm2
+  vmovdqu8 	 0x70($input),%xmm5
+  add 	         \$0x80,$input
+___
+  }
+
+
+  decrypt_by_eight_zmm("%zmm1", "%zmm2", "%zmm9", "%zmm10", "%zmm0", 0);
+
+  {
+  $code .= <<___;
+  vmovdqu8 	 %zmm1,($output)
+  vmovdqu8 	 %zmm2,0x40($output)
+  add 	 \$0x80,$output
+  sub 	 \$0x80,$length
+  cmp 	 \$0x80,$length
+  jge 	 .L_main_loop_run_8_${rndsuffix}
+  jmp 	 .L_do_n_blocks_${rndsuffix}
+
+  .L_steal_cipher_${rndsuffix}:
+  # start cipher stealing simplified: xmm8-last cipher block, xmm0-next tweak
+  vmovdqa 	 %xmm8,%xmm2
+
+  # shift xmm8 to the left by 16-N_val bytes
+  lea vpshufb_shf_table(%rip),$TWTEMPL
+  vmovdqu 	 ($TWTEMPL,$length,1),%xmm10
+  vpshufb 	 %xmm10,%xmm8,%xmm8
+
+
+  vmovdqu 	 -0x10($input,$length,1),%xmm3
+  vmovdqu 	 %xmm8,-0x10($output,$length,1)
+
+  # shift xmm3 to the right by 16-N_val bytes
+  lea vpshufb_shf_table(%rip), $TWTEMPL
+  add \$16, $TWTEMPL
+  sub 	 $length,$TWTEMPL
+  vmovdqu 	 ($TWTEMPL),%xmm10
+  vpxor mask1(%rip),%xmm10,%xmm10
+  vpshufb 	 %xmm10,%xmm3,%xmm3
+
+  vpblendvb 	 %xmm10,%xmm2,%xmm3,%xmm3
+
+  # xor Tweak value
+  vpxor 	 %xmm0,%xmm3,%xmm8
+
+  # decrypt last block with cipher stealing
+  vpxor 	 0x80(%rsp),%xmm8,%xmm8
+  vaesdec 	 0x90(%rsp),%xmm8,%xmm8
+  vaesdec 	 0xa0(%rsp),%xmm8,%xmm8
+  vaesdec 	 0xb0(%rsp),%xmm8,%xmm8
+  vaesdec 	 0xc0(%rsp),%xmm8,%xmm8
+  vaesdec 	 0xd0(%rsp),%xmm8,%xmm8
+  vaesdec 	 0xe0(%rsp),%xmm8,%xmm8
+  vaesdec 	 0xf0(%rsp),%xmm8,%xmm8
+  vaesdec 	 0x100(%rsp),%xmm8,%xmm8
+  vaesdec 	 0x110(%rsp),%xmm8,%xmm8
+  vaesdec 	 0x120(%rsp),%xmm8,%xmm8
+  vaesdec 	 0x130(%rsp),%xmm8,%xmm8
+  vaesdec 	 0x140(%rsp),%xmm8,%xmm8
+  vaesdec 	 0x150(%rsp),%xmm8,%xmm8
+  vaesdeclast 	 0x160(%rsp),%xmm8,%xmm8
+
+  # xor Tweak value
+  vpxor 	 %xmm0,%xmm8,%xmm8
+
+  .L_done_${rndsuffix}:
+  # store last ciphertext value
+  vmovdqu 	 %xmm8,-0x10($output)
+___
+  }
+  $code .= "\n.L_ret_${rndsuffix}:\n";
+  $code .= "mov 	 $GP_STORAGE($TW),%rbx\n";
+
+  if ($win64) {
+    $code .= "mov 	 $GP_STORAGE + 8*1($TW),%rdi\n";
+    $code .= "mov 	 $GP_STORAGE + 8*2($TW),%rsi\n";
+
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 0($TW), %xmm6\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 1($TW), %xmm7\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 2($TW), %xmm8\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 3($TW), %xmm9\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 4($TW), %xmm10\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 5($TW), %xmm11\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 6($TW), %xmm12\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 7($TW), %xmm13\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 8($TW), %xmm14\n";
+    $code .= "vmovdqa  $XMM_STORAGE + 16 * 9($TW), %xmm15\n";
+  }
+
+  {
+  $code .= <<___;
+  mov 	 %rbp,%rsp
+  pop 	 %rbp
+  ret
+
+  .L_less_than_128_bytes_${rndsuffix}:
+  cmp 	 \$0x10,$length
+  jb 	 .L_ret_${rndsuffix}
+
+  mov 	 $length,$tmp1
+  and 	 \$0x70,$tmp1
+  cmp 	 \$0x60,$tmp1
+  je 	 .L_num_blocks_is_6_${rndsuffix}
+  cmp 	 \$0x50,$tmp1
+  je 	 .L_num_blocks_is_5_${rndsuffix}
+  cmp 	 \$0x40,$tmp1
+  je 	 .L_num_blocks_is_4_${rndsuffix}
+  cmp 	 \$0x30,$tmp1
+  je 	 .L_num_blocks_is_3_${rndsuffix}
+  cmp 	 \$0x20,$tmp1
+  je 	 .L_num_blocks_is_2_${rndsuffix}
+  cmp 	 \$0x10,$tmp1
+  je 	 .L_num_blocks_is_1_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_7_${rndsuffix}:\n";
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 7);
+
+  {
+  $code .= <<___;
+  add    \$0x70,$input
+  and    \$0xf,$length
+  je      .L_done_7_${rndsuffix}
+
+  .L_steal_cipher_7_${rndsuffix}:
+   xor         $gf_poly_8b_temp, $gf_poly_8b_temp
+   shl         \$1, $TWTEMPL
+   adc         $TWTEMPH, $TWTEMPH
+   cmovc       $gf_poly_8b, $gf_poly_8b_temp
+   xor         $gf_poly_8b_temp, $TWTEMPL
+   mov         $TWTEMPL,0x10($TW)
+   mov         $TWTEMPH,0x18($TW)
+   vmovdqa64   %xmm15,%xmm16
+   vmovdqa     0x10(%rsp),%xmm15
+___
+  }
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 7, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  vmovdqu 	 %xmm4,0x30($output)
+  vmovdqu 	 %xmm5,0x40($output)
+  vmovdqu 	 %xmm6,0x50($output)
+  add 	         \$0x70,$output
+  vmovdqa64 	 %xmm16,%xmm0
+  vmovdqa 	 %xmm7,%xmm8
+  jmp 	         .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_done_7_${rndsuffix}:\n";
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 7, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  vmovdqu 	 %xmm4,0x30($output)
+  vmovdqu 	 %xmm5,0x40($output)
+  vmovdqu 	 %xmm6,0x50($output)
+  add 	         \$0x70,$output
+  vmovdqa 	 %xmm7,%xmm8
+  jmp 	         .L_done_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_6_${rndsuffix}:\n";
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 6);
+
+  {
+  $code .= <<___;
+  add    \$0x60,$input
+  and    \$0xf,$length
+  je      .L_done_6_${rndsuffix}
+
+  .L_steal_cipher_6_${rndsuffix}:
+   xor         $gf_poly_8b_temp, $gf_poly_8b_temp
+   shl         \$1, $TWTEMPL
+   adc         $TWTEMPH, $TWTEMPH
+   cmovc       $gf_poly_8b, $gf_poly_8b_temp
+   xor         $gf_poly_8b_temp, $TWTEMPL
+   mov         $TWTEMPL,0x10($TW)
+   mov         $TWTEMPH,0x18($TW)
+   vmovdqa64   %xmm14,%xmm15
+   vmovdqa     0x10(%rsp),%xmm14
+___
+  }
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 6, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  vmovdqu 	 %xmm4,0x30($output)
+  vmovdqu 	 %xmm5,0x40($output)
+  add 	         \$0x60,$output
+  vmovdqa 	 %xmm15,%xmm0
+  vmovdqa 	 %xmm6,%xmm8
+  jmp 	         .L_steal_cipher_${rndsuffix}
+___
+  }
+  $code .= "\n.L_done_6_${rndsuffix}:\n";
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 6, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  vmovdqu 	 %xmm4,0x30($output)
+  vmovdqu 	 %xmm5,0x40($output)
+  add 	         \$0x60,$output
+  vmovdqa 	 %xmm6,%xmm8
+  jmp 	         .L_done_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_5_${rndsuffix}:\n";
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 5);
+
+  {
+  $code .= <<___;
+  add    \$0x50,$input
+  and    \$0xf,$length
+  je      .L_done_5_${rndsuffix}
+
+  .L_steal_cipher_5_${rndsuffix}:
+   xor         $gf_poly_8b_temp, $gf_poly_8b_temp
+   shl         \$1, $TWTEMPL
+   adc         $TWTEMPH, $TWTEMPH
+   cmovc       $gf_poly_8b, $gf_poly_8b_temp
+   xor         $gf_poly_8b_temp, $TWTEMPL
+   mov         $TWTEMPL,0x10($TW)
+   mov         $TWTEMPH,0x18($TW)
+   vmovdqa64   %xmm13,%xmm14
+   vmovdqa     0x10($TW),%xmm13
+___
+  }
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 5, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  vmovdqu 	 %xmm4,0x30($output)
+  add 	         \$0x50,$output
+  vmovdqa 	 %xmm14,%xmm0
+  vmovdqa 	 %xmm5,%xmm8
+  jmp 	         .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_done_5_${rndsuffix}:\n";
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 5, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  vmovdqu 	 %xmm4,0x30($output)
+  add 	         \$0x50,$output
+  vmovdqa 	 %xmm5,%xmm8
+  jmp 	         .L_done_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_4_${rndsuffix}:\n";
+
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 4);
+
+  {
+  $code .= <<___;
+  add    \$0x40,$input
+  and    \$0xf,$length
+  je      .L_done_4_${rndsuffix}
+
+  .L_steal_cipher_4_${rndsuffix}:
+   xor         $gf_poly_8b_temp, $gf_poly_8b_temp
+   shl         \$1, $TWTEMPL
+   adc         $TWTEMPH, $TWTEMPH
+   cmovc       $gf_poly_8b, $gf_poly_8b_temp
+   xor         $gf_poly_8b_temp, $TWTEMPL
+   mov         $TWTEMPL,0x10($TW)
+   mov         $TWTEMPH,0x18($TW)
+   vmovdqa64   %xmm12,%xmm13
+   vmovdqa     0x10($TW),%xmm12
+___
+  }
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 4, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  add 	         \$0x40,$output
+  vmovdqa 	 %xmm13,%xmm0
+  vmovdqa 	 %xmm4,%xmm8
+  jmp 	         .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_done_4_${rndsuffix}:\n";
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 4, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  vmovdqu 	 %xmm3,0x20($output)
+  add 	         \$0x40,$output
+  vmovdqa 	 %xmm4,%xmm8
+  jmp 	         .L_done_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_3_${rndsuffix}:\n";
+
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 3);
+
+  {
+  $code .= <<___;
+  add    \$0x30,$input
+  and    \$0xf,$length
+  je      .L_done_3_${rndsuffix}
+
+  .L_steal_cipher_3_${rndsuffix}:
+   xor         $gf_poly_8b_temp, $gf_poly_8b_temp
+   shl         \$1, $TWTEMPL
+   adc         $TWTEMPH, $TWTEMPH
+   cmovc       $gf_poly_8b, $gf_poly_8b_temp
+   xor         $gf_poly_8b_temp, $TWTEMPL
+   mov         $TWTEMPL,0x10($TW)
+   mov         $TWTEMPH,0x18($TW)
+   vmovdqa64   %xmm11,%xmm12
+   vmovdqa     0x10($TW),%xmm11
+___
+  }
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 3, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  add 	         \$0x30,$output
+  vmovdqa 	 %xmm12,%xmm0
+  vmovdqa 	 %xmm3,%xmm8
+  jmp 	         .L_steal_cipher_${rndsuffix}
+___
+  }
+  $code .= "\n.L_done_3_${rndsuffix}:\n";
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 3, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  vmovdqu 	 %xmm2,0x10($output)
+  add 	         \$0x30,$output
+  vmovdqa 	 %xmm3,%xmm8
+  jmp 	         .L_done_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_2_${rndsuffix}:\n";
+
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 2);
+
+  {
+  $code .= <<___;
+  add    \$0x20,$input
+  and    \$0xf,$length
+  je      .L_done_2_${rndsuffix}
+
+  .L_steal_cipher_2_${rndsuffix}:
+   xor         $gf_poly_8b_temp, $gf_poly_8b_temp
+   shl         \$1, $TWTEMPL
+   adc         $TWTEMPH, $TWTEMPH
+   cmovc       $gf_poly_8b, $gf_poly_8b_temp
+   xor         $gf_poly_8b_temp, $TWTEMPL
+   mov         $TWTEMPL,0x10($TW)
+   mov         $TWTEMPH,0x18($TW)
+   vmovdqa64   %xmm10,%xmm11
+   vmovdqa     0x10($TW),%xmm10
+___
+  }
+
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 2, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  add 	         \$0x20,$output
+  vmovdqa 	 %xmm11,%xmm0
+  vmovdqa 	 %xmm2,%xmm8
+  jmp 	         .L_steal_cipher_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_done_2_${rndsuffix}:\n";
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 2, 1);
+
+  {
+  $code .= <<___;
+  vmovdqu 	 %xmm1,($output)
+  add 	         \$0x20,$output
+  vmovdqa 	 %xmm2,%xmm8
+  jmp 	         .L_done_${rndsuffix}
+___
+  }
+
+  $code .= "\n.L_num_blocks_is_1_${rndsuffix}:\n";
+
+  initialize("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+             "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+             "%xmm13", "%xmm14", "%xmm15", 1);
+
+  {
+  $code .= <<___;
+  add    \$0x10,$input
+  and    \$0xf,$length
+  je      .L_done_1_${rndsuffix}
+
+  .L_steal_cipher_1_${rndsuffix}:
+   xor         $gf_poly_8b_temp, $gf_poly_8b_temp
+   shl         \$1, $TWTEMPL
+   adc         $TWTEMPH, $TWTEMPH
+   cmovc       $gf_poly_8b, $gf_poly_8b_temp
+   xor         $gf_poly_8b_temp, $TWTEMPL
+   mov         $TWTEMPL,0x10($TW)
+   mov         $TWTEMPH,0x18($TW)
+   vmovdqa64   %xmm9,%xmm10
+   vmovdqa     0x10($TW),%xmm9
+___
+  }
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 1, 1);
+
+  {
+  $code .= <<___;
+  add 	         \$0x10,$output
+  vmovdqa 	 %xmm10,%xmm0
+  vmovdqa 	 %xmm1,%xmm8
+  jmp 	         .L_steal_cipher_${rndsuffix}
+___
+  }
+  $code .= "\n.L_done_1_${rndsuffix}:\n";
+  decrypt_initial("%xmm1", "%xmm2", "%xmm3", "%xmm4", "%xmm5", "%xmm6",
+                  "%xmm7", "%xmm8", "%xmm9", "%xmm10", "%xmm11", "%xmm12",
+                  "%xmm13", "%xmm14", "%xmm15", "%xmm0", 1, 1);
+
+  {
+  $code .= <<___;
+  add 	         \$0x10,$output
+  vmovdqa 	 %xmm1,%xmm8
+  jmp 	         .L_done_${rndsuffix}
+  ret
+  .cfi_endproc
+___
+  }
+
+  $code .= <<___;
+  .section .rodata
+  .align 16
+
+  vpshufb_shf_table:
+    .quad 0x8786858483828100, 0x8f8e8d8c8b8a8988
+    .quad 0x0706050403020100, 0x000e0d0c0b0a0908
+
+  mask1:
+    .quad 0x8080808080808080, 0x8080808080808080
+
+  const_dq3210:
+    .quad 0, 0, 1, 1, 2, 2, 3, 3
+  const_dq5678:
+    .quad 8, 8, 7, 7, 6, 6, 5, 5
+  const_dq7654:
+    .quad 4, 4, 5, 5, 6, 6, 7, 7
+  const_dq1234:
+    .quad 4, 4, 3, 3, 2, 2, 1, 1
+
+  shufb_15_7:
+    .byte  15, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 7, 0xff, 0xff
+    .byte  0xff, 0xff, 0xff, 0xff, 0xff
+
+.text
+___
+} else {
+    $code .= <<___;
+    .text
+    .globl  aes_hw_xts_encrypt_avx512
+    .globl  aes_hw_xts_decrypt_avx512
+
+    aes_hw_xts_encrypt_avx512:
+    aes_hw_xts_decrypt_avx512:
+    .byte   0x0f,0x0b    # ud2
+    ret
+___
+}
+
+# Bits 7 & 4 contain the src1 register's MSB in inverted form
+# Bits 6 & 5 contian the dst register's MSB in inverted form
+# Bits 1 & 0 is fixed to 10 for vaesenc* instrcutions and 11
+# for vpclmulqdq instruction
+sub evex_byte1 {
+  my ($mm, $src1, $dst) = @_;
+  # set default to zero
+  $src1 //= 0;
+  $dst //= 0;
+
+  my $byte = 0xf0 | $mm;
+
+  if (($src1 & 0x8) > 0) {
+      $byte = $byte & 0x7f;
+  }
+  if (($src1 & 0x10) > 0) {
+      $byte = $byte & 0xef;
+  }
+
+  if (($dst & 0x8) > 0) {
+      $byte = $byte & 0xdf;
+  }
+  if (($dst & 0x10) > 0) {
+      $byte = $byte & 0xbf;
+  }
+  return $byte;
+}
+
+# Bits 6->3 contians the lower 4 bits of src2 register in inverted form
+# Bits 0->2 is fixed to 101
+sub evex_byte2 {
+  my $src2 = shift;
+  $src2 = ($src2 & 0x0f) ^ 0x0f;
+  return (($src2 << 3) | 0x05);
+}
+
+# Bits 6 & 5 tells about the operand register types and bit 3 contains
+# the src2 register's MSB in inverted form
+sub evex_byte3 {
+  my ($type, $src2) = @_;
+  my $byte = 0x0; # default for xmm registers
+  if ($type eq 'y') {
+	$byte = 0x01;
+  } elsif ($type eq 'z') {
+	$byte = 0x02;
+  }
+
+  $byte = $byte << 5;
+
+  if (!($src2 & 0x10)) {
+      $byte = $byte | 0x08;
+  }
+  return $byte;
+}
+
+sub vpclmulqdq {
+  my $line = shift;
+  my @opcode = (0x62);
+  my $inst_type = 0x03; #vpclmulqdq
+  my %opcodelet = (
+     "vpclmulqdq" => 0x44,
+  );
+  if ($line=~/(vpclmul[a-z]+)\s+\$0x([0-9]+),\s*%([xyz])mm([0-9]+),\s*%[xyz]mm([0-9]+),\s*%[xyz]mm([0-9]+)/) {
+        return undef if (!defined($opcodelet{$1}));
+        my $byte1 = evex_byte1($inst_type, $6, $4);
+        my $byte2 = evex_byte2($5);
+        my $byte3 = evex_byte3($3, $5);
+        my $modrm = 0xc0 | (($4 & 7) | (($6 & 7) << 3));
+	push @opcode,$byte1,$byte2,$byte3;
+	push @opcode,($opcodelet{$1});
+	push @opcode,$modrm;
+	push @opcode,hex($2);
+        return ".byte\t".join(',',@opcode);
+  }
+  return $line;
+}
+
+sub vaesni {
+  my $line = shift;
+  my @opcode = (0x62);
+  my $inst_type = 0x02; # vaesenc
+  my $byte1, $byte2, $byte3;
+  my %opcodelet = (
+     "vaesenc" => 0xdc, "vaesdec" => 0xde,
+     "vaesenclast" => 0xdd, "vaesdeclast" => 0xdf,
+  );
+  if ($line=~/(vaes[a-z]+)\s+%([xyz])mm([0-9]+),\s*%[xyz]mm([0-9]+),\s*%[xyz]mm([0-9]*)/) {
+        return undef if (!defined($opcodelet{$1}));
+        $byte1 = evex_byte1($inst_type, $5, $3);
+        $byte2 = evex_byte2($4);
+        $byte3 = evex_byte3($2, $4);
+        my $modrm = 0xc0 | ((($5 & 7) << 3) | ($3 & 7));
+	push @opcode,$byte1,$byte2,$byte3;
+	push @opcode,($opcodelet{$1});
+	push @opcode,$modrm;
+        return ".byte\t".join(',',@opcode);
+  } elsif ($line=~/(vaes[a-z]+)\s+0x([a-f,0-9]+)\(%rsp\),\s*%([xyz])mm([0-9]+),\s*%[xyz]mm([0-9]+)/) {
+        return undef if (!defined($opcodelet{$1}));
+        $byte1 = evex_byte1($inst_type,$5);
+        $byte2 = evex_byte2($5);
+        $byte3 = evex_byte3($3, $5);
+        push @opcode,$byte1,$byte2,$byte3;
+        push @opcode,($opcodelet{$1});
+        my $rsp = 0x04;
+        my $modrm = 0x80 | ((($5 & 7) << 3) | $rsp);
+        push @opcode,$modrm;
+        push @opcode,0x24;
+        push @opcode, (hex($2) & 0xFF), ((hex($2) >> 8) & 0xFF);
+        push @opcode, ((hex($2) >> 16) & 0xFF), ((hex($2) >> 24) & 0xFF);
+        return ".byte\t".join(',',@opcode);
+  }
+  return $line;
+}
+
+$code =~ s/\`([^\`]*)\`/eval($1)/gem;
+$code =~ s/\b(vpclmul.*).*$/vpclmulqdq($1)/gem;
+$code =~ s/\b(vaesenc.*).*$/vaesni($1)/gem;
+$code =~ s/\b(vaesdec.*).*$/vaesni($1)/gem;
+
+print $code;
+
+close STDOUT or die "error closing STDOUT: $!";
diff --git a/crypto/fipsmodule/aes/internal.h b/crypto/fipsmodule/aes/internal.h
index dc694db57..8c808c32b 100644
--- a/crypto/fipsmodule/aes/internal.h
+++ b/crypto/fipsmodule/aes/internal.h
@@ -29,6 +29,15 @@ extern "C" {
 
 #if !defined(OPENSSL_NO_ASM)
 
+#if defined(OPENSSL_X86_64)
+OPENSSL_INLINE int avx512_xts_available(void) {
+  return (CRYPTO_is_VAES_capable() &&
+          CRYPTO_is_VBMI2_capable() &&
+          CRYPTO_is_AVX512_capable() &&
+          CRYPTO_is_VPCLMULQDQ_capable());
+}
+#endif
+
 #if defined(OPENSSL_X86) || defined(OPENSSL_X86_64)
 #define HWAES
 #define HWAES_ECB
@@ -151,6 +160,20 @@ void aes_hw_xts_decrypt(const uint8_t *in, uint8_t *out, size_t length,
 OPENSSL_EXPORT int aes_hw_xts_cipher(const uint8_t *in, uint8_t *out, size_t length,
                                       const AES_KEY *key1, const AES_KEY *key2,
                                       const uint8_t iv[16], int enc);
+
+#if defined(OPENSSL_X86_64) && !defined(MY_ASSEMBLER_IS_TOO_OLD_FOR_512AVX)
+#define AES_XTS_X86_64_AVX512
+void aes_hw_xts_encrypt_avx512(const uint8_t *in, uint8_t *out, size_t length,
+                               const AES_KEY *key1, const AES_KEY *key2,
+                               const uint8_t iv[16]);
+void aes_hw_xts_decrypt_avx512(const uint8_t *in, uint8_t *out, size_t length,
+                               const AES_KEY *key1, const AES_KEY *key2,
+                               const uint8_t iv[16]);
+int crypto_xts_avx512_enabled(void);
+
+#endif //AES_XTS_X86_64_AVX512
+
+
 #else
 OPENSSL_INLINE int hwaes_xts_available(void) { return 0; }
 OPENSSL_INLINE void aes_hw_xts_encrypt(const uint8_t *in, uint8_t *out, size_t length,
diff --git a/crypto/fipsmodule/aes/mode_wrappers.c b/crypto/fipsmodule/aes/mode_wrappers.c
index 5723f33d0..ced71adac 100644
--- a/crypto/fipsmodule/aes/mode_wrappers.c
+++ b/crypto/fipsmodule/aes/mode_wrappers.c
@@ -148,8 +148,20 @@ int aes_hw_xts_cipher(const uint8_t *in, uint8_t *out, size_t length,
   if (length < 16) return 0;
 
   if (enc) {
+#if defined(AES_XTS_X86_64_AVX512)
+    if (avx512_xts_available()) {
+      aes_hw_xts_encrypt_avx512(in, out, length, key1, key2, iv);
+      return 1;
+    }
+#endif
     aes_hw_xts_encrypt(in, out, length, key1, key2, iv);
   } else {
+#if defined(AES_XTS_X86_64_AVX512)
+    if (avx512_xts_available()) {
+      aes_hw_xts_decrypt_avx512(in, out, length, key1, key2, iv);
+      return 1;
+    }
+#endif
     aes_hw_xts_decrypt(in, out, length, key1, key2, iv);
   }
   return 1;
diff --git a/crypto/fipsmodule/cpucap/internal.h b/crypto/fipsmodule/cpucap/internal.h
index 03ac640d0..b2ff5bbb1 100644
--- a/crypto/fipsmodule/cpucap/internal.h
+++ b/crypto/fipsmodule/cpucap/internal.h
@@ -127,6 +127,11 @@ OPENSSL_INLINE int CRYPTO_is_VPCLMULQDQ_capable(void) {
   return (OPENSSL_ia32cap_get()[3] & (1u << (42 - 32))) != 0;
 }
 
+OPENSSL_INLINE int CRYPTO_is_VBMI2_capable(void) {
+  return (OPENSSL_ia32cap_get()[3] & (1 << 6)) != 0;
+}
+
+
 #endif  // OPENSSL_X86 || OPENSSL_X86_64
 
 #if defined(OPENSSL_ARM) || defined(OPENSSL_AARCH64)
diff --git a/crypto/fipsmodule/modes/xts_test.cc b/crypto/fipsmodule/modes/xts_test.cc
index d6d954222..07f071da2 100644
--- a/crypto/fipsmodule/modes/xts_test.cc
+++ b/crypto/fipsmodule/modes/xts_test.cc
@@ -365,6 +365,20 @@ static const XTSTestCase kXTSTestCases[] = {
         "8917567652e9b4ef3838baf35e400fe1aad32ff4d83b0af3f6a176025bd1321b"
         "ffe2f16c",
     },
+    // len = 126 bytes = 7 blocks + 14 bytes
+    {
+        "6df514f7b04518669d88dfe8e22683bc09081e7c6980ad768afc144bcb75263f"
+        "54176c5f69b1ebf5a3b116e2e77eb4f1b21d00cfc281e64bfe69e4f7714e312f",
+        "e3484c0248c9d1b18b51323838a883c2",
+        "a67f2bba6376b12ee8267d0e58bc3d3b04893d4c520efddd602f1698d7995a7d"
+        "1985387cfbe9abe31028f168e42ea3e9b8e1350aef33e84f62fee73a9741b7b0"
+        "c6ef2dc2d9d8a5e9009751e4c5f4cd7dd50388c5367014986efcd2053d8ab604"
+        "79e3c652bb6b3bbb028c9fc8816d455670cd1ba63d303eab2c11b1699b67",
+        "25d58c11702de5d39f73e4b651814272038b579df534ee096297d7cd0c95300a"
+        "d87ffe0d48fb4d8e6a1d01d3333c4e52113d921dcefca066a6b264ece99acdf1"
+        "283c56b49bfb42a1965080f06e5953d9414a432953d50784beec734cea1ae602"
+        "68fddc72e4c3f3613483bc15ab95ec62c9bc9edf48a2395dad69e4786f09"
+    },
     // len = 128 bytes = 8 blocks
     {
         "fffefdfcfbfaf9f8f7f6f5f4f3f2f1f0efeeedecebeae9e8e7e6e5e4e3e2e1e0"
@@ -583,6 +597,372 @@ static const XTSTestCase kXTSTestCases[] = {
         "ea39991882130ee45e95a3c6bc508f09c990add0cd3f1ca3403c096f9277e785"
         "4cd6a885857fba751416febc2cc41f2e6ea0659808172173e234f7dfd6",
     },
+    // len = 257 bytes = 16 blocks + 1 bytes
+    {
+        "aff22ec6b186ffbd4140241576ced5ae70e0783b67c0d58d0ef6dbd27ace07d5"
+        "0a5bd89cc096142790e00ea06e7d6685a09fa62ac1667d16cd320c51a1507c78",
+        "1db517955b52bfc2148028e9fdb6000b",
+        "83f63d5f0210b07db17fcdb8fba230185747aeb29a6d75aeed9d97eb5397f6d7"
+        "8e34369044e60df565daad617cde79d3252786bf95fb6d8298056dec9c64c32a"
+        "98f9bbdcdfc8d145a37ea61f5c1ff382477941dc74af5e0cb4ccf85030bb7bc8"
+        "b436a494fe75d9a1f37fc1509eb4d2e52d13c1a1c220ad76eca6c71c6142e416"
+        "7888aa76fd8318f002d940a08d1286ba26475be867085f53ae266f10685326e0"
+        "dbd056d8536ec9554709f5d41c7b8e42c3e92a2af2897ea0afedb01741d6f71c"
+        "a64ef5f9bcbe4e04c744d8e3bf6725825050ad42d92be3891893a0596a987610"
+        "e66b0aa22958a6f09c7fd45ce6f9de36498b7923b65caccfef4c2859e49e6aca"
+        "09",
+        "c98f54b11d1210f845e6c0a68baa37a274677ccc405e2ff38cd4f41fbe7f65b4"
+        "4d36fa9792541e8bbbceb8ecedef84bbb4fdf850ca6b4a17b50dee390063b431"
+        "c5fd389b14a04a399b2e0055fbeb390852861947a4d0b613e10eb469ba86f5cf"
+        "8d303750cfa35942d7b5ec66a5bae9ed8db2a062fd60bec11b1abea241355f7f"
+        "7564a7ca901ada44cd61b6266d25c7cc6118ce6b1a901ce83385701a722582f0"
+        "e238bf7e2a9396cf07aa7200c2cbd146738782863792652707271d0868f58f2d"
+        "0c45b826f61f030972fe32c6a55a0e6b39f2012e827318d623d545bf90fc68c6"
+        "d8c48e8fff9445248b0016897269022420c51e8472b9aa1a2350699815f62a66"
+        "3f"
+    },
+    // len = 276 bytes = 17 blocks + 4 bytes
+    {
+        "6dcc2392c5f0af2f5d84b455cd7b04abb785e0d19049fc675d625eeff162ef67"
+        "321369f778a33a28e50973dd3a37857177da17a5c23f7859eb12b72b6af60da7",
+        "85ec816504530b941554d311cc3a6a29",
+        "25cc3b83832aaf7595116b841fd22ba4bfad09c300145715682a263464905e89"
+        "5c990cdfc4bb5459cdbfddec92099051b69914b6ae6bcb1695f14bf981a982dd"
+        "428ebc064a115f17d03d03624693b3fc2cc7b2da327df1c76e3cc0efe542cc27"
+        "d1882e1b998d326aca35cc10c8800cf447becf7a3bc041a9fc0298e144640815"
+        "ed363086c462f08e97bd9f5f3dab54846a23fea5e3404fdf42e7c0864cc89c39"
+        "ffccbfc32fb051c66df026aa9c7a2e069d2dab806dfa5fafe21f352ee7d167e6"
+        "9e26a9cdd6fb9343ebb9ed87331c8dd0493950b633af6515ce9a43b66caa9c0a"
+        "d146d7a7416aeb2c24d8b457f441283d7a78f3ae2858c3f6f307ac5fb1496982"
+        "8f402ad0aa15fcceedb026e2f24e1f6cc6131aee",
+        "5451cf03fe8ed79c73f43b8595a93e4e5704856c72ad8e0ae454e021f880b26a"
+        "4fc1634b8fdb143bcbbf28ece2d08b34ddbd04223c507ef54c013f85be416127"
+        "db2e4ec5873541185ee8d345a04be6fc8c7711d4185c6999ae15e3ceb020b5ba"
+        "036f3a29176b369723c5d162e2e3c1ec464809995facb03c40b35d22df23b1e0"
+        "b16ddf322f77e88dd51b35137aafc5d63e0c939e003ed5cebf644ddb425fdb53"
+        "3f1f127eb08d44cf95d024e36d888033653b6dab815a3ff8ec6536ba5cd51403"
+        "ab938dbd6e4e63e34e6754869105dd87b0f8e53b6fa4e68834a5b15a74dae604"
+        "cb8e120115d5b9c66f6a7e973bcf2f418f37e19a51713577182ea38c5493ff46"
+        "966b2f87f3dd957226a4823848f42d943898a8c7"
+    },
+    // len = 299 bytes = 18 blocks + 11 bytes
+    {
+        "de5e919626694311364505d8471a5fc5ed22e159d26efc6d5f19dab8367bdad3"
+        "e5e5bdda19663958dfe62753455a083d247fb908c3157f32b264f121d29640c8",
+        "64f64746afa04e8163bc96603c03929b",
+        "b5abff8f9db7b0d3892b69646c3d2cd0337316e21364637721f9d75dfc69f8b1"
+        "15f741b2aff185381def9c892cc8595f3b70414fd4a4c6f59d9d5299064a4a1b"
+        "428bcdf17d53299a42c6236e8e7ccdcaec0e19c1b2dfb64f7c09e88253329e95"
+        "be6b863bbeb0d50076f86e04743bce6149e722fbc6d84a42e132c5356563ca23"
+        "ce515e8d01338d772bfc7b9f374a008131227cf8fbc73adcf9ff115e62dc8131"
+        "2ddfbe2e124ba53d4720dd7f6add009c007c94fb43ced73dcee99b30c51d61f2"
+        "fc1f200f6bc54cb2e52931500731ec07ae8002f14ed92e1cc2ca4d87e7ae79e3"
+        "ce99f2395e3feb44681d946f4e8076fc0078ee4e521c6b14e6b89ccd6615b134"
+        "afa36d0de259514b76e5bac46531c165a9afb4fbcb1f10b2d7ac7f3dc1307270"
+        "d4df7eb638cf01aeb5bc73",
+        "292987d5a1c8cb8392c9cd1f9a9a4de5490d0284b91295cf4bbcd06376ef9513"
+        "d18753f1dc7ae7ed2b8f081d108de07c4752eeccfa6c1558da2a176ead5127a3"
+        "06d080ad4af63c13dffe291d65937b65ef370652a13e99d5e3e4698a80c04cd1"
+        "a78618c7b956f8422e286eae175c296d9a8aa687c9eb570cf7754b90346c5e3a"
+        "cfad6d073c7f85fe1b8d09dc06c6b6b2583441d6101b6c6e7520bb7ab1310c8f"
+        "1216cd270dd610e957e1c3d104a859ea5fe3bf11ef10a838905de4c5134bb02a"
+        "921a56103584f72008e1cd4f3482d399d7f029d79a27cfad399fb2c3b1bd6975"
+        "6af7d26ca822c239d948463e3451fb137e8274966924669ed1d6b9cab12034ff"
+        "ea30464b626f017e03a9b8d13efe0f1b589bfabc4eee2c38977e345ccb30d7ab"
+        "f7b541294e651a657663b1"
+    },
+    // len = 314 bytes = 19 blocks + 10 bytes
+    {
+        "ed80e39253604e671080b99bcda0589d47dc51d810ce196d0f0eebb9453b6ce3"
+        "349634aea22ae00fd9e4fef19c8213451d2a6ea4395e3529edc8a9b9599ed8b3",
+        "f5bfdd462db6ea3defb89b08ee0931fd",
+        "f63fc5e2e87e9b2ed8d6cc44afaff8a46fd5eb9c8bd5da7a8e75827c7eb47975"
+        "f33e57dcbcf20a94c8d6d97785d11cf4a6079132dc6bac6ae02fe65fe35fd4d6"
+        "9d2bb25a1dbceee592c75d1898790c3f809d715c081dc7e94cad482f0d1c06aa"
+        "47b8046475f34907baa61f531f2c929fc903fcd220c3bb6d70039c7d1fa22866"
+        "5b2ccad01f13d7dabaf72dd923bf79ecc275bee238794fa87cec269b8e4e01e9"
+        "7acbb99adf91749988a172ab60eb972260560498cf54414c4067e7ceb5e9b82f"
+        "b471c993023d2c8ade9f353e8acd60eb236583f2b9c43ef92b26c7e00f7f10c3"
+        "f1d957f317837ef522b334ad809498a3f91b96b2e0d4ab0bfa73ec09f2fccde3"
+        "d524d7eca755e2ca08167789aa0f2ca42ac2560a9702169175029b67fe684bd3"
+        "8c22c03377a2fd7fb8740862833506aef75db88e5fce20d4d0bb",
+        "7a93f2efebbe2557ebcb22d5972ba884696ab0651fc1cfa1c5b88f8a2bc9ab78"
+        "17a97c0c46ffa8ac531f360700c94cad12ae081c03cd508b22123fd5d0ba4f1d"
+        "027d0f150b314177f3b2d6074686a1c548eaa243fb49604db2dbb3c32ecf5585"
+        "fcd023a72492413acd5427e3fff16a1bfee13d0be5333bda8621fb5a0f5814f5"
+        "04118a8b3b5dd5f40108c2baeff9ae59b84f20bd13f50e052083dfeaa80ff8c8"
+        "7be86dcd19930877712e6fe3442e4b80ed69467b2e208152f4f76f3c78f75934"
+        "4ed558d2ba69a85bd7019c8ff7900abe277bfb0d27cb921266565bb7891c77a3"
+        "973e8d6836ad7b8a389b27ffe70c19806810fed62fe029bf4b1d173bd297db6e"
+        "4ab4a9738f30a46646d386e9ade5b4ea00a5456535f7972a54dcda19f8d26e43"
+        "30c3d760bc4e278fa5ab8481dadde7829fde7563ece4094588f9"
+    },
+    // len = 331 bytes = 20 blocks + 11 bytes
+    {
+        "ce86af621fe0bca7d825d43ee182ddf0604e1ff2b4de268e62843598d0cb5859"
+        "23a2a8e2049f541edc8682620db53dac770fb03fd27085feb336c3161badbcb9",
+        "33a7c85257ba910b8c6f7bb3f50ab157",
+        "bd368df3f92509c940d5769991d052c4771a16cfd5a8da611756140c60c6631d"
+        "fcf010f5161abe56ef34ef800441447c5c5a4b310225921a7ba726db6d8af969"
+        "7a095e90231ce71250d6925518d6d174311ca53341374dbdde74984bfe91b478"
+        "9b1209be2ef0d17fc663d4de3aa5526bc1f79e022fecbf0d6058595ee90dd684"
+        "20df434ecf14cd9677a174b146c71c07bebb0aeda7c9fb072154650b613b8f81"
+        "1bd2d0eae69d805e3ff50f85bc2c8d7ae797688e60639582b7fa8d18351c9a50"
+        "ef6a3bd507bb3346b043cc6c6f59e756f04fe450b279d269735f81a87c1bf96b"
+        "8534408def74d3a0b79f0c26f8f37ce8426039f4d90b5d4c6bdff4e7faed5280"
+        "21920d1106e0b1bd80bde378b15f61f3bf9ae898a545e41024d9f71fc6499fe8"
+        "dcacf9e28caaa00c67838518e3e60ca280f43b25391f365ef82d7dbf771ca753"
+        "c8a035544ad561b159e6ca",
+        "23a76bbecc4187fc27809e98d66f013c4713c42331fb74f81db7c6ce8d788abd"
+        "1451538cfe68bb29d82ff90427ef00c85b6f63cea97b31285ed0f0873b203ce8"
+        "e73d16252f7478d6e920bf079aa38fb0df55fbcf613d5f866fc70ca9bac43dd7"
+        "2feca0c2cc4b596e6b949e43252023504dbc543d61534435f8994efe79d00e39"
+        "7de079b72006bf2b3c289a5b3535af26d8ccabe239e4f0dcbb7f9dc893fc7279"
+        "055df2edd27bb997819e61654408d8028903cc31e350417271fe6e0128c3d6b3"
+        "951b58a0ea72a5101063b01397171c30e1df0eb255be3afd2ce2be2e30278d61"
+        "b93d7e97cc697180ea16d2bb9dfd76106d8db120e9ffafcb8182e2c869f57706"
+        "facfb9b53e0d80d330d7db057087251d2947a6746034b34b549ce47207334ccf"
+        "6c8df1492055d0464d3800df0278609ddd277e8ff1d12ee78d623e2e816fb5ec"
+        "9d6aa67116f98db59ede5b"
+    },
+    // len = 348 bytes = 21 blocks + 12 bytes
+    {
+        "ccdeca713961d5f0fa9f3717aa335e3fd37637a08fa1e0b0299d23e22cd012b4"
+        "d64c1903a731de4c97c2d4817803fd2a1d9de770026492db4f61b4cd158a0fe5",
+        "2d2a82641223d4a12575050507b5e031",
+        "047e92273274f55f8ac5e99cd59e8202c80466da273a7b4caf8052b73532e839"
+        "b07a61e3ee5642781b2b15f0c997f2929b586cc392e80f426861f99e94e1d744"
+        "5b3827498e69c2aa95d79a5e6e8df009e55dcc7845dcbaad3db34bd1942316ef"
+        "5b3d38eaa7fa943cd12e9a3fbb8b49a1e815192df1d3da2f8626001a491609a4"
+        "54418efb3c22370d51d14d0c5c96ad44abc6719d994ccc1f72cc39bbe3425f37"
+        "84ee32c01069cd613a1a6e97b01bdb5ce24df97b99c59b0b91d4c6741725ab9b"
+        "13dd5b244628858143f318f30ff34ff140486cd90d07e49fdcaa13f3d0bf8ee3"
+        "9ce907e3118d6454807c488f6f9780b0e0ed89edf46e8cd018a0c3e85f51ccfb"
+        "3ad3de4c6042a0e1bee8702e80f1de60de674dd2d5daa3ee7a66d6d9b8a2d4f2"
+        "76b33ed6f5dfb7b4c728e24719c0a7f727f5c9fdcf6ceb49d3c1228b64f67dda"
+        "a9bcb09f9b6853629035aaa9f551a01c46691915d6045ea9c680342a",
+        "b044e504905a2a68940aea8eec020eb5bc82faabba1374429efb6bfc4d84bd7c"
+        "e5caef9b87708dae91268d8b67909ac537067b86f5361acd33d70130c8c2c838"
+        "4401e833bb5fed395135f314294e1fb6db2434b4f276cd852e69f98c41487351"
+        "d3fc82eb9169802b2e1a1e714125cef2ebacf4d363296e3cfa6d8488b31b6f95"
+        "8b64c686221df4ff989b3fda3a220648c17c1bc5dc8d5ab8e76829cb7ca8f55a"
+        "6a8adbb75f475974ace8b53fe4360bf03c80acc25182f3ece25cb9d194bb93c8"
+        "304f7aebdcad9689fd8b505089046c9e2f7e558c6a22b7a05e9cbc13f039ae0f"
+        "91bf0dd2dda37ddc2a03267bad9de5a7c4a279eaeabe1a5b1222aad48b99e1c3"
+        "19117b280fb1841aeba7403ce6b99b79b40cef584838ab5284d78c87f640a6ff"
+        "7fefc0a1c37727269e3440d765e2e3d736003e6d0408d2ebcabffc0d275f61e5"
+        "2ebeb0c06962e11a3ff932a3f32f74eddb06f26b50f88d1b9741e53f"
+    },
+    // len = 359 bytes = 22 blocks + 7 bytes
+    {
+        "b120b40812ac153cf5ad7235213d12186b31f83ca4523e20e8928fd1a552757b"
+        "046dbf1c6b475566595fc277dea167c3391f390be8b98f33cd5ac7b00eb76ae0",
+        "a3ac009ce53ca78a24f9436288639670",
+        "3028cbc0f09c7095abc24d202dc801d074016c593d13e3610d27c4958a5a06bb"
+        "82d17b726deb0818ae5539db1d3aab913b18ea782bced938f59dce7ff7d43a7a"
+        "a5b5ec12a1f42b4f49642a669ed5f7d9ede25119b02a51a5c81f24bff35f3998"
+        "1426abb51ad604643a2ecad804c2b1f1a4020a542c5cf9f47b1db46f7ced0791"
+        "13b2462e884b92c2795c9a7d1e4b6fc24d79167ad50f6e512d22c0a910c73a23"
+        "7a815102cce3c545405fc35eab3221f8ab37728147e1d27403921d13595837d3"
+        "d988d6a56c9beaacfaad0aa5df2b9e8b63100caaf1de1ef5703b08c9933f9d6c"
+        "c87311340efce008a9eaae89164c14795c20234efe41436e7c4b37108bd47c53"
+        "478e87558a675e33510cbc6758d0e0b4f00302ee44455cc19194d11c684d6fb0"
+        "dbf605655d6399ae6f5516c726f67c16fa7e053ec461ff55f5d0715e1ee00ef9"
+        "d6135f3377f8e1e64df7ae73ee2a8ae8a88f266cf026c1e6f632441412520ee8"
+        "656d1bdc65fdc3",
+        "4100e99913513a39bd379f672ac3fa13e6bab8412d3373531a801f70ea9b5e25"
+        "603229eac8b0af2d0619c1859bb980ee08e1d43e4c3fa206ff0928eaac063978"
+        "0e1e25dea0a6e4e25c8d27793425e740c1bac1bcf98986125372c12198819028"
+        "40b7d612adc275de1a1bbeea216458e1010cf7019a949a4a3f016edbd0b09226"
+        "9295e7924a2d9267c1babb021b0f0e207de84460c55cdb4abc6b2dae0960d9af"
+        "1a95f1dfcc83ec625bd7c42b1e17d219b15d5cc0e3e69063889f1ab18008c309"
+        "14b549815822b844b279324cd7cf8f596445b1abbd87ee97c13242bd11958c0d"
+        "45986cd54247c307224568d557d42188f39f1ce6338cb3aa3c18501095ab5316"
+        "db18cb23698aa57fb12728a747b2f64878bc23e260cae1d597214a817d8b0229"
+        "fa449b6e55aad6260b230049b4a7708a9d72d06208033b40305bacd253ec4c5b"
+        "e85d813d4adbd69083bcb7bcda1667d30e0315753e1801c165b7f8a8be6a0717"
+        "c6d5e3ce942c57"
+    },
+    // len = 370 bytes = 23 blocks + 2 bytes
+    {
+        "f4269bca3fb01715a422ab9f9dedb7b4218e8a004af9216e687cc024d55fc239"
+        "71e2b043f12f710d59b63011ba02b0acdabc59c9b0610ec590131b5d128d14e4",
+        "eec7a07920a0426a503bcb724a3a37b2",
+        "cab3c58bceeae8a4fc4831be5c6aa24a3143c352e306bc344188a68bc2dd3d8c"
+        "9103175fed0003e94834a7a49e4aeed08db22270b8dea4f9664a852828c2b5b9"
+        "c5cc18b2cc1c9b145043b8ef8da7bf1a59e18a11bf2f0a26798f4ea152035a17"
+        "d073ca9c8f65b1dfa869ce35108d4f696eda7a2e0985548214a22466a67e7e76"
+        "f1481280adc360562d2e8b3dbcdba72ab52158bea6ac40bb4f6421f5e39f6bd4"
+        "e77d559541b5eb6ee376ab9f5152ca067422c41acf05d51e69f7134c967e217e"
+        "fb76133c2bfeaa0e7456aec6a878cc1c9a913769960c87ff039a4c9a186d1814"
+        "e32b500e29fb1c9d51ca63f9423016ddc14d465759ce565d68a2f7810f0f95f2"
+        "3ae50063e01d0031e7642b2a944107558e4dace71b024484a53b05b44a9aa784"
+        "7fa7e760c4e891ac4cbcd6e0fddd358b2ae17346e3b7ca88f3cf3d3d69e4c2e8"
+        "8ba9485091dafcdd96d2bd94aff21fd9d3921fb74ae93f3db87c7a21603c0aec"
+        "e6523c772c3855c30a1257b9057692d809b2",
+        "74468138b84656e4292b2855019566852232e2126ad7e062b9a9320027e4067a"
+        "a19d70f4a338bdd30c6e08a3d42e0c4b5d75d4bda6078df47469fec4dac80b49"
+        "c80e0076dd1bf773919d8699455bbc905c1ba7d8e968d15ad0525ad31d080335"
+        "15c4e85068fe0eede28ecd3fd434e0f8db6db3d99c0c9120d060b21490e15b4c"
+        "23a5db14c06a02d5677f64f2e861ad7333180417d6eae190a9f5fcfbb629795a"
+        "c9b8d5792767ec63f7d5146aaf3294c8924c951a0b95221dc821d8bc428a7670"
+        "cf4c900f7d844bd0b48dfbf70f18718fe404bdd476fbf366a01a1bf69d3178b1"
+        "0981388a1d51c05e97cc5ae6061ea57736b22989e772fa413eb89e25357827f5"
+        "89d41dfbf6dc6cfeb063a2cd3cf2da200b99b4a856a8aff15f27769f9d20ceee"
+        "12bf2b09391d9c75d27f69417a90b198240df02fb4abb351fbd0b753f50c9349"
+        "59ad6b5d74d0677c792a89c3c20f22285ae56aaea231b27fc0be998320cc395c"
+        "42d9a1b72c80603465ca9bdb3960f0f7a534"
+    },
+    // len = 399 bytes = 24 blocks + 15 bytes
+    {
+        "53cf540aac7f2dd4fef9161811619879af7b9378ccd7ca9eb78aa39c319e11b4"
+        "9b904b754798d2a40cc10ccf8fe913eb4803853ff8f9abc897cda2b4fec917c0",
+        "5f2fc3f2b53c328134097bfeb519c66c",
+        "b0503a54f3d60824d4a6eee6bda2a61cd26a0f87a64108da4a83d8ff9c9e6c4d"
+        "efa6a1e27ca9065150f4370d97dd2a694739f0ed7af8c7c47c9fc4183e30652d"
+        "d6060f52b015a3000ada0da1b8370aff70faedeaf2b4af6e54738792a3ecbf79"
+        "f3cecba3e36fa3ed49b08e01e898015892ee4385a2f2f3f6657a88086747815a"
+        "154cfdf9bba0e605507506380d0791a0f5d42598c6188e2b931733fa5eb45474"
+        "00516dbcf153c141c8c77ad6cf0b76c4df9b5ca5b3ebd04602034060b794d4b7"
+        "e54173d69534185dfc9233cb9da98f7c44ec21f8d7f13ed9f47f39ab130e62f9"
+        "4fd6cfe40ae742067975d1161f6192634db35b24a49afd981936432c44a62594"
+        "7cf57886dcba8d56305e6c4fbfffb20cb20e3057a82defc16433eda8d9133c55"
+        "08b5dbe46f683a9fc7a7ee86a6a19358afc3af57f19f1855d205fdab183a0020"
+        "efdb055e443ffe0be6ec918c8d24e53ce89493d933ab2e05b12bb0c965b0ea54"
+        "8cefb3d02eb1db159d6ca12b918667791bfb524ea6805457ab042111b50b6541"
+        "fa181128c9ec3d6758df92e965f962",
+        "89119ff52d0dd7274f6bef39e395541f9a7db5d4bbefefd73a3126bfc9dec2fe"
+        "9af741c5b70a3322aa84c88ddae92c7c48ce2685950139c644eb5faad8e0d823"
+        "9e65b272cb4bde4562294445249a0bccfd27f06074217b45344b4ec1c83d0024"
+        "244a300b8fe9e6e4266c75fc8bdf219cbfa0c0286fc772e34e1742c12732251c"
+        "3d3c2b5425535214abe1f0aeabdd86a28d562df5c8f851a54c3517bca02029c0"
+        "7229b6493297b8c5173b72d7ad3732bb100d5e92ca02a3d339a6838dc7744d44"
+        "bedfe9696e2ed6869277c406c3b64148e2dec18c28c0db1afd557285046a851e"
+        "5f95ca48117900e21ae7204a01122135f0b667e12dc81476547081507fdf506e"
+        "7a14107a9f172c61948b92d92c6337c77bdaf6ba07691d105518b5887c22f526"
+        "4f3b7838f3e8b56d568d8249d28cd62457aad01b8b90a1ff4548cc02e794a12c"
+        "c7d8af55204bffda04dd8f53041876a3468a4d5747f57b225b4feb1fbb523103"
+        "30145923a945c02b6d79acf69e0c6b41408deab6ecb2f4b7fbb760372d7bdb8a"
+        "26f1ca0f722b377f1f9b5b1dc246d4"
+    },
+    // len = 512 bytes = 32 blocks
+    {
+        "471f9a9ee92d06701e84d8ee35773e00939620dcfe7679b4aa6eb0d6c59bec8a"
+        "f51000afca6d6fafc3d7775468b59e86202d34090a05e79738825de54e049b7f",
+        "bb20acdc5589e553935c580c430ca3ed",
+        "451170f56e46da349475388011c2ffcce2aca837358e8bc8eae3d42df0771a35"
+        "888a2af7d1042b657a63e68b25e5570791003fc68eca8e78ad62a59dd9bfd262"
+        "4afc591b0184807be766060c4c5d13dd5d52a4eb1c3263ca9508676ec83ad012"
+        "36292d37adadb29414b8a06016b43d7306e15f2314c2eda9cb5417938ee8a5c5"
+        "11d2fcbf7faf539367f4f37da831f1ae1250d12612becfdd13e770a1cf1566e0"
+        "e7639f6712f3fa79e7eef78f1fe83d31380f584acd2728e00e9882ddaee8be95"
+        "4b5dfc5d50f7d737e5cec604b60435ee138d38e0b560c1c3f943a1a72b5f3c77"
+        "bc39d40d30ab4415790b192f0f4e1d22dc560291b6c354af06f556325493a911"
+        "cc7d1efc296211a26d2ad27c78ef9e5445a1e5fc643aab6b2f029d8495469561"
+        "c3b35dec156e8f839861ff10509e65963f4a92a3843d0eb43fab38d4f1cd35b5"
+        "8092a195003018989118a9e2b60e78f5580a98dd47a7918752c95b449691f916"
+        "239aab24cbc4bc5cdc653e9273b687ccc01fa908c63a8f1903ea5d997b56af9f"
+        "f05ac3bb1e7f18fae5568c580d1324cd33cdd5f90764120a4f6fa3cac55269b6"
+        "ad2c71cbac89c691e052e9ed660eba99db9092e3f4a5ed4314910edae3779090"
+        "a4015c508b22e16b74ca58dad81273b4a2069797ab84dabf15e899f960298904"
+        "2be554b60735217cff7956d88bca8c2ed023c57ba79f3abd88d4b6e8fd3fec28",
+        "4df3fa60e97ddd3d9890bfc71e75f43aa2951f8b9f1eafbc7d85dcd52587af30"
+        "4ddcd513d4608123d797d3b734dced7c7549d54a0d18f19b3a3d8a46f1c480b8"
+        "4a986f47610907e764d214ee2af6831aad11daa8c55a483d7c99cdd418cb06a5"
+        "ee1be3f5cccefc1c5200f4320399b7430c90cf00bc901df08a20e2fbeee38e0a"
+        "65b4a43038ecbdfd7ecd3a8d138e862e6313fafc39453b2e610577749a1e0d11"
+        "7c3ec5018d3b56495d03e639c46233752371bcefb276b9b7b0623a6bf533cb72"
+        "016238ecc3ac227717c3c6b69b5f74ac27b2c966e552b85b67739c3540930cc0"
+        "e3a62d4a7362bd825d13fe1a6b68d6152ac6615eca89b3c1b8571e57b55766fb"
+        "a3ac11df6161dd6e5110e9047a6637349b0e49738a4ca52ba7e0f84e01a2af01"
+        "db63fa36bafc6e212089196b20d30b346ce865f0c1412d196e56b328cd1c2399"
+        "59202aeabc56aa08c847360ea1d3a5521748e853869b7a783874424c62b5da33"
+        "1d95880aa77ec6d04aa59b6e2e85a75a3fcc9be9537c3cf065d69537e1454024"
+        "4398137ae4d6cc84c5323df0b9cccf5bc51ad873847369c92e0bed692bc03b47"
+        "94b20ac22f672a64ecd15a69665f21233f47e6904b6cd2972ffc6aeb2961f666"
+        "923c13d3ab532009211c784f6a8553201907cdbf1db8176684ec59280fa20b82"
+        "9cfc069b2fc61d0a4c7cdcb9d7bee655cdb56932ca319152849f37b0b396674a"
+    },
+    // len = 523 bytes = 32 blocks + 11 bytes
+    {
+        "471f9a9ee92d06701e84d8ee35773e00939620dcfe7679b4aa6eb0d6c59bec8a"
+        "f51000afca6d6fafc3d7775468b59e86202d34090a05e79738825de54e049b7f",
+        "bb20acdc5589e553935c580c430ca3ed",
+        "451170f56e46da349475388011c2ffcce2aca837358e8bc8eae3d42df0771a35"
+        "888a2af7d1042b657a63e68b25e5570791003fc68eca8e78ad62a59dd9bfd262"
+        "4afc591b0184807be766060c4c5d13dd5d52a4eb1c3263ca9508676ec83ad012"
+        "36292d37adadb29414b8a06016b43d7306e15f2314c2eda9cb5417938ee8a5c5"
+        "11d2fcbf7faf539367f4f37da831f1ae1250d12612becfdd13e770a1cf1566e0"
+        "e7639f6712f3fa79e7eef78f1fe83d31380f584acd2728e00e9882ddaee8be95"
+        "4b5dfc5d50f7d737e5cec604b60435ee138d38e0b560c1c3f943a1a72b5f3c77"
+        "bc39d40d30ab4415790b192f0f4e1d22dc560291b6c354af06f556325493a911"
+        "cc7d1efc296211a26d2ad27c78ef9e5445a1e5fc643aab6b2f029d8495469561"
+        "c3b35dec156e8f839861ff10509e65963f4a92a3843d0eb43fab38d4f1cd35b5"
+        "8092a195003018989118a9e2b60e78f5580a98dd47a7918752c95b449691f916"
+        "239aab24cbc4bc5cdc653e9273b687ccc01fa908c63a8f1903ea5d997b56af9f"
+        "f05ac3bb1e7f18fae5568c580d1324cd33cdd5f90764120a4f6fa3cac55269b6"
+        "ad2c71cbac89c691e052e9ed660eba99db9092e3f4a5ed4314910edae3779090"
+        "a4015c508b22e16b74ca58dad81273b4a2069797ab84dabf15e899f960298904"
+        "2be554b60735217cff7956d88bca8c2ed023c57ba79f3abd88d4b6e8fd3fec28"
+        "2540de2c75ffa87478ff4c",
+        "4df3fa60e97ddd3d9890bfc71e75f43aa2951f8b9f1eafbc7d85dcd52587af30"
+        "4ddcd513d4608123d797d3b734dced7c7549d54a0d18f19b3a3d8a46f1c480b8"
+        "4a986f47610907e764d214ee2af6831aad11daa8c55a483d7c99cdd418cb06a5"
+        "ee1be3f5cccefc1c5200f4320399b7430c90cf00bc901df08a20e2fbeee38e0a"
+        "65b4a43038ecbdfd7ecd3a8d138e862e6313fafc39453b2e610577749a1e0d11"
+        "7c3ec5018d3b56495d03e639c46233752371bcefb276b9b7b0623a6bf533cb72"
+        "016238ecc3ac227717c3c6b69b5f74ac27b2c966e552b85b67739c3540930cc0"
+        "e3a62d4a7362bd825d13fe1a6b68d6152ac6615eca89b3c1b8571e57b55766fb"
+        "a3ac11df6161dd6e5110e9047a6637349b0e49738a4ca52ba7e0f84e01a2af01"
+        "db63fa36bafc6e212089196b20d30b346ce865f0c1412d196e56b328cd1c2399"
+        "59202aeabc56aa08c847360ea1d3a5521748e853869b7a783874424c62b5da33"
+        "1d95880aa77ec6d04aa59b6e2e85a75a3fcc9be9537c3cf065d69537e1454024"
+        "4398137ae4d6cc84c5323df0b9cccf5bc51ad873847369c92e0bed692bc03b47"
+        "94b20ac22f672a64ecd15a69665f21233f47e6904b6cd2972ffc6aeb2961f666"
+        "923c13d3ab532009211c784f6a8553201907cdbf1db8176684ec59280fa20b82"
+        "9cfc069b2fc61d0a4c7cdcb9d7bee655df0f08137ed40076a293a450b4602009"
+        "cdb56932ca319152849f37"
+    },
+    // len = 544 bytes = 34 blocks
+    {
+        "b0f0dc151095ec58d407195199de7a769a755b2bda7c52a039474db388ee3b2d"
+        "f04af5b44f1091da6520a660f008aa2a66778a6cc0c6d426a75298e6910364d6",
+        "cba24d262d789207390ecd8be26db11b",
+        "14f86e6190214719b2351ced9949c364ec108b19891d20c22ced4d0e5afe2a6e"
+        "f798cf87b916a06b4bbc58e4061c49f22cd40bb5f12b771d18c42c72c356e0ba"
+        "eeaf41a7c5e212109e6af4a4863d96b311a16803cce020e4a44c5667a2362190"
+        "e56337aa4549bae3b4af883aec1eedfec056018c362171da6ec74210fe63a1e3"
+        "c6d88e0b2248efd6f77710e495fee25554e3e28a045364721aa683180a24fcd0"
+        "fc8adc1ed2cbf4ca4205aed703902d57730fe1776245ea7cec6d95f69191c68d"
+        "1ba2aced6da0b7afa56587a8f5b4ff68c3e0e02526caa112373608c8c7ce55e2"
+        "7101d0dea2878e47ed15f0e2c9ef4b8cd02bb1f6f552082c8910f450de49334f"
+        "4b032eed8abc3477d1245a9a14a526e4d0d7dac529e2f1b2f2e503d02e362079"
+        "394e66c30a9b3bdbbf9575d33a9bb70a7291cf9b73c04e65a55136d387564dc0"
+        "a4b383ae4ebe890e53fee18d9999970b2a66a69e26f403cb45399fcc8fec8c33"
+        "9f10e1eece6afc2268ddaf0176470ca1adb33fd4a7429fed7c3eb90b2a463fca"
+        "5620b8248bb446f391f6f5083d01a9eab4e8be5c2a5e49a69c02b2c748f1919e"
+        "1149c39cfd09908eff85963c863f273b27e5975243e0f8e0e2aaa72b9b38c9ad"
+        "818c497e96d90c955ea3d2e5e2f9200adeb75c2297540279ffa9a49ae16e4762"
+        "fa91e0906aec26c98ff8ae72f1ce7ccf85d8f11c2cf3952b9c3ac67da80ddfa2"
+        "9ebf3309ac59d23b5180ad424e2911d30103ef2ef6845993be1f10662df009cb",
+        "864f5741c3d048ae75b1f0186a8eb07542e099799539f1c5e47c0643ce4de614"
+        "6742e685051ab2911a29ed18872ffb317731cc55af9a2ed5da2d4902a1fee005"
+        "8a818c8d1fc508c1c303760fcd580c2ee21e65dd094a41cd4c9fe34dbb0d75cc"
+        "ee2816cab10cdc1f9ed3cc1d36d75795ec299ca9357e92a782d822298db2f601"
+        "7cbc376141aea92cc67b6dfa8108d5d9021780fd852d38f593a30274c41379dc"
+        "8a34ea4ae767f811f357bf5742c86c2b46b5c059cd3c7695c70f1aca2ca2a669"
+        "196aff6f192af27bbfc85c55c31a5a0ece0ecc55016589c58c17ef921620347e"
+        "4be4c29f3333506b3303dd1759f03733169ccf7a93e2a24e6dbeeacefc382d5c"
+        "b00398da4b5704430b5675b82064f9b5215b0757d440a5de9422121891129f39"
+        "276171d1ad42f21239c6d49e17ab5fc989048dce4ffb68131e8aa674817cbed9"
+        "91ab240aa629d4ac29dcf407f0911dff119f8c02441fc72a8987ed46d9cde314"
+        "bc11f4232351386f1612ba71c3d18d432802754ca738df69c60650f309fec629"
+        "c3727cc0a715f55a8b623554cbd08092b833b645acfb34216b02c822e13c79d4"
+        "e6835cc8b5e88639a1fee056cfa654a4fbd238ac59f4b2c70060a71e0954a6b0"
+        "045e53711ba7043c1e5c713d5ccec41975bb3114dcbd63ec08e90074191f515f"
+        "e0f83bbc6990d010d6a30674d69f3d8355ca1db1b3a294b233949f4f0dc423bb"
+        "f02abbc2306a6791749c97045db72ead783f2e8b90127b4c12a4ab6a329d3ecd"
+    },
     // Test vectors from NIST
     // https://csrc.nist.gov/projects/cryptographic-algorithm-validation-program
     // 256-bit key, 256-bit data (32 bytes, 2 blocks)
diff --git a/generated-src/linux-x86_64/crypto/fipsmodule/aesni-xts-avx512.S b/generated-src/linux-x86_64/crypto/fipsmodule/aesni-xts-avx512.S
new file mode 100644
index 000000000..5812d16d6
--- /dev/null
+++ b/generated-src/linux-x86_64/crypto/fipsmodule/aesni-xts-avx512.S
@@ -0,0 +1,5211 @@
+// This file is generated from a similarly-named Perl script in the BoringSSL
+// source tree. Do not edit by hand.
+
+#if defined(__has_feature)
+#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+#define OPENSSL_NO_ASM
+#endif
+#endif
+
+#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM) && defined(__ELF__)
+#if defined(BORINGSSL_PREFIX)
+#include <boringssl_prefix_symbols_asm.h>
+#endif
+.text	
+.globl	aes_hw_xts_encrypt_avx512
+.hidden aes_hw_xts_encrypt_avx512
+.hidden	aes_hw_xts_encrypt_avx512
+.type	aes_hw_xts_encrypt_avx512,@function
+.align	32
+aes_hw_xts_encrypt_avx512:
+.cfi_startproc	
+.byte	243,15,30,250
+	pushq	%rbp
+	movq	%rsp,%rbp
+	subq	$376,%rsp
+	andq	$0xffffffffffffffc0,%rsp
+	movq	%rbx,368(%rsp)
+	movq	$0x87,%r10
+	vmovdqu	(%r9),%xmm1
+	vpxor	%xmm4,%xmm4,%xmm4
+	vmovdqu	(%r8),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+
+	vmovdqu	(%rcx),%xmm2
+	vmovdqa	%xmm2,128(%rsp)
+
+	vmovdqu	16(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	16(%rcx),%xmm2
+	vmovdqa	%xmm2,144(%rsp)
+
+	vmovdqu	32(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	32(%rcx),%xmm2
+	vmovdqa	%xmm2,160(%rsp)
+
+	vmovdqu	48(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	48(%rcx),%xmm2
+	vmovdqa	%xmm2,176(%rsp)
+
+	vmovdqu	64(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	64(%rcx),%xmm2
+	vmovdqa	%xmm2,192(%rsp)
+
+	vmovdqu	80(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	80(%rcx),%xmm2
+	vmovdqa	%xmm2,208(%rsp)
+
+	vmovdqu	96(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	96(%rcx),%xmm2
+	vmovdqa	%xmm2,224(%rsp)
+
+	vmovdqu	112(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	112(%rcx),%xmm2
+	vmovdqa	%xmm2,240(%rsp)
+
+	vmovdqu	128(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	128(%rcx),%xmm2
+	vmovdqa	%xmm2,256(%rsp)
+
+	vmovdqu	144(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	144(%rcx),%xmm2
+	vmovdqa	%xmm2,272(%rsp)
+
+	vmovdqu	160(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	160(%rcx),%xmm2
+	vmovdqa	%xmm2,288(%rsp)
+
+	vmovdqu	176(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	176(%rcx),%xmm2
+	vmovdqa	%xmm2,304(%rsp)
+
+	vmovdqu	192(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	192(%rcx),%xmm2
+	vmovdqa	%xmm2,320(%rsp)
+
+	vmovdqu	208(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	208(%rcx),%xmm2
+	vmovdqa	%xmm2,336(%rsp)
+
+	vmovdqu	224(%r8),%xmm0
+.byte	98,242,117,8,221,200
+
+	vmovdqu	224(%rcx),%xmm2
+	vmovdqa	%xmm2,352(%rsp)
+
+	vmovdqa	%xmm1,(%rsp)
+
+	cmpq	$0x80,%rdx
+	jl	.L_less_than_128_bytes_hEgxyDlCngwrfFe
+	vpbroadcastq	%r10,%zmm25
+	cmpq	$0x100,%rdx
+	jge	.L_start_by16_hEgxyDlCngwrfFe
+	cmpq	$0x80,%rdx
+	jge	.L_start_by8_hEgxyDlCngwrfFe
+
+.L_do_n_blocks_hEgxyDlCngwrfFe:
+	cmpq	$0x0,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	cmpq	$0x70,%rdx
+	jge	.L_remaining_num_blocks_is_7_hEgxyDlCngwrfFe
+	cmpq	$0x60,%rdx
+	jge	.L_remaining_num_blocks_is_6_hEgxyDlCngwrfFe
+	cmpq	$0x50,%rdx
+	jge	.L_remaining_num_blocks_is_5_hEgxyDlCngwrfFe
+	cmpq	$0x40,%rdx
+	jge	.L_remaining_num_blocks_is_4_hEgxyDlCngwrfFe
+	cmpq	$0x30,%rdx
+	jge	.L_remaining_num_blocks_is_3_hEgxyDlCngwrfFe
+	cmpq	$0x20,%rdx
+	jge	.L_remaining_num_blocks_is_2_hEgxyDlCngwrfFe
+	cmpq	$0x10,%rdx
+	jge	.L_remaining_num_blocks_is_1_hEgxyDlCngwrfFe
+	vmovdqa	%xmm0,%xmm8
+	vmovdqa	%xmm9,%xmm0
+	jmp	.L_steal_cipher_hEgxyDlCngwrfFe
+
+.L_remaining_num_blocks_is_7_hEgxyDlCngwrfFe:
+	movq	$0xffffffffffffffff,%r8
+	shrq	$0x10,%r8
+	kmovq	%r8,%k1
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2{%k1}
+	addq	$0x70,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi){%k1}
+	addq	$0x70,%rsi
+	vextracti32x4	$0x2,%zmm2,%xmm8
+	vextracti32x4	$0x3,%zmm10,%xmm0
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_hEgxyDlCngwrfFe
+
+.L_remaining_num_blocks_is_6_hEgxyDlCngwrfFe:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%ymm2
+	addq	$0x60,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%ymm2,64(%rsi)
+	addq	$0x60,%rsi
+	vextracti32x4	$0x1,%zmm2,%xmm8
+	vextracti32x4	$0x2,%zmm10,%xmm0
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_hEgxyDlCngwrfFe
+
+.L_remaining_num_blocks_is_5_hEgxyDlCngwrfFe:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu	64(%rdi),%xmm2
+	addq	$0x50,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu	%xmm2,64(%rsi)
+	addq	$0x50,%rsi
+	movdqa	%xmm2,%xmm8
+	vextracti32x4	$0x1,%zmm10,%xmm0
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_hEgxyDlCngwrfFe
+
+.L_remaining_num_blocks_is_4_hEgxyDlCngwrfFe:
+	vmovdqu8	(%rdi),%zmm1
+	addq	$0x40,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	addq	$0x40,%rsi
+	vextracti32x4	$0x3,%zmm1,%xmm8
+	vextracti32x4	$0x0,%zmm10,%xmm0
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_hEgxyDlCngwrfFe
+.L_remaining_num_blocks_is_3_hEgxyDlCngwrfFe:
+	vextracti32x4	$0x1,%zmm9,%xmm10
+	vextracti32x4	$0x2,%zmm9,%xmm11
+	vmovdqu	(%rdi),%xmm1
+	vmovdqu	16(%rdi),%xmm2
+	vmovdqu	32(%rdi),%xmm3
+	addq	$0x30,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	addq	$0x30,%rsi
+	vmovdqa	%xmm3,%xmm8
+	vextracti32x4	$0x3,%zmm9,%xmm0
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_hEgxyDlCngwrfFe
+.L_remaining_num_blocks_is_2_hEgxyDlCngwrfFe:
+	vextracti32x4	$0x1,%zmm9,%xmm10
+	vmovdqu	(%rdi),%xmm1
+	vmovdqu	16(%rdi),%xmm2
+	addq	$0x20,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	addq	$0x20,%rsi
+	vmovdqa	%xmm2,%xmm8
+	vextracti32x4	$0x2,%zmm9,%xmm0
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_hEgxyDlCngwrfFe
+.L_remaining_num_blocks_is_1_hEgxyDlCngwrfFe:
+	vmovdqu	(%rdi),%xmm1
+	addq	$0x10,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqu	%xmm1,(%rsi)
+	addq	$0x10,%rsi
+	vmovdqa	%xmm1,%xmm8
+	vextracti32x4	$0x1,%zmm9,%xmm0
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_hEgxyDlCngwrfFe
+
+.L_start_by16_hEgxyDlCngwrfFe:
+	vbroadcasti32x4	(%rsp),%zmm0
+	vbroadcasti32x4	shufb_15_7(%rip),%zmm8
+	movq	$0xaa,%r8
+	kmovq	%r8,%k2
+	vpshufb	%zmm8,%zmm0,%zmm1
+	vpsllvq	const_dq3210(%rip),%zmm0,%zmm4
+	vpsrlvq	const_dq5678(%rip),%zmm1,%zmm2
+.byte	98,147,109,72,68,217,0
+	vpxorq	%zmm2,%zmm4,%zmm4{%k2}
+	vpxord	%zmm4,%zmm3,%zmm9
+	vpsllvq	const_dq7654(%rip),%zmm0,%zmm5
+	vpsrlvq	const_dq1234(%rip),%zmm1,%zmm6
+.byte	98,147,77,72,68,249,0
+	vpxorq	%zmm6,%zmm5,%zmm5{%k2}
+	vpxord	%zmm5,%zmm7,%zmm10
+	vpsrldq	$0xf,%zmm9,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm9,%zmm11
+	vpxord	%zmm14,%zmm11,%zmm11
+	vpsrldq	$0xf,%zmm10,%zmm15
+.byte	98,131,5,72,68,193,0
+	vpslldq	$0x1,%zmm10,%zmm12
+	vpxord	%zmm16,%zmm12,%zmm12
+
+.L_main_loop_run_16_hEgxyDlCngwrfFe:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2
+	vmovdqu8	128(%rdi),%zmm3
+	vmovdqu8	192(%rdi),%zmm4
+	addq	$0x100,%rdi
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+	vpxorq	%zmm11,%zmm3,%zmm3
+	vpxorq	%zmm12,%zmm4,%zmm4
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vpxorq	%zmm0,%zmm3,%zmm3
+	vpxorq	%zmm0,%zmm4,%zmm4
+	vpsrldq	$0xf,%zmm11,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm11,%zmm15
+	vpxord	%zmm14,%zmm15,%zmm15
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vpsrldq	$0xf,%zmm12,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm12,%zmm16
+	vpxord	%zmm14,%zmm16,%zmm16
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vpsrldq	$0xf,%zmm15,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm15,%zmm17
+	vpxord	%zmm14,%zmm17,%zmm17
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vpsrldq	$0xf,%zmm16,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm16,%zmm18
+	vpxord	%zmm14,%zmm18,%zmm18
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+.byte	98,242,101,72,221,216
+.byte	98,242,93,72,221,224
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+	vpxorq	%zmm11,%zmm3,%zmm3
+	vpxorq	%zmm12,%zmm4,%zmm4
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqa32	%zmm17,%zmm11
+	vmovdqa32	%zmm18,%zmm12
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi)
+	vmovdqu8	%zmm3,128(%rsi)
+	vmovdqu8	%zmm4,192(%rsi)
+	addq	$0x100,%rsi
+	subq	$0x100,%rdx
+	cmpq	$0x100,%rdx
+	jge	.L_main_loop_run_16_hEgxyDlCngwrfFe
+	cmpq	$0x80,%rdx
+	jge	.L_main_loop_run_8_hEgxyDlCngwrfFe
+	vextracti32x4	$0x3,%zmm4,%xmm0
+	jmp	.L_do_n_blocks_hEgxyDlCngwrfFe
+
+.L_start_by8_hEgxyDlCngwrfFe:
+	vbroadcasti32x4	(%rsp),%zmm0
+	vbroadcasti32x4	shufb_15_7(%rip),%zmm8
+	movq	$0xaa,%r8
+	kmovq	%r8,%k2
+	vpshufb	%zmm8,%zmm0,%zmm1
+	vpsllvq	const_dq3210(%rip),%zmm0,%zmm4
+	vpsrlvq	const_dq5678(%rip),%zmm1,%zmm2
+.byte	98,147,109,72,68,217,0
+	vpxorq	%zmm2,%zmm4,%zmm4{%k2}
+	vpxord	%zmm4,%zmm3,%zmm9
+	vpsllvq	const_dq7654(%rip),%zmm0,%zmm5
+	vpsrlvq	const_dq1234(%rip),%zmm1,%zmm6
+.byte	98,147,77,72,68,249,0
+	vpxorq	%zmm6,%zmm5,%zmm5{%k2}
+	vpxord	%zmm5,%zmm7,%zmm10
+
+.L_main_loop_run_8_hEgxyDlCngwrfFe:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2
+	addq	$0x80,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vpsrldq	$0xf,%zmm9,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm9,%zmm15
+	vpxord	%zmm14,%zmm15,%zmm15
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+	vpsrldq	$0xf,%zmm10,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm10,%zmm16
+	vpxord	%zmm14,%zmm16,%zmm16
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi)
+	addq	$0x80,%rsi
+	subq	$0x80,%rdx
+	cmpq	$0x80,%rdx
+	jge	.L_main_loop_run_8_hEgxyDlCngwrfFe
+	vextracti32x4	$0x3,%zmm2,%xmm0
+	jmp	.L_do_n_blocks_hEgxyDlCngwrfFe
+
+.L_steal_cipher_next_hEgxyDlCngwrfFe:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,(%rsp)
+	movq	%rbx,8(%rsp)
+	vmovdqa	(%rsp),%xmm0
+
+.L_steal_cipher_hEgxyDlCngwrfFe:
+	vmovdqa	%xmm8,%xmm2
+	leaq	vpshufb_shf_table(%rip),%rax
+	vmovdqu	(%rax,%rdx,1),%xmm10
+	vpshufb	%xmm10,%xmm8,%xmm8
+	vmovdqu	-16(%rdi,%rdx,1),%xmm3
+	vmovdqu	%xmm8,-16(%rsi,%rdx,1)
+	leaq	vpshufb_shf_table(%rip),%rax
+	addq	$16,%rax
+	subq	%rdx,%rax
+	vmovdqu	(%rax),%xmm10
+	vpxor	mask1(%rip),%xmm10,%xmm10
+	vpshufb	%xmm10,%xmm3,%xmm3
+	vpblendvb	%xmm10,%xmm2,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm3,%xmm8
+	vpxor	128(%rsp),%xmm8,%xmm8
+.byte	98,114,61,8,220,132,36,144,0,0,0
+.byte	98,114,61,8,220,132,36,160,0,0,0
+.byte	98,114,61,8,220,132,36,176,0,0,0
+.byte	98,114,61,8,220,132,36,192,0,0,0
+.byte	98,114,61,8,220,132,36,208,0,0,0
+.byte	98,114,61,8,220,132,36,224,0,0,0
+.byte	98,114,61,8,220,132,36,240,0,0,0
+.byte	98,114,61,8,220,132,36,0,1,0,0
+.byte	98,114,61,8,220,132,36,16,1,0,0
+.byte	98,114,61,8,220,132,36,32,1,0,0
+.byte	98,114,61,8,220,132,36,48,1,0,0
+.byte	98,114,61,8,220,132,36,64,1,0,0
+.byte	98,114,61,8,220,132,36,80,1,0,0
+.byte	98,114,61,8,221,132,36,96,1,0,0
+	vpxor	%xmm0,%xmm8,%xmm8
+	vmovdqu	%xmm8,-16(%rsi)
+
+.L_ret_hEgxyDlCngwrfFe:
+	movq	368(%rsp),%rbx
+	movq	%rbp,%rsp
+	popq	%rbp
+	.byte	0xf3,0xc3
+
+.L_less_than_128_bytes_hEgxyDlCngwrfFe:
+	cmpq	$0x10,%rdx
+	jb	.L_ret_hEgxyDlCngwrfFe
+	movq	%rdx,%r8
+	andq	$0x70,%r8
+	cmpq	$0x60,%r8
+	je	.L_num_blocks_is_6_hEgxyDlCngwrfFe
+	cmpq	$0x50,%r8
+	je	.L_num_blocks_is_5_hEgxyDlCngwrfFe
+	cmpq	$0x40,%r8
+	je	.L_num_blocks_is_4_hEgxyDlCngwrfFe
+	cmpq	$0x30,%r8
+	je	.L_num_blocks_is_3_hEgxyDlCngwrfFe
+	cmpq	$0x20,%r8
+	je	.L_num_blocks_is_2_hEgxyDlCngwrfFe
+	cmpq	$0x10,%r8
+	je	.L_num_blocks_is_1_hEgxyDlCngwrfFe
+
+.L_num_blocks_is_7_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,80(%rsp)
+	movq	%rbx,88(%rsp)
+	vmovdqa	80(%rsp),%xmm14
+	vmovdqu	80(%rdi),%xmm6
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,96(%rsp)
+	movq	%rbx,104(%rsp)
+	vmovdqa	96(%rsp),%xmm15
+	vmovdqu	96(%rdi),%xmm7
+	addq	$0x70,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vpxor	%xmm0,%xmm7,%xmm7
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+.byte	98,242,93,8,221,224
+.byte	98,242,85,8,221,232
+.byte	98,242,77,8,221,240
+.byte	98,242,69,8,221,248
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	vmovdqu	%xmm6,80(%rsi)
+	vmovdqu	%xmm7,96(%rsi)
+	addq	$0x70,%rsi
+	vmovdqa	%xmm7,%xmm8
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_next_hEgxyDlCngwrfFe
+
+.L_num_blocks_is_6_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,80(%rsp)
+	movq	%rbx,88(%rsp)
+	vmovdqa	80(%rsp),%xmm14
+	vmovdqu	80(%rdi),%xmm6
+	addq	$0x60,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+.byte	98,242,93,8,221,224
+.byte	98,242,85,8,221,232
+.byte	98,242,77,8,221,240
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	vmovdqu	%xmm6,80(%rsi)
+	addq	$0x60,%rsi
+	vmovdqa	%xmm6,%xmm8
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_next_hEgxyDlCngwrfFe
+
+.L_num_blocks_is_5_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	addq	$0x50,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+.byte	98,242,93,8,221,224
+.byte	98,242,85,8,221,232
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	addq	$0x50,%rsi
+	vmovdqa	%xmm5,%xmm8
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_next_hEgxyDlCngwrfFe
+
+.L_num_blocks_is_4_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	addq	$0x40,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+.byte	98,242,93,8,221,224
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	addq	$0x40,%rsi
+	vmovdqa	%xmm4,%xmm8
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_next_hEgxyDlCngwrfFe
+
+.L_num_blocks_is_3_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	addq	$0x30,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	addq	$0x30,%rsi
+	vmovdqa	%xmm3,%xmm8
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_next_hEgxyDlCngwrfFe
+
+.L_num_blocks_is_2_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	addq	$0x20,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	addq	$0x20,%rsi
+	vmovdqa	%xmm2,%xmm8
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_next_hEgxyDlCngwrfFe
+
+.L_num_blocks_is_1_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	addq	$0x10,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqu	%xmm1,(%rsi)
+	addq	$0x10,%rsi
+	vmovdqa	%xmm1,%xmm8
+	andq	$0xf,%rdx
+	je	.L_ret_hEgxyDlCngwrfFe
+	jmp	.L_steal_cipher_next_hEgxyDlCngwrfFe
+	.byte	0xf3,0xc3
+.cfi_endproc	
+.globl	aes_hw_xts_decrypt_avx512
+.hidden aes_hw_xts_decrypt_avx512
+.hidden	aes_hw_xts_decrypt_avx512
+.type	aes_hw_xts_decrypt_avx512,@function
+.align	32
+aes_hw_xts_decrypt_avx512:
+.cfi_startproc	
+.byte	243,15,30,250
+	pushq	%rbp
+	movq	%rsp,%rbp
+	subq	$376,%rsp
+	andq	$0xffffffffffffffc0,%rsp
+	movq	%rbx,368(%rsp)
+	movq	$0x87,%r10
+	vmovdqu	(%r9),%xmm1
+	vpxor	%xmm4,%xmm4,%xmm4
+	vmovdqu	(%r8),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+
+	vmovdqu	224(%rcx),%xmm2
+	vmovdqa	%xmm2,352(%rsp)
+
+	vmovdqu	16(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	208(%rcx),%xmm2
+	vmovdqa	%xmm2,336(%rsp)
+
+	vmovdqu	32(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	192(%rcx),%xmm2
+	vmovdqa	%xmm2,320(%rsp)
+
+	vmovdqu	48(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	176(%rcx),%xmm2
+	vmovdqa	%xmm2,304(%rsp)
+
+	vmovdqu	64(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	160(%rcx),%xmm2
+	vmovdqa	%xmm2,288(%rsp)
+
+	vmovdqu	80(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	144(%rcx),%xmm2
+	vmovdqa	%xmm2,272(%rsp)
+
+	vmovdqu	96(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	128(%rcx),%xmm2
+	vmovdqa	%xmm2,256(%rsp)
+
+	vmovdqu	112(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	112(%rcx),%xmm2
+	vmovdqa	%xmm2,240(%rsp)
+
+	vmovdqu	128(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	96(%rcx),%xmm2
+	vmovdqa	%xmm2,224(%rsp)
+
+	vmovdqu	144(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	80(%rcx),%xmm2
+	vmovdqa	%xmm2,208(%rsp)
+
+	vmovdqu	160(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	64(%rcx),%xmm2
+	vmovdqa	%xmm2,192(%rsp)
+
+	vmovdqu	176(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	48(%rcx),%xmm2
+	vmovdqa	%xmm2,176(%rsp)
+
+	vmovdqu	192(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	32(%rcx),%xmm2
+	vmovdqa	%xmm2,160(%rsp)
+
+	vmovdqu	208(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	16(%rcx),%xmm2
+	vmovdqa	%xmm2,144(%rsp)
+
+	vmovdqu	224(%r8),%xmm0
+.byte	98,242,117,8,221,200
+
+	vmovdqu	(%rcx),%xmm2
+	vmovdqa	%xmm2,128(%rsp)
+
+	vmovdqa	%xmm1,(%rsp)
+
+	cmpq	$0x80,%rdx
+	jb	.L_less_than_128_bytes_amivrujEyduiFoi
+	vpbroadcastq	%r10,%zmm25
+	cmpq	$0x100,%rdx
+	jge	.L_start_by16_amivrujEyduiFoi
+	jmp	.L_start_by8_amivrujEyduiFoi
+
+.L_do_n_blocks_amivrujEyduiFoi:
+	cmpq	$0x0,%rdx
+	je	.L_ret_amivrujEyduiFoi
+	cmpq	$0x70,%rdx
+	jge	.L_remaining_num_blocks_is_7_amivrujEyduiFoi
+	cmpq	$0x60,%rdx
+	jge	.L_remaining_num_blocks_is_6_amivrujEyduiFoi
+	cmpq	$0x50,%rdx
+	jge	.L_remaining_num_blocks_is_5_amivrujEyduiFoi
+	cmpq	$0x40,%rdx
+	jge	.L_remaining_num_blocks_is_4_amivrujEyduiFoi
+	cmpq	$0x30,%rdx
+	jge	.L_remaining_num_blocks_is_3_amivrujEyduiFoi
+	cmpq	$0x20,%rdx
+	jge	.L_remaining_num_blocks_is_2_amivrujEyduiFoi
+	cmpq	$0x10,%rdx
+	jge	.L_remaining_num_blocks_is_1_amivrujEyduiFoi
+
+
+	vmovdqu	%xmm5,%xmm1
+
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqu	%xmm1,-16(%rsi)
+	vmovdqa	%xmm1,%xmm8
+
+
+	movq	$0x1,%r8
+	kmovq	%r8,%k1
+	vpsllq	$0x3f,%xmm9,%xmm13
+	vpsraq	$0x3f,%xmm13,%xmm14
+	vpandq	%xmm25,%xmm14,%xmm5
+	vpxorq	%xmm5,%xmm9,%xmm9{%k1}
+	vpsrldq	$0x8,%xmm9,%xmm10
+.byte	98, 211, 181, 8, 115, 194, 1
+	vpslldq	$0x8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm0,%xmm0
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_remaining_num_blocks_is_7_amivrujEyduiFoi:
+	movq	$0xffffffffffffffff,%r8
+	shrq	$0x10,%r8
+	kmovq	%r8,%k1
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2{%k1}
+	addq	$0x70,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_7_remain_amivrujEyduiFoi
+	vextracti32x4	$0x2,%zmm10,%xmm12
+	vextracti32x4	$0x3,%zmm10,%xmm13
+	vinserti32x4	$0x2,%xmm13,%zmm10,%zmm10
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi){%k1}
+	addq	$0x70,%rsi
+	vextracti32x4	$0x2,%zmm2,%xmm8
+	vmovdqa	%xmm12,%xmm0
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_7_remain_amivrujEyduiFoi:
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi){%k1}
+	jmp	.L_ret_amivrujEyduiFoi
+
+.L_remaining_num_blocks_is_6_amivrujEyduiFoi:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%ymm2
+	addq	$0x60,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_6_remain_amivrujEyduiFoi
+	vextracti32x4	$0x1,%zmm10,%xmm12
+	vextracti32x4	$0x2,%zmm10,%xmm13
+	vinserti32x4	$0x1,%xmm13,%zmm10,%zmm10
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%ymm2,64(%rsi)
+	addq	$0x60,%rsi
+	vextracti32x4	$0x1,%zmm2,%xmm8
+	vmovdqa	%xmm12,%xmm0
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_6_remain_amivrujEyduiFoi:
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%ymm2,64(%rsi)
+	jmp	.L_ret_amivrujEyduiFoi
+
+.L_remaining_num_blocks_is_5_amivrujEyduiFoi:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu	64(%rdi),%xmm2
+	addq	$0x50,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_5_remain_amivrujEyduiFoi
+	vmovdqa	%xmm10,%xmm12
+	vextracti32x4	$0x1,%zmm10,%xmm10
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu	%xmm2,64(%rsi)
+	addq	$0x50,%rsi
+	vmovdqa	%xmm2,%xmm8
+	vmovdqa	%xmm12,%xmm0
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_5_remain_amivrujEyduiFoi:
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%xmm2,64(%rsi)
+	jmp	.L_ret_amivrujEyduiFoi
+
+.L_remaining_num_blocks_is_4_amivrujEyduiFoi:
+	vmovdqu8	(%rdi),%zmm1
+	addq	$0x40,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_4_remain_amivrujEyduiFoi
+	vextracti32x4	$0x3,%zmm9,%xmm12
+	vinserti32x4	$0x3,%xmm10,%zmm9,%zmm9
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	addq	$0x40,%rsi
+	vextracti32x4	$0x3,%zmm1,%xmm8
+	vmovdqa	%xmm12,%xmm0
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_4_remain_amivrujEyduiFoi:
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	jmp	.L_ret_amivrujEyduiFoi
+
+.L_remaining_num_blocks_is_3_amivrujEyduiFoi:
+	vmovdqu	(%rdi),%xmm1
+	vmovdqu	16(%rdi),%xmm2
+	vmovdqu	32(%rdi),%xmm3
+	addq	$0x30,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_3_remain_amivrujEyduiFoi
+	vextracti32x4	$0x2,%zmm9,%xmm13
+	vextracti32x4	$0x1,%zmm9,%xmm10
+	vextracti32x4	$0x3,%zmm9,%xmm11
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	addq	$0x30,%rsi
+	vmovdqa	%xmm3,%xmm8
+	vmovdqa	%xmm13,%xmm0
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_3_remain_amivrujEyduiFoi:
+	vextracti32x4	$0x1,%zmm9,%xmm10
+	vextracti32x4	$0x2,%zmm9,%xmm11
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	jmp	.L_ret_amivrujEyduiFoi
+
+.L_remaining_num_blocks_is_2_amivrujEyduiFoi:
+	vmovdqu	(%rdi),%xmm1
+	vmovdqu	16(%rdi),%xmm2
+	addq	$0x20,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_2_remain_amivrujEyduiFoi
+	vextracti32x4	$0x2,%zmm9,%xmm10
+	vextracti32x4	$0x1,%zmm9,%xmm12
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	addq	$0x20,%rsi
+	vmovdqa	%xmm2,%xmm8
+	vmovdqa	%xmm12,%xmm0
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_2_remain_amivrujEyduiFoi:
+	vextracti32x4	$0x1,%zmm9,%xmm10
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	jmp	.L_ret_amivrujEyduiFoi
+
+.L_remaining_num_blocks_is_1_amivrujEyduiFoi:
+	vmovdqu	(%rdi),%xmm1
+	addq	$0x10,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_1_remain_amivrujEyduiFoi
+	vextracti32x4	$0x1,%zmm9,%xmm11
+	vpxor	%xmm11,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+	vpxor	%xmm11,%xmm1,%xmm1
+	vmovdqu	%xmm1,(%rsi)
+	addq	$0x10,%rsi
+	vmovdqa	%xmm1,%xmm8
+	vmovdqa	%xmm9,%xmm0
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_1_remain_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqu	%xmm1,(%rsi)
+	jmp	.L_ret_amivrujEyduiFoi
+
+.L_start_by16_amivrujEyduiFoi:
+	vbroadcasti32x4	(%rsp),%zmm0
+	vbroadcasti32x4	shufb_15_7(%rip),%zmm8
+	movq	$0xaa,%r8
+	kmovq	%r8,%k2
+
+
+	vpshufb	%zmm8,%zmm0,%zmm1
+	vpsllvq	const_dq3210(%rip),%zmm0,%zmm4
+	vpsrlvq	const_dq5678(%rip),%zmm1,%zmm2
+.byte	98,147,109,72,68,217,0
+	vpxorq	%zmm2,%zmm4,%zmm4{%k2}
+	vpxord	%zmm4,%zmm3,%zmm9
+
+
+	vpsllvq	const_dq7654(%rip),%zmm0,%zmm5
+	vpsrlvq	const_dq1234(%rip),%zmm1,%zmm6
+.byte	98,147,77,72,68,249,0
+	vpxorq	%zmm6,%zmm5,%zmm5{%k2}
+	vpxord	%zmm5,%zmm7,%zmm10
+
+
+	vpsrldq	$0xf,%zmm9,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm9,%zmm11
+	vpxord	%zmm14,%zmm11,%zmm11
+
+	vpsrldq	$0xf,%zmm10,%zmm15
+.byte	98,131,5,72,68,193,0
+	vpslldq	$0x1,%zmm10,%zmm12
+	vpxord	%zmm16,%zmm12,%zmm12
+
+.L_main_loop_run_16_amivrujEyduiFoi:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2
+	vmovdqu8	128(%rdi),%zmm3
+	vmovdqu8	192(%rdi),%zmm4
+	vmovdqu8	240(%rdi),%zmm5
+	addq	$0x100,%rdi
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+	vpxorq	%zmm11,%zmm3,%zmm3
+	vpxorq	%zmm12,%zmm4,%zmm4
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vpxorq	%zmm0,%zmm3,%zmm3
+	vpxorq	%zmm0,%zmm4,%zmm4
+	vpsrldq	$0xf,%zmm11,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm11,%zmm15
+	vpxord	%zmm14,%zmm15,%zmm15
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vpsrldq	$0xf,%zmm12,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm12,%zmm16
+	vpxord	%zmm14,%zmm16,%zmm16
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vpsrldq	$0xf,%zmm15,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm15,%zmm17
+	vpxord	%zmm14,%zmm17,%zmm17
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vpsrldq	$0xf,%zmm16,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm16,%zmm18
+	vpxord	%zmm14,%zmm18,%zmm18
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+.byte	98,242,101,72,223,216
+.byte	98,242,93,72,223,224
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+	vpxorq	%zmm11,%zmm3,%zmm3
+	vpxorq	%zmm12,%zmm4,%zmm4
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqa32	%zmm17,%zmm11
+	vmovdqa32	%zmm18,%zmm12
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi)
+	vmovdqu8	%zmm3,128(%rsi)
+	vmovdqu8	%zmm4,192(%rsi)
+	addq	$0x100,%rsi
+	subq	$0x100,%rdx
+	cmpq	$0x100,%rdx
+	jge	.L_main_loop_run_16_amivrujEyduiFoi
+
+	cmpq	$0x80,%rdx
+	jge	.L_main_loop_run_8_amivrujEyduiFoi
+	jmp	.L_do_n_blocks_amivrujEyduiFoi
+
+.L_start_by8_amivrujEyduiFoi:
+
+	vbroadcasti32x4	(%rsp),%zmm0
+	vbroadcasti32x4	shufb_15_7(%rip),%zmm8
+	movq	$0xaa,%r8
+	kmovq	%r8,%k2
+
+
+	vpshufb	%zmm8,%zmm0,%zmm1
+	vpsllvq	const_dq3210(%rip),%zmm0,%zmm4
+	vpsrlvq	const_dq5678(%rip),%zmm1,%zmm2
+.byte	98,147,109,72,68,217,0
+	vpxorq	%zmm2,%zmm4,%zmm4{%k2}
+	vpxord	%zmm4,%zmm3,%zmm9
+
+
+	vpsllvq	const_dq7654(%rip),%zmm0,%zmm5
+	vpsrlvq	const_dq1234(%rip),%zmm1,%zmm6
+.byte	98,147,77,72,68,249,0
+	vpxorq	%zmm6,%zmm5,%zmm5{%k2}
+	vpxord	%zmm5,%zmm7,%zmm10
+
+.L_main_loop_run_8_amivrujEyduiFoi:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2
+	vmovdqu8	112(%rdi),%xmm5
+	addq	$0x80,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vpsrldq	$0xf,%zmm9,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm9,%zmm15
+	vpxord	%zmm14,%zmm15,%zmm15
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+	vpsrldq	$0xf,%zmm10,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm10,%zmm16
+	vpxord	%zmm14,%zmm16,%zmm16
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi)
+	addq	$0x80,%rsi
+	subq	$0x80,%rdx
+	cmpq	$0x80,%rdx
+	jge	.L_main_loop_run_8_amivrujEyduiFoi
+	jmp	.L_do_n_blocks_amivrujEyduiFoi
+
+.L_steal_cipher_amivrujEyduiFoi:
+
+	vmovdqa	%xmm8,%xmm2
+
+
+	leaq	vpshufb_shf_table(%rip),%rax
+	vmovdqu	(%rax,%rdx,1),%xmm10
+	vpshufb	%xmm10,%xmm8,%xmm8
+
+
+	vmovdqu	-16(%rdi,%rdx,1),%xmm3
+	vmovdqu	%xmm8,-16(%rsi,%rdx,1)
+
+
+	leaq	vpshufb_shf_table(%rip),%rax
+	addq	$16,%rax
+	subq	%rdx,%rax
+	vmovdqu	(%rax),%xmm10
+	vpxor	mask1(%rip),%xmm10,%xmm10
+	vpshufb	%xmm10,%xmm3,%xmm3
+
+	vpblendvb	%xmm10,%xmm2,%xmm3,%xmm3
+
+
+	vpxor	%xmm0,%xmm3,%xmm8
+
+
+	vpxor	128(%rsp),%xmm8,%xmm8
+.byte	98,114,61,8,222,132,36,144,0,0,0
+.byte	98,114,61,8,222,132,36,160,0,0,0
+.byte	98,114,61,8,222,132,36,176,0,0,0
+.byte	98,114,61,8,222,132,36,192,0,0,0
+.byte	98,114,61,8,222,132,36,208,0,0,0
+.byte	98,114,61,8,222,132,36,224,0,0,0
+.byte	98,114,61,8,222,132,36,240,0,0,0
+.byte	98,114,61,8,222,132,36,0,1,0,0
+.byte	98,114,61,8,222,132,36,16,1,0,0
+.byte	98,114,61,8,222,132,36,32,1,0,0
+.byte	98,114,61,8,222,132,36,48,1,0,0
+.byte	98,114,61,8,222,132,36,64,1,0,0
+.byte	98,114,61,8,222,132,36,80,1,0,0
+.byte	98,114,61,8,223,132,36,96,1,0,0
+
+
+	vpxor	%xmm0,%xmm8,%xmm8
+
+.L_done_amivrujEyduiFoi:
+
+	vmovdqu	%xmm8,-16(%rsi)
+
+.L_ret_amivrujEyduiFoi:
+	movq	368(%rsp),%rbx
+	movq	%rbp,%rsp
+	popq	%rbp
+	.byte	0xf3,0xc3
+
+.L_less_than_128_bytes_amivrujEyduiFoi:
+	cmpq	$0x10,%rdx
+	jb	.L_ret_amivrujEyduiFoi
+
+	movq	%rdx,%r8
+	andq	$0x70,%r8
+	cmpq	$0x60,%r8
+	je	.L_num_blocks_is_6_amivrujEyduiFoi
+	cmpq	$0x50,%r8
+	je	.L_num_blocks_is_5_amivrujEyduiFoi
+	cmpq	$0x40,%r8
+	je	.L_num_blocks_is_4_amivrujEyduiFoi
+	cmpq	$0x30,%r8
+	je	.L_num_blocks_is_3_amivrujEyduiFoi
+	cmpq	$0x20,%r8
+	je	.L_num_blocks_is_2_amivrujEyduiFoi
+	cmpq	$0x10,%r8
+	je	.L_num_blocks_is_1_amivrujEyduiFoi
+
+.L_num_blocks_is_7_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,80(%rsp)
+	movq	%rbx,88(%rsp)
+	vmovdqa	80(%rsp),%xmm14
+	vmovdqu	80(%rdi),%xmm6
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,96(%rsp)
+	movq	%rbx,104(%rsp)
+	vmovdqa	96(%rsp),%xmm15
+	vmovdqu	96(%rdi),%xmm7
+	addq	$0x70,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_7_amivrujEyduiFoi
+
+.L_steal_cipher_7_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm15,%xmm16
+	vmovdqa	16(%rsp),%xmm15
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vpxor	%xmm0,%xmm7,%xmm7
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+.byte	98,242,77,8,223,240
+.byte	98,242,69,8,223,248
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	vmovdqu	%xmm6,80(%rsi)
+	addq	$0x70,%rsi
+	vmovdqa64	%xmm16,%xmm0
+	vmovdqa	%xmm7,%xmm8
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_7_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vpxor	%xmm0,%xmm7,%xmm7
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+.byte	98,242,77,8,223,240
+.byte	98,242,69,8,223,248
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	vmovdqu	%xmm6,80(%rsi)
+	addq	$0x70,%rsi
+	vmovdqa	%xmm7,%xmm8
+	jmp	.L_done_amivrujEyduiFoi
+
+.L_num_blocks_is_6_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,80(%rsp)
+	movq	%rbx,88(%rsp)
+	vmovdqa	80(%rsp),%xmm14
+	vmovdqu	80(%rdi),%xmm6
+	addq	$0x60,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_6_amivrujEyduiFoi
+
+.L_steal_cipher_6_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm14,%xmm15
+	vmovdqa	16(%rsp),%xmm14
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+.byte	98,242,77,8,223,240
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	addq	$0x60,%rsi
+	vmovdqa	%xmm15,%xmm0
+	vmovdqa	%xmm6,%xmm8
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_6_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+.byte	98,242,77,8,223,240
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	addq	$0x60,%rsi
+	vmovdqa	%xmm6,%xmm8
+	jmp	.L_done_amivrujEyduiFoi
+
+.L_num_blocks_is_5_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	addq	$0x50,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_5_amivrujEyduiFoi
+
+.L_steal_cipher_5_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm13,%xmm14
+	vmovdqa	16(%rsp),%xmm13
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	addq	$0x50,%rsi
+	vmovdqa	%xmm14,%xmm0
+	vmovdqa	%xmm5,%xmm8
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_5_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	addq	$0x50,%rsi
+	vmovdqa	%xmm5,%xmm8
+	jmp	.L_done_amivrujEyduiFoi
+
+.L_num_blocks_is_4_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	addq	$0x40,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_4_amivrujEyduiFoi
+
+.L_steal_cipher_4_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm12,%xmm13
+	vmovdqa	16(%rsp),%xmm12
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	addq	$0x40,%rsi
+	vmovdqa	%xmm13,%xmm0
+	vmovdqa	%xmm4,%xmm8
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_4_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	addq	$0x40,%rsi
+	vmovdqa	%xmm4,%xmm8
+	jmp	.L_done_amivrujEyduiFoi
+
+.L_num_blocks_is_3_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	addq	$0x30,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_3_amivrujEyduiFoi
+
+.L_steal_cipher_3_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm11,%xmm12
+	vmovdqa	16(%rsp),%xmm11
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	addq	$0x30,%rsi
+	vmovdqa	%xmm12,%xmm0
+	vmovdqa	%xmm3,%xmm8
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_3_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	addq	$0x30,%rsi
+	vmovdqa	%xmm3,%xmm8
+	jmp	.L_done_amivrujEyduiFoi
+
+.L_num_blocks_is_2_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	addq	$0x20,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_2_amivrujEyduiFoi
+
+.L_steal_cipher_2_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm10,%xmm11
+	vmovdqa	16(%rsp),%xmm10
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	addq	$0x20,%rsi
+	vmovdqa	%xmm11,%xmm0
+	vmovdqa	%xmm2,%xmm8
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_2_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	addq	$0x20,%rsi
+	vmovdqa	%xmm2,%xmm8
+	jmp	.L_done_amivrujEyduiFoi
+
+.L_num_blocks_is_1_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	addq	$0x10,%rdi
+	andq	$0xf,%rdx
+	je	.L_done_1_amivrujEyduiFoi
+
+.L_steal_cipher_1_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm9,%xmm10
+	vmovdqa	16(%rsp),%xmm9
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	addq	$0x10,%rsi
+	vmovdqa	%xmm10,%xmm0
+	vmovdqa	%xmm1,%xmm8
+	jmp	.L_steal_cipher_amivrujEyduiFoi
+
+.L_done_1_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	addq	$0x10,%rsi
+	vmovdqa	%xmm1,%xmm8
+	jmp	.L_done_amivrujEyduiFoi
+	.byte	0xf3,0xc3
+.cfi_endproc	
+.section	.rodata
+.align	16
+
+vpshufb_shf_table:
+.quad	0x8786858483828100, 0x8f8e8d8c8b8a8988
+.quad	0x0706050403020100, 0x000e0d0c0b0a0908
+
+mask1:
+.quad	0x8080808080808080, 0x8080808080808080
+
+const_dq3210:
+.quad	0, 0, 1, 1, 2, 2, 3, 3
+const_dq5678:
+.quad	8, 8, 7, 7, 6, 6, 5, 5
+const_dq7654:
+.quad	4, 4, 5, 5, 6, 6, 7, 7
+const_dq1234:
+.quad	4, 4, 3, 3, 2, 2, 1, 1
+
+shufb_15_7:
+.byte	15, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 7, 0xff, 0xff
+.byte	0xff, 0xff, 0xff, 0xff, 0xff
+
+.text	
+#endif
+#if defined(__ELF__)
+// See https://www.airs.com/blog/archives/518.
+.section .note.GNU-stack,"",%progbits
+#endif
diff --git a/generated-src/mac-x86_64/crypto/fipsmodule/aesni-xts-avx512.S b/generated-src/mac-x86_64/crypto/fipsmodule/aesni-xts-avx512.S
new file mode 100644
index 000000000..f6349c6e1
--- /dev/null
+++ b/generated-src/mac-x86_64/crypto/fipsmodule/aesni-xts-avx512.S
@@ -0,0 +1,5211 @@
+// This file is generated from a similarly-named Perl script in the BoringSSL
+// source tree. Do not edit by hand.
+
+#if defined(__has_feature)
+#if __has_feature(memory_sanitizer) && !defined(OPENSSL_NO_ASM)
+#define OPENSSL_NO_ASM
+#endif
+#endif
+
+#if defined(__x86_64__) && !defined(OPENSSL_NO_ASM) && defined(__APPLE__)
+#if defined(BORINGSSL_PREFIX)
+#include <boringssl_prefix_symbols_asm.h>
+#endif
+.text	
+.globl	_aes_hw_xts_encrypt_avx512
+.private_extern _aes_hw_xts_encrypt_avx512
+.private_extern	_aes_hw_xts_encrypt_avx512
+
+.p2align	5
+_aes_hw_xts_encrypt_avx512:
+
+.byte	243,15,30,250
+	pushq	%rbp
+	movq	%rsp,%rbp
+	subq	$376,%rsp
+	andq	$0xffffffffffffffc0,%rsp
+	movq	%rbx,368(%rsp)
+	movq	$0x87,%r10
+	vmovdqu	(%r9),%xmm1
+	vpxor	%xmm4,%xmm4,%xmm4
+	vmovdqu	(%r8),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+
+	vmovdqu	(%rcx),%xmm2
+	vmovdqa	%xmm2,128(%rsp)
+
+	vmovdqu	16(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	16(%rcx),%xmm2
+	vmovdqa	%xmm2,144(%rsp)
+
+	vmovdqu	32(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	32(%rcx),%xmm2
+	vmovdqa	%xmm2,160(%rsp)
+
+	vmovdqu	48(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	48(%rcx),%xmm2
+	vmovdqa	%xmm2,176(%rsp)
+
+	vmovdqu	64(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	64(%rcx),%xmm2
+	vmovdqa	%xmm2,192(%rsp)
+
+	vmovdqu	80(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	80(%rcx),%xmm2
+	vmovdqa	%xmm2,208(%rsp)
+
+	vmovdqu	96(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	96(%rcx),%xmm2
+	vmovdqa	%xmm2,224(%rsp)
+
+	vmovdqu	112(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	112(%rcx),%xmm2
+	vmovdqa	%xmm2,240(%rsp)
+
+	vmovdqu	128(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	128(%rcx),%xmm2
+	vmovdqa	%xmm2,256(%rsp)
+
+	vmovdqu	144(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	144(%rcx),%xmm2
+	vmovdqa	%xmm2,272(%rsp)
+
+	vmovdqu	160(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	160(%rcx),%xmm2
+	vmovdqa	%xmm2,288(%rsp)
+
+	vmovdqu	176(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	176(%rcx),%xmm2
+	vmovdqa	%xmm2,304(%rsp)
+
+	vmovdqu	192(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	192(%rcx),%xmm2
+	vmovdqa	%xmm2,320(%rsp)
+
+	vmovdqu	208(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	208(%rcx),%xmm2
+	vmovdqa	%xmm2,336(%rsp)
+
+	vmovdqu	224(%r8),%xmm0
+.byte	98,242,117,8,221,200
+
+	vmovdqu	224(%rcx),%xmm2
+	vmovdqa	%xmm2,352(%rsp)
+
+	vmovdqa	%xmm1,(%rsp)
+
+	cmpq	$0x80,%rdx
+	jl	L$_less_than_128_bytes_hEgxyDlCngwrfFe
+	vpbroadcastq	%r10,%zmm25
+	cmpq	$0x100,%rdx
+	jge	L$_start_by16_hEgxyDlCngwrfFe
+	cmpq	$0x80,%rdx
+	jge	L$_start_by8_hEgxyDlCngwrfFe
+
+L$_do_n_blocks_hEgxyDlCngwrfFe:
+	cmpq	$0x0,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	cmpq	$0x70,%rdx
+	jge	L$_remaining_num_blocks_is_7_hEgxyDlCngwrfFe
+	cmpq	$0x60,%rdx
+	jge	L$_remaining_num_blocks_is_6_hEgxyDlCngwrfFe
+	cmpq	$0x50,%rdx
+	jge	L$_remaining_num_blocks_is_5_hEgxyDlCngwrfFe
+	cmpq	$0x40,%rdx
+	jge	L$_remaining_num_blocks_is_4_hEgxyDlCngwrfFe
+	cmpq	$0x30,%rdx
+	jge	L$_remaining_num_blocks_is_3_hEgxyDlCngwrfFe
+	cmpq	$0x20,%rdx
+	jge	L$_remaining_num_blocks_is_2_hEgxyDlCngwrfFe
+	cmpq	$0x10,%rdx
+	jge	L$_remaining_num_blocks_is_1_hEgxyDlCngwrfFe
+	vmovdqa	%xmm0,%xmm8
+	vmovdqa	%xmm9,%xmm0
+	jmp	L$_steal_cipher_hEgxyDlCngwrfFe
+
+L$_remaining_num_blocks_is_7_hEgxyDlCngwrfFe:
+	movq	$0xffffffffffffffff,%r8
+	shrq	$0x10,%r8
+	kmovq	%r8,%k1
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2{%k1}
+	addq	$0x70,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi){%k1}
+	addq	$0x70,%rsi
+	vextracti32x4	$0x2,%zmm2,%xmm8
+	vextracti32x4	$0x3,%zmm10,%xmm0
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_hEgxyDlCngwrfFe
+
+L$_remaining_num_blocks_is_6_hEgxyDlCngwrfFe:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%ymm2
+	addq	$0x60,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%ymm2,64(%rsi)
+	addq	$0x60,%rsi
+	vextracti32x4	$0x1,%zmm2,%xmm8
+	vextracti32x4	$0x2,%zmm10,%xmm0
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_hEgxyDlCngwrfFe
+
+L$_remaining_num_blocks_is_5_hEgxyDlCngwrfFe:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu	64(%rdi),%xmm2
+	addq	$0x50,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu	%xmm2,64(%rsi)
+	addq	$0x50,%rsi
+	movdqa	%xmm2,%xmm8
+	vextracti32x4	$0x1,%zmm10,%xmm0
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_hEgxyDlCngwrfFe
+
+L$_remaining_num_blocks_is_4_hEgxyDlCngwrfFe:
+	vmovdqu8	(%rdi),%zmm1
+	addq	$0x40,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	addq	$0x40,%rsi
+	vextracti32x4	$0x3,%zmm1,%xmm8
+	vextracti32x4	$0x0,%zmm10,%xmm0
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_hEgxyDlCngwrfFe
+L$_remaining_num_blocks_is_3_hEgxyDlCngwrfFe:
+	vextracti32x4	$0x1,%zmm9,%xmm10
+	vextracti32x4	$0x2,%zmm9,%xmm11
+	vmovdqu	(%rdi),%xmm1
+	vmovdqu	16(%rdi),%xmm2
+	vmovdqu	32(%rdi),%xmm3
+	addq	$0x30,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	addq	$0x30,%rsi
+	vmovdqa	%xmm3,%xmm8
+	vextracti32x4	$0x3,%zmm9,%xmm0
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_hEgxyDlCngwrfFe
+L$_remaining_num_blocks_is_2_hEgxyDlCngwrfFe:
+	vextracti32x4	$0x1,%zmm9,%xmm10
+	vmovdqu	(%rdi),%xmm1
+	vmovdqu	16(%rdi),%xmm2
+	addq	$0x20,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	addq	$0x20,%rsi
+	vmovdqa	%xmm2,%xmm8
+	vextracti32x4	$0x2,%zmm9,%xmm0
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_hEgxyDlCngwrfFe
+L$_remaining_num_blocks_is_1_hEgxyDlCngwrfFe:
+	vmovdqu	(%rdi),%xmm1
+	addq	$0x10,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqu	%xmm1,(%rsi)
+	addq	$0x10,%rsi
+	vmovdqa	%xmm1,%xmm8
+	vextracti32x4	$0x1,%zmm9,%xmm0
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_hEgxyDlCngwrfFe
+
+L$_start_by16_hEgxyDlCngwrfFe:
+	vbroadcasti32x4	(%rsp),%zmm0
+	vbroadcasti32x4	shufb_15_7(%rip),%zmm8
+	movq	$0xaa,%r8
+	kmovq	%r8,%k2
+	vpshufb	%zmm8,%zmm0,%zmm1
+	vpsllvq	const_dq3210(%rip),%zmm0,%zmm4
+	vpsrlvq	const_dq5678(%rip),%zmm1,%zmm2
+.byte	98,147,109,72,68,217,0
+	vpxorq	%zmm2,%zmm4,%zmm4{%k2}
+	vpxord	%zmm4,%zmm3,%zmm9
+	vpsllvq	const_dq7654(%rip),%zmm0,%zmm5
+	vpsrlvq	const_dq1234(%rip),%zmm1,%zmm6
+.byte	98,147,77,72,68,249,0
+	vpxorq	%zmm6,%zmm5,%zmm5{%k2}
+	vpxord	%zmm5,%zmm7,%zmm10
+	vpsrldq	$0xf,%zmm9,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm9,%zmm11
+	vpxord	%zmm14,%zmm11,%zmm11
+	vpsrldq	$0xf,%zmm10,%zmm15
+.byte	98,131,5,72,68,193,0
+	vpslldq	$0x1,%zmm10,%zmm12
+	vpxord	%zmm16,%zmm12,%zmm12
+
+L$_main_loop_run_16_hEgxyDlCngwrfFe:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2
+	vmovdqu8	128(%rdi),%zmm3
+	vmovdqu8	192(%rdi),%zmm4
+	addq	$0x100,%rdi
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+	vpxorq	%zmm11,%zmm3,%zmm3
+	vpxorq	%zmm12,%zmm4,%zmm4
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vpxorq	%zmm0,%zmm3,%zmm3
+	vpxorq	%zmm0,%zmm4,%zmm4
+	vpsrldq	$0xf,%zmm11,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm11,%zmm15
+	vpxord	%zmm14,%zmm15,%zmm15
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vpsrldq	$0xf,%zmm12,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm12,%zmm16
+	vpxord	%zmm14,%zmm16,%zmm16
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vpsrldq	$0xf,%zmm15,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm15,%zmm17
+	vpxord	%zmm14,%zmm17,%zmm17
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vpsrldq	$0xf,%zmm16,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm16,%zmm18
+	vpxord	%zmm14,%zmm18,%zmm18
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+.byte	98,242,101,72,220,216
+.byte	98,242,93,72,220,224
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+.byte	98,242,101,72,221,216
+.byte	98,242,93,72,221,224
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+	vpxorq	%zmm11,%zmm3,%zmm3
+	vpxorq	%zmm12,%zmm4,%zmm4
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqa32	%zmm17,%zmm11
+	vmovdqa32	%zmm18,%zmm12
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi)
+	vmovdqu8	%zmm3,128(%rsi)
+	vmovdqu8	%zmm4,192(%rsi)
+	addq	$0x100,%rsi
+	subq	$0x100,%rdx
+	cmpq	$0x100,%rdx
+	jge	L$_main_loop_run_16_hEgxyDlCngwrfFe
+	cmpq	$0x80,%rdx
+	jge	L$_main_loop_run_8_hEgxyDlCngwrfFe
+	vextracti32x4	$0x3,%zmm4,%xmm0
+	jmp	L$_do_n_blocks_hEgxyDlCngwrfFe
+
+L$_start_by8_hEgxyDlCngwrfFe:
+	vbroadcasti32x4	(%rsp),%zmm0
+	vbroadcasti32x4	shufb_15_7(%rip),%zmm8
+	movq	$0xaa,%r8
+	kmovq	%r8,%k2
+	vpshufb	%zmm8,%zmm0,%zmm1
+	vpsllvq	const_dq3210(%rip),%zmm0,%zmm4
+	vpsrlvq	const_dq5678(%rip),%zmm1,%zmm2
+.byte	98,147,109,72,68,217,0
+	vpxorq	%zmm2,%zmm4,%zmm4{%k2}
+	vpxord	%zmm4,%zmm3,%zmm9
+	vpsllvq	const_dq7654(%rip),%zmm0,%zmm5
+	vpsrlvq	const_dq1234(%rip),%zmm1,%zmm6
+.byte	98,147,77,72,68,249,0
+	vpxorq	%zmm6,%zmm5,%zmm5{%k2}
+	vpxord	%zmm5,%zmm7,%zmm10
+
+L$_main_loop_run_8_hEgxyDlCngwrfFe:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2
+	addq	$0x80,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vpsrldq	$0xf,%zmm9,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm9,%zmm15
+	vpxord	%zmm14,%zmm15,%zmm15
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+	vpsrldq	$0xf,%zmm10,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm10,%zmm16
+	vpxord	%zmm14,%zmm16,%zmm16
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,220,200
+.byte	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,221,200
+.byte	98,242,109,72,221,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi)
+	addq	$0x80,%rsi
+	subq	$0x80,%rdx
+	cmpq	$0x80,%rdx
+	jge	L$_main_loop_run_8_hEgxyDlCngwrfFe
+	vextracti32x4	$0x3,%zmm2,%xmm0
+	jmp	L$_do_n_blocks_hEgxyDlCngwrfFe
+
+L$_steal_cipher_next_hEgxyDlCngwrfFe:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,(%rsp)
+	movq	%rbx,8(%rsp)
+	vmovdqa	(%rsp),%xmm0
+
+L$_steal_cipher_hEgxyDlCngwrfFe:
+	vmovdqa	%xmm8,%xmm2
+	leaq	vpshufb_shf_table(%rip),%rax
+	vmovdqu	(%rax,%rdx,1),%xmm10
+	vpshufb	%xmm10,%xmm8,%xmm8
+	vmovdqu	-16(%rdi,%rdx,1),%xmm3
+	vmovdqu	%xmm8,-16(%rsi,%rdx,1)
+	leaq	vpshufb_shf_table(%rip),%rax
+	addq	$16,%rax
+	subq	%rdx,%rax
+	vmovdqu	(%rax),%xmm10
+	vpxor	mask1(%rip),%xmm10,%xmm10
+	vpshufb	%xmm10,%xmm3,%xmm3
+	vpblendvb	%xmm10,%xmm2,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm3,%xmm8
+	vpxor	128(%rsp),%xmm8,%xmm8
+.byte	98,114,61,8,220,132,36,144,0,0,0
+.byte	98,114,61,8,220,132,36,160,0,0,0
+.byte	98,114,61,8,220,132,36,176,0,0,0
+.byte	98,114,61,8,220,132,36,192,0,0,0
+.byte	98,114,61,8,220,132,36,208,0,0,0
+.byte	98,114,61,8,220,132,36,224,0,0,0
+.byte	98,114,61,8,220,132,36,240,0,0,0
+.byte	98,114,61,8,220,132,36,0,1,0,0
+.byte	98,114,61,8,220,132,36,16,1,0,0
+.byte	98,114,61,8,220,132,36,32,1,0,0
+.byte	98,114,61,8,220,132,36,48,1,0,0
+.byte	98,114,61,8,220,132,36,64,1,0,0
+.byte	98,114,61,8,220,132,36,80,1,0,0
+.byte	98,114,61,8,221,132,36,96,1,0,0
+	vpxor	%xmm0,%xmm8,%xmm8
+	vmovdqu	%xmm8,-16(%rsi)
+
+L$_ret_hEgxyDlCngwrfFe:
+	movq	368(%rsp),%rbx
+	movq	%rbp,%rsp
+	popq	%rbp
+	.byte	0xf3,0xc3
+
+L$_less_than_128_bytes_hEgxyDlCngwrfFe:
+	cmpq	$0x10,%rdx
+	jb	L$_ret_hEgxyDlCngwrfFe
+	movq	%rdx,%r8
+	andq	$0x70,%r8
+	cmpq	$0x60,%r8
+	je	L$_num_blocks_is_6_hEgxyDlCngwrfFe
+	cmpq	$0x50,%r8
+	je	L$_num_blocks_is_5_hEgxyDlCngwrfFe
+	cmpq	$0x40,%r8
+	je	L$_num_blocks_is_4_hEgxyDlCngwrfFe
+	cmpq	$0x30,%r8
+	je	L$_num_blocks_is_3_hEgxyDlCngwrfFe
+	cmpq	$0x20,%r8
+	je	L$_num_blocks_is_2_hEgxyDlCngwrfFe
+	cmpq	$0x10,%r8
+	je	L$_num_blocks_is_1_hEgxyDlCngwrfFe
+
+L$_num_blocks_is_7_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,80(%rsp)
+	movq	%rbx,88(%rsp)
+	vmovdqa	80(%rsp),%xmm14
+	vmovdqu	80(%rdi),%xmm6
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,96(%rsp)
+	movq	%rbx,104(%rsp)
+	vmovdqa	96(%rsp),%xmm15
+	vmovdqu	96(%rdi),%xmm7
+	addq	$0x70,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vpxor	%xmm0,%xmm7,%xmm7
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+.byte	98,242,69,8,220,248
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+.byte	98,242,93,8,221,224
+.byte	98,242,85,8,221,232
+.byte	98,242,77,8,221,240
+.byte	98,242,69,8,221,248
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	vmovdqu	%xmm6,80(%rsi)
+	vmovdqu	%xmm7,96(%rsi)
+	addq	$0x70,%rsi
+	vmovdqa	%xmm7,%xmm8
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+L$_num_blocks_is_6_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,80(%rsp)
+	movq	%rbx,88(%rsp)
+	vmovdqa	80(%rsp),%xmm14
+	vmovdqu	80(%rdi),%xmm6
+	addq	$0x60,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+.byte	98,242,77,8,220,240
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+.byte	98,242,93,8,221,224
+.byte	98,242,85,8,221,232
+.byte	98,242,77,8,221,240
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	vmovdqu	%xmm6,80(%rsi)
+	addq	$0x60,%rsi
+	vmovdqa	%xmm6,%xmm8
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+L$_num_blocks_is_5_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	addq	$0x50,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+.byte	98,242,85,8,220,232
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+.byte	98,242,93,8,221,224
+.byte	98,242,85,8,221,232
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	addq	$0x50,%rsi
+	vmovdqa	%xmm5,%xmm8
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+L$_num_blocks_is_4_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	addq	$0x40,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+.byte	98,242,93,8,220,224
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+.byte	98,242,93,8,221,224
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	addq	$0x40,%rsi
+	vmovdqa	%xmm4,%xmm8
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+L$_num_blocks_is_3_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	addq	$0x30,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+.byte	98,242,101,8,220,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+.byte	98,242,101,8,221,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	addq	$0x30,%rsi
+	vmovdqa	%xmm3,%xmm8
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+L$_num_blocks_is_2_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	addq	$0x20,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+.byte	98,242,109,8,220,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+.byte	98,242,109,8,221,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	addq	$0x20,%rsi
+	vmovdqa	%xmm2,%xmm8
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+L$_num_blocks_is_1_hEgxyDlCngwrfFe:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	addq	$0x10,%rdi
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,220,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,221,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqu	%xmm1,(%rsi)
+	addq	$0x10,%rsi
+	vmovdqa	%xmm1,%xmm8
+	andq	$0xf,%rdx
+	je	L$_ret_hEgxyDlCngwrfFe
+	jmp	L$_steal_cipher_next_hEgxyDlCngwrfFe
+	.byte	0xf3,0xc3
+
+.globl	_aes_hw_xts_decrypt_avx512
+.private_extern _aes_hw_xts_decrypt_avx512
+.private_extern	_aes_hw_xts_decrypt_avx512
+
+.p2align	5
+_aes_hw_xts_decrypt_avx512:
+
+.byte	243,15,30,250
+	pushq	%rbp
+	movq	%rsp,%rbp
+	subq	$376,%rsp
+	andq	$0xffffffffffffffc0,%rsp
+	movq	%rbx,368(%rsp)
+	movq	$0x87,%r10
+	vmovdqu	(%r9),%xmm1
+	vpxor	%xmm4,%xmm4,%xmm4
+	vmovdqu	(%r8),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+
+	vmovdqu	224(%rcx),%xmm2
+	vmovdqa	%xmm2,352(%rsp)
+
+	vmovdqu	16(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	208(%rcx),%xmm2
+	vmovdqa	%xmm2,336(%rsp)
+
+	vmovdqu	32(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	192(%rcx),%xmm2
+	vmovdqa	%xmm2,320(%rsp)
+
+	vmovdqu	48(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	176(%rcx),%xmm2
+	vmovdqa	%xmm2,304(%rsp)
+
+	vmovdqu	64(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	160(%rcx),%xmm2
+	vmovdqa	%xmm2,288(%rsp)
+
+	vmovdqu	80(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	144(%rcx),%xmm2
+	vmovdqa	%xmm2,272(%rsp)
+
+	vmovdqu	96(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	128(%rcx),%xmm2
+	vmovdqa	%xmm2,256(%rsp)
+
+	vmovdqu	112(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	112(%rcx),%xmm2
+	vmovdqa	%xmm2,240(%rsp)
+
+	vmovdqu	128(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	96(%rcx),%xmm2
+	vmovdqa	%xmm2,224(%rsp)
+
+	vmovdqu	144(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	80(%rcx),%xmm2
+	vmovdqa	%xmm2,208(%rsp)
+
+	vmovdqu	160(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	64(%rcx),%xmm2
+	vmovdqa	%xmm2,192(%rsp)
+
+	vmovdqu	176(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	48(%rcx),%xmm2
+	vmovdqa	%xmm2,176(%rsp)
+
+	vmovdqu	192(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	32(%rcx),%xmm2
+	vmovdqa	%xmm2,160(%rsp)
+
+	vmovdqu	208(%r8),%xmm0
+.byte	98,242,117,8,220,200
+
+	vmovdqu	16(%rcx),%xmm2
+	vmovdqa	%xmm2,144(%rsp)
+
+	vmovdqu	224(%r8),%xmm0
+.byte	98,242,117,8,221,200
+
+	vmovdqu	(%rcx),%xmm2
+	vmovdqa	%xmm2,128(%rsp)
+
+	vmovdqa	%xmm1,(%rsp)
+
+	cmpq	$0x80,%rdx
+	jb	L$_less_than_128_bytes_amivrujEyduiFoi
+	vpbroadcastq	%r10,%zmm25
+	cmpq	$0x100,%rdx
+	jge	L$_start_by16_amivrujEyduiFoi
+	jmp	L$_start_by8_amivrujEyduiFoi
+
+L$_do_n_blocks_amivrujEyduiFoi:
+	cmpq	$0x0,%rdx
+	je	L$_ret_amivrujEyduiFoi
+	cmpq	$0x70,%rdx
+	jge	L$_remaining_num_blocks_is_7_amivrujEyduiFoi
+	cmpq	$0x60,%rdx
+	jge	L$_remaining_num_blocks_is_6_amivrujEyduiFoi
+	cmpq	$0x50,%rdx
+	jge	L$_remaining_num_blocks_is_5_amivrujEyduiFoi
+	cmpq	$0x40,%rdx
+	jge	L$_remaining_num_blocks_is_4_amivrujEyduiFoi
+	cmpq	$0x30,%rdx
+	jge	L$_remaining_num_blocks_is_3_amivrujEyduiFoi
+	cmpq	$0x20,%rdx
+	jge	L$_remaining_num_blocks_is_2_amivrujEyduiFoi
+	cmpq	$0x10,%rdx
+	jge	L$_remaining_num_blocks_is_1_amivrujEyduiFoi
+
+
+	vmovdqu	%xmm5,%xmm1
+
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqu	%xmm1,-16(%rsi)
+	vmovdqa	%xmm1,%xmm8
+
+
+	movq	$0x1,%r8
+	kmovq	%r8,%k1
+	vpsllq	$0x3f,%xmm9,%xmm13
+	vpsraq	$0x3f,%xmm13,%xmm14
+	vpandq	%xmm25,%xmm14,%xmm5
+	vpxorq	%xmm5,%xmm9,%xmm9{%k1}
+	vpsrldq	$0x8,%xmm9,%xmm10
+.byte	98, 211, 181, 8, 115, 194, 1
+	vpslldq	$0x8,%xmm13,%xmm13
+	vpxorq	%xmm13,%xmm0,%xmm0
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_remaining_num_blocks_is_7_amivrujEyduiFoi:
+	movq	$0xffffffffffffffff,%r8
+	shrq	$0x10,%r8
+	kmovq	%r8,%k1
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2{%k1}
+	addq	$0x70,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_7_remain_amivrujEyduiFoi
+	vextracti32x4	$0x2,%zmm10,%xmm12
+	vextracti32x4	$0x3,%zmm10,%xmm13
+	vinserti32x4	$0x2,%xmm13,%zmm10,%zmm10
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi){%k1}
+	addq	$0x70,%rsi
+	vextracti32x4	$0x2,%zmm2,%xmm8
+	vmovdqa	%xmm12,%xmm0
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_7_remain_amivrujEyduiFoi:
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi){%k1}
+	jmp	L$_ret_amivrujEyduiFoi
+
+L$_remaining_num_blocks_is_6_amivrujEyduiFoi:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%ymm2
+	addq	$0x60,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_6_remain_amivrujEyduiFoi
+	vextracti32x4	$0x1,%zmm10,%xmm12
+	vextracti32x4	$0x2,%zmm10,%xmm13
+	vinserti32x4	$0x1,%xmm13,%zmm10,%zmm10
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%ymm2,64(%rsi)
+	addq	$0x60,%rsi
+	vextracti32x4	$0x1,%zmm2,%xmm8
+	vmovdqa	%xmm12,%xmm0
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_6_remain_amivrujEyduiFoi:
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%ymm2,64(%rsi)
+	jmp	L$_ret_amivrujEyduiFoi
+
+L$_remaining_num_blocks_is_5_amivrujEyduiFoi:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu	64(%rdi),%xmm2
+	addq	$0x50,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_5_remain_amivrujEyduiFoi
+	vmovdqa	%xmm10,%xmm12
+	vextracti32x4	$0x1,%zmm10,%xmm10
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu	%xmm2,64(%rsi)
+	addq	$0x50,%rsi
+	vmovdqa	%xmm2,%xmm8
+	vmovdqa	%xmm12,%xmm0
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_5_remain_amivrujEyduiFoi:
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%xmm2,64(%rsi)
+	jmp	L$_ret_amivrujEyduiFoi
+
+L$_remaining_num_blocks_is_4_amivrujEyduiFoi:
+	vmovdqu8	(%rdi),%zmm1
+	addq	$0x40,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_4_remain_amivrujEyduiFoi
+	vextracti32x4	$0x3,%zmm9,%xmm12
+	vinserti32x4	$0x3,%xmm10,%zmm9,%zmm9
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	addq	$0x40,%rsi
+	vextracti32x4	$0x3,%zmm1,%xmm8
+	vmovdqa	%xmm12,%xmm0
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_4_remain_amivrujEyduiFoi:
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	jmp	L$_ret_amivrujEyduiFoi
+
+L$_remaining_num_blocks_is_3_amivrujEyduiFoi:
+	vmovdqu	(%rdi),%xmm1
+	vmovdqu	16(%rdi),%xmm2
+	vmovdqu	32(%rdi),%xmm3
+	addq	$0x30,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_3_remain_amivrujEyduiFoi
+	vextracti32x4	$0x2,%zmm9,%xmm13
+	vextracti32x4	$0x1,%zmm9,%xmm10
+	vextracti32x4	$0x3,%zmm9,%xmm11
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	addq	$0x30,%rsi
+	vmovdqa	%xmm3,%xmm8
+	vmovdqa	%xmm13,%xmm0
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_3_remain_amivrujEyduiFoi:
+	vextracti32x4	$0x1,%zmm9,%xmm10
+	vextracti32x4	$0x2,%zmm9,%xmm11
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	jmp	L$_ret_amivrujEyduiFoi
+
+L$_remaining_num_blocks_is_2_amivrujEyduiFoi:
+	vmovdqu	(%rdi),%xmm1
+	vmovdqu	16(%rdi),%xmm2
+	addq	$0x20,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_2_remain_amivrujEyduiFoi
+	vextracti32x4	$0x2,%zmm9,%xmm10
+	vextracti32x4	$0x1,%zmm9,%xmm12
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	addq	$0x20,%rsi
+	vmovdqa	%xmm2,%xmm8
+	vmovdqa	%xmm12,%xmm0
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_2_remain_amivrujEyduiFoi:
+	vextracti32x4	$0x1,%zmm9,%xmm10
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	jmp	L$_ret_amivrujEyduiFoi
+
+L$_remaining_num_blocks_is_1_amivrujEyduiFoi:
+	vmovdqu	(%rdi),%xmm1
+	addq	$0x10,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_1_remain_amivrujEyduiFoi
+	vextracti32x4	$0x1,%zmm9,%xmm11
+	vpxor	%xmm11,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+	vpxor	%xmm11,%xmm1,%xmm1
+	vmovdqu	%xmm1,(%rsi)
+	addq	$0x10,%rsi
+	vmovdqa	%xmm1,%xmm8
+	vmovdqa	%xmm9,%xmm0
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_1_remain_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqu	%xmm1,(%rsi)
+	jmp	L$_ret_amivrujEyduiFoi
+
+L$_start_by16_amivrujEyduiFoi:
+	vbroadcasti32x4	(%rsp),%zmm0
+	vbroadcasti32x4	shufb_15_7(%rip),%zmm8
+	movq	$0xaa,%r8
+	kmovq	%r8,%k2
+
+
+	vpshufb	%zmm8,%zmm0,%zmm1
+	vpsllvq	const_dq3210(%rip),%zmm0,%zmm4
+	vpsrlvq	const_dq5678(%rip),%zmm1,%zmm2
+.byte	98,147,109,72,68,217,0
+	vpxorq	%zmm2,%zmm4,%zmm4{%k2}
+	vpxord	%zmm4,%zmm3,%zmm9
+
+
+	vpsllvq	const_dq7654(%rip),%zmm0,%zmm5
+	vpsrlvq	const_dq1234(%rip),%zmm1,%zmm6
+.byte	98,147,77,72,68,249,0
+	vpxorq	%zmm6,%zmm5,%zmm5{%k2}
+	vpxord	%zmm5,%zmm7,%zmm10
+
+
+	vpsrldq	$0xf,%zmm9,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm9,%zmm11
+	vpxord	%zmm14,%zmm11,%zmm11
+
+	vpsrldq	$0xf,%zmm10,%zmm15
+.byte	98,131,5,72,68,193,0
+	vpslldq	$0x1,%zmm10,%zmm12
+	vpxord	%zmm16,%zmm12,%zmm12
+
+L$_main_loop_run_16_amivrujEyduiFoi:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2
+	vmovdqu8	128(%rdi),%zmm3
+	vmovdqu8	192(%rdi),%zmm4
+	vmovdqu8	240(%rdi),%zmm5
+	addq	$0x100,%rdi
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+	vpxorq	%zmm11,%zmm3,%zmm3
+	vpxorq	%zmm12,%zmm4,%zmm4
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vpxorq	%zmm0,%zmm3,%zmm3
+	vpxorq	%zmm0,%zmm4,%zmm4
+	vpsrldq	$0xf,%zmm11,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm11,%zmm15
+	vpxord	%zmm14,%zmm15,%zmm15
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vpsrldq	$0xf,%zmm12,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm12,%zmm16
+	vpxord	%zmm14,%zmm16,%zmm16
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vpsrldq	$0xf,%zmm15,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm15,%zmm17
+	vpxord	%zmm14,%zmm17,%zmm17
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vpsrldq	$0xf,%zmm16,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm16,%zmm18
+	vpxord	%zmm14,%zmm18,%zmm18
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+.byte	98,242,101,72,222,216
+.byte	98,242,93,72,222,224
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+.byte	98,242,101,72,223,216
+.byte	98,242,93,72,223,224
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+	vpxorq	%zmm11,%zmm3,%zmm3
+	vpxorq	%zmm12,%zmm4,%zmm4
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqa32	%zmm17,%zmm11
+	vmovdqa32	%zmm18,%zmm12
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi)
+	vmovdqu8	%zmm3,128(%rsi)
+	vmovdqu8	%zmm4,192(%rsi)
+	addq	$0x100,%rsi
+	subq	$0x100,%rdx
+	cmpq	$0x100,%rdx
+	jge	L$_main_loop_run_16_amivrujEyduiFoi
+
+	cmpq	$0x80,%rdx
+	jge	L$_main_loop_run_8_amivrujEyduiFoi
+	jmp	L$_do_n_blocks_amivrujEyduiFoi
+
+L$_start_by8_amivrujEyduiFoi:
+
+	vbroadcasti32x4	(%rsp),%zmm0
+	vbroadcasti32x4	shufb_15_7(%rip),%zmm8
+	movq	$0xaa,%r8
+	kmovq	%r8,%k2
+
+
+	vpshufb	%zmm8,%zmm0,%zmm1
+	vpsllvq	const_dq3210(%rip),%zmm0,%zmm4
+	vpsrlvq	const_dq5678(%rip),%zmm1,%zmm2
+.byte	98,147,109,72,68,217,0
+	vpxorq	%zmm2,%zmm4,%zmm4{%k2}
+	vpxord	%zmm4,%zmm3,%zmm9
+
+
+	vpsllvq	const_dq7654(%rip),%zmm0,%zmm5
+	vpsrlvq	const_dq1234(%rip),%zmm1,%zmm6
+.byte	98,147,77,72,68,249,0
+	vpxorq	%zmm6,%zmm5,%zmm5{%k2}
+	vpxord	%zmm5,%zmm7,%zmm10
+
+L$_main_loop_run_8_amivrujEyduiFoi:
+	vmovdqu8	(%rdi),%zmm1
+	vmovdqu8	64(%rdi),%zmm2
+	vmovdqu8	112(%rdi),%xmm5
+	addq	$0x80,%rdi
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vbroadcasti32x4	128(%rsp),%zmm0
+	vpxorq	%zmm0,%zmm1,%zmm1
+	vpxorq	%zmm0,%zmm2,%zmm2
+	vpsrldq	$0xf,%zmm9,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm9,%zmm15
+	vpxord	%zmm14,%zmm15,%zmm15
+	vbroadcasti32x4	144(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	160(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	176(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+	vpsrldq	$0xf,%zmm10,%zmm13
+.byte	98,19,21,72,68,241,0
+	vpslldq	$0x1,%zmm10,%zmm16
+	vpxord	%zmm14,%zmm16,%zmm16
+
+	vbroadcasti32x4	192(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	208(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	224(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	240(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	256(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	272(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	288(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	304(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	320(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	336(%rsp),%zmm0
+.byte	98,242,117,72,222,200
+.byte	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	352(%rsp),%zmm0
+.byte	98,242,117,72,223,200
+.byte	98,242,109,72,223,208
+
+
+	vpxorq	%zmm9,%zmm1,%zmm1
+	vpxorq	%zmm10,%zmm2,%zmm2
+
+
+	vmovdqa32	%zmm15,%zmm9
+	vmovdqa32	%zmm16,%zmm10
+	vmovdqu8	%zmm1,(%rsi)
+	vmovdqu8	%zmm2,64(%rsi)
+	addq	$0x80,%rsi
+	subq	$0x80,%rdx
+	cmpq	$0x80,%rdx
+	jge	L$_main_loop_run_8_amivrujEyduiFoi
+	jmp	L$_do_n_blocks_amivrujEyduiFoi
+
+L$_steal_cipher_amivrujEyduiFoi:
+
+	vmovdqa	%xmm8,%xmm2
+
+
+	leaq	vpshufb_shf_table(%rip),%rax
+	vmovdqu	(%rax,%rdx,1),%xmm10
+	vpshufb	%xmm10,%xmm8,%xmm8
+
+
+	vmovdqu	-16(%rdi,%rdx,1),%xmm3
+	vmovdqu	%xmm8,-16(%rsi,%rdx,1)
+
+
+	leaq	vpshufb_shf_table(%rip),%rax
+	addq	$16,%rax
+	subq	%rdx,%rax
+	vmovdqu	(%rax),%xmm10
+	vpxor	mask1(%rip),%xmm10,%xmm10
+	vpshufb	%xmm10,%xmm3,%xmm3
+
+	vpblendvb	%xmm10,%xmm2,%xmm3,%xmm3
+
+
+	vpxor	%xmm0,%xmm3,%xmm8
+
+
+	vpxor	128(%rsp),%xmm8,%xmm8
+.byte	98,114,61,8,222,132,36,144,0,0,0
+.byte	98,114,61,8,222,132,36,160,0,0,0
+.byte	98,114,61,8,222,132,36,176,0,0,0
+.byte	98,114,61,8,222,132,36,192,0,0,0
+.byte	98,114,61,8,222,132,36,208,0,0,0
+.byte	98,114,61,8,222,132,36,224,0,0,0
+.byte	98,114,61,8,222,132,36,240,0,0,0
+.byte	98,114,61,8,222,132,36,0,1,0,0
+.byte	98,114,61,8,222,132,36,16,1,0,0
+.byte	98,114,61,8,222,132,36,32,1,0,0
+.byte	98,114,61,8,222,132,36,48,1,0,0
+.byte	98,114,61,8,222,132,36,64,1,0,0
+.byte	98,114,61,8,222,132,36,80,1,0,0
+.byte	98,114,61,8,223,132,36,96,1,0,0
+
+
+	vpxor	%xmm0,%xmm8,%xmm8
+
+L$_done_amivrujEyduiFoi:
+
+	vmovdqu	%xmm8,-16(%rsi)
+
+L$_ret_amivrujEyduiFoi:
+	movq	368(%rsp),%rbx
+	movq	%rbp,%rsp
+	popq	%rbp
+	.byte	0xf3,0xc3
+
+L$_less_than_128_bytes_amivrujEyduiFoi:
+	cmpq	$0x10,%rdx
+	jb	L$_ret_amivrujEyduiFoi
+
+	movq	%rdx,%r8
+	andq	$0x70,%r8
+	cmpq	$0x60,%r8
+	je	L$_num_blocks_is_6_amivrujEyduiFoi
+	cmpq	$0x50,%r8
+	je	L$_num_blocks_is_5_amivrujEyduiFoi
+	cmpq	$0x40,%r8
+	je	L$_num_blocks_is_4_amivrujEyduiFoi
+	cmpq	$0x30,%r8
+	je	L$_num_blocks_is_3_amivrujEyduiFoi
+	cmpq	$0x20,%r8
+	je	L$_num_blocks_is_2_amivrujEyduiFoi
+	cmpq	$0x10,%r8
+	je	L$_num_blocks_is_1_amivrujEyduiFoi
+
+L$_num_blocks_is_7_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,80(%rsp)
+	movq	%rbx,88(%rsp)
+	vmovdqa	80(%rsp),%xmm14
+	vmovdqu	80(%rdi),%xmm6
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,96(%rsp)
+	movq	%rbx,104(%rsp)
+	vmovdqa	96(%rsp),%xmm15
+	vmovdqu	96(%rdi),%xmm7
+	addq	$0x70,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_7_amivrujEyduiFoi
+
+L$_steal_cipher_7_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm15,%xmm16
+	vmovdqa	16(%rsp),%xmm15
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vpxor	%xmm0,%xmm7,%xmm7
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+.byte	98,242,77,8,223,240
+.byte	98,242,69,8,223,248
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	vmovdqu	%xmm6,80(%rsi)
+	addq	$0x70,%rsi
+	vmovdqa64	%xmm16,%xmm0
+	vmovdqa	%xmm7,%xmm8
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_7_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vpxor	%xmm0,%xmm7,%xmm7
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+.byte	98,242,69,8,222,248
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+.byte	98,242,77,8,223,240
+.byte	98,242,69,8,223,248
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vpxor	%xmm15,%xmm7,%xmm7
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	vmovdqu	%xmm6,80(%rsi)
+	addq	$0x70,%rsi
+	vmovdqa	%xmm7,%xmm8
+	jmp	L$_done_amivrujEyduiFoi
+
+L$_num_blocks_is_6_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,80(%rsp)
+	movq	%rbx,88(%rsp)
+	vmovdqa	80(%rsp),%xmm14
+	vmovdqu	80(%rdi),%xmm6
+	addq	$0x60,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_6_amivrujEyduiFoi
+
+L$_steal_cipher_6_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm14,%xmm15
+	vmovdqa	16(%rsp),%xmm14
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+.byte	98,242,77,8,223,240
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	addq	$0x60,%rsi
+	vmovdqa	%xmm15,%xmm0
+	vmovdqa	%xmm6,%xmm8
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_6_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vpxor	%xmm0,%xmm6,%xmm6
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+.byte	98,242,77,8,222,240
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+.byte	98,242,77,8,223,240
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vpxor	%xmm14,%xmm6,%xmm6
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	vmovdqu	%xmm5,64(%rsi)
+	addq	$0x60,%rsi
+	vmovdqa	%xmm6,%xmm8
+	jmp	L$_done_amivrujEyduiFoi
+
+L$_num_blocks_is_5_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,64(%rsp)
+	movq	%rbx,72(%rsp)
+	vmovdqa	64(%rsp),%xmm13
+	vmovdqu	64(%rdi),%xmm5
+	addq	$0x50,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_5_amivrujEyduiFoi
+
+L$_steal_cipher_5_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm13,%xmm14
+	vmovdqa	16(%rsp),%xmm13
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	addq	$0x50,%rsi
+	vmovdqa	%xmm14,%xmm0
+	vmovdqa	%xmm5,%xmm8
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_5_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vpxor	%xmm0,%xmm5,%xmm5
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+.byte	98,242,85,8,222,232
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+.byte	98,242,85,8,223,232
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vpxor	%xmm13,%xmm5,%xmm5
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	vmovdqu	%xmm4,48(%rsi)
+	addq	$0x50,%rsi
+	vmovdqa	%xmm5,%xmm8
+	jmp	L$_done_amivrujEyduiFoi
+
+L$_num_blocks_is_4_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,48(%rsp)
+	movq	%rbx,56(%rsp)
+	vmovdqa	48(%rsp),%xmm12
+	vmovdqu	48(%rdi),%xmm4
+	addq	$0x40,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_4_amivrujEyduiFoi
+
+L$_steal_cipher_4_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm12,%xmm13
+	vmovdqa	16(%rsp),%xmm12
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	addq	$0x40,%rsi
+	vmovdqa	%xmm13,%xmm0
+	vmovdqa	%xmm4,%xmm8
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_4_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vpxor	%xmm0,%xmm4,%xmm4
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+.byte	98,242,93,8,222,224
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+.byte	98,242,93,8,223,224
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vpxor	%xmm12,%xmm4,%xmm4
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	vmovdqu	%xmm3,32(%rsi)
+	addq	$0x40,%rsi
+	vmovdqa	%xmm4,%xmm8
+	jmp	L$_done_amivrujEyduiFoi
+
+L$_num_blocks_is_3_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,32(%rsp)
+	movq	%rbx,40(%rsp)
+	vmovdqa	32(%rsp),%xmm11
+	vmovdqu	32(%rdi),%xmm3
+	addq	$0x30,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_3_amivrujEyduiFoi
+
+L$_steal_cipher_3_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm11,%xmm12
+	vmovdqa	16(%rsp),%xmm11
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	addq	$0x30,%rsi
+	vmovdqa	%xmm12,%xmm0
+	vmovdqa	%xmm3,%xmm8
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_3_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vpxor	%xmm0,%xmm3,%xmm3
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+.byte	98,242,101,8,222,216
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+.byte	98,242,101,8,223,216
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vpxor	%xmm11,%xmm3,%xmm3
+	vmovdqu	%xmm1,(%rsi)
+	vmovdqu	%xmm2,16(%rsi)
+	addq	$0x30,%rsi
+	vmovdqa	%xmm3,%xmm8
+	jmp	L$_done_amivrujEyduiFoi
+
+L$_num_blocks_is_2_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa	16(%rsp),%xmm10
+	vmovdqu	16(%rdi),%xmm2
+	addq	$0x20,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_2_amivrujEyduiFoi
+
+L$_steal_cipher_2_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm10,%xmm11
+	vmovdqa	16(%rsp),%xmm10
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	addq	$0x20,%rsi
+	vmovdqa	%xmm11,%xmm0
+	vmovdqa	%xmm2,%xmm8
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_2_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vpxor	%xmm0,%xmm2,%xmm2
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+.byte	98,242,109,8,222,208
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+.byte	98,242,109,8,223,208
+	vpxor	%xmm9,%xmm1,%xmm1
+	vpxor	%xmm10,%xmm2,%xmm2
+	vmovdqu	%xmm1,(%rsi)
+	addq	$0x20,%rsi
+	vmovdqa	%xmm2,%xmm8
+	jmp	L$_done_amivrujEyduiFoi
+
+L$_num_blocks_is_1_amivrujEyduiFoi:
+	vmovdqa	0(%rsp),%xmm9
+	movq	0(%rsp),%rax
+	movq	8(%rsp),%rbx
+	vmovdqu	0(%rdi),%xmm1
+	addq	$0x10,%rdi
+	andq	$0xf,%rdx
+	je	L$_done_1_amivrujEyduiFoi
+
+L$_steal_cipher_1_amivrujEyduiFoi:
+	xorq	%r11,%r11
+	shlq	$1,%rax
+	adcq	%rbx,%rbx
+	cmovcq	%r10,%r11
+	xorq	%r11,%rax
+	movq	%rax,16(%rsp)
+	movq	%rbx,24(%rsp)
+	vmovdqa64	%xmm9,%xmm10
+	vmovdqa	16(%rsp),%xmm9
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	addq	$0x10,%rsi
+	vmovdqa	%xmm10,%xmm0
+	vmovdqa	%xmm1,%xmm8
+	jmp	L$_steal_cipher_amivrujEyduiFoi
+
+L$_done_1_amivrujEyduiFoi:
+	vpxor	%xmm9,%xmm1,%xmm1
+	vmovdqa	128(%rsp),%xmm0
+	vpxor	%xmm0,%xmm1,%xmm1
+	vmovdqa	144(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	160(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	176(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	192(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	208(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	224(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	240(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	256(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	272(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	288(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	304(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	320(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	336(%rsp),%xmm0
+.byte	98,242,117,8,222,200
+	vmovdqa	352(%rsp),%xmm0
+.byte	98,242,117,8,223,200
+	vpxor	%xmm9,%xmm1,%xmm1
+	addq	$0x10,%rsi
+	vmovdqa	%xmm1,%xmm8
+	jmp	L$_done_amivrujEyduiFoi
+	.byte	0xf3,0xc3
+
+.section	__DATA,__const
+.p2align	4
+
+vpshufb_shf_table:
+.quad	0x8786858483828100, 0x8f8e8d8c8b8a8988
+.quad	0x0706050403020100, 0x000e0d0c0b0a0908
+
+mask1:
+.quad	0x8080808080808080, 0x8080808080808080
+
+const_dq3210:
+.quad	0, 0, 1, 1, 2, 2, 3, 3
+const_dq5678:
+.quad	8, 8, 7, 7, 6, 6, 5, 5
+const_dq7654:
+.quad	4, 4, 5, 5, 6, 6, 7, 7
+const_dq1234:
+.quad	4, 4, 3, 3, 2, 2, 1, 1
+
+shufb_15_7:
+.byte	15, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 7, 0xff, 0xff
+.byte	0xff, 0xff, 0xff, 0xff, 0xff
+
+.text	
+#endif
+#if defined(__ELF__)
+// See https://www.airs.com/blog/archives/518.
+.section .note.GNU-stack,"",%progbits
+#endif
diff --git a/generated-src/win-x86_64/crypto/fipsmodule/aesni-xts-avx512.asm b/generated-src/win-x86_64/crypto/fipsmodule/aesni-xts-avx512.asm
new file mode 100644
index 000000000..887fd187c
--- /dev/null
+++ b/generated-src/win-x86_64/crypto/fipsmodule/aesni-xts-avx512.asm
@@ -0,0 +1,5261 @@
+; This file is generated from a similarly-named Perl script in the BoringSSL
+; source tree. Do not edit by hand.
+
+%ifidn __OUTPUT_FORMAT__, win64
+default	rel
+%define XMMWORD
+%define YMMWORD
+%define ZMMWORD
+
+%ifdef BORINGSSL_PREFIX
+%include "boringssl_prefix_symbols_nasm.inc"
+%endif
+section	.text code align=64
+
+global	aes_hw_xts_encrypt_avx512
+
+
+ALIGN	32
+aes_hw_xts_encrypt_avx512:
+
+DB	243,15,30,250
+	push	rbp
+	mov	rbp,rsp
+	sub	rsp,552
+	and	rsp,0xffffffffffffffc0
+	mov	QWORD[528+rsp],rbx
+	mov	QWORD[((528 + 8))+rsp],rdi
+	mov	QWORD[((528 + 16))+rsp],rsi
+	vmovdqa	XMMWORD[(368 + 0)+rsp],xmm6
+	vmovdqa	XMMWORD[(368 + 16)+rsp],xmm7
+	vmovdqa	XMMWORD[(368 + 32)+rsp],xmm8
+	vmovdqa	XMMWORD[(368 + 48)+rsp],xmm9
+	vmovdqa	XMMWORD[(368 + 64)+rsp],xmm10
+	vmovdqa	XMMWORD[(368 + 80)+rsp],xmm11
+	vmovdqa	XMMWORD[(368 + 96)+rsp],xmm12
+	vmovdqa	XMMWORD[(368 + 112)+rsp],xmm13
+	vmovdqa	XMMWORD[(368 + 128)+rsp],xmm14
+	vmovdqa	XMMWORD[(368 + 144)+rsp],xmm15
+	mov	rdi,0x87
+	vmovdqu	xmm1,XMMWORD[r11]
+	vpxor	xmm4,xmm4,xmm4
+	vmovdqu	xmm0,XMMWORD[r10]
+	vpxor	xmm1,xmm1,xmm0
+
+	vmovdqu	xmm2,XMMWORD[r9]
+	vmovdqa	XMMWORD[128+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[16+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[16+r9]
+	vmovdqa	XMMWORD[144+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[32+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[32+r9]
+	vmovdqa	XMMWORD[160+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[48+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[48+r9]
+	vmovdqa	XMMWORD[176+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[64+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[64+r9]
+	vmovdqa	XMMWORD[192+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[80+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[80+r9]
+	vmovdqa	XMMWORD[208+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[96+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[96+r9]
+	vmovdqa	XMMWORD[224+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[112+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[112+r9]
+	vmovdqa	XMMWORD[240+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[128+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[128+r9]
+	vmovdqa	XMMWORD[256+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[144+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[144+r9]
+	vmovdqa	XMMWORD[272+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[160+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[160+r9]
+	vmovdqa	XMMWORD[288+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[176+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[176+r9]
+	vmovdqa	XMMWORD[304+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[192+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[192+r9]
+	vmovdqa	XMMWORD[320+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[208+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[208+r9]
+	vmovdqa	XMMWORD[336+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[224+r10]
+	DB	98,242,117,8,221,200
+
+	vmovdqu	xmm2,XMMWORD[224+r9]
+	vmovdqa	XMMWORD[352+rsp],xmm2
+
+	vmovdqa	XMMWORD[rsp],xmm1
+	mov	QWORD[((8 + 40))+rbp],rcx
+	mov	QWORD[((8 + 48))+rbp],rdx
+
+	cmp	r8,0x80
+	jl	NEAR $L$_less_than_128_bytes_hEgxyDlCngwrfFe
+	vpbroadcastq	zmm25,rdi
+	cmp	r8,0x100
+	jge	NEAR $L$_start_by16_hEgxyDlCngwrfFe
+	cmp	r8,0x80
+	jge	NEAR $L$_start_by8_hEgxyDlCngwrfFe
+
+$L$_do_n_blocks_hEgxyDlCngwrfFe:
+	cmp	r8,0x0
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	cmp	r8,0x70
+	jge	NEAR $L$_remaining_num_blocks_is_7_hEgxyDlCngwrfFe
+	cmp	r8,0x60
+	jge	NEAR $L$_remaining_num_blocks_is_6_hEgxyDlCngwrfFe
+	cmp	r8,0x50
+	jge	NEAR $L$_remaining_num_blocks_is_5_hEgxyDlCngwrfFe
+	cmp	r8,0x40
+	jge	NEAR $L$_remaining_num_blocks_is_4_hEgxyDlCngwrfFe
+	cmp	r8,0x30
+	jge	NEAR $L$_remaining_num_blocks_is_3_hEgxyDlCngwrfFe
+	cmp	r8,0x20
+	jge	NEAR $L$_remaining_num_blocks_is_2_hEgxyDlCngwrfFe
+	cmp	r8,0x10
+	jge	NEAR $L$_remaining_num_blocks_is_1_hEgxyDlCngwrfFe
+	vmovdqa	xmm8,xmm0
+	vmovdqa	xmm0,xmm9
+	jmp	NEAR $L$_steal_cipher_hEgxyDlCngwrfFe
+
+$L$_remaining_num_blocks_is_7_hEgxyDlCngwrfFe:
+	mov	r10,0xffffffffffffffff
+	shr	r10,0x10
+	kmovq	k1,r10
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	vmovdqu8	zmm2{k1},[64+rcx]
+	add	rcx,0x70
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,221,200
+	DB	98,242,109,72,221,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu8	ZMMWORD[64+rdx]{k1},zmm2
+	add	rdx,0x70
+	vextracti32x4	xmm8,zmm2,0x2
+	vextracti32x4	xmm0,zmm10,0x3
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_hEgxyDlCngwrfFe
+
+$L$_remaining_num_blocks_is_6_hEgxyDlCngwrfFe:
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	vmovdqu8	ymm2,YMMWORD[64+rcx]
+	add	rcx,0x60
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,221,200
+	DB	98,242,109,72,221,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu8	YMMWORD[64+rdx],ymm2
+	add	rdx,0x60
+	vextracti32x4	xmm8,zmm2,0x1
+	vextracti32x4	xmm0,zmm10,0x2
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_hEgxyDlCngwrfFe
+
+$L$_remaining_num_blocks_is_5_hEgxyDlCngwrfFe:
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	vmovdqu	xmm2,XMMWORD[64+rcx]
+	add	rcx,0x50
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,221,200
+	DB	98,242,109,72,221,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu	XMMWORD[64+rdx],xmm2
+	add	rdx,0x50
+	movdqa	xmm8,xmm2
+	vextracti32x4	xmm0,zmm10,0x1
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_hEgxyDlCngwrfFe
+
+$L$_remaining_num_blocks_is_4_hEgxyDlCngwrfFe:
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	add	rcx,0x40
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,221,200
+	DB	98,242,109,72,221,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	add	rdx,0x40
+	vextracti32x4	xmm8,zmm1,0x3
+	vextracti32x4	xmm0,zmm10,0x0
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_hEgxyDlCngwrfFe
+$L$_remaining_num_blocks_is_3_hEgxyDlCngwrfFe:
+	vextracti32x4	xmm10,zmm9,0x1
+	vextracti32x4	xmm11,zmm9,0x2
+	vmovdqu	xmm1,XMMWORD[rcx]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	add	rcx,0x30
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,221,200
+	DB	98,242,109,8,221,208
+	DB	98,242,101,8,221,216
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	add	rdx,0x30
+	vmovdqa	xmm8,xmm3
+	vextracti32x4	xmm0,zmm9,0x3
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_hEgxyDlCngwrfFe
+$L$_remaining_num_blocks_is_2_hEgxyDlCngwrfFe:
+	vextracti32x4	xmm10,zmm9,0x1
+	vmovdqu	xmm1,XMMWORD[rcx]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	add	rcx,0x20
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,221,200
+	DB	98,242,109,8,221,208
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	add	rdx,0x20
+	vmovdqa	xmm8,xmm2
+	vextracti32x4	xmm0,zmm9,0x2
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_hEgxyDlCngwrfFe
+$L$_remaining_num_blocks_is_1_hEgxyDlCngwrfFe:
+	vmovdqu	xmm1,XMMWORD[rcx]
+	add	rcx,0x10
+	vpxor	xmm1,xmm1,xmm9
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,221,200
+	vpxor	xmm1,xmm1,xmm9
+	vmovdqu	XMMWORD[rdx],xmm1
+	add	rdx,0x10
+	vmovdqa	xmm8,xmm1
+	vextracti32x4	xmm0,zmm9,0x1
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_hEgxyDlCngwrfFe
+
+$L$_start_by16_hEgxyDlCngwrfFe:
+	vbroadcasti32x4	zmm0,ZMMWORD[rsp]
+	vbroadcasti32x4	zmm8,ZMMWORD[shufb_15_7]
+	mov	r10,0xaa
+	kmovq	k2,r10
+	vpshufb	zmm1,zmm0,zmm8
+	vpsllvq	zmm4,zmm0,ZMMWORD[const_dq3210]
+	vpsrlvq	zmm2,zmm1,ZMMWORD[const_dq5678]
+	DB	98,147,109,72,68,217,0
+	vpxorq	zmm4{k2},zmm4,zmm2
+	vpxord	zmm9,zmm3,zmm4
+	vpsllvq	zmm5,zmm0,ZMMWORD[const_dq7654]
+	vpsrlvq	zmm6,zmm1,ZMMWORD[const_dq1234]
+	DB	98,147,77,72,68,249,0
+	vpxorq	zmm5{k2},zmm5,zmm6
+	vpxord	zmm10,zmm7,zmm5
+	vpsrldq	zmm13,zmm9,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm11,zmm9,0x1
+	vpxord	zmm11,zmm11,zmm14
+	vpsrldq	zmm15,zmm10,0xf
+	DB	98,131,5,72,68,193,0
+	vpslldq	zmm12,zmm10,0x1
+	vpxord	zmm12,zmm12,zmm16
+
+$L$_main_loop_run_16_hEgxyDlCngwrfFe:
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	vmovdqu8	zmm2,ZMMWORD[64+rcx]
+	vmovdqu8	zmm3,ZMMWORD[128+rcx]
+	vmovdqu8	zmm4,ZMMWORD[192+rcx]
+	add	rcx,0x100
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+	vpxorq	zmm3,zmm3,zmm11
+	vpxorq	zmm4,zmm4,zmm12
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vpxorq	zmm3,zmm3,zmm0
+	vpxorq	zmm4,zmm4,zmm0
+	vpsrldq	zmm13,zmm11,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm15,zmm11,0x1
+	vpxord	zmm15,zmm15,zmm14
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vpsrldq	zmm13,zmm12,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm16,zmm12,0x1
+	vpxord	zmm16,zmm16,zmm14
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vpsrldq	zmm13,zmm15,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm17,zmm15,0x1
+	vpxord	zmm17,zmm17,zmm14
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vpsrldq	zmm13,zmm16,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm18,zmm16,0x1
+	vpxord	zmm18,zmm18,zmm14
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	DB	98,242,101,72,220,216
+	DB	98,242,93,72,220,224
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,221,200
+	DB	98,242,109,72,221,208
+	DB	98,242,101,72,221,216
+	DB	98,242,93,72,221,224
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+	vpxorq	zmm3,zmm3,zmm11
+	vpxorq	zmm4,zmm4,zmm12
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqa32	zmm11,zmm17
+	vmovdqa32	zmm12,zmm18
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu8	ZMMWORD[64+rdx],zmm2
+	vmovdqu8	ZMMWORD[128+rdx],zmm3
+	vmovdqu8	ZMMWORD[192+rdx],zmm4
+	add	rdx,0x100
+	sub	r8,0x100
+	cmp	r8,0x100
+	jge	NEAR $L$_main_loop_run_16_hEgxyDlCngwrfFe
+	cmp	r8,0x80
+	jge	NEAR $L$_main_loop_run_8_hEgxyDlCngwrfFe
+	vextracti32x4	xmm0,zmm4,0x3
+	jmp	NEAR $L$_do_n_blocks_hEgxyDlCngwrfFe
+
+$L$_start_by8_hEgxyDlCngwrfFe:
+	vbroadcasti32x4	zmm0,ZMMWORD[rsp]
+	vbroadcasti32x4	zmm8,ZMMWORD[shufb_15_7]
+	mov	r10,0xaa
+	kmovq	k2,r10
+	vpshufb	zmm1,zmm0,zmm8
+	vpsllvq	zmm4,zmm0,ZMMWORD[const_dq3210]
+	vpsrlvq	zmm2,zmm1,ZMMWORD[const_dq5678]
+	DB	98,147,109,72,68,217,0
+	vpxorq	zmm4{k2},zmm4,zmm2
+	vpxord	zmm9,zmm3,zmm4
+	vpsllvq	zmm5,zmm0,ZMMWORD[const_dq7654]
+	vpsrlvq	zmm6,zmm1,ZMMWORD[const_dq1234]
+	DB	98,147,77,72,68,249,0
+	vpxorq	zmm5{k2},zmm5,zmm6
+	vpxord	zmm10,zmm7,zmm5
+
+$L$_main_loop_run_8_hEgxyDlCngwrfFe:
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	vmovdqu8	zmm2,ZMMWORD[64+rcx]
+	add	rcx,0x80
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vpsrldq	zmm13,zmm9,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm15,zmm9,0x1
+	vpxord	zmm15,zmm15,zmm14
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+	vpsrldq	zmm13,zmm10,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm16,zmm10,0x1
+	vpxord	zmm16,zmm16,zmm14
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,220,200
+	DB	98,242,109,72,220,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,221,200
+	DB	98,242,109,72,221,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu8	ZMMWORD[64+rdx],zmm2
+	add	rdx,0x80
+	sub	r8,0x80
+	cmp	r8,0x80
+	jge	NEAR $L$_main_loop_run_8_hEgxyDlCngwrfFe
+	vextracti32x4	xmm0,zmm2,0x3
+	jmp	NEAR $L$_do_n_blocks_hEgxyDlCngwrfFe
+
+$L$_steal_cipher_next_hEgxyDlCngwrfFe:
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[rsp],rax
+	mov	QWORD[8+rsp],rbx
+	vmovdqa	xmm0,XMMWORD[rsp]
+
+$L$_steal_cipher_hEgxyDlCngwrfFe:
+	vmovdqa	xmm2,xmm8
+	lea	rax,[vpshufb_shf_table]
+	vmovdqu	xmm10,XMMWORD[r8*1+rax]
+	vpshufb	xmm8,xmm8,xmm10
+	vmovdqu	xmm3,XMMWORD[((-16))+r8*1+rcx]
+	vmovdqu	XMMWORD[(-16)+r8*1+rdx],xmm8
+	lea	rax,[vpshufb_shf_table]
+	add	rax,16
+	sub	rax,r8
+	vmovdqu	xmm10,XMMWORD[rax]
+	vpxor	xmm10,xmm10,XMMWORD[mask1]
+	vpshufb	xmm3,xmm3,xmm10
+	vpblendvb	xmm3,xmm3,xmm2,xmm10
+	vpxor	xmm8,xmm3,xmm0
+	vpxor	xmm8,xmm8,XMMWORD[128+rsp]
+	DB	98,114,61,8,220,132,36,144,0,0,0
+	DB	98,114,61,8,220,132,36,160,0,0,0
+	DB	98,114,61,8,220,132,36,176,0,0,0
+	DB	98,114,61,8,220,132,36,192,0,0,0
+	DB	98,114,61,8,220,132,36,208,0,0,0
+	DB	98,114,61,8,220,132,36,224,0,0,0
+	DB	98,114,61,8,220,132,36,240,0,0,0
+	DB	98,114,61,8,220,132,36,0,1,0,0
+	DB	98,114,61,8,220,132,36,16,1,0,0
+	DB	98,114,61,8,220,132,36,32,1,0,0
+	DB	98,114,61,8,220,132,36,48,1,0,0
+	DB	98,114,61,8,220,132,36,64,1,0,0
+	DB	98,114,61,8,220,132,36,80,1,0,0
+	DB	98,114,61,8,221,132,36,96,1,0,0
+	vpxor	xmm8,xmm8,xmm0
+	vmovdqu	XMMWORD[(-16)+rdx],xmm8
+
+$L$_ret_hEgxyDlCngwrfFe:
+	mov	rbx,QWORD[528+rsp]
+	mov	rdi,QWORD[((528 + 8))+rsp]
+	mov	rsi,QWORD[((528 + 16))+rsp]
+	vmovdqa	xmm6,XMMWORD[((368 + 0))+rsp]
+	vmovdqa	xmm7,XMMWORD[((368 + 16))+rsp]
+	vmovdqa	xmm8,XMMWORD[((368 + 32))+rsp]
+	vmovdqa	xmm9,XMMWORD[((368 + 48))+rsp]
+	vmovdqa	xmm10,XMMWORD[((368 + 64))+rsp]
+	vmovdqa	xmm11,XMMWORD[((368 + 80))+rsp]
+	vmovdqa	xmm12,XMMWORD[((368 + 96))+rsp]
+	vmovdqa	xmm13,XMMWORD[((368 + 112))+rsp]
+	vmovdqa	xmm14,XMMWORD[((368 + 128))+rsp]
+	vmovdqa	xmm15,XMMWORD[((368 + 144))+rsp]
+	mov	rsp,rbp
+	pop	rbp
+	DB	0F3h,0C3h		;repret
+
+$L$_less_than_128_bytes_hEgxyDlCngwrfFe:
+	cmp	r8,0x10
+	jb	NEAR $L$_ret_hEgxyDlCngwrfFe
+	mov	r10,r8
+	and	r10,0x70
+	cmp	r10,0x60
+	je	NEAR $L$_num_blocks_is_6_hEgxyDlCngwrfFe
+	cmp	r10,0x50
+	je	NEAR $L$_num_blocks_is_5_hEgxyDlCngwrfFe
+	cmp	r10,0x40
+	je	NEAR $L$_num_blocks_is_4_hEgxyDlCngwrfFe
+	cmp	r10,0x30
+	je	NEAR $L$_num_blocks_is_3_hEgxyDlCngwrfFe
+	cmp	r10,0x20
+	je	NEAR $L$_num_blocks_is_2_hEgxyDlCngwrfFe
+	cmp	r10,0x10
+	je	NEAR $L$_num_blocks_is_1_hEgxyDlCngwrfFe
+
+$L$_num_blocks_is_7_hEgxyDlCngwrfFe:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[32+rsp],rax
+	mov	QWORD[40+rsp],rbx
+	vmovdqa	xmm11,XMMWORD[32+rsp]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[48+rsp],rax
+	mov	QWORD[56+rsp],rbx
+	vmovdqa	xmm12,XMMWORD[48+rsp]
+	vmovdqu	xmm4,XMMWORD[48+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[64+rsp],rax
+	mov	QWORD[72+rsp],rbx
+	vmovdqa	xmm13,XMMWORD[64+rsp]
+	vmovdqu	xmm5,XMMWORD[64+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[80+rsp],rax
+	mov	QWORD[88+rsp],rbx
+	vmovdqa	xmm14,XMMWORD[80+rsp]
+	vmovdqu	xmm6,XMMWORD[80+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[96+rsp],rax
+	mov	QWORD[104+rsp],rbx
+	vmovdqa	xmm15,XMMWORD[96+rsp]
+	vmovdqu	xmm7,XMMWORD[96+rcx]
+	add	rcx,0x70
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vpxor	xmm7,xmm7,xmm15
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vpxor	xmm5,xmm5,xmm0
+	vpxor	xmm6,xmm6,xmm0
+	vpxor	xmm7,xmm7,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	DB	98,242,69,8,220,248
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,221,200
+	DB	98,242,109,8,221,208
+	DB	98,242,101,8,221,216
+	DB	98,242,93,8,221,224
+	DB	98,242,85,8,221,232
+	DB	98,242,77,8,221,240
+	DB	98,242,69,8,221,248
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vpxor	xmm7,xmm7,xmm15
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	vmovdqu	XMMWORD[48+rdx],xmm4
+	vmovdqu	XMMWORD[64+rdx],xmm5
+	vmovdqu	XMMWORD[80+rdx],xmm6
+	vmovdqu	XMMWORD[96+rdx],xmm7
+	add	rdx,0x70
+	vmovdqa	xmm8,xmm7
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+$L$_num_blocks_is_6_hEgxyDlCngwrfFe:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[32+rsp],rax
+	mov	QWORD[40+rsp],rbx
+	vmovdqa	xmm11,XMMWORD[32+rsp]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[48+rsp],rax
+	mov	QWORD[56+rsp],rbx
+	vmovdqa	xmm12,XMMWORD[48+rsp]
+	vmovdqu	xmm4,XMMWORD[48+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[64+rsp],rax
+	mov	QWORD[72+rsp],rbx
+	vmovdqa	xmm13,XMMWORD[64+rsp]
+	vmovdqu	xmm5,XMMWORD[64+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[80+rsp],rax
+	mov	QWORD[88+rsp],rbx
+	vmovdqa	xmm14,XMMWORD[80+rsp]
+	vmovdqu	xmm6,XMMWORD[80+rcx]
+	add	rcx,0x60
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vpxor	xmm5,xmm5,xmm0
+	vpxor	xmm6,xmm6,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	DB	98,242,77,8,220,240
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,221,200
+	DB	98,242,109,8,221,208
+	DB	98,242,101,8,221,216
+	DB	98,242,93,8,221,224
+	DB	98,242,85,8,221,232
+	DB	98,242,77,8,221,240
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	vmovdqu	XMMWORD[48+rdx],xmm4
+	vmovdqu	XMMWORD[64+rdx],xmm5
+	vmovdqu	XMMWORD[80+rdx],xmm6
+	add	rdx,0x60
+	vmovdqa	xmm8,xmm6
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+$L$_num_blocks_is_5_hEgxyDlCngwrfFe:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[32+rsp],rax
+	mov	QWORD[40+rsp],rbx
+	vmovdqa	xmm11,XMMWORD[32+rsp]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[48+rsp],rax
+	mov	QWORD[56+rsp],rbx
+	vmovdqa	xmm12,XMMWORD[48+rsp]
+	vmovdqu	xmm4,XMMWORD[48+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[64+rsp],rax
+	mov	QWORD[72+rsp],rbx
+	vmovdqa	xmm13,XMMWORD[64+rsp]
+	vmovdqu	xmm5,XMMWORD[64+rcx]
+	add	rcx,0x50
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vpxor	xmm5,xmm5,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	DB	98,242,85,8,220,232
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,221,200
+	DB	98,242,109,8,221,208
+	DB	98,242,101,8,221,216
+	DB	98,242,93,8,221,224
+	DB	98,242,85,8,221,232
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	vmovdqu	XMMWORD[48+rdx],xmm4
+	vmovdqu	XMMWORD[64+rdx],xmm5
+	add	rdx,0x50
+	vmovdqa	xmm8,xmm5
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+$L$_num_blocks_is_4_hEgxyDlCngwrfFe:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[32+rsp],rax
+	mov	QWORD[40+rsp],rbx
+	vmovdqa	xmm11,XMMWORD[32+rsp]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[48+rsp],rax
+	mov	QWORD[56+rsp],rbx
+	vmovdqa	xmm12,XMMWORD[48+rsp]
+	vmovdqu	xmm4,XMMWORD[48+rcx]
+	add	rcx,0x40
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	DB	98,242,93,8,220,224
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,221,200
+	DB	98,242,109,8,221,208
+	DB	98,242,101,8,221,216
+	DB	98,242,93,8,221,224
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	vmovdqu	XMMWORD[48+rdx],xmm4
+	add	rdx,0x40
+	vmovdqa	xmm8,xmm4
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+$L$_num_blocks_is_3_hEgxyDlCngwrfFe:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[32+rsp],rax
+	mov	QWORD[40+rsp],rbx
+	vmovdqa	xmm11,XMMWORD[32+rsp]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	add	rcx,0x30
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	DB	98,242,101,8,220,216
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,221,200
+	DB	98,242,109,8,221,208
+	DB	98,242,101,8,221,216
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	add	rdx,0x30
+	vmovdqa	xmm8,xmm3
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+$L$_num_blocks_is_2_hEgxyDlCngwrfFe:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	add	rcx,0x20
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,220,200
+	DB	98,242,109,8,220,208
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,221,200
+	DB	98,242,109,8,221,208
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	add	rdx,0x20
+	vmovdqa	xmm8,xmm2
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_next_hEgxyDlCngwrfFe
+
+$L$_num_blocks_is_1_hEgxyDlCngwrfFe:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	add	rcx,0x10
+	vpxor	xmm1,xmm1,xmm9
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,220,200
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,221,200
+	vpxor	xmm1,xmm1,xmm9
+	vmovdqu	XMMWORD[rdx],xmm1
+	add	rdx,0x10
+	vmovdqa	xmm8,xmm1
+	and	r8,0xf
+	je	NEAR $L$_ret_hEgxyDlCngwrfFe
+	jmp	NEAR $L$_steal_cipher_next_hEgxyDlCngwrfFe
+	DB	0F3h,0C3h		;repret
+
+global	aes_hw_xts_decrypt_avx512
+
+
+ALIGN	32
+aes_hw_xts_decrypt_avx512:
+
+DB	243,15,30,250
+	push	rbp
+	mov	rbp,rsp
+	sub	rsp,552
+	and	rsp,0xffffffffffffffc0
+	mov	QWORD[528+rsp],rbx
+	mov	QWORD[((528 + 8))+rsp],rdi
+	mov	QWORD[((528 + 16))+rsp],rsi
+	vmovdqa	XMMWORD[(368 + 0)+rsp],xmm6
+	vmovdqa	XMMWORD[(368 + 16)+rsp],xmm7
+	vmovdqa	XMMWORD[(368 + 32)+rsp],xmm8
+	vmovdqa	XMMWORD[(368 + 48)+rsp],xmm9
+	vmovdqa	XMMWORD[(368 + 64)+rsp],xmm10
+	vmovdqa	XMMWORD[(368 + 80)+rsp],xmm11
+	vmovdqa	XMMWORD[(368 + 96)+rsp],xmm12
+	vmovdqa	XMMWORD[(368 + 112)+rsp],xmm13
+	vmovdqa	XMMWORD[(368 + 128)+rsp],xmm14
+	vmovdqa	XMMWORD[(368 + 144)+rsp],xmm15
+	mov	rdi,0x87
+	vmovdqu	xmm1,XMMWORD[r11]
+	vpxor	xmm4,xmm4,xmm4
+	vmovdqu	xmm0,XMMWORD[r10]
+	vpxor	xmm1,xmm1,xmm0
+
+	vmovdqu	xmm2,XMMWORD[224+r9]
+	vmovdqa	XMMWORD[352+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[16+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[208+r9]
+	vmovdqa	XMMWORD[336+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[32+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[192+r9]
+	vmovdqa	XMMWORD[320+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[48+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[176+r9]
+	vmovdqa	XMMWORD[304+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[64+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[160+r9]
+	vmovdqa	XMMWORD[288+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[80+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[144+r9]
+	vmovdqa	XMMWORD[272+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[96+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[128+r9]
+	vmovdqa	XMMWORD[256+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[112+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[112+r9]
+	vmovdqa	XMMWORD[240+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[128+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[96+r9]
+	vmovdqa	XMMWORD[224+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[144+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[80+r9]
+	vmovdqa	XMMWORD[208+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[160+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[64+r9]
+	vmovdqa	XMMWORD[192+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[176+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[48+r9]
+	vmovdqa	XMMWORD[176+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[192+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[32+r9]
+	vmovdqa	XMMWORD[160+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[208+r10]
+	DB	98,242,117,8,220,200
+
+	vmovdqu	xmm2,XMMWORD[16+r9]
+	vmovdqa	XMMWORD[144+rsp],xmm2
+
+	vmovdqu	xmm0,XMMWORD[224+r10]
+	DB	98,242,117,8,221,200
+
+	vmovdqu	xmm2,XMMWORD[r9]
+	vmovdqa	XMMWORD[128+rsp],xmm2
+
+	vmovdqa	XMMWORD[rsp],xmm1
+	mov	QWORD[((8 + 40))+rbp],rcx
+	mov	QWORD[((8 + 48))+rbp],rdx
+
+	cmp	r8,0x80
+	jb	NEAR $L$_less_than_128_bytes_amivrujEyduiFoi
+	vpbroadcastq	zmm25,rdi
+	cmp	r8,0x100
+	jge	NEAR $L$_start_by16_amivrujEyduiFoi
+	jmp	NEAR $L$_start_by8_amivrujEyduiFoi
+
+$L$_do_n_blocks_amivrujEyduiFoi:
+	cmp	r8,0x0
+	je	NEAR $L$_ret_amivrujEyduiFoi
+	cmp	r8,0x70
+	jge	NEAR $L$_remaining_num_blocks_is_7_amivrujEyduiFoi
+	cmp	r8,0x60
+	jge	NEAR $L$_remaining_num_blocks_is_6_amivrujEyduiFoi
+	cmp	r8,0x50
+	jge	NEAR $L$_remaining_num_blocks_is_5_amivrujEyduiFoi
+	cmp	r8,0x40
+	jge	NEAR $L$_remaining_num_blocks_is_4_amivrujEyduiFoi
+	cmp	r8,0x30
+	jge	NEAR $L$_remaining_num_blocks_is_3_amivrujEyduiFoi
+	cmp	r8,0x20
+	jge	NEAR $L$_remaining_num_blocks_is_2_amivrujEyduiFoi
+	cmp	r8,0x10
+	jge	NEAR $L$_remaining_num_blocks_is_1_amivrujEyduiFoi
+
+
+	vmovdqu	xmm1,xmm5
+
+	vpxor	xmm1,xmm1,xmm9
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	vpxor	xmm1,xmm1,xmm9
+	vmovdqu	XMMWORD[(-16)+rdx],xmm1
+	vmovdqa	xmm8,xmm1
+
+
+	mov	r10,0x1
+	kmovq	k1,r10
+	vpsllq	xmm13,xmm9,0x3f
+	vpsraq	xmm14,xmm13,0x3f
+	vpandq	xmm5,xmm14,xmm25
+	vpxorq	xmm9{k1},xmm9,xmm5
+	vpsrldq	xmm10,xmm9,0x8
+	DB	98,211,181,8,115,194,1
+	vpslldq	xmm13,xmm13,0x8
+	vpxorq	xmm0,xmm0,xmm13
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_remaining_num_blocks_is_7_amivrujEyduiFoi:
+	mov	r10,0xffffffffffffffff
+	shr	r10,0x10
+	kmovq	k1,r10
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	vmovdqu8	zmm2{k1},[64+rcx]
+	add	rcx,0x70
+	and	r8,0xf
+	je	NEAR $L$_done_7_remain_amivrujEyduiFoi
+	vextracti32x4	xmm12,zmm10,0x2
+	vextracti32x4	xmm13,zmm10,0x3
+	vinserti32x4	zmm10,zmm10,xmm13,0x2
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,223,200
+	DB	98,242,109,72,223,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu8	ZMMWORD[64+rdx]{k1},zmm2
+	add	rdx,0x70
+	vextracti32x4	xmm8,zmm2,0x2
+	vmovdqa	xmm0,xmm12
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_7_remain_amivrujEyduiFoi:
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,223,200
+	DB	98,242,109,72,223,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu8	ZMMWORD[64+rdx]{k1},zmm2
+	jmp	NEAR $L$_ret_amivrujEyduiFoi
+
+$L$_remaining_num_blocks_is_6_amivrujEyduiFoi:
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	vmovdqu8	ymm2,YMMWORD[64+rcx]
+	add	rcx,0x60
+	and	r8,0xf
+	je	NEAR $L$_done_6_remain_amivrujEyduiFoi
+	vextracti32x4	xmm12,zmm10,0x1
+	vextracti32x4	xmm13,zmm10,0x2
+	vinserti32x4	zmm10,zmm10,xmm13,0x1
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,223,200
+	DB	98,242,109,72,223,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu8	YMMWORD[64+rdx],ymm2
+	add	rdx,0x60
+	vextracti32x4	xmm8,zmm2,0x1
+	vmovdqa	xmm0,xmm12
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_6_remain_amivrujEyduiFoi:
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,223,200
+	DB	98,242,109,72,223,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu8	YMMWORD[64+rdx],ymm2
+	jmp	NEAR $L$_ret_amivrujEyduiFoi
+
+$L$_remaining_num_blocks_is_5_amivrujEyduiFoi:
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	vmovdqu	xmm2,XMMWORD[64+rcx]
+	add	rcx,0x50
+	and	r8,0xf
+	je	NEAR $L$_done_5_remain_amivrujEyduiFoi
+	vmovdqa	xmm12,xmm10
+	vextracti32x4	xmm10,zmm10,0x1
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,223,200
+	DB	98,242,109,72,223,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu	XMMWORD[64+rdx],xmm2
+	add	rdx,0x50
+	vmovdqa	xmm8,xmm2
+	vmovdqa	xmm0,xmm12
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_5_remain_amivrujEyduiFoi:
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,223,200
+	DB	98,242,109,72,223,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu8	XMMWORD[64+rdx],xmm2
+	jmp	NEAR $L$_ret_amivrujEyduiFoi
+
+$L$_remaining_num_blocks_is_4_amivrujEyduiFoi:
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	add	rcx,0x40
+	and	r8,0xf
+	je	NEAR $L$_done_4_remain_amivrujEyduiFoi
+	vextracti32x4	xmm12,zmm9,0x3
+	vinserti32x4	zmm9,zmm9,xmm10,0x3
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,223,200
+	DB	98,242,109,72,223,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	add	rdx,0x40
+	vextracti32x4	xmm8,zmm1,0x3
+	vmovdqa	xmm0,xmm12
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_4_remain_amivrujEyduiFoi:
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,223,200
+	DB	98,242,109,72,223,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	jmp	NEAR $L$_ret_amivrujEyduiFoi
+
+$L$_remaining_num_blocks_is_3_amivrujEyduiFoi:
+	vmovdqu	xmm1,XMMWORD[rcx]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	add	rcx,0x30
+	and	r8,0xf
+	je	NEAR $L$_done_3_remain_amivrujEyduiFoi
+	vextracti32x4	xmm13,zmm9,0x2
+	vextracti32x4	xmm10,zmm9,0x1
+	vextracti32x4	xmm11,zmm9,0x3
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	add	rdx,0x30
+	vmovdqa	xmm8,xmm3
+	vmovdqa	xmm0,xmm13
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_3_remain_amivrujEyduiFoi:
+	vextracti32x4	xmm10,zmm9,0x1
+	vextracti32x4	xmm11,zmm9,0x2
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	jmp	NEAR $L$_ret_amivrujEyduiFoi
+
+$L$_remaining_num_blocks_is_2_amivrujEyduiFoi:
+	vmovdqu	xmm1,XMMWORD[rcx]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	add	rcx,0x20
+	and	r8,0xf
+	je	NEAR $L$_done_2_remain_amivrujEyduiFoi
+	vextracti32x4	xmm10,zmm9,0x2
+	vextracti32x4	xmm12,zmm9,0x1
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	add	rdx,0x20
+	vmovdqa	xmm8,xmm2
+	vmovdqa	xmm0,xmm12
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_2_remain_amivrujEyduiFoi:
+	vextracti32x4	xmm10,zmm9,0x1
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	jmp	NEAR $L$_ret_amivrujEyduiFoi
+
+$L$_remaining_num_blocks_is_1_amivrujEyduiFoi:
+	vmovdqu	xmm1,XMMWORD[rcx]
+	add	rcx,0x10
+	and	r8,0xf
+	je	NEAR $L$_done_1_remain_amivrujEyduiFoi
+	vextracti32x4	xmm11,zmm9,0x1
+	vpxor	xmm1,xmm1,xmm11
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	vpxor	xmm1,xmm1,xmm11
+	vmovdqu	XMMWORD[rdx],xmm1
+	add	rdx,0x10
+	vmovdqa	xmm8,xmm1
+	vmovdqa	xmm0,xmm9
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_1_remain_amivrujEyduiFoi:
+	vpxor	xmm1,xmm1,xmm9
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	vpxor	xmm1,xmm1,xmm9
+	vmovdqu	XMMWORD[rdx],xmm1
+	jmp	NEAR $L$_ret_amivrujEyduiFoi
+
+$L$_start_by16_amivrujEyduiFoi:
+	vbroadcasti32x4	zmm0,ZMMWORD[rsp]
+	vbroadcasti32x4	zmm8,ZMMWORD[shufb_15_7]
+	mov	r10,0xaa
+	kmovq	k2,r10
+
+
+	vpshufb	zmm1,zmm0,zmm8
+	vpsllvq	zmm4,zmm0,ZMMWORD[const_dq3210]
+	vpsrlvq	zmm2,zmm1,ZMMWORD[const_dq5678]
+	DB	98,147,109,72,68,217,0
+	vpxorq	zmm4{k2},zmm4,zmm2
+	vpxord	zmm9,zmm3,zmm4
+
+
+	vpsllvq	zmm5,zmm0,ZMMWORD[const_dq7654]
+	vpsrlvq	zmm6,zmm1,ZMMWORD[const_dq1234]
+	DB	98,147,77,72,68,249,0
+	vpxorq	zmm5{k2},zmm5,zmm6
+	vpxord	zmm10,zmm7,zmm5
+
+
+	vpsrldq	zmm13,zmm9,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm11,zmm9,0x1
+	vpxord	zmm11,zmm11,zmm14
+
+	vpsrldq	zmm15,zmm10,0xf
+	DB	98,131,5,72,68,193,0
+	vpslldq	zmm12,zmm10,0x1
+	vpxord	zmm12,zmm12,zmm16
+
+$L$_main_loop_run_16_amivrujEyduiFoi:
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	vmovdqu8	zmm2,ZMMWORD[64+rcx]
+	vmovdqu8	zmm3,ZMMWORD[128+rcx]
+	vmovdqu8	zmm4,ZMMWORD[192+rcx]
+	vmovdqu8	zmm5,ZMMWORD[240+rcx]
+	add	rcx,0x100
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+	vpxorq	zmm3,zmm3,zmm11
+	vpxorq	zmm4,zmm4,zmm12
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vpxorq	zmm3,zmm3,zmm0
+	vpxorq	zmm4,zmm4,zmm0
+	vpsrldq	zmm13,zmm11,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm15,zmm11,0x1
+	vpxord	zmm15,zmm15,zmm14
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vpsrldq	zmm13,zmm12,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm16,zmm12,0x1
+	vpxord	zmm16,zmm16,zmm14
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vpsrldq	zmm13,zmm15,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm17,zmm15,0x1
+	vpxord	zmm17,zmm17,zmm14
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vpsrldq	zmm13,zmm16,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm18,zmm16,0x1
+	vpxord	zmm18,zmm18,zmm14
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	DB	98,242,101,72,222,216
+	DB	98,242,93,72,222,224
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,223,200
+	DB	98,242,109,72,223,208
+	DB	98,242,101,72,223,216
+	DB	98,242,93,72,223,224
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+	vpxorq	zmm3,zmm3,zmm11
+	vpxorq	zmm4,zmm4,zmm12
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqa32	zmm11,zmm17
+	vmovdqa32	zmm12,zmm18
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu8	ZMMWORD[64+rdx],zmm2
+	vmovdqu8	ZMMWORD[128+rdx],zmm3
+	vmovdqu8	ZMMWORD[192+rdx],zmm4
+	add	rdx,0x100
+	sub	r8,0x100
+	cmp	r8,0x100
+	jge	NEAR $L$_main_loop_run_16_amivrujEyduiFoi
+
+	cmp	r8,0x80
+	jge	NEAR $L$_main_loop_run_8_amivrujEyduiFoi
+	jmp	NEAR $L$_do_n_blocks_amivrujEyduiFoi
+
+$L$_start_by8_amivrujEyduiFoi:
+
+	vbroadcasti32x4	zmm0,ZMMWORD[rsp]
+	vbroadcasti32x4	zmm8,ZMMWORD[shufb_15_7]
+	mov	r10,0xaa
+	kmovq	k2,r10
+
+
+	vpshufb	zmm1,zmm0,zmm8
+	vpsllvq	zmm4,zmm0,ZMMWORD[const_dq3210]
+	vpsrlvq	zmm2,zmm1,ZMMWORD[const_dq5678]
+	DB	98,147,109,72,68,217,0
+	vpxorq	zmm4{k2},zmm4,zmm2
+	vpxord	zmm9,zmm3,zmm4
+
+
+	vpsllvq	zmm5,zmm0,ZMMWORD[const_dq7654]
+	vpsrlvq	zmm6,zmm1,ZMMWORD[const_dq1234]
+	DB	98,147,77,72,68,249,0
+	vpxorq	zmm5{k2},zmm5,zmm6
+	vpxord	zmm10,zmm7,zmm5
+
+$L$_main_loop_run_8_amivrujEyduiFoi:
+	vmovdqu8	zmm1,ZMMWORD[rcx]
+	vmovdqu8	zmm2,ZMMWORD[64+rcx]
+	vmovdqu8	xmm5,XMMWORD[112+rcx]
+	add	rcx,0x80
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[128+rsp]
+	vpxorq	zmm1,zmm1,zmm0
+	vpxorq	zmm2,zmm2,zmm0
+	vpsrldq	zmm13,zmm9,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm15,zmm9,0x1
+	vpxord	zmm15,zmm15,zmm14
+	vbroadcasti32x4	zmm0,ZMMWORD[144+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[160+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[176+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+	vpsrldq	zmm13,zmm10,0xf
+	DB	98,19,21,72,68,241,0
+	vpslldq	zmm16,zmm10,0x1
+	vpxord	zmm16,zmm16,zmm14
+
+	vbroadcasti32x4	zmm0,ZMMWORD[192+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[208+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[224+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[240+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[256+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[272+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[288+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[304+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[320+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[336+rsp]
+	DB	98,242,117,72,222,200
+	DB	98,242,109,72,222,208
+
+
+	vbroadcasti32x4	zmm0,ZMMWORD[352+rsp]
+	DB	98,242,117,72,223,200
+	DB	98,242,109,72,223,208
+
+
+	vpxorq	zmm1,zmm1,zmm9
+	vpxorq	zmm2,zmm2,zmm10
+
+
+	vmovdqa32	zmm9,zmm15
+	vmovdqa32	zmm10,zmm16
+	vmovdqu8	ZMMWORD[rdx],zmm1
+	vmovdqu8	ZMMWORD[64+rdx],zmm2
+	add	rdx,0x80
+	sub	r8,0x80
+	cmp	r8,0x80
+	jge	NEAR $L$_main_loop_run_8_amivrujEyduiFoi
+	jmp	NEAR $L$_do_n_blocks_amivrujEyduiFoi
+
+$L$_steal_cipher_amivrujEyduiFoi:
+
+	vmovdqa	xmm2,xmm8
+
+
+	lea	rax,[vpshufb_shf_table]
+	vmovdqu	xmm10,XMMWORD[r8*1+rax]
+	vpshufb	xmm8,xmm8,xmm10
+
+
+	vmovdqu	xmm3,XMMWORD[((-16))+r8*1+rcx]
+	vmovdqu	XMMWORD[(-16)+r8*1+rdx],xmm8
+
+
+	lea	rax,[vpshufb_shf_table]
+	add	rax,16
+	sub	rax,r8
+	vmovdqu	xmm10,XMMWORD[rax]
+	vpxor	xmm10,xmm10,XMMWORD[mask1]
+	vpshufb	xmm3,xmm3,xmm10
+
+	vpblendvb	xmm3,xmm3,xmm2,xmm10
+
+
+	vpxor	xmm8,xmm3,xmm0
+
+
+	vpxor	xmm8,xmm8,XMMWORD[128+rsp]
+	DB	98,114,61,8,222,132,36,144,0,0,0
+	DB	98,114,61,8,222,132,36,160,0,0,0
+	DB	98,114,61,8,222,132,36,176,0,0,0
+	DB	98,114,61,8,222,132,36,192,0,0,0
+	DB	98,114,61,8,222,132,36,208,0,0,0
+	DB	98,114,61,8,222,132,36,224,0,0,0
+	DB	98,114,61,8,222,132,36,240,0,0,0
+	DB	98,114,61,8,222,132,36,0,1,0,0
+	DB	98,114,61,8,222,132,36,16,1,0,0
+	DB	98,114,61,8,222,132,36,32,1,0,0
+	DB	98,114,61,8,222,132,36,48,1,0,0
+	DB	98,114,61,8,222,132,36,64,1,0,0
+	DB	98,114,61,8,222,132,36,80,1,0,0
+	DB	98,114,61,8,223,132,36,96,1,0,0
+
+
+	vpxor	xmm8,xmm8,xmm0
+
+$L$_done_amivrujEyduiFoi:
+
+	vmovdqu	XMMWORD[(-16)+rdx],xmm8
+
+$L$_ret_amivrujEyduiFoi:
+	mov	rbx,QWORD[528+rsp]
+	mov	rdi,QWORD[((528 + 8))+rsp]
+	mov	rsi,QWORD[((528 + 16))+rsp]
+	vmovdqa	xmm6,XMMWORD[((368 + 0))+rsp]
+	vmovdqa	xmm7,XMMWORD[((368 + 16))+rsp]
+	vmovdqa	xmm8,XMMWORD[((368 + 32))+rsp]
+	vmovdqa	xmm9,XMMWORD[((368 + 48))+rsp]
+	vmovdqa	xmm10,XMMWORD[((368 + 64))+rsp]
+	vmovdqa	xmm11,XMMWORD[((368 + 80))+rsp]
+	vmovdqa	xmm12,XMMWORD[((368 + 96))+rsp]
+	vmovdqa	xmm13,XMMWORD[((368 + 112))+rsp]
+	vmovdqa	xmm14,XMMWORD[((368 + 128))+rsp]
+	vmovdqa	xmm15,XMMWORD[((368 + 144))+rsp]
+	mov	rsp,rbp
+	pop	rbp
+	DB	0F3h,0C3h		;repret
+
+$L$_less_than_128_bytes_amivrujEyduiFoi:
+	cmp	r8,0x10
+	jb	NEAR $L$_ret_amivrujEyduiFoi
+
+	mov	r10,r8
+	and	r10,0x70
+	cmp	r10,0x60
+	je	NEAR $L$_num_blocks_is_6_amivrujEyduiFoi
+	cmp	r10,0x50
+	je	NEAR $L$_num_blocks_is_5_amivrujEyduiFoi
+	cmp	r10,0x40
+	je	NEAR $L$_num_blocks_is_4_amivrujEyduiFoi
+	cmp	r10,0x30
+	je	NEAR $L$_num_blocks_is_3_amivrujEyduiFoi
+	cmp	r10,0x20
+	je	NEAR $L$_num_blocks_is_2_amivrujEyduiFoi
+	cmp	r10,0x10
+	je	NEAR $L$_num_blocks_is_1_amivrujEyduiFoi
+
+$L$_num_blocks_is_7_amivrujEyduiFoi:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[32+rsp],rax
+	mov	QWORD[40+rsp],rbx
+	vmovdqa	xmm11,XMMWORD[32+rsp]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[48+rsp],rax
+	mov	QWORD[56+rsp],rbx
+	vmovdqa	xmm12,XMMWORD[48+rsp]
+	vmovdqu	xmm4,XMMWORD[48+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[64+rsp],rax
+	mov	QWORD[72+rsp],rbx
+	vmovdqa	xmm13,XMMWORD[64+rsp]
+	vmovdqu	xmm5,XMMWORD[64+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[80+rsp],rax
+	mov	QWORD[88+rsp],rbx
+	vmovdqa	xmm14,XMMWORD[80+rsp]
+	vmovdqu	xmm6,XMMWORD[80+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[96+rsp],rax
+	mov	QWORD[104+rsp],rbx
+	vmovdqa	xmm15,XMMWORD[96+rsp]
+	vmovdqu	xmm7,XMMWORD[96+rcx]
+	add	rcx,0x70
+	and	r8,0xf
+	je	NEAR $L$_done_7_amivrujEyduiFoi
+
+$L$_steal_cipher_7_amivrujEyduiFoi:
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa64	xmm16,xmm15
+	vmovdqa	xmm15,XMMWORD[16+rsp]
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vpxor	xmm7,xmm7,xmm15
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vpxor	xmm5,xmm5,xmm0
+	vpxor	xmm6,xmm6,xmm0
+	vpxor	xmm7,xmm7,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	DB	98,242,93,8,223,224
+	DB	98,242,85,8,223,232
+	DB	98,242,77,8,223,240
+	DB	98,242,69,8,223,248
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vpxor	xmm7,xmm7,xmm15
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	vmovdqu	XMMWORD[48+rdx],xmm4
+	vmovdqu	XMMWORD[64+rdx],xmm5
+	vmovdqu	XMMWORD[80+rdx],xmm6
+	add	rdx,0x70
+	vmovdqa64	xmm0,xmm16
+	vmovdqa	xmm8,xmm7
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_7_amivrujEyduiFoi:
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vpxor	xmm7,xmm7,xmm15
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vpxor	xmm5,xmm5,xmm0
+	vpxor	xmm6,xmm6,xmm0
+	vpxor	xmm7,xmm7,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	DB	98,242,69,8,222,248
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	DB	98,242,93,8,223,224
+	DB	98,242,85,8,223,232
+	DB	98,242,77,8,223,240
+	DB	98,242,69,8,223,248
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vpxor	xmm7,xmm7,xmm15
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	vmovdqu	XMMWORD[48+rdx],xmm4
+	vmovdqu	XMMWORD[64+rdx],xmm5
+	vmovdqu	XMMWORD[80+rdx],xmm6
+	add	rdx,0x70
+	vmovdqa	xmm8,xmm7
+	jmp	NEAR $L$_done_amivrujEyduiFoi
+
+$L$_num_blocks_is_6_amivrujEyduiFoi:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[32+rsp],rax
+	mov	QWORD[40+rsp],rbx
+	vmovdqa	xmm11,XMMWORD[32+rsp]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[48+rsp],rax
+	mov	QWORD[56+rsp],rbx
+	vmovdqa	xmm12,XMMWORD[48+rsp]
+	vmovdqu	xmm4,XMMWORD[48+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[64+rsp],rax
+	mov	QWORD[72+rsp],rbx
+	vmovdqa	xmm13,XMMWORD[64+rsp]
+	vmovdqu	xmm5,XMMWORD[64+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[80+rsp],rax
+	mov	QWORD[88+rsp],rbx
+	vmovdqa	xmm14,XMMWORD[80+rsp]
+	vmovdqu	xmm6,XMMWORD[80+rcx]
+	add	rcx,0x60
+	and	r8,0xf
+	je	NEAR $L$_done_6_amivrujEyduiFoi
+
+$L$_steal_cipher_6_amivrujEyduiFoi:
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa64	xmm15,xmm14
+	vmovdqa	xmm14,XMMWORD[16+rsp]
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vpxor	xmm5,xmm5,xmm0
+	vpxor	xmm6,xmm6,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	DB	98,242,93,8,223,224
+	DB	98,242,85,8,223,232
+	DB	98,242,77,8,223,240
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	vmovdqu	XMMWORD[48+rdx],xmm4
+	vmovdqu	XMMWORD[64+rdx],xmm5
+	add	rdx,0x60
+	vmovdqa	xmm0,xmm15
+	vmovdqa	xmm8,xmm6
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_6_amivrujEyduiFoi:
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vpxor	xmm5,xmm5,xmm0
+	vpxor	xmm6,xmm6,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	DB	98,242,77,8,222,240
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	DB	98,242,93,8,223,224
+	DB	98,242,85,8,223,232
+	DB	98,242,77,8,223,240
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vpxor	xmm6,xmm6,xmm14
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	vmovdqu	XMMWORD[48+rdx],xmm4
+	vmovdqu	XMMWORD[64+rdx],xmm5
+	add	rdx,0x60
+	vmovdqa	xmm8,xmm6
+	jmp	NEAR $L$_done_amivrujEyduiFoi
+
+$L$_num_blocks_is_5_amivrujEyduiFoi:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[32+rsp],rax
+	mov	QWORD[40+rsp],rbx
+	vmovdqa	xmm11,XMMWORD[32+rsp]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[48+rsp],rax
+	mov	QWORD[56+rsp],rbx
+	vmovdqa	xmm12,XMMWORD[48+rsp]
+	vmovdqu	xmm4,XMMWORD[48+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[64+rsp],rax
+	mov	QWORD[72+rsp],rbx
+	vmovdqa	xmm13,XMMWORD[64+rsp]
+	vmovdqu	xmm5,XMMWORD[64+rcx]
+	add	rcx,0x50
+	and	r8,0xf
+	je	NEAR $L$_done_5_amivrujEyduiFoi
+
+$L$_steal_cipher_5_amivrujEyduiFoi:
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa64	xmm14,xmm13
+	vmovdqa	xmm13,XMMWORD[16+rsp]
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vpxor	xmm5,xmm5,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	DB	98,242,93,8,223,224
+	DB	98,242,85,8,223,232
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	vmovdqu	XMMWORD[48+rdx],xmm4
+	add	rdx,0x50
+	vmovdqa	xmm0,xmm14
+	vmovdqa	xmm8,xmm5
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_5_amivrujEyduiFoi:
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vpxor	xmm5,xmm5,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	DB	98,242,85,8,222,232
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	DB	98,242,93,8,223,224
+	DB	98,242,85,8,223,232
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vpxor	xmm5,xmm5,xmm13
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	vmovdqu	XMMWORD[48+rdx],xmm4
+	add	rdx,0x50
+	vmovdqa	xmm8,xmm5
+	jmp	NEAR $L$_done_amivrujEyduiFoi
+
+$L$_num_blocks_is_4_amivrujEyduiFoi:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[32+rsp],rax
+	mov	QWORD[40+rsp],rbx
+	vmovdqa	xmm11,XMMWORD[32+rsp]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[48+rsp],rax
+	mov	QWORD[56+rsp],rbx
+	vmovdqa	xmm12,XMMWORD[48+rsp]
+	vmovdqu	xmm4,XMMWORD[48+rcx]
+	add	rcx,0x40
+	and	r8,0xf
+	je	NEAR $L$_done_4_amivrujEyduiFoi
+
+$L$_steal_cipher_4_amivrujEyduiFoi:
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa64	xmm13,xmm12
+	vmovdqa	xmm12,XMMWORD[16+rsp]
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	DB	98,242,93,8,223,224
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	add	rdx,0x40
+	vmovdqa	xmm0,xmm13
+	vmovdqa	xmm8,xmm4
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_4_amivrujEyduiFoi:
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vpxor	xmm4,xmm4,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	DB	98,242,93,8,222,224
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	DB	98,242,93,8,223,224
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vpxor	xmm4,xmm4,xmm12
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	vmovdqu	XMMWORD[32+rdx],xmm3
+	add	rdx,0x40
+	vmovdqa	xmm8,xmm4
+	jmp	NEAR $L$_done_amivrujEyduiFoi
+
+$L$_num_blocks_is_3_amivrujEyduiFoi:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[32+rsp],rax
+	mov	QWORD[40+rsp],rbx
+	vmovdqa	xmm11,XMMWORD[32+rsp]
+	vmovdqu	xmm3,XMMWORD[32+rcx]
+	add	rcx,0x30
+	and	r8,0xf
+	je	NEAR $L$_done_3_amivrujEyduiFoi
+
+$L$_steal_cipher_3_amivrujEyduiFoi:
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa64	xmm12,xmm11
+	vmovdqa	xmm11,XMMWORD[16+rsp]
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	add	rdx,0x30
+	vmovdqa	xmm0,xmm12
+	vmovdqa	xmm8,xmm3
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_3_amivrujEyduiFoi:
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vpxor	xmm3,xmm3,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	DB	98,242,101,8,222,216
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	DB	98,242,101,8,223,216
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vpxor	xmm3,xmm3,xmm11
+	vmovdqu	XMMWORD[rdx],xmm1
+	vmovdqu	XMMWORD[16+rdx],xmm2
+	add	rdx,0x30
+	vmovdqa	xmm8,xmm3
+	jmp	NEAR $L$_done_amivrujEyduiFoi
+
+$L$_num_blocks_is_2_amivrujEyduiFoi:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
+	add	rcx,0x20
+	and	r8,0xf
+	je	NEAR $L$_done_2_amivrujEyduiFoi
+
+$L$_steal_cipher_2_amivrujEyduiFoi:
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa64	xmm11,xmm10
+	vmovdqa	xmm10,XMMWORD[16+rsp]
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqu	XMMWORD[rdx],xmm1
+	add	rdx,0x20
+	vmovdqa	xmm0,xmm11
+	vmovdqa	xmm8,xmm2
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_2_amivrujEyduiFoi:
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vpxor	xmm2,xmm2,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	DB	98,242,109,8,222,208
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	DB	98,242,109,8,223,208
+	vpxor	xmm1,xmm1,xmm9
+	vpxor	xmm2,xmm2,xmm10
+	vmovdqu	XMMWORD[rdx],xmm1
+	add	rdx,0x20
+	vmovdqa	xmm8,xmm2
+	jmp	NEAR $L$_done_amivrujEyduiFoi
+
+$L$_num_blocks_is_1_amivrujEyduiFoi:
+	vmovdqa	xmm9,XMMWORD[rsp]
+	mov	rax,QWORD[rsp]
+	mov	rbx,QWORD[8+rsp]
+	vmovdqu	xmm1,XMMWORD[rcx]
+	add	rcx,0x10
+	and	r8,0xf
+	je	NEAR $L$_done_1_amivrujEyduiFoi
+
+$L$_steal_cipher_1_amivrujEyduiFoi:
+	xor	rsi,rsi
+	shl	rax,1
+	adc	rbx,rbx
+	cmovc	rsi,rdi
+	xor	rax,rsi
+	mov	QWORD[16+rsp],rax
+	mov	QWORD[24+rsp],rbx
+	vmovdqa64	xmm10,xmm9
+	vmovdqa	xmm9,XMMWORD[16+rsp]
+	vpxor	xmm1,xmm1,xmm9
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	vpxor	xmm1,xmm1,xmm9
+	add	rdx,0x10
+	vmovdqa	xmm0,xmm10
+	vmovdqa	xmm8,xmm1
+	jmp	NEAR $L$_steal_cipher_amivrujEyduiFoi
+
+$L$_done_1_amivrujEyduiFoi:
+	vpxor	xmm1,xmm1,xmm9
+	vmovdqa	xmm0,XMMWORD[128+rsp]
+	vpxor	xmm1,xmm1,xmm0
+	vmovdqa	xmm0,XMMWORD[144+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[160+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[176+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[192+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[208+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[224+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[240+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[256+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[272+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[288+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[304+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[320+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[336+rsp]
+	DB	98,242,117,8,222,200
+	vmovdqa	xmm0,XMMWORD[352+rsp]
+	DB	98,242,117,8,223,200
+	vpxor	xmm1,xmm1,xmm9
+	add	rdx,0x10
+	vmovdqa	xmm8,xmm1
+	jmp	NEAR $L$_done_amivrujEyduiFoi
+	DB	0F3h,0C3h		;repret
+
+section	.rdata rdata align=8
+ALIGN	16
+
+vpshufb_shf_table:
+	DQ	0x8786858483828100,0x8f8e8d8c8b8a8988
+	DQ	0x0706050403020100,0x000e0d0c0b0a0908
+
+mask1:
+	DQ	0x8080808080808080,0x8080808080808080
+
+const_dq3210:
+	DQ	0,0,1,1,2,2,3,3
+const_dq5678:
+	DQ	8,8,7,7,6,6,5,5
+const_dq7654:
+	DQ	4,4,5,5,6,6,7,7
+const_dq1234:
+	DQ	4,4,3,3,2,2,1,1
+
+shufb_15_7:
+	DB	15,0xff,0xff,0xff,0xff,0xff,0xff,0xff,7,0xff,0xff
+	DB	0xff,0xff,0xff,0xff,0xff
+
+section	.text
+
+%else
+; Work around https://bugzilla.nasm.us/show_bug.cgi?id=3392738
+ret
+%endif
