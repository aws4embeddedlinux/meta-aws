From f25fa28a5751949bcb07bf90921ea4bb07f06a1d Mon Sep 17 00:00:00 2001
From: David Benjamin <davidben@google.com>
Date: Tue, 2 May 2023 17:28:01 -0400
Subject: [PATCH] Don't make assumptions about GCM128_CONTEXT layout in
 aesni-gcm-x86_64.pl

This is a little trickier because Intel architectures are so
inconveniently register-starved. This code was already using every
register. However, since Xi is only needed at the start and end of the
function, I just swapped the Xi and Htable parameters. Xi is passed on
the stack, so we don't need to explicitly spill it.

Change-Id: I2ef4552fc181a5350c9b1c733cf2319377a06b74
Reviewed-on: https://boringssl-review.googlesource.com/c/boringssl/+/59525
Reviewed-by: Adam Langley <agl@google.com>
Commit-Queue: David Benjamin <davidben@google.com>
(cherry picked from commit 62f9751ade8373ae3339daee581c80a173206321)
---
 .../fipsmodule/modes/asm/aesni-gcm-x86_64.pl  | 85 +++++++++++--------
 crypto/fipsmodule/modes/gcm.c                 |  8 +-
 crypto/fipsmodule/modes/gcm_test.cc           | 15 +---
 crypto/fipsmodule/modes/internal.h            |  6 +-
 4 files changed, 59 insertions(+), 55 deletions(-)

diff --git a/crypto/fipsmodule/modes/asm/aesni-gcm-x86_64.pl b/crypto/fipsmodule/modes/asm/aesni-gcm-x86_64.pl
index 443a986c9..2c6c50c5e 100644
--- a/crypto/fipsmodule/modes/asm/aesni-gcm-x86_64.pl
+++ b/crypto/fipsmodule/modes/asm/aesni-gcm-x86_64.pl
@@ -77,10 +77,19 @@ if ($avx>1) {{{
 
 # On Windows, only four parameters are passed in registers. The last two
 # parameters will be manually loaded into %rdi and %rsi.
-my ($inp, $out, $len, $key, $ivp, $Xip) =
+my ($inp, $out, $len, $key, $ivp, $Htable) =
     $win64 ? ("%rcx", "%rdx", "%r8", "%r9", "%rdi", "%rsi") :
              ("%rdi", "%rsi", "%rdx", "%rcx", "%r8", "%r9");
 
+# The offset from %rbp to the Xip parameter. On Windows, all parameters have
+# corresponding stack positions, not just ones passed on the stack.
+# (0x40 = 6*8 + 0x10)
+#
+# Xip only needs to be accessed at the beginning and end of the function, and
+# this function is short on registers, so we make it the last parameter for
+# convenience.
+my $Xip_offset = $win64 ? 0x40 : 0x10;
+
 ($Ii,$T1,$T2,$Hkey,
  $Z0,$Z1,$Z2,$Z3,$Xi) = map("%xmm$_",(0..8));
 
@@ -112,7 +121,7 @@ _aesni_ctr32_ghash_6x:
 .Loop6x:
 	add		\$`6<<24`,$counter
 	jc		.Lhandle_ctr32		# discard $inout[1-5]?
-	vmovdqu		0x00-0x20($Xip),$Hkey	# $Hkey^1
+	vmovdqu		0x00-0x20($Htable),$Hkey	# $Hkey^1
 	  vpaddb	$T2,$inout5,$T1		# next counter value
 	  vpxor		$rndkey,$inout1,$inout1
 	  vpxor		$rndkey,$inout2,$inout2
@@ -152,7 +161,7 @@ _aesni_ctr32_ghash_6x:
 	setnc		%r12b
 	vpclmulqdq	\$0x11,$Hkey,$Z3,$Z3
 	  vaesenc	$T2,$inout2,$inout2
-	vmovdqu		0x10-0x20($Xip),$Hkey	# $Hkey^2
+	vmovdqu		0x10-0x20($Htable),$Hkey	# $Hkey^2
 	neg		%r12
 	  vaesenc	$T2,$inout3,$inout3
 	 vpxor		$Z1,$Z2,$Z2
@@ -179,7 +188,7 @@ _aesni_ctr32_ghash_6x:
 	mov		%r13,0x20+8(%rsp)
 	  vaesenc	$rndkey,$inout4,$inout4
 	mov		%r12,0x28+8(%rsp)
-	vmovdqu		0x30-0x20($Xip),$Z1	# borrow $Z1 for $Hkey^3
+	vmovdqu		0x30-0x20($Htable),$Z1	# borrow $Z1 for $Hkey^3
 	  vaesenc	$rndkey,$inout5,$inout5
 
 	  vmovups	0x30-0x80($key),$rndkey
@@ -197,7 +206,7 @@ _aesni_ctr32_ghash_6x:
 	  vaesenc	$rndkey,$inout3,$inout3
 	  vaesenc	$rndkey,$inout4,$inout4
 	 vpxor		$T1,$Z0,$Z0
-	vmovdqu		0x40-0x20($Xip),$T1	# borrow $T1 for $Hkey^4
+	vmovdqu		0x40-0x20($Htable),$T1	# borrow $T1 for $Hkey^4
 	  vaesenc	$rndkey,$inout5,$inout5
 
 	  vmovups	0x40-0x80($key),$rndkey
@@ -219,7 +228,7 @@ _aesni_ctr32_ghash_6x:
 	  vaesenc	$rndkey,$inout4,$inout4
 	mov		%r12,0x38+8(%rsp)
 	 vpxor		$T2,$Z0,$Z0
-	vmovdqu		0x60-0x20($Xip),$T2	# borrow $T2 for $Hkey^5
+	vmovdqu		0x60-0x20($Htable),$T2	# borrow $T2 for $Hkey^5
 	  vaesenc	$rndkey,$inout5,$inout5
 
 	  vmovups	0x50-0x80($key),$rndkey
@@ -241,7 +250,7 @@ _aesni_ctr32_ghash_6x:
 	  vaesenc	$rndkey,$inout4,$inout4
 	mov		%r12,0x48+8(%rsp)
 	 vpxor		$Hkey,$Z0,$Z0
-	 vmovdqu	0x70-0x20($Xip),$Hkey	# $Hkey^6
+	 vmovdqu	0x70-0x20($Htable),$Hkey	# $Hkey^6
 	  vaesenc	$rndkey,$inout5,$inout5
 
 	  vmovups	0x60-0x80($key),$rndkey
@@ -343,7 +352,7 @@ _aesni_ctr32_ghash_6x:
 	  vmovdqu	0x30($const),$Z1	# borrow $Z1, .Ltwo_lsb
 	  vpaddd	0x40($const),$Z2,$inout1	# .Lone_lsb
 	  vpaddd	$Z1,$Z2,$inout2
-	vmovdqu		0x00-0x20($Xip),$Hkey	# $Hkey^1
+	vmovdqu		0x00-0x20($Htable),$Hkey	# $Hkey^1
 	  vpaddd	$Z1,$inout1,$inout3
 	  vpshufb	$Ii,$inout1,$inout1
 	  vpaddd	$Z1,$inout2,$inout4
@@ -425,8 +434,8 @@ ___
 ######################################################################
 #
 # size_t aesni_gcm_[en|de]crypt(const void *inp, void *out, size_t len,
-#		const AES_KEY *key, unsigned char iv[16],
-#		struct { u128 Xi,H,Htbl[9]; } *Xip);
+#		const AES_KEY *key, unsigned char iv[16], const u128 *Htbl[9],
+#		u128 *Xip);
 $code.=<<___;
 .globl	aesni_gcm_decrypt
 .type	aesni_gcm_decrypt,\@abi-omnipotent
@@ -475,7 +484,7 @@ $code.=<<___
 	mov	%rsi, 0x18(%rbp)
 .seh_savereg	%rsi, 0xa8+5*8+0x18
 	mov	0x30(%rbp), $ivp
-	mov	0x38(%rbp), $Xip
+	mov	0x38(%rbp), $Htable
 	# Save non-volatile XMM registers.
 	movaps	%xmm6,-0xd0(%rbp)
 .seh_savexmm128	%xmm6, 0xa8+5*8-0xd0
@@ -502,17 +511,18 @@ ___
 $code.=<<___;
 	vzeroupper
 
+	mov		$Xip_offset(%rbp), %r12
 	vmovdqu		($ivp),$T1		# input counter value
 	add		\$-128,%rsp
 	mov		12($ivp),$counter
 	lea		.Lbswap_mask(%rip),$const
 	lea		-0x80($key),$in0	# borrow $in0
 	mov		\$0xf80,$end0		# borrow $end0
-	vmovdqu		($Xip),$Xi		# load Xi
+	vmovdqu		(%r12),$Xi		# load Xi
 	and		\$-128,%rsp		# ensure stack alignment
 	vmovdqu		($const),$Ii		# borrow $Ii for .Lbswap_mask
 	lea		0x80($key),$key		# size optimization
-	lea		0x20+0x20($Xip),$Xip	# size optimization
+	lea		0x20($Htable),$Htable	# size optimization
 	mov		0xf0-0x80($key),$rounds
 	vpshufb		$Ii,$Xi,$Xi
 
@@ -557,6 +567,7 @@ $code.=<<___;
 
 	call		_aesni_ctr32_ghash_6x
 
+	mov		$Xip_offset(%rbp), %r12
 	vmovups		$inout0,-0x60($out)	# save output
 	vmovups		$inout1,-0x50($out)
 	vmovups		$inout2,-0x40($out)
@@ -565,7 +576,7 @@ $code.=<<___;
 	vmovups		$inout5,-0x10($out)
 
 	vpshufb		($const),$Xi,$Xi	# .Lbswap_mask
-	vmovdqu		$Xi,-0x40($Xip)		# output Xi
+	vmovdqu		$Xi,(%r12)		# output Xi
 
 	vzeroupper
 ___
@@ -751,7 +762,7 @@ $code.=<<___
 	mov	%rsi, 0x18(%rbp)
 .seh_savereg	%rsi, 0xa8+5*8+0x18
 	mov	0x30(%rbp), $ivp
-	mov	0x38(%rbp), $Xip
+	mov	0x38(%rbp), $Htable
 	# Save non-volatile XMM registers.
 	movaps	%xmm6,-0xd0(%rbp)
 .seh_savexmm128	%xmm6, 0xa8+5*8-0xd0
@@ -826,8 +837,9 @@ $code.=<<___;
 
 	call		_aesni_ctr32_6x
 
-	vmovdqu		($Xip),$Xi		# load Xi
-	lea		0x20+0x20($Xip),$Xip	# size optimization
+	mov		$Xip_offset(%rbp), %r12
+	lea		0x20($Htable),$Htable	# size optimization
+	vmovdqu		(%r12),$Xi		# load Xi
 	sub		\$12,$len
 	mov		\$0x60*2,%rax
 	vpshufb		$Ii,$Xi,$Xi
@@ -835,9 +847,9 @@ $code.=<<___;
 	call		_aesni_ctr32_ghash_6x
 	vmovdqu		0x20(%rsp),$Z3		# I[5]
 	 vmovdqu	($const),$Ii		# borrow $Ii for .Lbswap_mask
-	vmovdqu		0x00-0x20($Xip),$Hkey	# $Hkey^1
+	vmovdqu		0x00-0x20($Htable),$Hkey	# $Hkey^1
 	vpunpckhqdq	$Z3,$Z3,$T1
-	vmovdqu		0x20-0x20($Xip),$rndkey	# borrow $rndkey for $HK
+	vmovdqu		0x20-0x20($Htable),$rndkey	# borrow $rndkey for $HK
 	 vmovups	$inout0,-0x60($out)	# save output
 	 vpshufb	$Ii,$inout0,$inout0	# but keep bswapped copy
 	vpxor		$Z3,$T1,$T1
@@ -857,7 +869,7 @@ ___
 
 $code.=<<___;
 	 vmovdqu	0x30(%rsp),$Z2		# I[4]
-	 vmovdqu	0x10-0x20($Xip),$Ii	# borrow $Ii for $Hkey^2
+	 vmovdqu	0x10-0x20($Htable),$Ii	# borrow $Ii for $Hkey^2
 	 vpunpckhqdq	$Z2,$Z2,$T2
 	vpclmulqdq	\$0x00,$Hkey,$Z3,$Z1
 	 vpxor		$Z2,$T2,$T2
@@ -866,19 +878,19 @@ $code.=<<___;
 
 	 vmovdqu	0x40(%rsp),$T3		# I[3]
 	vpclmulqdq	\$0x00,$Ii,$Z2,$Z0
-	 vmovdqu	0x30-0x20($Xip),$Hkey	# $Hkey^3
+	 vmovdqu	0x30-0x20($Htable),$Hkey	# $Hkey^3
 	vpxor		$Z1,$Z0,$Z0
 	 vpunpckhqdq	$T3,$T3,$Z1
 	vpclmulqdq	\$0x11,$Ii,$Z2,$Z2
 	 vpxor		$T3,$Z1,$Z1
 	vpxor		$Z3,$Z2,$Z2
 	vpclmulqdq	\$0x10,$HK,$T2,$T2
-	 vmovdqu	0x50-0x20($Xip),$HK
+	 vmovdqu	0x50-0x20($Htable),$HK
 	vpxor		$T1,$T2,$T2
 
 	 vmovdqu	0x50(%rsp),$T1		# I[2]
 	vpclmulqdq	\$0x00,$Hkey,$T3,$Z3
-	 vmovdqu	0x40-0x20($Xip),$Ii	# borrow $Ii for $Hkey^4
+	 vmovdqu	0x40-0x20($Htable),$Ii	# borrow $Ii for $Hkey^4
 	vpxor		$Z0,$Z3,$Z3
 	 vpunpckhqdq	$T1,$T1,$Z0
 	vpclmulqdq	\$0x11,$Hkey,$T3,$T3
@@ -889,19 +901,19 @@ $code.=<<___;
 
 	 vmovdqu	0x60(%rsp),$T2		# I[1]
 	vpclmulqdq	\$0x00,$Ii,$T1,$Z2
-	 vmovdqu	0x60-0x20($Xip),$Hkey	# $Hkey^5
+	 vmovdqu	0x60-0x20($Htable),$Hkey	# $Hkey^5
 	vpxor		$Z3,$Z2,$Z2
 	 vpunpckhqdq	$T2,$T2,$Z3
 	vpclmulqdq	\$0x11,$Ii,$T1,$T1
 	 vpxor		$T2,$Z3,$Z3
 	vpxor		$T3,$T1,$T1
 	vpclmulqdq	\$0x10,$HK,$Z0,$Z0
-	 vmovdqu	0x80-0x20($Xip),$HK
+	 vmovdqu	0x80-0x20($Htable),$HK
 	vpxor		$Z1,$Z0,$Z0
 
 	 vpxor		0x70(%rsp),$Xi,$Xi	# accumulate I[0]
 	vpclmulqdq	\$0x00,$Hkey,$T2,$Z1
-	 vmovdqu	0x70-0x20($Xip),$Ii	# borrow $Ii for $Hkey^6
+	 vmovdqu	0x70-0x20($Htable),$Ii	# borrow $Ii for $Hkey^6
 	 vpunpckhqdq	$Xi,$Xi,$T3
 	vpxor		$Z2,$Z1,$Z1
 	vpclmulqdq	\$0x11,$Hkey,$T2,$T2
@@ -911,17 +923,17 @@ $code.=<<___;
 	vpxor		$Z0,$Z3,$Z0
 
 	vpclmulqdq	\$0x00,$Ii,$Xi,$Z2
-	 vmovdqu	0x00-0x20($Xip),$Hkey	# $Hkey^1
+	 vmovdqu	0x00-0x20($Htable),$Hkey	# $Hkey^1
 	 vpunpckhqdq	$inout5,$inout5,$T1
 	vpclmulqdq	\$0x11,$Ii,$Xi,$Xi
 	 vpxor		$inout5,$T1,$T1
 	vpxor		$Z1,$Z2,$Z1
 	vpclmulqdq	\$0x10,$HK,$T3,$T3
-	 vmovdqu	0x20-0x20($Xip),$HK
+	 vmovdqu	0x20-0x20($Htable),$HK
 	vpxor		$T2,$Xi,$Z3
 	vpxor		$Z0,$T3,$Z2
 
-	 vmovdqu	0x10-0x20($Xip),$Ii	# borrow $Ii for $Hkey^2
+	 vmovdqu	0x10-0x20($Htable),$Ii	# borrow $Ii for $Hkey^2
 	  vpxor		$Z1,$Z3,$T3		# aggregated Karatsuba post-processing
 	vpclmulqdq	\$0x00,$Hkey,$inout5,$Z0
 	  vpxor		$T3,$Z2,$Z2
@@ -935,7 +947,7 @@ $code.=<<___;
 	  vpxor		$Z2,$Z3,$Z3
 
 	vpclmulqdq	\$0x00,$Ii,$inout4,$Z1
-	 vmovdqu	0x30-0x20($Xip),$Hkey	# $Hkey^3
+	 vmovdqu	0x30-0x20($Htable),$Hkey	# $Hkey^3
 	vpxor		$Z0,$Z1,$Z1
 	 vpunpckhqdq	$inout3,$inout3,$T3
 	vpclmulqdq	\$0x11,$Ii,$inout4,$inout4
@@ -943,11 +955,11 @@ $code.=<<___;
 	vpxor		$inout5,$inout4,$inout4
 	  vpalignr	\$8,$Xi,$Xi,$inout5	# 1st phase
 	vpclmulqdq	\$0x10,$HK,$T2,$T2
-	 vmovdqu	0x50-0x20($Xip),$HK
+	 vmovdqu	0x50-0x20($Htable),$HK
 	vpxor		$T1,$T2,$T2
 
 	vpclmulqdq	\$0x00,$Hkey,$inout3,$Z0
-	 vmovdqu	0x40-0x20($Xip),$Ii	# borrow $Ii for $Hkey^4
+	 vmovdqu	0x40-0x20($Htable),$Ii	# borrow $Ii for $Hkey^4
 	vpxor		$Z1,$Z0,$Z0
 	 vpunpckhqdq	$inout2,$inout2,$T1
 	vpclmulqdq	\$0x11,$Hkey,$inout3,$inout3
@@ -961,7 +973,7 @@ $code.=<<___;
 	  vxorps	$inout5,$Xi,$Xi
 
 	vpclmulqdq	\$0x00,$Ii,$inout2,$Z1
-	 vmovdqu	0x60-0x20($Xip),$Hkey	# $Hkey^5
+	 vmovdqu	0x60-0x20($Htable),$Hkey	# $Hkey^5
 	vpxor		$Z0,$Z1,$Z1
 	 vpunpckhqdq	$inout1,$inout1,$T2
 	vpclmulqdq	\$0x11,$Ii,$inout2,$inout2
@@ -969,7 +981,7 @@ $code.=<<___;
 	  vpalignr	\$8,$Xi,$Xi,$inout5	# 2nd phase
 	vpxor		$inout3,$inout2,$inout2
 	vpclmulqdq	\$0x10,$HK,$T1,$T1
-	 vmovdqu	0x80-0x20($Xip),$HK
+	 vmovdqu	0x80-0x20($Htable),$HK
 	vpxor		$T3,$T1,$T1
 
 	  vxorps	$Z3,$inout5,$inout5
@@ -977,7 +989,7 @@ $code.=<<___;
 	  vxorps	$inout5,$Xi,$Xi
 
 	vpclmulqdq	\$0x00,$Hkey,$inout1,$Z0
-	 vmovdqu	0x70-0x20($Xip),$Ii	# borrow $Ii for $Hkey^6
+	 vmovdqu	0x70-0x20($Htable),$Ii	# borrow $Ii for $Hkey^6
 	vpxor		$Z1,$Z0,$Z0
 	 vpunpckhqdq	$Xi,$Xi,$T3
 	vpclmulqdq	\$0x11,$Hkey,$inout1,$inout1
@@ -1012,8 +1024,9 @@ $code.=<<___;
 ___
 }
 $code.=<<___;
+	mov		$Xip_offset(%rbp), %r12
 	vpshufb		($const),$Xi,$Xi	# .Lbswap_mask
-	vmovdqu		$Xi,-0x40($Xip)		# output Xi
+	vmovdqu		$Xi,(%r12)		# output Xi
 
 	vzeroupper
 ___
diff --git a/crypto/fipsmodule/modes/gcm.c b/crypto/fipsmodule/modes/gcm.c
index b0a7c2ecf..1bf91bc4f 100644
--- a/crypto/fipsmodule/modes/gcm.c
+++ b/crypto/fipsmodule/modes/gcm.c
@@ -136,17 +136,13 @@ void gcm_init_ssse3(u128 Htable[16], const uint64_t H[2]) {
 static size_t hw_gcm_encrypt(const uint8_t *in, uint8_t *out, size_t len,
                              const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi,
                              const u128 Htable[16]) {
-  // TODO(davidben): |aesni_gcm_encrypt| accesses |Htable| but does so assuming
-  // it is a known offset from |Xi|.
-  return aesni_gcm_encrypt(in, out, len, key, ivec, Xi);
+  return aesni_gcm_encrypt(in, out, len, key, ivec, Htable, Xi);
 }
 
 static size_t hw_gcm_decrypt(const uint8_t *in, uint8_t *out, size_t len,
                              const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi,
                              const u128 Htable[16]) {
-  // TODO(davidben): |aesni_gcm_decrypt| accesses |Htable| but does so assuming
-  // it is a known offset from |Xi|.
-  return aesni_gcm_decrypt(in, out, len, key, ivec, Xi);
+  return aesni_gcm_decrypt(in, out, len, key, ivec, Htable, Xi);
 }
 #endif  // HW_GCM && X86_64
 
diff --git a/crypto/fipsmodule/modes/gcm_test.cc b/crypto/fipsmodule/modes/gcm_test.cc
index 81e9392bf..8c5ccc7bf 100644
--- a/crypto/fipsmodule/modes/gcm_test.cc
+++ b/crypto/fipsmodule/modes/gcm_test.cc
@@ -164,28 +164,21 @@ TEST(GCMTest, ABI) {
       if (hwaes_capable()) {
         AES_KEY aes_key;
         static const uint8_t kKey[16] = {0};
-
-        // aesni_gcm_* makes assumptions about |GCM128_CONTEXT|'s layout.
-        GCM128_CONTEXT gcm;
-        memset(&gcm, 0, sizeof(gcm));
-        memcpy(&gcm.gcm_key.H, kH, sizeof(kH));
-        memcpy(&gcm.gcm_key.Htable, Htable, sizeof(Htable));
-        memcpy(&gcm.Xi, X, sizeof(X));
         uint8_t iv[16] = {0};
 
         aes_hw_set_encrypt_key(kKey, 128, &aes_key);
         for (size_t blocks : kBlockCounts) {
           CHECK_ABI_SEH(aesni_gcm_encrypt, buf, buf, blocks * 16, &aes_key, iv,
-                        gcm.Xi.u);
+                        Htable, X);
           CHECK_ABI_SEH(aesni_gcm_encrypt, buf, buf, blocks * 16 + 7, &aes_key,
-                        iv, gcm.Xi.u);
+                        iv, Htable, X);
         }
         aes_hw_set_decrypt_key(kKey, 128, &aes_key);
         for (size_t blocks : kBlockCounts) {
           CHECK_ABI_SEH(aesni_gcm_decrypt, buf, buf, blocks * 16, &aes_key, iv,
-                        gcm.Xi.u);
+                        Htable, X);
           CHECK_ABI_SEH(aesni_gcm_decrypt, buf, buf, blocks * 16 + 7, &aes_key,
-                        iv, gcm.Xi.u);
+                        iv, Htable, X);
         }
       }
     }
diff --git a/crypto/fipsmodule/modes/internal.h b/crypto/fipsmodule/modes/internal.h
index ffff77fa8..9c40d220c 100644
--- a/crypto/fipsmodule/modes/internal.h
+++ b/crypto/fipsmodule/modes/internal.h
@@ -292,9 +292,11 @@ void gcm_ghash_avx512(uint64_t Xi[2], const u128 Htable[16], const uint8_t *in,
                       size_t len);
 #define HW_GCM
 size_t aesni_gcm_encrypt(const uint8_t *in, uint8_t *out, size_t len,
-                         const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi);
+                         const AES_KEY *key, uint8_t ivec[16],
+                         const u128 Htable[16], uint64_t *Xi);
 size_t aesni_gcm_decrypt(const uint8_t *in, uint8_t *out, size_t len,
-                         const AES_KEY *key, uint8_t ivec[16], uint64_t *Xi);
+                         const AES_KEY *key, uint8_t ivec[16],
+                         const u128 Htable[16], uint64_t *Xi);
 void gcm_setiv_avx512(const AES_KEY *key, const GCM128_CONTEXT *ctx,
                       const uint8_t *iv, size_t ivlen);
 void aes_gcm_encrypt_avx512(const AES_KEY *key, const GCM128_CONTEXT *ctx,
