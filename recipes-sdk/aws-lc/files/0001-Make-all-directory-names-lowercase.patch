From 1390c56c1cf2a13f082a8ecbbb568b27257f27fa Mon Sep 17 00:00:00 2001
From: John Harrison <jargh@amazon.com>
Date: Wed, 9 Jun 2021 18:08:17 -0700
Subject: [PATCH] Make all directory names lowercase

s2n-bignum original commit: https://github.com/awslabs/s2n-bignum/commit/e135d896544a0d63b7bed74675216d7f36d76aa8
---
 arm/fastmul/bignum_emontredc_8n.S | 386 +++++++++++++++
 arm/fastmul/bignum_kmul_16_32.S   | 784 ++++++++++++++++++++++++++++++
 arm/fastmul/bignum_ksqr_16_32.S   | 545 +++++++++++++++++++++
 arm/generic/bignum_ge.S           |  93 ++++
 arm/generic/bignum_mul.S          | 118 +++++
 arm/generic/bignum_optsub.S       |  74 +++
 arm/p384/Makefile                 |  58 +++
 arm/p384/bignum_add_p384.S        |  97 ++++
 arm/p384/bignum_amontmul_p384.S   | 423 ++++++++++++++++
 arm/p384/bignum_amontsqr_p384.S   | 356 ++++++++++++++
 arm/p384/bignum_cmul_p384.S       | 141 ++++++
 arm/p384/bignum_deamont_p384.S    | 146 ++++++
 arm/p384/bignum_demont_p384.S     | 117 +++++
 arm/p384/bignum_double_p384.S     |  91 ++++
 arm/p384/bignum_half_p384.S       |  89 ++++
 arm/p384/bignum_mod_n384.S        | 205 ++++++++
 arm/p384/bignum_mod_n384_6.S      |  95 ++++
 arm/p384/bignum_mod_p384.S        | 177 +++++++
 arm/p384/bignum_mod_p384_6.S      |  86 ++++
 arm/p384/bignum_montmul_p384.S    | 423 ++++++++++++++++
 arm/p384/bignum_montsqr_p384.S    | 380 +++++++++++++++
 arm/p384/bignum_neg_p384.S        |  87 ++++
 arm/p384/bignum_optneg_p384.S     | 101 ++++
 arm/p384/bignum_sub_p384.S        |  84 ++++
 arm/p384/bignum_tomont_p384.S     | 133 +++++
 arm/p384/bignum_triple_p384.S     | 130 +++++
 26 files changed, 5419 insertions(+)
 create mode 100644 arm/fastmul/bignum_emontredc_8n.S
 create mode 100644 arm/fastmul/bignum_kmul_16_32.S
 create mode 100644 arm/fastmul/bignum_ksqr_16_32.S
 create mode 100644 arm/generic/bignum_ge.S
 create mode 100644 arm/generic/bignum_mul.S
 create mode 100644 arm/generic/bignum_optsub.S
 create mode 100644 arm/p384/Makefile
 create mode 100644 arm/p384/bignum_add_p384.S
 create mode 100644 arm/p384/bignum_amontmul_p384.S
 create mode 100644 arm/p384/bignum_amontsqr_p384.S
 create mode 100644 arm/p384/bignum_cmul_p384.S
 create mode 100644 arm/p384/bignum_deamont_p384.S
 create mode 100644 arm/p384/bignum_demont_p384.S
 create mode 100644 arm/p384/bignum_double_p384.S
 create mode 100644 arm/p384/bignum_half_p384.S
 create mode 100644 arm/p384/bignum_mod_n384.S
 create mode 100644 arm/p384/bignum_mod_n384_6.S
 create mode 100644 arm/p384/bignum_mod_p384.S
 create mode 100644 arm/p384/bignum_mod_p384_6.S
 create mode 100644 arm/p384/bignum_montmul_p384.S
 create mode 100644 arm/p384/bignum_montsqr_p384.S
 create mode 100644 arm/p384/bignum_neg_p384.S
 create mode 100644 arm/p384/bignum_optneg_p384.S
 create mode 100644 arm/p384/bignum_sub_p384.S
 create mode 100644 arm/p384/bignum_tomont_p384.S
 create mode 100644 arm/p384/bignum_triple_p384.S

diff --git a/arm/fastmul/bignum_emontredc_8n.S b/arm/fastmul/bignum_emontredc_8n.S
new file mode 100644
index 000000000..426bd73db
--- /dev/null
+++ b/arm/fastmul/bignum_emontredc_8n.S
@@ -0,0 +1,386 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Extended Montgomery reduce in 8-digit blocks, results in input-output buffer
+// Inputs z[2*k], m[k], w; outputs function return (extra result bit) and z[2*k]
+//
+//    extern uint64_t bignum_emontredc_8n
+//     (uint64_t k, uint64_t *z, uint64_t *m, uint64_t w);
+//
+// Functionally equivalent to bignum_emontredc (see that file for more detail).
+// But in general assumes that the input k is a multiple of 8.
+//
+// Standard ARM ABI: X0 = k, X1 = z, X2 = m, X3 = w, returns X0
+// ----------------------------------------------------------------------------
+
+.globl bignum_emontredc_8n
+
+// ---------------------------------------------------------------------------
+// Macro computing (c,h,l) = 3-word 1s complement (x - y) * (w - z)
+// and adding it with carry to (b,a) so that we have CF+c in the 2 position.
+//
+// c,h,l,t should all be different
+// t,h should not overlap w,z
+// ---------------------------------------------------------------------------
+
+.macro muldiffnadd b,a, c,h,l,t, x,y, w,z
+        subs    \t, \x, \y
+        cneg    \t, \t, cc
+        csetm   \c, cc
+        subs    \h, \w, \z
+        cneg    \h, \h, cc
+        mul     \l, \t, \h
+        umulh   \h, \t, \h
+        cinv    \c, \c, cc
+        adds    xzr, \c, 1
+        eor     \l, \l, \c
+        adcs    \a, \a, \l
+        eor     \h, \h, \c
+        adcs    \b, \b, \h
+.endm
+
+// The inputs, though k gets processed so we use a different name
+
+#define k4m1 x0
+#define z x1
+#define m x2
+#define w x3
+
+// Additional variables
+
+#define a0 x4
+#define a1 x5
+#define a2 x6
+#define a3 x7
+
+#define b0 x8
+#define b1 x9
+#define b2 x10
+#define b3 x11
+
+#define c0 x12
+#define c1 x13
+#define c2 x14
+#define c3 x15
+#define c4 x16
+
+#define u0 x17
+#define u1 x19
+#define u2 x20
+#define u3 x21
+#define u4 x22
+#define u5 x23
+#define u6 x24
+#define u7 x25
+
+// These temp registers are aliased to c0..c3, which is safe here
+
+#define c x12
+#define h x13
+#define l x14
+#define t x15
+
+// Loop counters, outer and inner
+
+#define i x26
+#define j x27
+
+// Top carry, eventually returned when aligned with top
+// It's maintained as a bitmask since this seems slightly easier(?)
+
+#define tc x28
+
+// -----------------------------------------------------------------------
+// The basic 4x4->8 multiply-add block, which does in ADK style (10 muls):
+//
+// [c3;c2;c1;c0;z_3;z_2;z_1;z_0] :=
+//   [a3;a2;a1;a0] * [b3;b2;b1;b0] + [c3;c2;c1;c0] + [z_3;z_2;z_1;z_0]
+// -----------------------------------------------------------------------
+
+.macro madd4
+                mul     u0, a0, b0
+                mul     u4, a1, b1
+                mul     u5, a2, b2
+                mul     u6, a3, b3
+
+// Accumulate the simple products as [u7,u6,u5,u4,u0]
+
+                umulh   c4, a0, b0
+                adds    u4, u4, c4
+                umulh   c4, a1, b1
+                adcs    u5, u5, c4
+                umulh   c4, a2, b2
+                adcs    u6, u6, c4
+                umulh   c4, a3, b3
+                adc     u7, c4, xzr
+
+// Add up the carry-in and the existing z contents
+
+                ldp     u2, u3, [z]
+                adds    c0, c0, u2
+                adcs    c1, c1, u3
+                ldp     u2, u3, [z, 16]
+                adcs    c2, c2, u2
+                adcs    c3, c3, u3
+                adc     c4, xzr, xzr
+
+// Multiply by B + 1 to get [u7;u6;u5;u4;u1;u0]
+
+                adds    u1, u4, u0
+                adcs    u4, u5, u4
+                adcs    u5, u6, u5
+                adcs    u6, u7, u6
+                adc     u7, xzr, u7
+
+// Multiply by B^2 + 1 to get [u6;u5;u4;u3;u2;u1;u0]
+
+                adds    u2, u4, u0
+                adcs    u3, u5, u1
+                adcs    u4, u6, u4
+                adcs    u5, u7, u5
+                adcs    u6, xzr, u6
+                adc     u7, xzr, u7
+
+// Add in the carry-in and original z contents
+
+                adds    u0, u0, c0
+                adcs    u1, u1, c1
+                adcs    u2, u2, c2
+                adcs    u3, u3, c3
+                adcs    u4, u4, c4
+                adcs    u5, u5, xzr
+                adcs    u6, u6, xzr
+                adc     u7, u7, xzr
+
+// Now add in all the "complicated" terms.
+
+                muldiffnadd u6,u5, c,h,l,t, a2,a3, b3,b2
+                adc     u7, u7, c
+
+                muldiffnadd u2,u1, c,h,l,t, a0,a1, b1,b0
+                adcs    u3, u3, c
+                adcs    u4, u4, c
+                adcs    u5, u5, c
+                adcs    u6, u6, c
+                adc     u7, u7, c
+
+                muldiffnadd u5,u4, c,h,l,t, a1,a3, b3,b1
+                adcs    u6, u6, c
+                adc     u7, u7, c
+
+                muldiffnadd u3,u2, c,h,l,t, a0,a2, b2,b0
+                adcs    u4, u4, c
+                adcs    u5, u5, c
+                adcs    u6, u6, c
+                adc     u7, u7, c
+
+                muldiffnadd u4,u3, c,h,l,t, a0,a3, b3,b0
+                adcs    u5, u5, c
+                adcs    u6, u6, c
+                adc     u7, u7, c
+                muldiffnadd u4,u3, c,h,l,t, a1,a2, b2,b1
+                adcs    c1, u5, c
+                adcs    c2, u6, c
+                adc     c3, u7, c
+                mov     c0, u4
+
+                stp     u0, u1, [z]
+                stp     u2, u3, [z, 16]
+.endm
+
+// *****************************************************
+// Main code
+// *****************************************************
+
+bignum_emontredc_8n:
+
+                stp     x19, x20, [sp, -16]!
+                stp     x21, x22, [sp, -16]!
+                stp     x23, x24, [sp, -16]!
+                stp     x25, x26, [sp, -16]!
+                stp     x27, x28, [sp, -16]!
+
+// Set up (k/4 - 1)<<5 which is used as inner count and pointer fixup
+// ns i = k/4 as the outer loop count.
+// At this point skip everything if k/4 = 0, returning our x0 = 0 value
+
+                lsr     k4m1, x0, 2
+                mov     i, k4m1
+                subs    c, k4m1, 1
+                bcc     end
+                mov     tc, xzr
+                lsl     k4m1, c, 5
+
+// Outer loop, one digit of Montgomery reduction adding in word * m.
+// Rather than propagating the carry to the end each time, we
+// stop at the "natural" end and store top carry in tc as a bitmask.
+
+outerloop:
+
+// Load [u3;u2;u1;u0] = bottom 4 digits of the input at current window
+
+                ldp     u0, u1, [z]
+                ldp     u2, u3, [z, 16]
+
+// Load the bottom 4 digits of m
+
+                ldp     b0, b1, [m]
+                ldp     b2, b3, [m, 16]
+
+// Montgomery step 0
+
+                mul     a0, u0, w
+                mul     c0, a0, b0
+                mul     c1, a0, b1
+                mul     c2, a0, b2
+                mul     c3, a0, b3
+                adds    u0, u0, c0
+                umulh   c0, a0, b0
+                adcs    u1, u1, c1
+                umulh   c1, a0, b1
+                adcs    u2, u2, c2
+                umulh   c2, a0, b2
+                adcs    u3, u3, c3
+                umulh   c3, a0, b3
+                adc     u4, xzr, xzr
+                adds    u1, u1, c0
+                adcs    u2, u2, c1
+                adcs    u3, u3, c2
+                adc     u4, u4, c3
+
+// Montgomery step 1
+
+                mul     a1, u1, w
+                mul     c0, a1, b0
+                mul     c1, a1, b1
+                mul     c2, a1, b2
+                mul     c3, a1, b3
+                adds    u1, u1, c0
+                umulh   c0, a1, b0
+                adcs    u2, u2, c1
+                umulh   c1, a1, b1
+                adcs    u3, u3, c2
+                umulh   c2, a1, b2
+                adcs    u4, u4, c3
+                umulh   c3, a1, b3
+                adc     u5, xzr, xzr
+                adds    u2, u2, c0
+                adcs    u3, u3, c1
+                adcs    u4, u4, c2
+                adc     u5, u5, c3
+
+// Montgomery step 2
+
+                mul     a2, u2, w
+                mul     c0, a2, b0
+                mul     c1, a2, b1
+                mul     c2, a2, b2
+                mul     c3, a2, b3
+                adds    u2, u2, c0
+                umulh   c0, a2, b0
+                adcs    u3, u3, c1
+                umulh   c1, a2, b1
+                adcs    u4, u4, c2
+                umulh   c2, a2, b2
+                adcs    u5, u5, c3
+                umulh   c3, a2, b3
+                adc     u6, xzr, xzr
+                adds    u3, u3, c0
+                adcs    u4, u4, c1
+                adcs    u5, u5, c2
+                adc     u6, u6, c3
+
+// Montgomery step 3. In the last four instructions we put the top in
+// the carry variables expected by the "madd" block next, which is why
+// the pattern is slightly different.
+
+                mul     a3, u3, w
+                mul     c0, a3, b0
+                mul     c1, a3, b1
+                mul     c2, a3, b2
+                mul     c3, a3, b3
+                adds    u3, u3, c0
+                umulh   c0, a3, b0
+                adcs    u4, u4, c1
+                umulh   c1, a3, b1
+                adcs    u5, u5, c2
+                umulh   c2, a3, b2
+                adcs    u6, u6, c3
+                umulh   c3, a3, b3
+                adc     u7, xzr, xzr
+                adds    c0, u4, c0
+                adcs    c1, u5, c1
+                adcs    c2, u6, c2
+                adc     c3, u7, c3
+
+// Stash the multipliers as expected by the bignum_emontredc interface
+// We don't use these ourselves again though; they stay in [a3;a2;a1;a0]
+
+                stp     a0, a1, [z]
+                stp     a2, a3, [z, 16]
+
+// Repeated multiply-add block to do the k/4-1 remaining 4-digit chunks
+
+                cbz     k4m1, madddone
+                mov     j, k4m1
+maddloop:
+                add     m, m, 32
+                add     z, z, 32
+
+                ldp     b0, b1, [m]
+                ldp     b2, b3, [m, 16]
+                madd4
+                subs    j, j, 32
+                bne     maddloop
+madddone:
+
+// Add the carry out to the existing z contents, propagating the
+// top carry tc up by 32 places as we move "leftwards".
+
+                ldp     u0, u1, [z, 32]
+                ldp     u2, u3, [z, 48]
+                adds    xzr, tc, tc
+                adcs    u0, u0, c0
+                adcs    u1, u1, c1
+                adcs    u2, u2, c2
+                adcs    u3, u3, c3
+                csetm   tc, cs
+                stp     u0, u1, [z, 32]
+                stp     u2, u3, [z, 48]
+
+// Compensate for the repeated bumps in m and z in the inner loop
+
+                sub     z, z, k4m1
+                sub     m, m, k4m1
+
+// Bump up z only and keep going
+
+                add     z, z, 32
+                subs    i, i, 1
+                bne     outerloop
+
+// Return the top carry as 0 or 1 (it's currently a bitmask)
+
+                neg     x0, tc
+
+end:
+                ldp     x27, x28, [sp], 16
+                ldp     x25, x26, [sp], 16
+                ldp     x23, x24, [sp], 16
+                ldp     x21, x22, [sp], 16
+                ldp     x19, x20, [sp], 16
+
+                ret
diff --git a/arm/fastmul/bignum_kmul_16_32.S b/arm/fastmul/bignum_kmul_16_32.S
new file mode 100644
index 000000000..7da0dc558
--- /dev/null
+++ b/arm/fastmul/bignum_kmul_16_32.S
@@ -0,0 +1,784 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Multiply z := x * y
+// Inputs x[16], y[16]; output z[32]; temporary buffer t[>=32]
+//
+//    extern void bignum_kmul_16_32
+//     (uint64_t z[static 32], uint64_t x[static 16], uint64_t y[static 16],
+//      uint64_t t[static 32])
+//
+// This is a Karatsuba-style function multiplying half-sized results
+// internally and using temporary buffer t for intermediate results.
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = y, X3 = t
+// ----------------------------------------------------------------------------
+
+.globl bignum_kmul_16_32
+
+// Subroutine-safe copies of the output, inputs and temporary buffer pointers
+
+#define z x25
+#define x x26
+#define y x27
+#define t x28
+
+// More variables for sign masks, with s also necessarily subroutine-safe
+
+#define s x29
+#define m x19
+
+bignum_kmul_16_32:
+
+// Save registers, including return address
+
+        stp     x19, x20, [sp, -16]!
+        stp     x21, x22, [sp, -16]!
+        stp     x23, x24, [sp, -16]!
+        stp     x25, x26, [sp, -16]!
+        stp     x27, x28, [sp, -16]!
+        stp     x29, x30, [sp, -16]!
+
+// Move parameters into subroutine-safe places
+
+        mov     z, x0
+        mov     x, x1
+        mov     y, x2
+        mov     t, x3
+
+// Compute L = x_lo * y_lo in bottom half of buffer (size 8 x 8 -> 16)
+
+        bl      local_mul_8_16
+
+// Compute absolute difference [t..] = |x_lo - x_hi|
+// and the sign s = sgn(x_lo - x_hi) as a bitmask (all 1s for negative)
+
+        ldp     x10, x11, [x, 0]
+        ldp     x8, x9, [x, 64]
+        subs    x10, x10, x8
+        sbcs    x11, x11, x9
+        ldp     x12, x13, [x, 16]
+        ldp     x8, x9, [x, 80]
+        sbcs    x12, x12, x8
+        sbcs    x13, x13, x9
+        ldp     x14, x15, [x, 32]
+        ldp     x8, x9, [x, 96]
+        sbcs    x14, x14, x8
+        sbcs    x15, x15, x9
+        ldp     x16, x17, [x, 48]
+        ldp     x8, x9, [x, 112]
+        sbcs    x16, x16, x8
+        sbcs    x17, x17, x9
+        csetm   s, cc
+        adds    xzr, s, s
+        eor     x10, x10, s
+        adcs    x10, x10, xzr
+        eor     x11, x11, s
+        adcs    x11, x11, xzr
+        stp     x10, x11, [t]
+        eor     x12, x12, s
+        adcs    x12, x12, xzr
+        eor     x13, x13, s
+        adcs    x13, x13, xzr
+        stp     x12, x13, [t, 16]
+        eor     x14, x14, s
+        adcs    x14, x14, xzr
+        eor     x15, x15, s
+        adcs    x15, x15, xzr
+        stp     x14, x15, [t, 32]
+        eor     x16, x16, s
+        adcs    x16, x16, xzr
+        eor     x17, x17, s
+        adcs    x17, x17, xzr
+        stp     x16, x17, [t, 48]
+
+// Compute H = x_hi * y_hi in top half of buffer (size 8 x 8 -> 16)
+
+        add     x0, z, 128
+        add     x1, x, 64
+        add     x2, y, 64
+        bl      local_mul_8_16
+
+// Compute the other absolute difference [t+8..] = |y_hi - y_lo|
+// Collect the combined product sign bitmask (all 1s for negative) in s
+
+        ldp     x10, x11, [y, 0]
+        ldp     x8, x9, [y, 64]
+        subs    x10, x8, x10
+        sbcs    x11, x9, x11
+        ldp     x12, x13, [y, 16]
+        ldp     x8, x9, [y, 80]
+        sbcs    x12, x8, x12
+        sbcs    x13, x9, x13
+        ldp     x14, x15, [y, 32]
+        ldp     x8, x9, [y, 96]
+        sbcs    x14, x8, x14
+        sbcs    x15, x9, x15
+        ldp     x16, x17, [y, 48]
+        ldp     x8, x9, [y, 112]
+        sbcs    x16, x8, x16
+        sbcs    x17, x9, x17
+        csetm   m, cc
+        adds    xzr, m, m
+        eor     x10, x10, m
+        adcs    x10, x10, xzr
+        eor     x11, x11, m
+        adcs    x11, x11, xzr
+        stp     x10, x11, [t, 64]
+        eor     x12, x12, m
+        adcs    x12, x12, xzr
+        eor     x13, x13, m
+        adcs    x13, x13, xzr
+        stp     x12, x13, [t, 80]
+        eor     x14, x14, m
+        adcs    x14, x14, xzr
+        eor     x15, x15, m
+        adcs    x15, x15, xzr
+        stp     x14, x15, [t, 96]
+        eor     x16, x16, m
+        adcs    x16, x16, xzr
+        eor     x17, x17, m
+        adcs    x17, x17, xzr
+        stp     x16, x17, [t, 112]
+        eor     s, s, m
+
+// Compute H' = H + L_top in place of H (it cannot overflow)
+// First add 8-sized block then propagate carry through next 8
+
+        .set    i, 0
+
+        ldp     x10, x11, [z, 128+8*i]
+        ldp     x12, x13, [z, 64+8*i]
+        adds    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, 128+8*i]
+        .set    i, (i+2)
+
+.rep 3
+        ldp     x10, x11, [z, 128+8*i]
+        ldp     x12, x13, [z, 64+8*i]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, 128+8*i]
+        .set    i, (i+2)
+.endr
+
+.rep 4
+        ldp     x10, x11, [z, 128+8*i]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [z, 128+8*i]
+        .set    i, (i+2)
+.endr
+
+// Compute M = |x_lo - x_hi| * |y_hi - y_lo| in [t+16...], size 16
+
+        add     x0, t, 128
+        mov     x1, t
+        add     x2, t, 64
+        bl      local_mul_8_16
+
+// Add the interlocking H' and L_bot terms, storing in registers x15..x0
+// Intercept the carry at the 8 + 16 = 24 position and store it in x.
+// (Note that we no longer need the input x was pointing at.)
+
+        ldp     x0, x1, [z]
+        ldp     x16, x17, [z, 128]
+        adds    x0, x0, x16
+        adcs    x1, x1, x17
+        ldp     x2, x3, [z, 16]
+        ldp     x16, x17, [z, 144]
+        adcs    x2, x2, x16
+        adcs    x3, x3, x17
+        ldp     x4, x5, [z, 32]
+        ldp     x16, x17, [z, 160]
+        adcs    x4, x4, x16
+        adcs    x5, x5, x17
+        ldp     x6, x7, [z, 48]
+        ldp     x16, x17, [z, 176]
+        adcs    x6, x6, x16
+        adcs    x7, x7, x17
+        ldp     x8, x9, [z, 128]
+        ldp     x16, x17, [z, 192]
+        adcs    x8, x8, x16
+        adcs    x9, x9, x17
+        ldp     x10, x11, [z, 144]
+        ldp     x16, x17, [z, 208]
+        adcs    x10, x10, x16
+        adcs    x11, x11, x17
+        ldp     x12, x13, [z, 160]
+        ldp     x16, x17, [z, 224]
+        adcs    x12, x12, x16
+        adcs    x13, x13, x17
+        ldp     x14, x15, [z, 176]
+        ldp     x16, x17, [z, 240]
+        adcs    x14, x14, x16
+        adcs    x15, x15, x17
+
+        cset    x, cs
+
+// Add the sign-adjusted mid-term cross product M
+
+        cmn     s, s
+
+        ldp     x16, x17, [t, 128]
+        eor     x16, x16, s
+        adcs    x0, x0, x16
+        eor     x17, x17, s
+        adcs    x1, x1, x17
+        stp     x0, x1, [z, 64]
+        ldp     x16, x17, [t, 144]
+        eor     x16, x16, s
+        adcs    x2, x2, x16
+        eor     x17, x17, s
+        adcs    x3, x3, x17
+        stp     x2, x3, [z, 80]
+        ldp     x16, x17, [t, 160]
+        eor     x16, x16, s
+        adcs    x4, x4, x16
+        eor     x17, x17, s
+        adcs    x5, x5, x17
+        stp     x4, x5, [z, 96]
+        ldp     x16, x17, [t, 176]
+        eor     x16, x16, s
+        adcs    x6, x6, x16
+        eor     x17, x17, s
+        adcs    x7, x7, x17
+        stp     x6, x7, [z, 112]
+        ldp     x16, x17, [t, 192]
+        eor     x16, x16, s
+        adcs    x8, x8, x16
+        eor     x17, x17, s
+        adcs    x9, x9, x17
+        stp     x8, x9, [z, 128]
+        ldp     x16, x17, [t, 208]
+        eor     x16, x16, s
+        adcs    x10, x10, x16
+        eor     x17, x17, s
+        adcs    x11, x11, x17
+        stp     x10, x11, [z, 144]
+        ldp     x16, x17, [t, 224]
+        eor     x16, x16, s
+        adcs    x12, x12, x16
+        eor     x17, x17, s
+        adcs    x13, x13, x17
+        stp     x12, x13, [z, 160]
+        ldp     x16, x17, [t, 240]
+        eor     x16, x16, s
+        adcs    x14, x14, x16
+        eor     x17, x17, s
+        adcs    x15, x15, x17
+        stp     x14, x15, [z, 176]
+
+// Get the next digits effectively resulting so far starting at 24
+
+        adcs    y, s, x
+        adc     t, s, xzr
+
+// Now the final 8 digits of padding; the first one is special in using y
+// and also in getting the carry chain started
+
+        ldp     x10, x11, [z, 192]
+        adds    x10, x10, y
+        adcs    x11, x11, t
+        stp     x10, x11, [z, 192]
+        ldp     x10, x11, [z, 208]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, 208]
+        ldp     x10, x11, [z, 224]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, 224]
+        ldp     x10, x11, [z, 240]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, 240]
+
+// Restore registers and return
+
+        ldp     x29, x30, [sp], 16
+        ldp     x27, x28, [sp], 16
+        ldp     x25, x26, [sp], 16
+        ldp     x23, x24, [sp], 16
+        ldp     x21, x22, [sp], 16
+        ldp     x19, x20, [sp], 16
+
+        ret
+
+// -----------------------------------------------------------------------
+// Local copy of bignum_mul_8_16 without the scratch register save/restore
+// -----------------------------------------------------------------------
+
+local_mul_8_16:
+        ldp     x3, x4, [x1]
+        ldp     x7, x8, [x2]
+        ldp     x5, x6, [x1, 16]
+        ldp     x9, x10, [x2, 16]
+        mul     x11, x3, x7
+        mul     x15, x4, x8
+        mul     x16, x5, x9
+        mul     x17, x6, x10
+        umulh   x19, x3, x7
+        adds    x15, x15, x19
+        umulh   x19, x4, x8
+        adcs    x16, x16, x19
+        umulh   x19, x5, x9
+        adcs    x17, x17, x19
+        umulh   x19, x6, x10
+        adc     x19, x19, xzr
+        adds    x12, x15, x11
+        adcs    x15, x16, x15
+        adcs    x16, x17, x16
+        adcs    x17, x19, x17
+        adc     x19, xzr, x19
+        adds    x13, x15, x11
+        adcs    x14, x16, x12
+        adcs    x15, x17, x15
+        adcs    x16, x19, x16
+        adcs    x17, xzr, x17
+        adc     x19, xzr, x19
+        subs    x24, x5, x6
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x10, x9
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x16, x16, x22
+        eor     x21, x21, x20
+        adcs    x17, x17, x21
+        adc     x19, x19, x20
+        subs    x24, x3, x4
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x8, x7
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x12, x12, x22
+        eor     x21, x21, x20
+        adcs    x13, x13, x21
+        adcs    x14, x14, x20
+        adcs    x15, x15, x20
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x4, x6
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x10, x8
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x15, x15, x22
+        eor     x21, x21, x20
+        adcs    x16, x16, x21
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x3, x5
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x9, x7
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x13, x13, x22
+        eor     x21, x21, x20
+        adcs    x14, x14, x21
+        adcs    x15, x15, x20
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x3, x6
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x10, x7
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x14, x14, x22
+        eor     x21, x21, x20
+        adcs    x15, x15, x21
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x4, x5
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x9, x8
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x14, x14, x22
+        eor     x21, x21, x20
+        adcs    x15, x15, x21
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        ldp     x3, x4, [x1, 32]
+        stp     x11, x12, [x0]
+        ldp     x7, x8, [x2, 32]
+        stp     x13, x14, [x0, 16]
+        ldp     x5, x6, [x1, 48]
+        stp     x15, x16, [x0, 32]
+        ldp     x9, x10, [x2, 48]
+        stp     x17, x19, [x0, 48]
+        mul     x11, x3, x7
+        mul     x15, x4, x8
+        mul     x16, x5, x9
+        mul     x17, x6, x10
+        umulh   x19, x3, x7
+        adds    x15, x15, x19
+        umulh   x19, x4, x8
+        adcs    x16, x16, x19
+        umulh   x19, x5, x9
+        adcs    x17, x17, x19
+        umulh   x19, x6, x10
+        adc     x19, x19, xzr
+        adds    x12, x15, x11
+        adcs    x15, x16, x15
+        adcs    x16, x17, x16
+        adcs    x17, x19, x17
+        adc     x19, xzr, x19
+        adds    x13, x15, x11
+        adcs    x14, x16, x12
+        adcs    x15, x17, x15
+        adcs    x16, x19, x16
+        adcs    x17, xzr, x17
+        adc     x19, xzr, x19
+        ldp     x22, x21, [x0, 32]
+        adds    x11, x11, x22
+        adcs    x12, x12, x21
+        ldp     x22, x21, [x0, 48]
+        adcs    x13, x13, x22
+        adcs    x14, x14, x21
+        adcs    x15, x15, xzr
+        adcs    x16, x16, xzr
+        adcs    x17, x17, xzr
+        adc     x19, x19, xzr
+        subs    x24, x5, x6
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x10, x9
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x16, x16, x22
+        eor     x21, x21, x20
+        adcs    x17, x17, x21
+        adc     x19, x19, x20
+        subs    x24, x3, x4
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x8, x7
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x12, x12, x22
+        eor     x21, x21, x20
+        adcs    x13, x13, x21
+        adcs    x14, x14, x20
+        adcs    x15, x15, x20
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x4, x6
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x10, x8
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x15, x15, x22
+        eor     x21, x21, x20
+        adcs    x16, x16, x21
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x3, x5
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x9, x7
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x13, x13, x22
+        eor     x21, x21, x20
+        adcs    x14, x14, x21
+        adcs    x15, x15, x20
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x3, x6
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x10, x7
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x14, x14, x22
+        eor     x21, x21, x20
+        adcs    x15, x15, x21
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x4, x5
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x9, x8
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x14, x14, x22
+        eor     x21, x21, x20
+        adcs    x15, x15, x21
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        ldp     x22, x21, [x1]
+        subs    x3, x3, x22
+        sbcs    x4, x4, x21
+        ldp     x22, x21, [x1, 16]
+        sbcs    x5, x5, x22
+        sbcs    x6, x6, x21
+        csetm   x24, cc
+        stp     x11, x12, [x0, 64]
+        ldp     x22, x21, [x2]
+        subs    x7, x22, x7
+        sbcs    x8, x21, x8
+        ldp     x22, x21, [x2, 16]
+        sbcs    x9, x22, x9
+        sbcs    x10, x21, x10
+        csetm   x1, cc
+        stp     x13, x14, [x0, 80]
+        eor     x3, x3, x24
+        subs    x3, x3, x24
+        eor     x4, x4, x24
+        sbcs    x4, x4, x24
+        eor     x5, x5, x24
+        sbcs    x5, x5, x24
+        eor     x6, x6, x24
+        sbc     x6, x6, x24
+        stp     x15, x16, [x0, 96]
+        eor     x7, x7, x1
+        subs    x7, x7, x1
+        eor     x8, x8, x1
+        sbcs    x8, x8, x1
+        eor     x9, x9, x1
+        sbcs    x9, x9, x1
+        eor     x10, x10, x1
+        sbc     x10, x10, x1
+        stp     x17, x19, [x0, 112]
+        eor     x1, x1, x24
+        mul     x11, x3, x7
+        mul     x15, x4, x8
+        mul     x16, x5, x9
+        mul     x17, x6, x10
+        umulh   x19, x3, x7
+        adds    x15, x15, x19
+        umulh   x19, x4, x8
+        adcs    x16, x16, x19
+        umulh   x19, x5, x9
+        adcs    x17, x17, x19
+        umulh   x19, x6, x10
+        adc     x19, x19, xzr
+        adds    x12, x15, x11
+        adcs    x15, x16, x15
+        adcs    x16, x17, x16
+        adcs    x17, x19, x17
+        adc     x19, xzr, x19
+        adds    x13, x15, x11
+        adcs    x14, x16, x12
+        adcs    x15, x17, x15
+        adcs    x16, x19, x16
+        adcs    x17, xzr, x17
+        adc     x19, xzr, x19
+        subs    x24, x5, x6
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x10, x9
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x16, x16, x22
+        eor     x21, x21, x20
+        adcs    x17, x17, x21
+        adc     x19, x19, x20
+        subs    x24, x3, x4
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x8, x7
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x12, x12, x22
+        eor     x21, x21, x20
+        adcs    x13, x13, x21
+        adcs    x14, x14, x20
+        adcs    x15, x15, x20
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x4, x6
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x10, x8
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x15, x15, x22
+        eor     x21, x21, x20
+        adcs    x16, x16, x21
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x3, x5
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x9, x7
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x13, x13, x22
+        eor     x21, x21, x20
+        adcs    x14, x14, x21
+        adcs    x15, x15, x20
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x3, x6
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x10, x7
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x14, x14, x22
+        eor     x21, x21, x20
+        adcs    x15, x15, x21
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        subs    x24, x4, x5
+        cneg    x24, x24, cc
+        csetm   x20, cc
+        subs    x21, x9, x8
+        cneg    x21, x21, cc
+        mul     x22, x24, x21
+        umulh   x21, x24, x21
+        cinv    x20, x20, cc
+        cmn     x20, 0x1
+        eor     x22, x22, x20
+        adcs    x14, x14, x22
+        eor     x21, x21, x20
+        adcs    x15, x15, x21
+        adcs    x16, x16, x20
+        adcs    x17, x17, x20
+        adc     x19, x19, x20
+        ldp     x3, x4, [x0]
+        ldp     x7, x8, [x0, 64]
+        adds    x3, x3, x7
+        adcs    x4, x4, x8
+        ldp     x5, x6, [x0, 16]
+        ldp     x9, x10, [x0, 80]
+        adcs    x5, x5, x9
+        adcs    x6, x6, x10
+        ldp     x20, x21, [x0, 96]
+        adcs    x7, x7, x20
+        adcs    x8, x8, x21
+        ldp     x22, x23, [x0, 112]
+        adcs    x9, x9, x22
+        adcs    x10, x10, x23
+        adcs    x24, x1, xzr
+        adc     x2, x1, xzr
+        cmn     x1, 0x1
+        eor     x11, x11, x1
+        adcs    x3, x11, x3
+        eor     x12, x12, x1
+        adcs    x4, x12, x4
+        eor     x13, x13, x1
+        adcs    x5, x13, x5
+        eor     x14, x14, x1
+        adcs    x6, x14, x6
+        eor     x15, x15, x1
+        adcs    x7, x15, x7
+        eor     x16, x16, x1
+        adcs    x8, x16, x8
+        eor     x17, x17, x1
+        adcs    x9, x17, x9
+        eor     x19, x19, x1
+        adcs    x10, x19, x10
+        adcs    x20, x20, x24
+        adcs    x21, x21, x2
+        adcs    x22, x22, x2
+        adc     x23, x23, x2
+        stp     x3, x4, [x0, 32]
+        stp     x5, x6, [x0, 48]
+        stp     x7, x8, [x0, 64]
+        stp     x9, x10, [x0, 80]
+        stp     x20, x21, [x0, 96]
+        stp     x22, x23, [x0, 112]
+        ret
diff --git a/arm/fastmul/bignum_ksqr_16_32.S b/arm/fastmul/bignum_ksqr_16_32.S
new file mode 100644
index 000000000..2ea51ee85
--- /dev/null
+++ b/arm/fastmul/bignum_ksqr_16_32.S
@@ -0,0 +1,545 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Square, z := x^2
+// Input x[16]; output z[32]; temporary buffer t[>=24]
+//
+//    extern void bignum_ksqr_16_32
+//     (uint64_t z[static 32], uint64_t x[static 16], uint64_t t[static 24]);
+//
+// This is a Karatsuba-style function squaring half-sized results
+// and using temporary buffer t for intermediate results.
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = t
+// ----------------------------------------------------------------------------
+
+// Subroutine-safe copies of the output, inputs and temporary buffer pointers
+
+#define z x23
+#define x x24
+#define t x25
+
+// More variables for sign masks, with s also necessarily subroutine-safe
+
+#define s x19
+
+.globl bignum_ksqr_16_32
+
+bignum_ksqr_16_32:
+
+// Save registers, including return address
+
+        stp     x19, x20, [sp, -16]!
+        stp     x21, x22, [sp, -16]!
+        stp     x23, x24, [sp, -16]!
+        stp     x25, x30, [sp, -16]!
+
+// Move parameters into subroutine-safe places
+
+        mov     z, x0
+        mov     x, x1
+        mov     t, x2
+
+// Compute L = x_lo * y_lo in bottom half of buffer (size 8 x 8 -> 16)
+
+        bl      local_sqr_8_16
+
+// Compute absolute difference [t..] = |x_lo - x_hi|
+
+        ldp     x10, x11, [x, 0]
+        ldp     x8, x9, [x, 64]
+        subs    x10, x10, x8
+        sbcs    x11, x11, x9
+        ldp     x12, x13, [x, 16]
+        ldp     x8, x9, [x, 80]
+        sbcs    x12, x12, x8
+        sbcs    x13, x13, x9
+        ldp     x14, x15, [x, 32]
+        ldp     x8, x9, [x, 96]
+        sbcs    x14, x14, x8
+        sbcs    x15, x15, x9
+        ldp     x16, x17, [x, 48]
+        ldp     x8, x9, [x, 112]
+        sbcs    x16, x16, x8
+        sbcs    x17, x17, x9
+        csetm   s, cc
+        adds    xzr, s, s
+        eor     x10, x10, s
+        adcs    x10, x10, xzr
+        eor     x11, x11, s
+        adcs    x11, x11, xzr
+        stp     x10, x11, [t]
+        eor     x12, x12, s
+        adcs    x12, x12, xzr
+        eor     x13, x13, s
+        adcs    x13, x13, xzr
+        stp     x12, x13, [t, 16]
+        eor     x14, x14, s
+        adcs    x14, x14, xzr
+        eor     x15, x15, s
+        adcs    x15, x15, xzr
+        stp     x14, x15, [t, 32]
+        eor     x16, x16, s
+        adcs    x16, x16, xzr
+        eor     x17, x17, s
+        adcs    x17, x17, xzr
+        stp     x16, x17, [t, 48]
+
+// Compute H = x_hi * y_hi in top half of buffer (size 8 x 8 -> 16)
+
+        add     x0, z, 128
+        add     x1, x, 64
+        bl      local_sqr_8_16
+
+// Compute H' = H + L_top in place of H (it cannot overflow)
+// First add 8-sized block then propagate carry through next 8
+
+        .set    i, 0
+
+        ldp     x10, x11, [z, 128+8*i]
+        ldp     x12, x13, [z, 64+8*i]
+        adds    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, 128+8*i]
+        .set    i, (i+2)
+
+.rep 3
+        ldp     x10, x11, [z, 128+8*i]
+        ldp     x12, x13, [z, 64+8*i]
+        adcs    x10, x10, x12
+        adcs    x11, x11, x13
+        stp     x10, x11, [z, 128+8*i]
+        .set    i, (i+2)
+.endr
+
+.rep 4
+        ldp     x10, x11, [z, 128+8*i]
+        adcs    x10, x10, xzr
+        adcs    x11, x11, xzr
+        stp     x10, x11, [z, 128+8*i]
+        .set    i, (i+2)
+.endr
+
+// Compute M = |x_lo - x_hi| * |y_hi - y_lo| in [t+8...], size 16
+
+        add     x0, t, 64
+        mov     x1, t
+        bl      local_sqr_8_16
+
+// Add the interlocking H' and L_bot terms, storing in registers x15..x0
+// Intercept the carry at the 8 + 16 = 24 position and store it in x.
+// (Note that we no longer need the input x was pointing at.)
+
+        ldp     x0, x1, [z]
+        ldp     x16, x17, [z, 128]
+        adds    x0, x0, x16
+        adcs    x1, x1, x17
+        ldp     x2, x3, [z, 16]
+        ldp     x16, x17, [z, 144]
+        adcs    x2, x2, x16
+        adcs    x3, x3, x17
+        ldp     x4, x5, [z, 32]
+        ldp     x16, x17, [z, 160]
+        adcs    x4, x4, x16
+        adcs    x5, x5, x17
+        ldp     x6, x7, [z, 48]
+        ldp     x16, x17, [z, 176]
+        adcs    x6, x6, x16
+        adcs    x7, x7, x17
+        ldp     x8, x9, [z, 128]
+        ldp     x16, x17, [z, 192]
+        adcs    x8, x8, x16
+        adcs    x9, x9, x17
+        ldp     x10, x11, [z, 144]
+        ldp     x16, x17, [z, 208]
+        adcs    x10, x10, x16
+        adcs    x11, x11, x17
+        ldp     x12, x13, [z, 160]
+        ldp     x16, x17, [z, 224]
+        adcs    x12, x12, x16
+        adcs    x13, x13, x17
+        ldp     x14, x15, [z, 176]
+        ldp     x16, x17, [z, 240]
+        adcs    x14, x14, x16
+        adcs    x15, x15, x17
+        cset    x, cs
+
+// Subtract the mid-term cross product M
+
+        ldp     x16, x17, [t, 64]
+        subs    x0, x0, x16
+        sbcs    x1, x1, x17
+        stp     x0, x1, [z, 64]
+        ldp     x16, x17, [t, 80]
+        sbcs    x2, x2, x16
+        sbcs    x3, x3, x17
+        stp     x2, x3, [z, 80]
+        ldp     x16, x17, [t, 96]
+        sbcs    x4, x4, x16
+        sbcs    x5, x5, x17
+        stp     x4, x5, [z, 96]
+        ldp     x16, x17, [t, 112]
+        sbcs    x6, x6, x16
+        sbcs    x7, x7, x17
+        stp     x6, x7, [z, 112]
+        ldp     x16, x17, [t, 128]
+        sbcs    x8, x8, x16
+        sbcs    x9, x9, x17
+        stp     x8, x9, [z, 128]
+        ldp     x16, x17, [t, 144]
+        sbcs    x10, x10, x16
+        sbcs    x11, x11, x17
+        stp     x10, x11, [z, 144]
+        ldp     x16, x17, [t, 160]
+        sbcs    x12, x12, x16
+        sbcs    x13, x13, x17
+        stp     x12, x13, [z, 160]
+        ldp     x16, x17, [t, 176]
+        sbcs    x14, x14, x16
+        sbcs    x15, x15, x17
+        stp     x14, x15, [z, 176]
+
+// Get the next digits effectively resulting so far starting at 24
+
+        sbcs    x, x, xzr
+        csetm   t, cc
+
+// Now the final 8 digits of padding; the first one is special in using x
+// and also in getting the carry chain started
+
+        ldp     x10, x11, [z, 192]
+        adds    x10, x10, x
+        adcs    x11, x11, t
+        stp     x10, x11, [z, 192]
+        ldp     x10, x11, [z, 208]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, 208]
+        ldp     x10, x11, [z, 224]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, 224]
+        ldp     x10, x11, [z, 240]
+        adcs    x10, x10, t
+        adcs    x11, x11, t
+        stp     x10, x11, [z, 240]
+
+// Restore registers and return
+
+        ldp     x25, x30, [sp], 16
+        ldp     x23, x24, [sp], 16
+        ldp     x21, x22, [sp], 16
+        ldp     x19, x20, [sp], 16
+
+        ret
+
+// -----------------------------------------------------------------------------
+// Local 8x8->16 squaring routine, shared to reduce code size. Effectively
+// the same as bignum_sqr_8_16 without the scratch register preservation.
+// -----------------------------------------------------------------------------
+
+local_sqr_8_16:
+        ldp     x2, x3, [x1]
+        ldp     x4, x5, [x1, 16]
+        ldp     x6, x7, [x1, 32]
+        ldp     x8, x9, [x1, 48]
+        mul     x17, x2, x4
+        mul     x14, x3, x5
+        umulh   x20, x2, x4
+        subs    x21, x2, x3
+        cneg    x21, x21, cc
+        csetm   x11, cc
+        subs    x12, x5, x4
+        cneg    x12, x12, cc
+        mul     x13, x21, x12
+        umulh   x12, x21, x12
+        cinv    x11, x11, cc
+        eor     x13, x13, x11
+        eor     x12, x12, x11
+        adds    x19, x17, x20
+        adc     x20, x20, xzr
+        umulh   x21, x3, x5
+        adds    x19, x19, x14
+        adcs    x20, x20, x21
+        adc     x21, x21, xzr
+        adds    x20, x20, x14
+        adc     x21, x21, xzr
+        cmn     x11, 0x1
+        adcs    x19, x19, x13
+        adcs    x20, x20, x12
+        adc     x21, x21, x11
+        adds    x17, x17, x17
+        adcs    x19, x19, x19
+        adcs    x20, x20, x20
+        adcs    x21, x21, x21
+        adc     x10, xzr, xzr
+        mul     x12, x2, x2
+        mul     x13, x3, x3
+        mul     x15, x2, x3
+        umulh   x11, x2, x2
+        umulh   x14, x3, x3
+        umulh   x16, x2, x3
+        adds    x11, x11, x15
+        adcs    x13, x13, x16
+        adc     x14, x14, xzr
+        adds    x11, x11, x15
+        adcs    x13, x13, x16
+        adc     x14, x14, xzr
+        stp     x12, x11, [x0]
+        adds    x17, x17, x13
+        adcs    x19, x19, x14
+        adcs    x20, x20, xzr
+        adcs    x21, x21, xzr
+        adc     x10, x10, xzr
+        stp     x17, x19, [x0, 16]
+        mul     x12, x4, x4
+        mul     x13, x5, x5
+        mul     x15, x4, x5
+        umulh   x11, x4, x4
+        umulh   x14, x5, x5
+        umulh   x16, x4, x5
+        adds    x11, x11, x15
+        adcs    x13, x13, x16
+        adc     x14, x14, xzr
+        adds    x11, x11, x15
+        adcs    x13, x13, x16
+        adc     x14, x14, xzr
+        adds    x12, x12, x20
+        adcs    x11, x11, x21
+        stp     x12, x11, [x0, 32]
+        adcs    x13, x13, x10
+        adc     x14, x14, xzr
+        stp     x13, x14, [x0, 48]
+        mul     x17, x6, x8
+        mul     x14, x7, x9
+        umulh   x20, x6, x8
+        subs    x21, x6, x7
+        cneg    x21, x21, cc
+        csetm   x11, cc
+        subs    x12, x9, x8
+        cneg    x12, x12, cc
+        mul     x13, x21, x12
+        umulh   x12, x21, x12
+        cinv    x11, x11, cc
+        eor     x13, x13, x11
+        eor     x12, x12, x11
+        adds    x19, x17, x20
+        adc     x20, x20, xzr
+        umulh   x21, x7, x9
+        adds    x19, x19, x14
+        adcs    x20, x20, x21
+        adc     x21, x21, xzr
+        adds    x20, x20, x14
+        adc     x21, x21, xzr
+        cmn     x11, 0x1
+        adcs    x19, x19, x13
+        adcs    x20, x20, x12
+        adc     x21, x21, x11
+        adds    x17, x17, x17
+        adcs    x19, x19, x19
+        adcs    x20, x20, x20
+        adcs    x21, x21, x21
+        adc     x10, xzr, xzr
+        mul     x12, x6, x6
+        mul     x13, x7, x7
+        mul     x15, x6, x7
+        umulh   x11, x6, x6
+        umulh   x14, x7, x7
+        umulh   x16, x6, x7
+        adds    x11, x11, x15
+        adcs    x13, x13, x16
+        adc     x14, x14, xzr
+        adds    x11, x11, x15
+        adcs    x13, x13, x16
+        adc     x14, x14, xzr
+        stp     x12, x11, [x0, 64]
+        adds    x17, x17, x13
+        adcs    x19, x19, x14
+        adcs    x20, x20, xzr
+        adcs    x21, x21, xzr
+        adc     x10, x10, xzr
+        stp     x17, x19, [x0, 80]
+        mul     x12, x8, x8
+        mul     x13, x9, x9
+        mul     x15, x8, x9
+        umulh   x11, x8, x8
+        umulh   x14, x9, x9
+        umulh   x16, x8, x9
+        adds    x11, x11, x15
+        adcs    x13, x13, x16
+        adc     x14, x14, xzr
+        adds    x11, x11, x15
+        adcs    x13, x13, x16
+        adc     x14, x14, xzr
+        adds    x12, x12, x20
+        adcs    x11, x11, x21
+        stp     x12, x11, [x0, 96]
+        adcs    x13, x13, x10
+        adc     x14, x14, xzr
+        stp     x13, x14, [x0, 112]
+        mul     x10, x2, x6
+        mul     x14, x3, x7
+        mul     x15, x4, x8
+        mul     x16, x5, x9
+        umulh   x17, x2, x6
+        adds    x14, x14, x17
+        umulh   x17, x3, x7
+        adcs    x15, x15, x17
+        umulh   x17, x4, x8
+        adcs    x16, x16, x17
+        umulh   x17, x5, x9
+        adc     x17, x17, xzr
+        adds    x11, x14, x10
+        adcs    x14, x15, x14
+        adcs    x15, x16, x15
+        adcs    x16, x17, x16
+        adc     x17, xzr, x17
+        adds    x12, x14, x10
+        adcs    x13, x15, x11
+        adcs    x14, x16, x14
+        adcs    x15, x17, x15
+        adcs    x16, xzr, x16
+        adc     x17, xzr, x17
+        subs    x22, x4, x5
+        cneg    x22, x22, cc
+        csetm   x19, cc
+        subs    x20, x9, x8
+        cneg    x20, x20, cc
+        mul     x21, x22, x20
+        umulh   x20, x22, x20
+        cinv    x19, x19, cc
+        cmn     x19, 0x1
+        eor     x21, x21, x19
+        adcs    x15, x15, x21
+        eor     x20, x20, x19
+        adcs    x16, x16, x20
+        adc     x17, x17, x19
+        subs    x22, x2, x3
+        cneg    x22, x22, cc
+        csetm   x19, cc
+        subs    x20, x7, x6
+        cneg    x20, x20, cc
+        mul     x21, x22, x20
+        umulh   x20, x22, x20
+        cinv    x19, x19, cc
+        cmn     x19, 0x1
+        eor     x21, x21, x19
+        adcs    x11, x11, x21
+        eor     x20, x20, x19
+        adcs    x12, x12, x20
+        adcs    x13, x13, x19
+        adcs    x14, x14, x19
+        adcs    x15, x15, x19
+        adcs    x16, x16, x19
+        adc     x17, x17, x19
+        subs    x22, x3, x5
+        cneg    x22, x22, cc
+        csetm   x19, cc
+        subs    x20, x9, x7
+        cneg    x20, x20, cc
+        mul     x21, x22, x20
+        umulh   x20, x22, x20
+        cinv    x19, x19, cc
+        cmn     x19, 0x1
+        eor     x21, x21, x19
+        adcs    x14, x14, x21
+        eor     x20, x20, x19
+        adcs    x15, x15, x20
+        adcs    x16, x16, x19
+        adc     x17, x17, x19
+        subs    x22, x2, x4
+        cneg    x22, x22, cc
+        csetm   x19, cc
+        subs    x20, x8, x6
+        cneg    x20, x20, cc
+        mul     x21, x22, x20
+        umulh   x20, x22, x20
+        cinv    x19, x19, cc
+        cmn     x19, 0x1
+        eor     x21, x21, x19
+        adcs    x12, x12, x21
+        eor     x20, x20, x19
+        adcs    x13, x13, x20
+        adcs    x14, x14, x19
+        adcs    x15, x15, x19
+        adcs    x16, x16, x19
+        adc     x17, x17, x19
+        subs    x22, x2, x5
+        cneg    x22, x22, cc
+        csetm   x19, cc
+        subs    x20, x9, x6
+        cneg    x20, x20, cc
+        mul     x21, x22, x20
+        umulh   x20, x22, x20
+        cinv    x19, x19, cc
+        cmn     x19, 0x1
+        eor     x21, x21, x19
+        adcs    x13, x13, x21
+        eor     x20, x20, x19
+        adcs    x14, x14, x20
+        adcs    x15, x15, x19
+        adcs    x16, x16, x19
+        adc     x17, x17, x19
+        subs    x22, x3, x4
+        cneg    x22, x22, cc
+        csetm   x19, cc
+        subs    x20, x8, x7
+        cneg    x20, x20, cc
+        mul     x21, x22, x20
+        umulh   x20, x22, x20
+        cinv    x19, x19, cc
+        cmn     x19, 0x1
+        eor     x21, x21, x19
+        adcs    x13, x13, x21
+        eor     x20, x20, x19
+        adcs    x14, x14, x20
+        adcs    x15, x15, x19
+        adcs    x16, x16, x19
+        adc     x17, x17, x19
+        adds    x10, x10, x10
+        adcs    x11, x11, x11
+        adcs    x12, x12, x12
+        adcs    x13, x13, x13
+        adcs    x14, x14, x14
+        adcs    x15, x15, x15
+        adcs    x16, x16, x16
+        adcs    x17, x17, x17
+        adc     x19, xzr, xzr
+        ldp     x2, x3, [x0, 32]
+        adds    x10, x10, x2
+        adcs    x11, x11, x3
+        stp     x10, x11, [x0, 32]
+        ldp     x2, x3, [x0, 48]
+        adcs    x12, x12, x2
+        adcs    x13, x13, x3
+        stp     x12, x13, [x0, 48]
+        ldp     x2, x3, [x0, 64]
+        adcs    x14, x14, x2
+        adcs    x15, x15, x3
+        stp     x14, x15, [x0, 64]
+        ldp     x2, x3, [x0, 80]
+        adcs    x16, x16, x2
+        adcs    x17, x17, x3
+        stp     x16, x17, [x0, 80]
+        ldp     x2, x3, [x0, 96]
+        adcs    x2, x2, x19
+        adcs    x3, x3, xzr
+        stp     x2, x3, [x0, 96]
+        ldp     x2, x3, [x0, 112]
+        adcs    x2, x2, xzr
+        adc     x3, x3, xzr
+        stp     x2, x3, [x0, 112]
+        ret
diff --git a/arm/generic/bignum_ge.S b/arm/generic/bignum_ge.S
new file mode 100644
index 000000000..6d44be00e
--- /dev/null
+++ b/arm/generic/bignum_ge.S
@@ -0,0 +1,93 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Compare bignums, x >= y
+// Inputs x[m], y[n]; output function return
+//
+//    extern uint64_t bignum_ge
+//     (uint64_t m, uint64_t *x, uint64_t n, uint64_t *y);
+//
+// Standard ARM ABI: X0 = m, X1 = x, X2 = n, X3 = y, returns X0
+// ----------------------------------------------------------------------------
+
+#define m x0
+#define x x1
+#define n x2
+#define y x3
+#define i x4
+#define a x5
+#define d x6
+
+.text
+.globl bignum_ge
+
+bignum_ge:
+
+// Zero the main index counter for both branches
+
+                mov     i, xzr
+
+// Speculatively form m := m - n and do case split
+
+                subs    m, m, n
+                bcc     ylonger
+
+// The case where x is longer or of the same size (m >= n)
+// Note that CF=1 initially by the fact that we reach this point
+
+                cbz     n, xtest
+xmainloop:
+                ldr     a, [x, i, LSL 3]
+                ldr     d, [y, i, LSL 3]
+                sbcs    xzr, a, d
+                add     i, i, 1
+                sub     n, n, 1
+                cbnz    n, xmainloop
+xtest:
+                cbz     m, xskip
+xtoploop:
+                ldr     a, [x, i, LSL 3]
+                sbcs    xzr, a, xzr
+                add     i, i, 1
+                sub     m, m, 1
+                cbnz    m, xtoploop
+xskip:
+                cset    x0, cs
+                ret
+
+// The case where y is longer (n > m)
+// The first "adds" also makes sure CF=1 initially in this branch
+
+ylonger:
+                adds    m, m, n
+                cbz     m, ytoploop
+                sub     n, n, m
+ymainloop:
+                ldr     a, [x, i, LSL 3]
+                ldr     d, [y, i, LSL 3]
+                sbcs    xzr, a, d
+                add     i, i, 1
+                sub     m, m, 1
+                cbnz    m, ymainloop
+ytoploop:
+                ldr     a, [y, i, LSL 3]
+                sbcs    xzr, xzr, a
+                add     i, i, 1
+                sub     n, n, 1
+                cbnz    n, ytoploop
+
+                cset    x0, cs
+                ret
diff --git a/arm/generic/bignum_mul.S b/arm/generic/bignum_mul.S
new file mode 100644
index 000000000..225066c5a
--- /dev/null
+++ b/arm/generic/bignum_mul.S
@@ -0,0 +1,118 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Multiply z := x * y
+// Inputs x[m], y[n]; output z[k]
+//
+//    extern void bignum_mul
+//     (uint64_t k, uint64_t *z,
+//      uint64_t m, uint64_t *x, uint64_t n, uint64_t *y);
+//
+// Does the "z := x * y" operation where x is m digits, y is n, result z is p.
+// Truncates the result in general unless p >= m + n
+//
+// Standard ARM ABI: X0 = k, X1 = z, X2 = m, X3 = x, X4 = n, X5 = y
+// ----------------------------------------------------------------------------
+
+#define p x0
+#define z x1
+#define m x2
+#define x x3
+#define n x4
+#define y x5
+#define l x6
+#define h x7
+#define c x8
+#define k x9
+#define i x10
+#define a x11
+#define b x12
+#define d x13
+#define xx x14
+#define yy x15
+
+.text
+.globl bignum_mul
+
+bignum_mul:
+
+// If p = 0 the result is trivial and nothing needs doing
+
+                cbz     p, end
+
+// initialize (h,l) = 0, saving c = 0 for inside the loop
+
+                mov     l, xzr
+                mov     h, xzr
+
+// Iterate outer loop from k = 0 ... k = p - 1 producing result digits
+
+                mov     k, xzr
+outerloop:
+
+// Zero the carry for this stage
+
+                mov     c, xzr
+
+// First let a = MAX 0 (k + 1 - n) and b = MIN (k + 1) m
+// We want to accumulate all x[i] * y[k - i] for a <= i < b
+
+                add     a, k, 1
+                cmp     a, m
+                csel    b, a, m, cc
+                subs    a, a, n
+                csel    a, a, xzr, cs
+
+// Set loop count i = b - a, and skip everything if it's <= 0
+
+                subs    i, b, a
+                bls     innerend
+
+// Use temporary pointers xx = x + 8 * a and yy = y + 8 * (k - b)
+// Increment xx per iteration but just use loop counter with yy
+// So we start with [xx] = x[a] and [yy] = y[(k - b) + (b - a)] = y[k - a]
+
+                lsl     xx, a, 3
+                add     xx, xx, x
+
+                sub     yy, k, b
+                lsl     yy, yy, 3
+                add     yy, yy, y
+
+// And index using the loop counter i = b - a, ..., i = 1
+
+innerloop:
+                ldr     a, [xx], #8
+                ldr     b, [yy, i, LSL 3]
+                mul     d, a, b
+                umulh   a, a, b
+                adds    l, l, d
+                adcs    h, h, a
+                adc     c, c, xzr
+                subs    i, i, 1
+                bne     innerloop
+
+innerend:
+                str     l, [z, k, LSL 3]
+                mov     l, h
+                mov     h, c
+
+                add     k, k, 1
+                cmp     k, p
+                bcc     outerloop                       // Inverted carry flag!
+
+end:
+                ret
diff --git a/arm/generic/bignum_optsub.S b/arm/generic/bignum_optsub.S
new file mode 100644
index 000000000..c25cddd6b
--- /dev/null
+++ b/arm/generic/bignum_optsub.S
@@ -0,0 +1,74 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Optionally subtract, z := x - y (if p nonzero) or z := x (if p zero)
+// Inputs x[k], p, y[k]; outputs function return (carry-out) and z[k]
+//
+//    extern uint64_t bignum_optsub
+//     (uint64_t k, uint64_t *z, uint64_t *x, uint64_t p, uint64_t *y);
+//
+// It is assumed that all numbers x, y and z have the same size k digits.
+// Returns carry-out as per usual subtraction, always 0 if p was zero.
+//
+// Standard ARM ABI: X0 = k, X1 = z, X2 = x, X3 = p, X4 = y, returns X0
+// ----------------------------------------------------------------------------
+
+#define k x0
+#define z x1
+#define x x2
+#define p x3
+#define m x3
+#define y x4
+#define a x5
+#define b x6
+#define i x7
+
+.text
+.globl bignum_optsub
+
+bignum_optsub:
+
+// if k = 0 do nothing. This is also the right top carry in X0
+
+                cbz     k, end
+
+// Convert p into a strict bitmask (same register in fact)
+
+                cmp     p, xzr
+                csetm   m, ne
+
+// Set i = 0 *and* make sure initial ~CF = 0
+
+                subs    i, xzr, xzr
+
+// Main loop
+
+loop:
+                ldr     a, [x, i]
+                ldr     b, [y, i]
+                and     b, b, m
+                sbcs    a, a, b
+                str     a, [z, i]
+                add     i, i, 8
+                sub     k, k, 1
+                cbnz    k, loop
+
+// Return (non-inverted) carry flag
+
+                cset    x0, cc
+
+end:
+                ret
diff --git a/arm/p384/Makefile b/arm/p384/Makefile
new file mode 100644
index 000000000..91c7a4c6e
--- /dev/null
+++ b/arm/p384/Makefile
@@ -0,0 +1,58 @@
+#############################################################################
+# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License").
+# You may not use this file except in compliance with the License.
+# A copy of the License is located at
+#
+#  http://aws.amazon.com/apache2.0
+#
+# or in the "LICENSE" file accompanying this file. This file is distributed
+# on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+# express or implied. See the License for the specific language governing
+# permissions and limitations under the License.
+#############################################################################
+
+# If actually on an ARM8 machine, just use the GNU assmbler (as). Otherwise
+# use a cross-assembling version so that the code can still be assembled
+# and the proofs checked against the object files (though you won't be able
+# to run code without additional emulation infrastructure). The aarch64
+# cross-assembling version can be installed manually by something like:
+#
+#  sudo apt-get install binutils-aarch64-linux-gnu
+
+UNAME_RESULT=$(shell uname -p)
+
+ifeq ($(UNAME_RESULT),aarch64)
+GAS=as
+else
+GAS=aarch64-linux-gnu-as
+endif
+
+# List of object files
+
+OBJ = bignum_add_p384.o \
+      bignum_amontmul_p384.o \
+      bignum_amontsqr_p384.o \
+      bignum_cmul_p384.o \
+      bignum_deamont_p384.o \
+      bignum_demont_p384.o \
+      bignum_double_p384.o \
+      bignum_half_p384.o \
+      bignum_mod_n384.o \
+      bignum_mod_n384_6.o \
+      bignum_mod_p384.o \
+      bignum_mod_p384_6.o \
+      bignum_montmul_p384.o \
+      bignum_montsqr_p384.o \
+      bignum_neg_p384.o \
+      bignum_optneg_p384.o \
+      bignum_sub_p384.o \
+      bignum_tomont_p384.o \
+      bignum_triple_p384.o
+
+%.o : %.S ; cpp $< | $(GAS) -o $@ -
+
+default: $(OBJ);
+
+clean:; rm -f *.o *.correct
diff --git a/arm/p384/bignum_add_p384.S b/arm/p384/bignum_add_p384.S
new file mode 100644
index 000000000..e3d47b9a0
--- /dev/null
+++ b/arm/p384/bignum_add_p384.S
@@ -0,0 +1,97 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Add modulo p_384, z := (x + y) mod p_384, assuming x and y reduced
+// Inputs x[6], y[6]; output z[6]
+//
+//    extern void bignum_add_p384
+//     (uint64_t z[static 6], uint64_t x[static 6], uint64_t y[static 6]);
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = y
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define x x1
+#define y x2
+#define c x3
+#define l x4
+#define d0 x5
+#define d1 x6
+#define d2 x7
+#define d3 x8
+#define d4 x9
+#define d5 x10
+
+.text
+.globl bignum_add_p384
+
+bignum_add_p384:
+
+// First just add the numbers as c + [d5; d4; d3; d2; d1; d0]
+
+                ldp     d0, d1, [x]
+                ldp     l, c, [y]
+                adds    d0, d0, l
+                adcs    d1, d1, c
+                ldp     d2, d3, [x, #16]
+                ldp     l, c, [y, #16]
+                adcs    d2, d2, l
+                adcs    d3, d3, c
+                ldp     d4, d5, [x, #32]
+                ldp     l, c, [y, #32]
+                adcs    d4, d4, l
+                adcs    d5, d5, c
+                adc     c, xzr, xzr
+
+// Now compare [d5; d4; d3; d2; d1; d0] with p_384
+
+                mov     l, 0x00000000ffffffff
+                subs    xzr, d0, l
+                mov     l, 0xffffffff00000000
+                sbcs    xzr, d1, l
+                mov     l, 0xfffffffffffffffe
+                sbcs    xzr, d2, l
+                adcs    xzr, d3, xzr
+                adcs    xzr, d4, xzr
+                adcs    xzr, d5, xzr
+
+// Now CF is set (because of inversion) if (x + y) % 2^384 >= p_384
+// Thus we want to correct if either this is set or the original carry c was
+
+                adcs    c, c, xzr
+                csetm   c, ne
+
+// Now correct by subtracting masked p_384
+
+                mov     l, 0x00000000ffffffff
+                and     l, l, c
+                subs    d0, d0, l
+                eor     l, l, c
+                sbcs    d1, d1, l
+                mov     l, 0xfffffffffffffffe
+                and     l, l, c
+                sbcs    d2, d2, l
+                sbcs    d3, d3, c
+                sbcs    d4, d4, c
+                sbc     d5, d5, c
+
+// Store the result
+
+                stp     d0, d1, [z]
+                stp     d2, d3, [z, #16]
+                stp     d4, d5, [z, #32]
+
+                ret
diff --git a/arm/p384/bignum_amontmul_p384.S b/arm/p384/bignum_amontmul_p384.S
new file mode 100644
index 000000000..6f38ebf87
--- /dev/null
+++ b/arm/p384/bignum_amontmul_p384.S
@@ -0,0 +1,423 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Almost-Montgomery multiply, z :== (x * y / 2^384) (congruent mod p_384)
+// Inputs x[6], y[6]; output z[6]
+//
+//    extern void bignum_amontmul_p384
+//     (uint64_t z[static 6], uint64_t x[static 6], uint64_t y[static 6]);
+//
+// Does z :== (x * y / 2^384) mod p_384, meaning that the result, in the native
+// size 6, is congruent modulo p_384, but might not be fully reduced mod p_384.
+// This is why it is called *almost* Montgomery multiplication.
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = y
+// ----------------------------------------------------------------------------
+
+.globl   bignum_amontmul_p384
+
+// ---------------------------------------------------------------------------
+// Macro returning (c,h,l) = 3-word 1s complement (x - y) * (w - z)
+// c,h,l,t should all be different
+// t,h should not overlap w,z
+// ---------------------------------------------------------------------------
+
+.macro muldiffn c,h,l, t, x,y, w,z
+        subs    \t, \x, \y
+        cneg    \t, \t, cc
+        csetm   \c, cc
+        subs    \h, \w, \z
+        cneg    \h, \h, cc
+        mul     \l, \t, \h
+        umulh   \h, \t, \h
+        cinv    \c, \c, cc
+        eor     \l, \l, \c
+        eor     \h, \h, \c
+.endm
+
+// ---------------------------------------------------------------------------
+// Core one-step "short" Montgomery reduction macro. Takes input in
+// [d5;d4;d3;d2;d1;d0] and returns result in [d6;d5;d4;d3;d2;d1],
+// adding to the existing contents of [d5;d4;d3;d2;d1]. It is fine
+// for d6 to be the same register as d0.
+//
+// We want to add (2^384 - 2^128 - 2^96 + 2^32 - 1) * w
+// where w = [d0 + (d0<<32)] mod 2^64
+// ---------------------------------------------------------------------------
+
+.macro          montreds d6,d5,d4,d3,d2,d1,d0, t3,t2,t1
+// Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64
+// Recycle d0 (which we know gets implicitly cancelled) to store it
+                lsl     \t1, \d0, 32
+                add     \d0, \t1, \d0
+// Now let [t2;t1] = 2^64 * w - w + w_hi where w_hi = floor(w/2^32)
+// We need to subtract 2^32 * this, and we can ignore its lower 32
+// bits since by design it will cancel anyway; we only need the w_hi
+// part to get the carry propagation going.
+                lsr     \t1, \d0, 32
+                subs    \t1, \t1, \d0
+                sbc     \t2, \d0, xzr
+// Now select in t1 the field to subtract from d1
+                extr    \t1, \t2, \t1, 32
+// And now get the terms to subtract from d2 and d3
+                lsr     \t2, \t2, 32
+                adds    \t2, \t2, \d0
+                adc     \t3, xzr, xzr
+// Do the subtraction of that portion
+                subs    \d1, \d1, \t1
+                sbcs    \d2, \d2, \t2
+                sbcs    \d3, \d3, \t3
+                sbcs    \d4, \d4, xzr
+                sbcs    \d5, \d5, xzr
+// Now effectively add 2^384 * w by taking d0 as the input for the last sbc
+                sbc     \d6, \d0, xzr
+.endm
+
+#define a0 x3
+#define a1 x4
+#define a2 x5
+#define a3 x6
+#define a4 x7
+#define a5 x8
+#define b0 x9
+#define b1 x10
+#define b2 x11
+#define b3 x12
+#define b4 x13
+#define b5 x14
+
+#define s0 x15
+#define s1 x16
+#define s2 x17
+#define s3 x19
+#define s4 x20
+#define s5 x1
+#define s6 x2
+
+#define t1 x21
+#define t2 x22
+#define t3 x23
+#define t4 x24
+
+bignum_amontmul_p384:
+
+// Save some registers
+
+                stp     x19, x20, [sp, -16]!
+                stp     x21, x22, [sp, -16]!
+                stp     x23, x24, [sp, -16]!
+
+// Load in all words of both inputs
+
+                ldp     a0, a1, [x1]
+                ldp     a2, a3, [x1, 16]
+                ldp     a4, a5, [x1, 32]
+                ldp     b0, b1, [x2]
+                ldp     b2, b3, [x2, 16]
+                ldp     b4, b5, [x2, 32]
+
+// Multiply low halves with a 3x3->6 ADK multiplier as [s5;s4;s3;s2;s1;s0]
+
+                mul     s0, a0, b0
+                mul     t1, a1, b1
+                mul     t2, a2, b2
+                umulh   t3, a0, b0
+                umulh   t4, a1, b1
+                umulh   s5, a2, b2
+
+                adds    t3, t3, t1
+                adcs    t4, t4, t2
+                adc     s5, s5, xzr
+
+                adds    s1, t3, s0
+                adcs    s2, t4, t3
+                adcs    s3, s5, t4
+                adc     s4, s5, xzr
+
+                adds    s2, s2, s0
+                adcs    s3, s3, t3
+                adcs    s4, s4, t4
+                adc     s5, s5, xzr
+
+                muldiffn t3,t2,t1, t4, a0,a1, b1,b0
+                adds    xzr, t3, 1
+                adcs    s1, s1, t1
+                adcs    s2, s2, t2
+                adcs    s3, s3, t3
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a0,a2, b2,b0
+                adds    xzr, t3, 1
+                adcs    s2, s2, t1
+                adcs    s3, s3, t2
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a1,a2, b2,b1
+                adds    xzr, t3, 1
+                adcs    s3, s3, t1
+                adcs    s4, s4, t2
+                adc     s5, s5, t3
+
+// Perform three "short" Montgomery steps on the low product
+// This shifts it to an offset compatible with middle terms
+// Stash the result temporarily in the output buffer
+// We could keep this in registers by directly adding to it in the next
+// ADK block, but if anything that seems to be slightly slower
+
+                montreds s0,s5,s4,s3,s2,s1,s0, t1,t2,t3
+
+                montreds s1,s0,s5,s4,s3,s2,s1, t1,t2,t3
+
+                montreds s2,s1,s0,s5,s4,s3,s2, t1,t2,t3
+
+                stp     s3, s4, [x0]
+                stp     s5, s0, [x0, 16]
+                stp     s1, s2, [x0, 32]
+
+// Multiply high halves with a 3x3->6 ADK multiplier as [s5;s4;s3;s2;s1;s0]
+
+                mul     s0, a3, b3
+                mul     t1, a4, b4
+                mul     t2, a5, b5
+                umulh   t3, a3, b3
+                umulh   t4, a4, b4
+                umulh   s5, a5, b5
+
+                adds    t3, t3, t1
+                adcs    t4, t4, t2
+                adc     s5, s5, xzr
+
+                adds    s1, t3, s0
+                adcs    s2, t4, t3
+                adcs    s3, s5, t4
+                adc     s4, s5, xzr
+
+                adds    s2, s2, s0
+                adcs    s3, s3, t3
+                adcs    s4, s4, t4
+                adc     s5, s5, xzr
+
+                muldiffn t3,t2,t1, t4, a3,a4, b4,b3
+                adds    xzr, t3, 1
+                adcs    s1, s1, t1
+                adcs    s2, s2, t2
+                adcs    s3, s3, t3
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a3,a5, b5,b3
+                adds    xzr, t3, 1
+                adcs    s2, s2, t1
+                adcs    s3, s3, t2
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a4,a5, b5,b4
+                adds    xzr, t3, 1
+                adcs    s3, s3, t1
+                adcs    s4, s4, t2
+                adc     s5, s5, t3
+
+// Compute sign-magnitude a0,[a5,a4,a3] = x_hi - x_lo
+
+                subs    a3, a3, a0
+                sbcs    a4, a4, a1
+                sbcs    a5, a5, a2
+                sbc     a0, xzr, xzr
+                adds    xzr, a0, 1
+                eor     a3, a3, a0
+                adcs    a3, a3, xzr
+                eor     a4, a4, a0
+                adcs    a4, a4, xzr
+                eor     a5, a5, a0
+                adc     a5, a5, xzr
+
+// Compute sign-magnitude b5,[b2,b1,b0] = y_lo - y_hi
+
+                subs    b0, b0, b3
+                sbcs    b1, b1, b4
+                sbcs    b2, b2, b5
+                sbc     b5, xzr, xzr
+
+                adds    xzr, b5, 1
+                eor     b0, b0, b5
+                adcs    b0, b0, xzr
+                eor     b1, b1, b5
+                adcs    b1, b1, xzr
+                eor     b2, b2, b5
+                adc     b2, b2, xzr
+
+// Save the correct sign for the sub-product in b5
+
+                eor     b5, a0, b5
+
+// Add the high H to the modified low term L' and re-stash 6 words,
+// keeping top word in s6
+
+                ldp     t1, t2, [x0]
+                adds    s0, s0, t1
+                adcs    s1, s1, t2
+                ldp     t1, t2, [x0, 16]
+                adcs    s2, s2, t1
+                adcs    s3, s3, t2
+                ldp     t1, t2, [x0, 32]
+                adcs    s4, s4, t1
+                adcs    s5, s5, t2
+                adc     s6, xzr, xzr
+                stp     s0, s1, [x0]
+                stp     s2, s3, [x0, 16]
+                stp     s4, s5, [x0, 32]
+
+// Multiply with yet a third 3x3 ADK for the complex mid-term
+
+                mul     s0, a3, b0
+                mul     t1, a4, b1
+                mul     t2, a5, b2
+                umulh   t3, a3, b0
+                umulh   t4, a4, b1
+                umulh   s5, a5, b2
+
+                adds    t3, t3, t1
+                adcs    t4, t4, t2
+                adc     s5, s5, xzr
+
+                adds    s1, t3, s0
+                adcs    s2, t4, t3
+                adcs    s3, s5, t4
+                adc     s4, s5, xzr
+
+                adds    s2, s2, s0
+                adcs    s3, s3, t3
+                adcs    s4, s4, t4
+                adc     s5, s5, xzr
+
+                muldiffn t3,t2,t1, t4, a3,a4, b1,b0
+                adds    xzr, t3, 1
+                adcs    s1, s1, t1
+                adcs    s2, s2, t2
+                adcs    s3, s3, t3
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a3,a5, b2,b0
+                adds    xzr, t3, 1
+                adcs    s2, s2, t1
+                adcs    s3, s3, t2
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a4,a5, b2,b1
+                adds    xzr, t3, 1
+                adcs    s3, s3, t1
+                adcs    s4, s4, t2
+                adc     s5, s5, t3
+
+// Unstash the H + L' sum to add in twice
+
+                ldp     a0, a1, [x0]
+                ldp     a2, a3, [x0, 16]
+                ldp     a4, a5, [x0, 32]
+
+// Set up a sign-modified version of the mid-product in a long accumulator
+// as [b3;b2;b1;b0;s5;s4;s3;s2;s1;s0], adding in the H + L' term once with
+// zero offset as this signed value is created
+
+                adds    xzr, b5, 1
+                eor     s0, s0, b5
+                adcs    s0, s0, a0
+                eor     s1, s1, b5
+                adcs    s1, s1, a1
+                eor     s2, s2, b5
+                adcs    s2, s2, a2
+                eor     s3, s3, b5
+                adcs    s3, s3, a3
+                eor     s4, s4, b5
+                adcs    s4, s4, a4
+                eor     s5, s5, b5
+                adcs    s5, s5, a5
+                adcs    b0, b5, s6
+                adcs    b1, b5, xzr
+                adcs    b2, b5, xzr
+                adc     b3, b5, xzr
+
+// Add in the stashed H + L' term an offset of 3 words as well
+
+                adds    s3, s3, a0
+                adcs    s4, s4, a1
+                adcs    s5, s5, a2
+                adcs    b0, b0, a3
+                adcs    b1, b1, a4
+                adcs    b2, b2, a5
+                adc     b3, b3, s6
+
+// Do three more Montgomery steps on the composed term
+
+                montreds s0,s5,s4,s3,s2,s1,s0, t1,t2,t3
+                montreds s1,s0,s5,s4,s3,s2,s1, t1,t2,t3
+                montreds s2,s1,s0,s5,s4,s3,s2, t1,t2,t3
+
+                adds    b0, b0, s0
+                adcs    b1, b1, s1
+                adcs    b2, b2, s2
+                adc     b3, b3, xzr
+
+// Because of the way we added L' in two places, we can overspill by
+// more than usual in Montgomery, with the result being only known to
+// be < 3 * p_384, not the usual < 2 * p_384. So now we do a more
+// elaborate final correction in the style of bignum_cmul_p384, just
+// a little bit simpler because we know q is small.
+
+                add     t2, b3, 1
+                lsl     t1, t2, 32
+                subs    t4, t2, t1
+                sbc     t1, t1, xzr
+
+                adds    s3, s3, t4
+                adcs    s4, s4, t1
+                adcs    s5, s5, t2
+                adcs    b0, b0, xzr
+                adcs    b1, b1, xzr
+                adcs    b2, b2, xzr
+
+                csetm   t2, cc
+
+                mov     t3, 0x00000000ffffffff
+                and     t3, t3, t2
+                adds    s3, s3, t3
+                eor     t3, t3, t2
+                adcs    s4, s4, t3
+                mov     t3, 0xfffffffffffffffe
+                and     t3, t3, t2
+                adcs    s5, s5, t3
+                adcs    b0, b0, t2
+                adcs    b1, b1, t2
+                adc     b2, b2, t2
+
+// Write back the result
+
+                stp     s3, s4, [x0]
+                stp     s5, b0, [x0, 16]
+                stp     b1, b2, [x0, 32]
+
+// Restore registers and return
+
+                ldp     x23, x24, [sp], 16
+                ldp     x21, x22, [sp], 16
+                ldp     x19, x20, [sp], 16
+
+                ret
diff --git a/arm/p384/bignum_amontsqr_p384.S b/arm/p384/bignum_amontsqr_p384.S
new file mode 100644
index 000000000..099ead642
--- /dev/null
+++ b/arm/p384/bignum_amontsqr_p384.S
@@ -0,0 +1,356 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Almost-Montgomery square, z :== (x^2 / 2^384) (congruent mod p_384)
+// Input x[6]; output z[6]
+//
+//    extern void bignum_amontsqr_p384
+//     (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// "Almost" means the 6-digit result is correct mod p_384 but might not be
+// fully reduced, i.e. it might be >= p_384. Use "bignum_montsqr_p384" instead
+// where full reduction to a result < p_384 is desired/needed.
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+.globl   bignum_amontsqr_p384
+
+// ---------------------------------------------------------------------------
+// Macro returning (c,h,l) = 3-word 1s complement (x - y) * (w - z)
+// c,h,l,t should all be different
+// t,h should not overlap w,z
+// ---------------------------------------------------------------------------
+
+.macro muldiffn c,h,l, t, x,y, w,z
+        subs    \t, \x, \y
+        cneg    \t, \t, cc
+        csetm   \c, cc
+        subs    \h, \w, \z
+        cneg    \h, \h, cc
+        mul     \l, \t, \h
+        umulh   \h, \t, \h
+        cinv    \c, \c, cc
+        eor     \l, \l, \c
+        eor     \h, \h, \c
+.endm
+
+// ---------------------------------------------------------------------------
+// Core one-step "short" Montgomery reduction macro. Takes input in
+// [d5;d4;d3;d2;d1;d0] and returns result in [d6;d5;d4;d3;d2;d1],
+// adding to the existing contents of [d5;d4;d3;d2;d1]. It is fine
+// for d6 to be the same register as d0.
+//
+// We want to add (2^384 - 2^128 - 2^96 + 2^32 - 1) * w
+// where w = [d0 + (d0<<32)] mod 2^64
+// ---------------------------------------------------------------------------
+
+.macro          montreds d6,d5,d4,d3,d2,d1,d0, t3,t2,t1
+// Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64
+// Recycle d0 (which we know gets implicitly cancelled) to store it
+                lsl     \t1, \d0, 32
+                add     \d0, \t1, \d0
+// Now let [t2;t1] = 2^64 * w - w + w_hi where w_hi = floor(w/2^32)
+// We need to subtract 2^32 * this, and we can ignore its lower 32
+// bits since by design it will cancel anyway; we only need the w_hi
+// part to get the carry propagation going.
+                lsr     \t1, \d0, 32
+                subs    \t1, \t1, \d0
+                sbc     \t2, \d0, xzr
+// Now select in t1 the field to subtract from d1
+                extr    \t1, \t2, \t1, 32
+// And now get the terms to subtract from d2 and d3
+                lsr     \t2, \t2, 32
+                adds    \t2, \t2, \d0
+                adc     \t3, xzr, xzr
+// Do the subtraction of that portion
+                subs    \d1, \d1, \t1
+                sbcs    \d2, \d2, \t2
+                sbcs    \d3, \d3, \t3
+                sbcs    \d4, \d4, xzr
+                sbcs    \d5, \d5, xzr
+// Now effectively add 2^384 * w by taking d0 as the input for the last sbc
+                sbc     \d6, \d0, xzr
+.endm
+
+#define a0 x2
+#define a1 x3
+#define a2 x4
+#define a3 x5
+#define a4 x6
+#define a5 x7
+
+#define c0 x8
+#define c1 x9
+#define c2 x10
+#define c3 x11
+#define c4 x12
+#define c5 x13
+#define d1 x14
+#define d2 x15
+#define d3 x16
+#define d4 x17
+
+bignum_amontsqr_p384:
+
+// Load in all words of the input
+
+                ldp     a0, a1, [x1]
+                ldp     a2, a3, [x1, 16]
+                ldp     a4, a5, [x1, 32]
+
+// Square the low half getting a result in [c5;c4;c3;c2;c1;c0]
+
+                mul     d1, a0, a1
+                mul     d2, a0, a2
+                mul     d3, a1, a2
+                mul     c0, a0, a0
+                mul     c2, a1, a1
+                mul     c4, a2, a2
+
+                umulh   d4, a0, a1
+                adds    d2, d2, d4
+                umulh   d4, a0, a2
+                adcs    d3, d3, d4
+                umulh   d4, a1, a2
+                adcs    d4, d4, xzr
+
+                umulh   c1, a0, a0
+                umulh   c3, a1, a1
+                umulh   c5, a2, a2
+
+                adds    d1, d1, d1
+                adcs    d2, d2, d2
+                adcs    d3, d3, d3
+                adcs    d4, d4, d4
+                adc     c5, c5, xzr
+
+                adds    c1, c1, d1
+                adcs    c2, c2, d2
+                adcs    c3, c3, d3
+                adcs    c4, c4, d4
+                adc     c5, c5, xzr
+
+// Perform three "short" Montgomery steps on the low square
+// This shifts it to an offset compatible with middle product
+// Stash the result temporarily in the output buffer (to avoid more registers)
+
+                montreds c0,c5,c4,c3,c2,c1,c0, d1,d2,d3
+
+                montreds c1,c0,c5,c4,c3,c2,c1, d1,d2,d3
+
+                montreds c2,c1,c0,c5,c4,c3,c2, d1,d2,d3
+
+                stp     c3, c4, [x0]
+                stp     c5, c0, [x0, 16]
+                stp     c1, c2, [x0, 32]
+
+// Compute product of the cross-term with ADK 3x3->6 multiplier
+
+#define a0 x2
+#define a1 x3
+#define a2 x4
+#define a3 x5
+#define a4 x6
+#define a5 x7
+#define s0 x8
+#define s1 x9
+#define s2 x10
+#define s3 x11
+#define s4 x12
+#define s5 x13
+
+#define l1 x14
+#define l2 x15
+#define h0 x16
+#define h1 x17
+#define h2 x1
+
+#define s6 h1
+#define c  l1
+#define h  l2
+#define l  h0
+#define t  h1
+
+                mul     s0, a0, a3
+                mul     l1, a1, a4
+                mul     l2, a2, a5
+                umulh   h0, a0, a3
+                umulh   h1, a1, a4
+                umulh   h2, a2, a5
+
+                adds    h0, h0, l1
+                adcs    h1, h1, l2
+                adc     h2, h2, xzr
+
+                adds    s1, h0, s0
+                adcs    s2, h1, h0
+                adcs    s3, h2, h1
+                adc     s4, h2, xzr
+
+                adds    s2, s2, s0
+                adcs    s3, s3, h0
+                adcs    s4, s4, h1
+                adc     s5, h2, xzr
+
+                muldiffn c,h,l, t, a0,a1, a4,a3
+                adds    xzr, c, 1
+                adcs    s1, s1, l
+                adcs    s2, s2, h
+                adcs    s3, s3, c
+                adcs    s4, s4, c
+                adc     s5, s5, c
+
+                muldiffn c,h,l, t, a0,a2, a5,a3
+                adds    xzr, c, 1
+                adcs    s2, s2, l
+                adcs    s3, s3, h
+                adcs    s4, s4, c
+                adc     s5, s5, c
+
+                muldiffn c,h,l, t, a1,a2, a5,a4
+                adds    xzr, c, 1
+                adcs    s3, s3, l
+                adcs    s4, s4, h
+                adc     s5, s5, c
+
+// Double it and add the stashed Montgomerified low square
+
+                adds    s0, s0, s0
+                adcs    s1, s1, s1
+                adcs    s2, s2, s2
+                adcs    s3, s3, s3
+                adcs    s4, s4, s4
+                adcs    s5, s5, s5
+                adc     s6, xzr, xzr
+
+                ldp     a0, a1, [x0]
+                adds    s0, s0, a0
+                adcs    s1, s1, a1
+                ldp     a0, a1, [x0, 16]
+                adcs    s2, s2, a0
+                adcs    s3, s3, a1
+                ldp     a0, a1, [x0, 32]
+                adcs    s4, s4, a0
+                adcs    s5, s5, a1
+                adc     s6, s6, xzr
+
+// Montgomery-reduce the combined low and middle term another thrice
+
+                montreds s0,s5,s4,s3,s2,s1,s0, a0,a1,a2
+
+                montreds s1,s0,s5,s4,s3,s2,s1, a0,a1,a2
+
+                montreds s2,s1,s0,s5,s4,s3,s2, a0,a1,a2
+
+                adds    s6, s6, s0
+                adcs    s0, s1, xzr
+                adcs    s1, s2, xzr
+                adcs    s2, xzr, xzr
+
+// Our sum so far is in [s2;s1;s0;s6;s5;s4;s3]
+// Choose more intuitive names
+
+#define r0 x11
+#define r1 x12
+#define r2 x13
+#define r3 x17
+#define r4 x8
+#define r5 x9
+#define r6 x10
+
+// Remind ourselves what else we can't destroy
+
+#define a3 x5
+#define a4 x6
+#define a5 x7
+
+// So we can have these as temps
+
+#define t1 x1
+#define t2 x14
+#define t3 x15
+#define t4 x16
+
+// Add in all the pure squares 33 + 44 + 55
+
+                mul     t1, a3, a3
+                adds    r0, r0, t1
+                mul     t2, a4, a4
+                mul     t3, a5, a5
+                umulh   t1, a3, a3
+                adcs    r1, r1, t1
+                umulh   t1, a4, a4
+                adcs    r2, r2, t2
+                adcs    r3, r3, t1
+                umulh   t1, a5, a5
+                adcs    r4, r4, t3
+                adcs    r5, r5, t1
+                adc     r6, r6, xzr
+
+// Now compose the 34 + 35 + 45 terms, which need doubling
+
+                mul     t1, a3, a4
+                mul     t2, a3, a5
+                mul     t3, a4, a5
+                umulh   t4, a3, a4
+                adds    t2, t2, t4
+                umulh   t4, a3, a5
+                adcs    t3, t3, t4
+                umulh   t4, a4, a5
+                adc     t4, t4, xzr
+
+// Double and add. Recycle one of the no-longer-needed inputs as a temp
+
+#define t5 x5
+
+                adds    t1, t1, t1
+                adcs    t2, t2, t2
+                adcs    t3, t3, t3
+                adcs    t4, t4, t4
+                adc     t5, xzr, xzr
+
+                adds    r1, r1, t1
+                adcs    r2, r2, t2
+                adcs    r3, r3, t3
+                adcs    r4, r4, t4
+                adcs    r5, r5, t5
+                adc     r6, r6, xzr
+
+// Turn r6 into a bitmask and do a masked addition of
+// [0;0;0;t3;t2;t1] = 2^384 - p_384, hence masked subtraction of p_384
+
+                neg     r6, r6
+                mov     t1, 0xffffffff00000001
+                and     t1, t1, r6
+                adds    r0, r0, t1
+                mov     t2, 0x00000000ffffffff
+                and     t2, t2, r6
+                adcs    r1, r1, t2
+                mov     t3, 0x0000000000000001
+                and     t3, t3, r6
+                adcs    r2, r2, t3
+                adcs    r3, r3, xzr
+                adcs    r4, r4, xzr
+                adc     r5, r5, xzr
+
+// Store it back
+
+                stp     r0, r1, [x0]
+                stp     r2, r3, [x0, 16]
+                stp     r4, r5, [x0, 32]
+
+                ret
diff --git a/arm/p384/bignum_cmul_p384.S b/arm/p384/bignum_cmul_p384.S
new file mode 100644
index 000000000..e693876a2
--- /dev/null
+++ b/arm/p384/bignum_cmul_p384.S
@@ -0,0 +1,141 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Multiply by a single word modulo p_384, z := (c * x) mod p_384, assuming
+// x reduced
+// Inputs c, x[6]; output z[6]
+//
+//    extern void bignum_cmul_p384
+//     (uint64_t z[static 6], uint64_t c, uint64_t x[static 6]);
+//
+// Standard ARM ABI: X0 = z, X1 = c, X2 = x
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define c x1
+#define x x2
+
+#define d0 x2
+#define d1 x3
+#define d2 x4
+#define d3 x5
+#define d4 x6
+#define d5 x7
+#define a0 x8
+#define a1 x9
+#define a2 x10
+#define a3 x11
+#define a4 x12
+#define a5 x13
+
+// Some shared here
+
+#define h x1
+#define h1 x12
+#define hn x13
+#define m x8
+#define l x9
+
+.text
+.globl bignum_cmul_p384
+
+bignum_cmul_p384:
+
+// First do the multiply, straightforwardly, getting [h; d5; ...; d0]
+
+                ldp     a0, a1, [x]
+                ldp     a2, a3, [x, #16]
+                ldp     a4, a5, [x, #32]
+                mul     d0, c, a0
+                mul     d1, c, a1
+                mul     d2, c, a2
+                mul     d3, c, a3
+                mul     d4, c, a4
+                mul     d5, c, a5
+                umulh   a0, c, a0
+                umulh   a1, c, a1
+                umulh   a2, c, a2
+                umulh   a3, c, a3
+                umulh   a4, c, a4
+                umulh   h, c, a5
+                adds    d1, d1, a0
+                adcs    d2, d2, a1
+                adcs    d3, d3, a2
+                adcs    d4, d4, a3
+                adcs    d5, d5, a4
+                adc     h, h, xzr
+
+// Let h be the top word of this intermediate product and l the low 6 words.
+// By the range hypothesis on the input, we know h1 = h + 1 does not wrap
+// And then -p_384 <= z - h1 * p_384 < p_384, so we just need to subtract
+// h1 * p_384 and then correct if that is negative by adding p_384.
+//
+// Write p_384 = 2^384 - r where r = 2^128 + 2^96 - 2^32 + 1
+//
+// We want z - (h + 1) * (2^384 - r)
+//       = (2^384 * h + l) - (h + 1) * (2^384 - r)
+//       = (l + (h + 1) * r) - 2^384.
+//
+// Thus we can do the computation in 6 words of l + (h + 1) * r, and if it
+// does *not* carry we need to add p_384. We can rewrite this as the following,
+// using ~h = 2^64 - (h + 1) and absorbing the 2^64 in the higher term
+// using h instead of h + 1.
+//
+//         l + (h + 1) * r
+//       = l + 2^128 * (h + 1) + 2^96 * (h + 1) - 2^32 * (h + 1) + (h + 1)
+//       = l + 2^128 * (h + 1) + 2^96 * h + 2^32 * ~h + (h + 1)
+
+                add     h1, h, 1
+                orn     hn, xzr, h
+                lsl     a0, hn, 32
+                extr    a1, h, hn, 32
+                lsr     a2, h, 32
+
+                adds    a0, a0, h1
+                adcs    a1, a1, xzr
+                adcs    a2, a2, h1
+                adc     a3, xzr, xzr
+
+                adds    d0, d0, a0
+                adcs    d1, d1, a1
+                adcs    d2, d2, a2
+                adcs    d3, d3, a3
+                adcs    d4, d4, xzr
+                adcs    d5, d5, xzr
+
+// Catch the carry and do a masked addition of p_384
+
+                csetm   m, cc
+
+                mov     l, 0x00000000ffffffff
+                and     l, l, m
+                adds    d0, d0, l
+                eor     l, l, m
+                adcs    d1, d1, l
+                mov     l, 0xfffffffffffffffe
+                and     l, l, m
+                adcs    d2, d2, l
+                adcs    d3, d3, m
+                adcs    d4, d4, m
+                adc     d5, d5, m
+
+// Store the result
+
+                stp     d0, d1, [z]
+                stp     d2, d3, [z, #16]
+                stp     d4, d5, [z, #32]
+
+                ret
diff --git a/arm/p384/bignum_deamont_p384.S b/arm/p384/bignum_deamont_p384.S
new file mode 100644
index 000000000..0d300a724
--- /dev/null
+++ b/arm/p384/bignum_deamont_p384.S
@@ -0,0 +1,146 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Convert from almost-Montgomery form, z := (x / 2^384) mod p_384
+// Input x[6]; output z[6]
+//
+//    extern void bignum_deamont_p384
+//     (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// Convert a 6-digit bignum x out of its (optionally almost) Montgomery form,
+// "almost" meaning any 6-digit input will work, with no range restriction.
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+.globl   bignum_deamont_p384
+
+// ---------------------------------------------------------------------------
+// Core one-step "short" Montgomery reduction macro. Takes input in
+// [d5;d4;d3;d2;d1;d0] and returns result in [d6;d5;d4;d3;d2;d1],
+// adding to the existing contents of [d5;d4;d3;d2;d1]. It is fine
+// for d6 to be the same register as d0.
+//
+// We want to add (2^384 - 2^128 - 2^96 + 2^32 - 1) * w
+// where w = [d0 + (d0<<32)] mod 2^64
+// ---------------------------------------------------------------------------
+
+.macro          montreds d6,d5,d4,d3,d2,d1,d0, t3,t2,t1
+// Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64
+// Recycle d0 (which we know gets implicitly cancelled) to store it
+                lsl     \t1, \d0, 32
+                add     \d0, \t1, \d0
+// Now let [t2;t1] = 2^64 * w - w + w_hi where w_hi = floor(w/2^32)
+// We need to subtract 2^32 * this, and we can ignore its lower 32
+// bits since by design it will cancel anyway; we only need the w_hi
+// part to get the carry propagation going.
+                lsr     \t1, \d0, 32
+                subs    \t1, \t1, \d0
+                sbc     \t2, \d0, xzr
+// Now select in t1 the field to subtract from d1
+                extr    \t1, \t2, \t1, 32
+// And now get the terms to subtract from d2 and d3
+                lsr     \t2, \t2, 32
+                adds    \t2, \t2, \d0
+                adc     \t3, xzr, xzr
+// Do the subtraction of that portion
+                subs    \d1, \d1, \t1
+                sbcs    \d2, \d2, \t2
+                sbcs    \d3, \d3, \t3
+                sbcs    \d4, \d4, xzr
+                sbcs    \d5, \d5, xzr
+// Now effectively add 2^384 * w by taking d0 as the input for the last sbc
+                sbc     \d6, \d0, xzr
+.endm
+
+// Input parameters
+
+#define z x0
+#define x x1
+
+// Rotating registers for the intermediate windows
+
+#define d0 x2
+#define d1 x3
+#define d2 x4
+#define d3 x5
+#define d4 x6
+#define d5 x7
+
+// Other temporaries
+
+#define u x8
+#define v x9
+#define w x10
+
+bignum_deamont_p384:
+
+// Set up an initial window with the input x and an extra leading zero
+
+                ldp     d0, d1, [x]
+                ldp     d2, d3, [x, 16]
+                ldp     d4, d5, [x, 32]
+
+// Systematically scroll left doing 1-step reductions
+
+                montreds d0,d5,d4,d3,d2,d1,d0, u,v,w
+
+                montreds d1,d0,d5,d4,d3,d2,d1, u,v,w
+
+                montreds d2,d1,d0,d5,d4,d3,d2, u,v,w
+
+                montreds d3,d2,d1,d0,d5,d4,d3, u,v,w
+
+                montreds d4,d3,d2,d1,d0,d5,d4, u,v,w
+
+                montreds d5,d4,d3,d2,d1,d0,d5, u,v,w
+
+// Now compare end result in [d5;d4;d3;d2;d1;d0] = dd with p_384 by *adding*
+// 2^384 - p_384 = [0;0;0;w;v;u]. This will set CF if
+// dd + (2^384 - p_384) >= 2^384, hence iff dd >= p_384
+
+                mov     u, 0xffffffff00000001
+                mov     v, 0x00000000ffffffff
+                mov     w, 0x0000000000000001
+
+                adds    xzr, d0, u
+                adcs    xzr, d1, v
+                adcs    xzr, d2, w
+                adcs    xzr, d3, xzr
+                adcs    xzr, d4, xzr
+                adcs    xzr, d5, xzr
+
+// Convert the condition dd >= p_384 into a bitmask in w and do a masked
+// subtraction of p_384, via a masked addition of 2^384 - p_384:
+
+                csetm   w, cs
+                and     u, u, w
+                adds    d0, d0, u
+                and     v, v, w
+                adcs    d1, d1, v
+                and     w, w, 1
+                adcs    d2, d2, w
+                adcs    d3, d3, xzr
+                adcs    d4, d4, xzr
+                adc     d5, d5, xzr
+
+// Store it back
+
+                stp     d0, d1, [z]
+                stp     d2, d3, [z, 16]
+                stp     d4, d5, [z, 32]
+
+                ret
diff --git a/arm/p384/bignum_demont_p384.S b/arm/p384/bignum_demont_p384.S
new file mode 100644
index 000000000..1f147d45b
--- /dev/null
+++ b/arm/p384/bignum_demont_p384.S
@@ -0,0 +1,117 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Convert from Montgomery form z := (x / 2^384) mod p_384, assuming x reduced
+// Input x[6]; output z[6]
+//
+//    extern void bignum_demont_p384
+//     (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// This assumes the input is < p_384 for correctness. If this is not the case,
+// use the variant "bignum_deamont_p384" instead.
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+.globl   bignum_demont_p384
+
+// ---------------------------------------------------------------------------
+// Core one-step "short" Montgomery reduction macro. Takes input in
+// [d5;d4;d3;d2;d1;d0] and returns result in [d6;d5;d4;d3;d2;d1],
+// adding to the existing contents of [d5;d4;d3;d2;d1]. It is fine
+// for d6 to be the same register as d0.
+//
+// We want to add (2^384 - 2^128 - 2^96 + 2^32 - 1) * w
+// where w = [d0 + (d0<<32)] mod 2^64
+// ---------------------------------------------------------------------------
+
+.macro          montreds d6,d5,d4,d3,d2,d1,d0, t3,t2,t1
+// Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64
+// Recycle d0 (which we know gets implicitly cancelled) to store it
+                lsl     \t1, \d0, 32
+                add     \d0, \t1, \d0
+// Now let [t2;t1] = 2^64 * w - w + w_hi where w_hi = floor(w/2^32)
+// We need to subtract 2^32 * this, and we can ignore its lower 32
+// bits since by design it will cancel anyway; we only need the w_hi
+// part to get the carry propagation going.
+                lsr     \t1, \d0, 32
+                subs    \t1, \t1, \d0
+                sbc     \t2, \d0, xzr
+// Now select in t1 the field to subtract from d1
+                extr    \t1, \t2, \t1, 32
+// And now get the terms to subtract from d2 and d3
+                lsr     \t2, \t2, 32
+                adds    \t2, \t2, \d0
+                adc     \t3, xzr, xzr
+// Do the subtraction of that portion
+                subs    \d1, \d1, \t1
+                sbcs    \d2, \d2, \t2
+                sbcs    \d3, \d3, \t3
+                sbcs    \d4, \d4, xzr
+                sbcs    \d5, \d5, xzr
+// Now effectively add 2^384 * w by taking d0 as the input for the last sbc
+                sbc     \d6, \d0, xzr
+.endm
+
+// Input parameters
+
+#define z x0
+#define x x1
+
+// Rotating registers for the intermediate windows
+
+#define d0 x2
+#define d1 x3
+#define d2 x4
+#define d3 x5
+#define d4 x6
+#define d5 x7
+
+// Other temporaries
+
+#define u x8
+#define v x9
+#define w x10
+
+bignum_demont_p384:
+
+// Set up an initial window with the input x and an extra leading zero
+
+                ldp     d0, d1, [x]
+                ldp     d2, d3, [x, 16]
+                ldp     d4, d5, [x, 32]
+
+// Systematically scroll left doing 1-step reductions
+
+                montreds d0,d5,d4,d3,d2,d1,d0, u,v,w
+
+                montreds d1,d0,d5,d4,d3,d2,d1, u,v,w
+
+                montreds d2,d1,d0,d5,d4,d3,d2, u,v,w
+
+                montreds d3,d2,d1,d0,d5,d4,d3, u,v,w
+
+                montreds d4,d3,d2,d1,d0,d5,d4, u,v,w
+
+                montreds d5,d4,d3,d2,d1,d0,d5, u,v,w
+
+// This is already our answer with no correction needed
+
+                stp     d0, d1, [z]
+                stp     d2, d3, [z, 16]
+                stp     d4, d5, [z, 32]
+
+                ret
diff --git a/arm/p384/bignum_double_p384.S b/arm/p384/bignum_double_p384.S
new file mode 100644
index 000000000..89a5361c7
--- /dev/null
+++ b/arm/p384/bignum_double_p384.S
@@ -0,0 +1,91 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Double modulo p_384, z := (2 * x) mod p_384, assuming x reduced
+// Input x[6]; output z[6]
+//
+//    extern void bignum_double_p384
+//     (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define x x1
+#define d0 x2
+#define d1 x3
+#define d2 x4
+#define d3 x5
+#define d4 x6
+#define d5 x7
+#define c x8
+#define n0 x9
+#define n1 x10
+#define n2 x11
+#define n3 x12
+#define n4 x13
+#define n5 x14
+
+.text
+.globl bignum_double_p384
+
+bignum_double_p384:
+
+// Double the input number as 2 * x = c + [d5; d4; d3; d2; d1; d0]
+// It's worth considering doing this with extr...63 instead
+
+                ldp     d0, d1, [x]
+                ldp     d2, d3, [x, #16]
+                ldp     d4, d5, [x, #32]
+                adds    d0, d0, d0
+                adcs    d1, d1, d1
+                adcs    d2, d2, d2
+                adcs    d3, d3, d3
+                adcs    d4, d4, d4
+                adcs    d5, d5, d5
+                adc     c, xzr, xzr
+
+// Subtract p_384 to give 2 * x - p_384 = c + [n5; n4; n3; n2; n1; n0]
+
+                mov     n0, 0x00000000ffffffff
+                subs    n0, d0, n0
+                mov     n1, 0xffffffff00000000
+                sbcs    n1, d1, n1
+                mov     n2, 0xfffffffffffffffe
+                sbcs    n2, d2, n2
+                adcs    n3, d3, xzr
+                adcs    n4, d4, xzr
+                adcs    n5, d5, xzr
+                sbcs    c, c, xzr
+
+// Now CF is set (because of inversion) if 2 * x >= p_384, in which case the
+// correct result is [n5; n4; n3; n2; n1; n0], otherwise
+// [d5; d4; d3; d2; d1; d0]
+
+                csel    d0, d0, n0, cc
+                csel    d1, d1, n1, cc
+                csel    d2, d2, n2, cc
+                csel    d3, d3, n3, cc
+                csel    d4, d4, n4, cc
+                csel    d5, d5, n5, cc
+
+// Store the result
+
+                stp     d0, d1, [z]
+                stp     d2, d3, [z, #16]
+                stp     d4, d5, [z, #32]
+
+                ret
diff --git a/arm/p384/bignum_half_p384.S b/arm/p384/bignum_half_p384.S
new file mode 100644
index 000000000..2e7007eae
--- /dev/null
+++ b/arm/p384/bignum_half_p384.S
@@ -0,0 +1,89 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Halve modulo p_384, z := (x / 2) mod p_384, assuming x reduced
+// Input x[6]; output z[6]
+//
+//    extern void bignum_half_p384
+//     (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define x x1
+
+#define d0 x2
+#define d1 x3
+#define d2 x4
+#define d3 x5
+#define d4 x6
+#define d5 x7
+#define d6 x8
+#define d7 x9
+#define m x10
+#define n x11
+
+.text
+.globl bignum_half_p384
+
+bignum_half_p384:
+
+// Load the 4 digits of x
+
+                ldp     d0, d1, [x]
+                ldp     d2, d3, [x, 16]
+                ldp     d4, d5, [x, 32]
+
+// Get a bitmask corresponding to the lowest bit of the input
+
+                and     m, d0, 1
+                neg     m, m
+
+// Do a masked addition of p_384, catching carry in a 7th word
+
+                mov     n, 0x00000000ffffffff
+                and     n, n, m
+                adds    d0, d0, n
+                mov     n, 0xffffffff00000000
+                and     n, n, m
+                adcs    d1, d1, n
+                mov     n, 0xfffffffffffffffe
+                and     n, n, m
+                adcs    d2, d2, n
+                adcs    d3, d3, m
+                adcs    d4, d4, m
+                adcs    d5, d5, m
+                adc     d6, xzr, xzr
+
+// Now shift that sum right one place
+
+                extr    d0, d1, d0, 1
+                extr    d1, d2, d1, 1
+                extr    d2, d3, d2, 1
+                extr    d3, d4, d3, 1
+                extr    d4, d5, d4, 1
+                extr    d5, d6, d5, 1
+
+// Store back
+
+                stp     d0, d1, [z]
+                stp     d2, d3, [z, 16]
+                stp     d4, d5, [z, 32]
+
+// Return
+
+                ret
diff --git a/arm/p384/bignum_mod_n384.S b/arm/p384/bignum_mod_n384.S
new file mode 100644
index 000000000..530ae91be
--- /dev/null
+++ b/arm/p384/bignum_mod_n384.S
@@ -0,0 +1,205 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Reduce modulo group order, z := x mod n_384
+// Input x[k]; output z[6]
+//
+//    extern void bignum_mod_n384
+//     (uint64_t z[static 6], uint64_t k, uint64_t *x);
+//
+// Reduction is modulo the group order of the NIST curve P-384.
+//
+// Standard ARM ABI: X0 = z, X1 = k, X2 = x
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define k x1
+#define x x2
+
+#define m0 x3
+#define m1 x4
+#define m2 x5
+#define m3 x6
+#define m4 x7
+#define m5 x8
+
+#define t0 x9
+#define t1 x10
+#define t2 x11
+#define t3 x12
+#define t4 x13
+#define t5 x14
+
+#define n0 x15
+#define n1 x16
+#define n2 x17
+
+// Aliased to t4
+
+#define q x13
+
+// Aliased to t5
+
+#define d x14
+
+// This is aliased to t5; we get one extra (free-ish?) reg-reg move in the
+// main loop by not using an additional register, which seems an OK decision.
+
+#define t x14
+
+// Loading large constants
+
+.macro movbig nn, n3, n2, n1, n0
+                movz    \nn, \n0
+                movk    \nn, \n1, lsl 16
+                movk    \nn, \n2, lsl 32
+                movk    \nn, \n3, lsl 48
+.endm
+
+.text
+.globl bignum_mod_n384
+
+bignum_mod_n384:
+
+// If the input is already <= 5 words long, go to a trivial "copy" path
+
+                cmp     k, 6
+                bcc     short
+
+// Otherwise load the top 6 digits (top-down) and reduce k by 6
+
+                sub     k, k, 6
+                lsl     t0, k, 3
+                add     t0, t0, x
+                ldp     m4, m5, [t0, 32]
+                ldp     m2, m3, [t0, 16]
+                ldp     m0, m1, [t0]
+
+// Load the complicated three words of 2^384 - n_384 = [0; 0; 0; n2; n1; n0]
+
+                movbig  n0, 0x1313, 0xe695, 0x333a, 0xd68d
+                movbig  n1, 0xa7e5, 0xf24d, 0xb74f, 0x5885
+                movbig  n2, 0x389c, 0xb27e, 0x0bc8, 0xd220
+
+// Reduce the top 6 digits mod n_384 (a conditional subtraction of n_384)
+
+                adds    t0, m0, n0
+                adcs    t1, m1, n1
+                adcs    t2, m2, n2
+                adcs    t3, m3, xzr
+                adcs    t4, m4, xzr
+                adcs    t5, m5, xzr
+                csel    m0, m0, t0, cc
+                csel    m1, m1, t1, cc
+                csel    m2, m2, t2, cc
+                csel    m3, m3, t3, cc
+                csel    m4, m4, t4, cc
+                csel    m5, m5, t5, cc
+
+// Now do (k-6) iterations of 7->6 word modular reduction
+
+                cbz     k, writeback
+loop:
+
+// Compute q = min (m5 + 1) (2^64 - 1)
+
+                adds    q, m5, 1
+                csetm   t0, cs
+                orr     q, q, t0
+
+// [t3;t2;t1;t0] = q * (2^384 - n_384)
+
+                mul     t0, n0, q
+                mul     t1, n1, q
+                mul     t2, n2, q
+
+                umulh   t3, n0, q
+                adds    t1, t1, t3
+                umulh   t3, n1, q
+                adcs    t2, t2, t3
+                umulh   t3, n2, q
+                adc     t3, xzr, t3
+
+// Decrement k and load the next digit
+
+                sub     k, k, 1
+                ldr     d, [x, k, LSL 3]
+
+// Compensate for 2^384 * q
+
+                sub     m5, m5, q
+
+// [m5;m4;t4;t3;t2;t1;t0] = [m5;m4;m3;m2;m1;m0;d] - q * n_384
+
+                adds    t0, d, t0
+                adcs    t1, m0, t1
+                adcs    t2, m1, t2
+                adcs    t3, m2, t3
+                adcs    t4, m3, xzr
+                adcs    m4, m4, xzr
+                adc     m5, m5, xzr
+
+// Now our top word m5 is either zero or all 1s. Use it for a masked
+// addition of n_384, which we can do by a *subtraction* of
+// 2^384 - n_384 from our portion, re-using the constants
+
+                and     t, m5, n0
+                subs    m0, t0, t
+                and     t, m5, n1
+                sbcs    m1, t1, t
+                and     t, m5, n2
+                sbcs    m2, t2, t
+                sbcs    m3, t3, xzr
+                sbcs    t, t4, xzr
+                sbc     m5, m4, xzr
+                mov     m4, t
+
+                cbnz    k, loop
+
+// Finally write back [m5;m4;m3;m2;m1;m0] and return
+
+writeback:
+                stp     m0, m1, [z]
+                stp     m2, m3, [z, 16]
+                stp     m4, m5, [z, 32]
+
+                ret
+
+// Short case: just copy the input with zero-padding
+
+short:
+                mov     m0, xzr
+                mov     m1, xzr
+                mov     m2, xzr
+                mov     m3, xzr
+                mov     m4, xzr
+                mov     m5, xzr
+
+                cbz     k, writeback
+                ldr     m0, [x]
+                subs    k, k, 1
+                beq     writeback
+                ldr     m1, [x, 8]
+                subs    k, k, 1
+                beq     writeback
+                ldr     m2, [x, 16]
+                subs    k, k, 1
+                beq     writeback
+                ldr     m3, [x, 24]
+                subs    k, k, 1
+                beq     writeback
+                ldr     m4, [x, 32]
+                b       writeback
diff --git a/arm/p384/bignum_mod_n384_6.S b/arm/p384/bignum_mod_n384_6.S
new file mode 100644
index 000000000..60b0fa577
--- /dev/null
+++ b/arm/p384/bignum_mod_n384_6.S
@@ -0,0 +1,95 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Reduce modulo group order, z := x mod n_384
+// Input x[6]; output z[6]
+//
+//    extern void bignum_mod_n384_6
+//      (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// Reduction is modulo the group order of the NIST curve P-384.
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define x x1
+
+#define n0 x2
+#define n1 x3
+#define n2 x4
+#define n3 x5
+#define n4 x6
+#define n5 x7
+
+#define d0 x8
+#define d1 x9
+#define d2 x10
+#define d3 x11
+#define d4 x12
+#define d5 x13
+
+.macro movbig x, n3, n2, n1, n0
+                movz    \x, \n0
+                movk    \x, \n1, lsl 16
+                movk    \x, \n2, lsl 32
+                movk    \x, \n3, lsl 48
+.endm
+
+.text
+.globl bignum_mod_n384_6
+
+bignum_mod_n384_6:
+
+// Load the complicated lower three words of n_384
+
+                movbig  n0, 0xecec, 0x196a, 0xccc5, 0x2973
+                movbig  n1, 0x581a, 0x0db2, 0x48b0, 0xa77a
+                movbig  n2, 0xc763, 0x4d81, 0xf437, 0x2ddf
+
+// Load the input number
+
+                ldp     d0, d1, [x]
+                ldp     d2, d3, [x, #16]
+                ldp     d4, d5, [x, #32]
+
+// Do the subtraction. Since the top three words of n_384 are all 1s
+// we can devolve the top to adding 0, thanks to the inverted carry.
+
+                subs    n0, d0, n0
+                sbcs    n1, d1, n1
+                sbcs    n2, d2, n2
+                adcs    n3, d3, xzr
+                adcs    n4, d4, xzr
+                adcs    n5, d5, xzr
+
+// Now if the carry is *clear* (inversion at work) the subtraction carried
+// and hence we should have done nothing, so we reset each n_i = d_i
+
+                csel    n0, d0, n0, cc
+                csel    n1, d1, n1, cc
+                csel    n2, d2, n2, cc
+                csel    n3, d3, n3, cc
+                csel    n4, d4, n4, cc
+                csel    n5, d5, n5, cc
+
+// Store the end result
+
+                stp     n0, n1, [z]
+                stp     n2, n3, [z, #16]
+                stp     n4, n5, [z, #32]
+
+                ret
diff --git a/arm/p384/bignum_mod_p384.S b/arm/p384/bignum_mod_p384.S
new file mode 100644
index 000000000..853b14704
--- /dev/null
+++ b/arm/p384/bignum_mod_p384.S
@@ -0,0 +1,177 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Reduce modulo field characteristic, z := x mod p_384
+// Input x[k]; output z[6]
+//
+//    extern void bignum_mod_p384
+//     (uint64_t z[static 6], uint64_t k, uint64_t *x);
+//
+// Standard ARM ABI: X0 = z, X1 = k, X2 = x
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define k x1
+#define x x2
+
+#define m0 x3
+#define m1 x4
+#define m2 x5
+#define m3 x6
+#define m4 x7
+#define m5 x8
+
+#define t0 x9
+#define t1 x10
+#define t2 x11
+#define t3 x12
+#define t4 x13
+#define t5 x14
+
+#define n0 x15
+#define n1 x16
+#define n2 x17
+
+.text
+.globl bignum_mod_p384
+
+bignum_mod_p384:
+
+// If the input is already <= 5 words long, go to a trivial "copy" path
+
+                cmp     k, 6
+                bcc     short
+
+// Otherwise load the top 6 digits (top-down) and reduce k by 6
+
+                sub     k, k, 6
+                lsl     t0, k, 3
+                add     t0, t0, x
+                ldp     m4, m5, [t0, 32]
+                ldp     m2, m3, [t0, 16]
+                ldp     m0, m1, [t0]
+
+// Load the complicated lower three words of p_384 = [-1;-1;-1;n2;n1;n0]
+
+                mov     n0, 0x00000000ffffffff
+                mov     n1, 0xffffffff00000000
+                mov     n2, 0xfffffffffffffffe
+
+// Reduce the top 6 digits mod p_384 (a conditional subtraction of p_384)
+
+                subs    t0, m0, n0
+                sbcs    t1, m1, n1
+                sbcs    t2, m2, n2
+                adcs    t3, m3, xzr
+                adcs    t4, m4, xzr
+                adcs    t5, m5, xzr
+                csel    m0, m0, t0, cc
+                csel    m1, m1, t1, cc
+                csel    m2, m2, t2, cc
+                csel    m3, m3, t3, cc
+                csel    m4, m4, t4, cc
+                csel    m5, m5, t5, cc
+
+// Now do (k-6) iterations of 7->6 word modular reduction
+
+                cbz     k, writeback
+loop:
+
+// Decrement k and load the next digit as t5. We now want to reduce
+// [m5;m4;m3;m2;m1;m0;t5] |-> [m5;m4;m3;m2;m1;m0]; the shuffling downwards is
+// absorbed into the various ALU operations
+
+                sub     k, k, 1
+                ldr     t5, [x, k, LSL 3]
+
+// Initial quotient approximation q = min (h + 1) (2^64 - 1)
+
+                adds    m5, m5, 1
+                csetm   t3, cs
+                add     m5, m5, t3
+                orn     n1, xzr, t3
+                sub     t2, m5, 1
+                sub     t1, xzr, m5
+
+// Correction term [m5;t2;t1;t0] = q * (2^384 - p_384), using m5 as a temp
+
+                lsl     t0, t1, 32
+                extr    t1, t2, t1, 32
+                lsr     t2, t2, 32
+                adds    t0, t0, m5
+                adcs    t1, t1, xzr
+                adcs    t2, t2, m5
+                adc     m5, xzr, xzr
+
+// Addition to the initial value
+
+                adds    t0, t5, t0
+                adcs    t1, m0, t1
+                adcs    t2, m1, t2
+                adcs    t3, m2, m5
+                adcs    t4, m3, xzr
+                adcs    t5, m4, xzr
+                adc     n1, n1, xzr
+
+// Use net top of the 7-word answer (now in n1) for masked correction
+
+                and     m5, n0, n1
+                adds    m0, t0, m5
+                eor     m5, m5, n1
+                adcs    m1, t1, m5
+                and     m5, n2, n1
+                adcs    m2, t2, m5
+                adcs    m3, t3, n1
+                adcs    m4, t4, n1
+                adc     m5, t5, n1
+
+                cbnz    k, loop
+
+// Finally write back [m5;m4;m3;m2;m1;m0] and return
+
+writeback:
+                stp     m0, m1, [z]
+                stp     m2, m3, [z, 16]
+                stp     m4, m5, [z, 32]
+
+                ret
+
+// Short case: just copy the input with zero-padding
+
+short:
+                mov     m0, xzr
+                mov     m1, xzr
+                mov     m2, xzr
+                mov     m3, xzr
+                mov     m4, xzr
+                mov     m5, xzr
+
+                cbz     k, writeback
+                ldr     m0, [x]
+                subs    k, k, 1
+                beq     writeback
+                ldr     m1, [x, 8]
+                subs    k, k, 1
+                beq     writeback
+                ldr     m2, [x, 16]
+                subs    k, k, 1
+                beq     writeback
+                ldr     m3, [x, 24]
+                subs    k, k, 1
+                beq     writeback
+                ldr     m4, [x, 32]
+                b       writeback
+
diff --git a/arm/p384/bignum_mod_p384_6.S b/arm/p384/bignum_mod_p384_6.S
new file mode 100644
index 000000000..e9aed5820
--- /dev/null
+++ b/arm/p384/bignum_mod_p384_6.S
@@ -0,0 +1,86 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Reduce modulo field characteristic, z := x mod p_384
+// Input x[6]; output z[6]
+//
+//    extern void bignum_mod_p384_6
+//     (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define x x1
+
+#define n0 x2
+#define n1 x3
+#define n2 x4
+#define n3 x5
+#define n4 x6
+#define n5 x7
+
+#define d0 x8
+#define d1 x9
+#define d2 x10
+#define d3 x11
+#define d4 x12
+#define d5 x13
+
+.text
+.globl bignum_mod_p384_6
+
+bignum_mod_p384_6:
+
+// Load the complicated lower three words of p_384 = [-1;-1;-1;n2;n1;n0]
+
+                mov     n0, 0x00000000ffffffff
+                mov     n1, 0xffffffff00000000
+                mov     n2, 0xfffffffffffffffe
+
+// Load the input number
+
+                ldp     d0, d1, [x]
+                ldp     d2, d3, [x, #16]
+                ldp     d4, d5, [x, #32]
+
+// Do the subtraction. Since the top three words of p_384 are all 1s
+// we can devolve the top to adding 0, thanks to the inverted carry.
+
+                subs    n0, d0, n0
+                sbcs    n1, d1, n1
+                sbcs    n2, d2, n2
+                adcs    n3, d3, xzr
+                adcs    n4, d4, xzr
+                adcs    n5, d5, xzr
+
+// Now if the carry is *clear* (inversion at work) the subtraction carried
+// and hence we should have done nothing, so we reset each n_i = d_i
+
+                csel    n0, d0, n0, cc
+                csel    n1, d1, n1, cc
+                csel    n2, d2, n2, cc
+                csel    n3, d3, n3, cc
+                csel    n4, d4, n4, cc
+                csel    n5, d5, n5, cc
+
+// Store the end result
+
+                stp     n0, n1, [z]
+                stp     n2, n3, [z, #16]
+                stp     n4, n5, [z, #32]
+
+                ret
diff --git a/arm/p384/bignum_montmul_p384.S b/arm/p384/bignum_montmul_p384.S
new file mode 100644
index 000000000..8cf49d930
--- /dev/null
+++ b/arm/p384/bignum_montmul_p384.S
@@ -0,0 +1,423 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Montgomery multiply, z := (x * y / 2^384) mod p_384
+// Inputs x[6], y[6]; output z[6]
+//
+//    extern void bignum_montmul_p384
+//     (uint64_t z[static 6], uint64_t x[static 6], uint64_t y[static 6]);
+//
+// Does z := (2^{-384} * x * y) mod p_384, assuming that the inputs x and y
+// satisfy x * y <= 2^384 * p_384 (in particular this is true if we are in
+// the "usual" case x < p_384 and y < p_384).
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = y
+// ----------------------------------------------------------------------------
+
+.globl   bignum_montmul_p384
+
+// ---------------------------------------------------------------------------
+// Macro returning (c,h,l) = 3-word 1s complement (x - y) * (w - z)
+// c,h,l,t should all be different
+// t,h should not overlap w,z
+// ---------------------------------------------------------------------------
+
+.macro muldiffn c,h,l, t, x,y, w,z
+        subs    \t, \x, \y
+        cneg    \t, \t, cc
+        csetm   \c, cc
+        subs    \h, \w, \z
+        cneg    \h, \h, cc
+        mul     \l, \t, \h
+        umulh   \h, \t, \h
+        cinv    \c, \c, cc
+        eor     \l, \l, \c
+        eor     \h, \h, \c
+.endm
+
+// ---------------------------------------------------------------------------
+// Core one-step "short" Montgomery reduction macro. Takes input in
+// [d5;d4;d3;d2;d1;d0] and returns result in [d6;d5;d4;d3;d2;d1],
+// adding to the existing contents of [d5;d4;d3;d2;d1]. It is fine
+// for d6 to be the same register as d0.
+//
+// We want to add (2^384 - 2^128 - 2^96 + 2^32 - 1) * w
+// where w = [d0 + (d0<<32)] mod 2^64
+// ---------------------------------------------------------------------------
+
+.macro          montreds d6,d5,d4,d3,d2,d1,d0, t3,t2,t1
+// Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64
+// Recycle d0 (which we know gets implicitly cancelled) to store it
+                lsl     \t1, \d0, 32
+                add     \d0, \t1, \d0
+// Now let [t2;t1] = 2^64 * w - w + w_hi where w_hi = floor(w/2^32)
+// We need to subtract 2^32 * this, and we can ignore its lower 32
+// bits since by design it will cancel anyway; we only need the w_hi
+// part to get the carry propagation going.
+                lsr     \t1, \d0, 32
+                subs    \t1, \t1, \d0
+                sbc     \t2, \d0, xzr
+// Now select in t1 the field to subtract from d1
+                extr    \t1, \t2, \t1, 32
+// And now get the terms to subtract from d2 and d3
+                lsr     \t2, \t2, 32
+                adds    \t2, \t2, \d0
+                adc     \t3, xzr, xzr
+// Do the subtraction of that portion
+                subs    \d1, \d1, \t1
+                sbcs    \d2, \d2, \t2
+                sbcs    \d3, \d3, \t3
+                sbcs    \d4, \d4, xzr
+                sbcs    \d5, \d5, xzr
+// Now effectively add 2^384 * w by taking d0 as the input for the last sbc
+                sbc     \d6, \d0, xzr
+.endm
+
+#define a0 x3
+#define a1 x4
+#define a2 x5
+#define a3 x6
+#define a4 x7
+#define a5 x8
+#define b0 x9
+#define b1 x10
+#define b2 x11
+#define b3 x12
+#define b4 x13
+#define b5 x14
+
+#define s0 x15
+#define s1 x16
+#define s2 x17
+#define s3 x19
+#define s4 x20
+#define s5 x1
+#define s6 x2
+
+#define t1 x21
+#define t2 x22
+#define t3 x23
+#define t4 x24
+
+bignum_montmul_p384:
+
+// Save some registers
+
+                stp     x19, x20, [sp, -16]!
+                stp     x21, x22, [sp, -16]!
+                stp     x23, x24, [sp, -16]!
+
+// Load in all words of both inputs
+
+                ldp     a0, a1, [x1]
+                ldp     a2, a3, [x1, 16]
+                ldp     a4, a5, [x1, 32]
+                ldp     b0, b1, [x2]
+                ldp     b2, b3, [x2, 16]
+                ldp     b4, b5, [x2, 32]
+
+// Multiply low halves with a 3x3->6 ADK multiplier as [s5;s4;s3;s2;s1;s0]
+
+                mul     s0, a0, b0
+                mul     t1, a1, b1
+                mul     t2, a2, b2
+                umulh   t3, a0, b0
+                umulh   t4, a1, b1
+                umulh   s5, a2, b2
+
+                adds    t3, t3, t1
+                adcs    t4, t4, t2
+                adc     s5, s5, xzr
+
+                adds    s1, t3, s0
+                adcs    s2, t4, t3
+                adcs    s3, s5, t4
+                adc     s4, s5, xzr
+
+                adds    s2, s2, s0
+                adcs    s3, s3, t3
+                adcs    s4, s4, t4
+                adc     s5, s5, xzr
+
+                muldiffn t3,t2,t1, t4, a0,a1, b1,b0
+                adds    xzr, t3, 1
+                adcs    s1, s1, t1
+                adcs    s2, s2, t2
+                adcs    s3, s3, t3
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a0,a2, b2,b0
+                adds    xzr, t3, 1
+                adcs    s2, s2, t1
+                adcs    s3, s3, t2
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a1,a2, b2,b1
+                adds    xzr, t3, 1
+                adcs    s3, s3, t1
+                adcs    s4, s4, t2
+                adc     s5, s5, t3
+
+// Perform three "short" Montgomery steps on the low product
+// This shifts it to an offset compatible with middle terms
+// Stash the result temporarily in the output buffer
+// We could keep this in registers by directly adding to it in the next
+// ADK block, but if anything that seems to be slightly slower
+
+                montreds s0,s5,s4,s3,s2,s1,s0, t1,t2,t3
+
+                montreds s1,s0,s5,s4,s3,s2,s1, t1,t2,t3
+
+                montreds s2,s1,s0,s5,s4,s3,s2, t1,t2,t3
+
+                stp     s3, s4, [x0]
+                stp     s5, s0, [x0, 16]
+                stp     s1, s2, [x0, 32]
+
+// Multiply high halves with a 3x3->6 ADK multiplier as [s5;s4;s3;s2;s1;s0]
+
+                mul     s0, a3, b3
+                mul     t1, a4, b4
+                mul     t2, a5, b5
+                umulh   t3, a3, b3
+                umulh   t4, a4, b4
+                umulh   s5, a5, b5
+
+                adds    t3, t3, t1
+                adcs    t4, t4, t2
+                adc     s5, s5, xzr
+
+                adds    s1, t3, s0
+                adcs    s2, t4, t3
+                adcs    s3, s5, t4
+                adc     s4, s5, xzr
+
+                adds    s2, s2, s0
+                adcs    s3, s3, t3
+                adcs    s4, s4, t4
+                adc     s5, s5, xzr
+
+                muldiffn t3,t2,t1, t4, a3,a4, b4,b3
+                adds    xzr, t3, 1
+                adcs    s1, s1, t1
+                adcs    s2, s2, t2
+                adcs    s3, s3, t3
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a3,a5, b5,b3
+                adds    xzr, t3, 1
+                adcs    s2, s2, t1
+                adcs    s3, s3, t2
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a4,a5, b5,b4
+                adds    xzr, t3, 1
+                adcs    s3, s3, t1
+                adcs    s4, s4, t2
+                adc     s5, s5, t3
+
+// Compute sign-magnitude a0,[a5,a4,a3] = x_hi - x_lo
+
+                subs    a3, a3, a0
+                sbcs    a4, a4, a1
+                sbcs    a5, a5, a2
+                sbc     a0, xzr, xzr
+                adds    xzr, a0, 1
+                eor     a3, a3, a0
+                adcs    a3, a3, xzr
+                eor     a4, a4, a0
+                adcs    a4, a4, xzr
+                eor     a5, a5, a0
+                adc     a5, a5, xzr
+
+// Compute sign-magnitude b5,[b2,b1,b0] = y_lo - y_hi
+
+                subs    b0, b0, b3
+                sbcs    b1, b1, b4
+                sbcs    b2, b2, b5
+                sbc     b5, xzr, xzr
+
+                adds    xzr, b5, 1
+                eor     b0, b0, b5
+                adcs    b0, b0, xzr
+                eor     b1, b1, b5
+                adcs    b1, b1, xzr
+                eor     b2, b2, b5
+                adc     b2, b2, xzr
+
+// Save the correct sign for the sub-product in b5
+
+                eor     b5, a0, b5
+
+// Add the high H to the modified low term L' and re-stash 6 words,
+// keeping top word in s6
+
+                ldp     t1, t2, [x0]
+                adds    s0, s0, t1
+                adcs    s1, s1, t2
+                ldp     t1, t2, [x0, 16]
+                adcs    s2, s2, t1
+                adcs    s3, s3, t2
+                ldp     t1, t2, [x0, 32]
+                adcs    s4, s4, t1
+                adcs    s5, s5, t2
+                adc     s6, xzr, xzr
+                stp     s0, s1, [x0]
+                stp     s2, s3, [x0, 16]
+                stp     s4, s5, [x0, 32]
+
+// Multiply with yet a third 3x3 ADK for the complex mid-term
+
+                mul     s0, a3, b0
+                mul     t1, a4, b1
+                mul     t2, a5, b2
+                umulh   t3, a3, b0
+                umulh   t4, a4, b1
+                umulh   s5, a5, b2
+
+                adds    t3, t3, t1
+                adcs    t4, t4, t2
+                adc     s5, s5, xzr
+
+                adds    s1, t3, s0
+                adcs    s2, t4, t3
+                adcs    s3, s5, t4
+                adc     s4, s5, xzr
+
+                adds    s2, s2, s0
+                adcs    s3, s3, t3
+                adcs    s4, s4, t4
+                adc     s5, s5, xzr
+
+                muldiffn t3,t2,t1, t4, a3,a4, b1,b0
+                adds    xzr, t3, 1
+                adcs    s1, s1, t1
+                adcs    s2, s2, t2
+                adcs    s3, s3, t3
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a3,a5, b2,b0
+                adds    xzr, t3, 1
+                adcs    s2, s2, t1
+                adcs    s3, s3, t2
+                adcs    s4, s4, t3
+                adc     s5, s5, t3
+
+                muldiffn t3,t2,t1, t4, a4,a5, b2,b1
+                adds    xzr, t3, 1
+                adcs    s3, s3, t1
+                adcs    s4, s4, t2
+                adc     s5, s5, t3
+
+// Unstash the H + L' sum to add in twice
+
+                ldp     a0, a1, [x0]
+                ldp     a2, a3, [x0, 16]
+                ldp     a4, a5, [x0, 32]
+
+// Set up a sign-modified version of the mid-product in a long accumulator
+// as [b3;b2;b1;b0;s5;s4;s3;s2;s1;s0], adding in the H + L' term once with
+// zero offset as this signed value is created
+
+                adds    xzr, b5, 1
+                eor     s0, s0, b5
+                adcs    s0, s0, a0
+                eor     s1, s1, b5
+                adcs    s1, s1, a1
+                eor     s2, s2, b5
+                adcs    s2, s2, a2
+                eor     s3, s3, b5
+                adcs    s3, s3, a3
+                eor     s4, s4, b5
+                adcs    s4, s4, a4
+                eor     s5, s5, b5
+                adcs    s5, s5, a5
+                adcs    b0, b5, s6
+                adcs    b1, b5, xzr
+                adcs    b2, b5, xzr
+                adc     b3, b5, xzr
+
+// Add in the stashed H + L' term an offset of 3 words as well
+
+                adds    s3, s3, a0
+                adcs    s4, s4, a1
+                adcs    s5, s5, a2
+                adcs    b0, b0, a3
+                adcs    b1, b1, a4
+                adcs    b2, b2, a5
+                adc     b3, b3, s6
+
+// Do three more Montgomery steps on the composed term
+
+                montreds s0,s5,s4,s3,s2,s1,s0, t1,t2,t3
+                montreds s1,s0,s5,s4,s3,s2,s1, t1,t2,t3
+                montreds s2,s1,s0,s5,s4,s3,s2, t1,t2,t3
+
+                adds    b0, b0, s0
+                adcs    b1, b1, s1
+                adcs    b2, b2, s2
+                adc     b3, b3, xzr
+
+// Because of the way we added L' in two places, we can overspill by
+// more than usual in Montgomery, with the result being only known to
+// be < 3 * p_384, not the usual < 2 * p_384. So now we do a more
+// elaborate final correction in the style of bignum_cmul_p384, just
+// a little bit simpler because we know q is small.
+
+                add     t2, b3, 1
+                lsl     t1, t2, 32
+                subs    t4, t2, t1
+                sbc     t1, t1, xzr
+
+                adds    s3, s3, t4
+                adcs    s4, s4, t1
+                adcs    s5, s5, t2
+                adcs    b0, b0, xzr
+                adcs    b1, b1, xzr
+                adcs    b2, b2, xzr
+
+                csetm   t2, cc
+
+                mov     t3, 0x00000000ffffffff
+                and     t3, t3, t2
+                adds    s3, s3, t3
+                eor     t3, t3, t2
+                adcs    s4, s4, t3
+                mov     t3, 0xfffffffffffffffe
+                and     t3, t3, t2
+                adcs    s5, s5, t3
+                adcs    b0, b0, t2
+                adcs    b1, b1, t2
+                adc     b2, b2, t2
+
+// Write back the result
+
+                stp     s3, s4, [x0]
+                stp     s5, b0, [x0, 16]
+                stp     b1, b2, [x0, 32]
+
+// Restore registers and return
+
+                ldp     x23, x24, [sp], 16
+                ldp     x21, x22, [sp], 16
+                ldp     x19, x20, [sp], 16
+
+                ret
diff --git a/arm/p384/bignum_montsqr_p384.S b/arm/p384/bignum_montsqr_p384.S
new file mode 100644
index 000000000..789c377f1
--- /dev/null
+++ b/arm/p384/bignum_montsqr_p384.S
@@ -0,0 +1,380 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Montgomery square, z := (x^2 / 2^384) mod p_384
+// Input x[6]; output z[6]
+//
+//    extern void bignum_montsqr_p384
+//     (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// Does z := (x^2 / 2^384) mod p_384, assuming x^2 <= 2^384 * p_384, which is
+// guaranteed in particular if x < p_384 initially (the "intended" case).
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+.globl   bignum_montsqr_p384
+
+// ---------------------------------------------------------------------------
+// Macro returning (c,h,l) = 3-word 1s complement (x - y) * (w - z)
+// c,h,l,t should all be different
+// t,h should not overlap w,z
+// ---------------------------------------------------------------------------
+
+.macro muldiffn c,h,l, t, x,y, w,z
+        subs    \t, \x, \y
+        cneg    \t, \t, cc
+        csetm   \c, cc
+        subs    \h, \w, \z
+        cneg    \h, \h, cc
+        mul     \l, \t, \h
+        umulh   \h, \t, \h
+        cinv    \c, \c, cc
+        eor     \l, \l, \c
+        eor     \h, \h, \c
+.endm
+
+// ---------------------------------------------------------------------------
+// Core one-step "short" Montgomery reduction macro. Takes input in
+// [d5;d4;d3;d2;d1;d0] and returns result in [d6;d5;d4;d3;d2;d1],
+// adding to the existing contents of [d5;d4;d3;d2;d1]. It is fine
+// for d6 to be the same register as d0.
+//
+// We want to add (2^384 - 2^128 - 2^96 + 2^32 - 1) * w
+// where w = [d0 + (d0<<32)] mod 2^64
+// ---------------------------------------------------------------------------
+
+.macro          montreds d6,d5,d4,d3,d2,d1,d0, t3,t2,t1
+// Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64
+// Recycle d0 (which we know gets implicitly cancelled) to store it
+                lsl     \t1, \d0, 32
+                add     \d0, \t1, \d0
+// Now let [t2;t1] = 2^64 * w - w + w_hi where w_hi = floor(w/2^32)
+// We need to subtract 2^32 * this, and we can ignore its lower 32
+// bits since by design it will cancel anyway; we only need the w_hi
+// part to get the carry propagation going.
+                lsr     \t1, \d0, 32
+                subs    \t1, \t1, \d0
+                sbc     \t2, \d0, xzr
+// Now select in t1 the field to subtract from d1
+                extr    \t1, \t2, \t1, 32
+// And now get the terms to subtract from d2 and d3
+                lsr     \t2, \t2, 32
+                adds    \t2, \t2, \d0
+                adc     \t3, xzr, xzr
+// Do the subtraction of that portion
+                subs    \d1, \d1, \t1
+                sbcs    \d2, \d2, \t2
+                sbcs    \d3, \d3, \t3
+                sbcs    \d4, \d4, xzr
+                sbcs    \d5, \d5, xzr
+// Now effectively add 2^384 * w by taking d0 as the input for the last sbc
+                sbc     \d6, \d0, xzr
+.endm
+
+#define a0 x2
+#define a1 x3
+#define a2 x4
+#define a3 x5
+#define a4 x6
+#define a5 x7
+
+#define c0 x8
+#define c1 x9
+#define c2 x10
+#define c3 x11
+#define c4 x12
+#define c5 x13
+#define d1 x14
+#define d2 x15
+#define d3 x16
+#define d4 x17
+
+bignum_montsqr_p384:
+
+// Load in all words of the input
+
+                ldp     a0, a1, [x1]
+                ldp     a2, a3, [x1, 16]
+                ldp     a4, a5, [x1, 32]
+
+// Square the low half getting a result in [c5;c4;c3;c2;c1;c0]
+
+                mul     d1, a0, a1
+                mul     d2, a0, a2
+                mul     d3, a1, a2
+                mul     c0, a0, a0
+                mul     c2, a1, a1
+                mul     c4, a2, a2
+
+                umulh   d4, a0, a1
+                adds    d2, d2, d4
+                umulh   d4, a0, a2
+                adcs    d3, d3, d4
+                umulh   d4, a1, a2
+                adcs    d4, d4, xzr
+
+                umulh   c1, a0, a0
+                umulh   c3, a1, a1
+                umulh   c5, a2, a2
+
+                adds    d1, d1, d1
+                adcs    d2, d2, d2
+                adcs    d3, d3, d3
+                adcs    d4, d4, d4
+                adc     c5, c5, xzr
+
+                adds    c1, c1, d1
+                adcs    c2, c2, d2
+                adcs    c3, c3, d3
+                adcs    c4, c4, d4
+                adc     c5, c5, xzr
+
+// Perform three "short" Montgomery steps on the low square
+// This shifts it to an offset compatible with middle product
+// Stash the result temporarily in the output buffer (to avoid more registers)
+
+                montreds c0,c5,c4,c3,c2,c1,c0, d1,d2,d3
+
+                montreds c1,c0,c5,c4,c3,c2,c1, d1,d2,d3
+
+                montreds c2,c1,c0,c5,c4,c3,c2, d1,d2,d3
+
+                stp     c3, c4, [x0]
+                stp     c5, c0, [x0, 16]
+                stp     c1, c2, [x0, 32]
+
+// Compute product of the cross-term with ADK 3x3->6 multiplier
+
+#define a0 x2
+#define a1 x3
+#define a2 x4
+#define a3 x5
+#define a4 x6
+#define a5 x7
+#define s0 x8
+#define s1 x9
+#define s2 x10
+#define s3 x11
+#define s4 x12
+#define s5 x13
+
+#define l1 x14
+#define l2 x15
+#define h0 x16
+#define h1 x17
+#define h2 x1
+
+#define s6 h1
+#define c  l1
+#define h  l2
+#define l  h0
+#define t  h1
+
+                mul     s0, a0, a3
+                mul     l1, a1, a4
+                mul     l2, a2, a5
+                umulh   h0, a0, a3
+                umulh   h1, a1, a4
+                umulh   h2, a2, a5
+
+                adds    h0, h0, l1
+                adcs    h1, h1, l2
+                adc     h2, h2, xzr
+
+                adds    s1, h0, s0
+                adcs    s2, h1, h0
+                adcs    s3, h2, h1
+                adc     s4, h2, xzr
+
+                adds    s2, s2, s0
+                adcs    s3, s3, h0
+                adcs    s4, s4, h1
+                adc     s5, h2, xzr
+
+                muldiffn c,h,l, t, a0,a1, a4,a3
+                adds    xzr, c, 1
+                adcs    s1, s1, l
+                adcs    s2, s2, h
+                adcs    s3, s3, c
+                adcs    s4, s4, c
+                adc     s5, s5, c
+
+                muldiffn c,h,l, t, a0,a2, a5,a3
+                adds    xzr, c, 1
+                adcs    s2, s2, l
+                adcs    s3, s3, h
+                adcs    s4, s4, c
+                adc     s5, s5, c
+
+                muldiffn c,h,l, t, a1,a2, a5,a4
+                adds    xzr, c, 1
+                adcs    s3, s3, l
+                adcs    s4, s4, h
+                adc     s5, s5, c
+
+// Double it and add the stashed Montgomerified low square
+
+                adds    s0, s0, s0
+                adcs    s1, s1, s1
+                adcs    s2, s2, s2
+                adcs    s3, s3, s3
+                adcs    s4, s4, s4
+                adcs    s5, s5, s5
+                adc     s6, xzr, xzr
+
+                ldp     a0, a1, [x0]
+                adds    s0, s0, a0
+                adcs    s1, s1, a1
+                ldp     a0, a1, [x0, 16]
+                adcs    s2, s2, a0
+                adcs    s3, s3, a1
+                ldp     a0, a1, [x0, 32]
+                adcs    s4, s4, a0
+                adcs    s5, s5, a1
+                adc     s6, s6, xzr
+
+// Montgomery-reduce the combined low and middle term another thrice
+
+                montreds s0,s5,s4,s3,s2,s1,s0, a0,a1,a2
+
+                montreds s1,s0,s5,s4,s3,s2,s1, a0,a1,a2
+
+                montreds s2,s1,s0,s5,s4,s3,s2, a0,a1,a2
+
+                adds    s6, s6, s0
+                adcs    s0, s1, xzr
+                adcs    s1, s2, xzr
+                adcs    s2, xzr, xzr
+
+// Our sum so far is in [s2;s1;s0;s6;s5;s4;s3]
+// Choose more intuitive names
+
+#define r0 x11
+#define r1 x12
+#define r2 x13
+#define r3 x17
+#define r4 x8
+#define r5 x9
+#define r6 x10
+
+// Remind ourselves what else we can't destroy
+
+#define a3 x5
+#define a4 x6
+#define a5 x7
+
+// So we can have these as temps
+
+#define t1 x1
+#define t2 x14
+#define t3 x15
+#define t4 x16
+
+// Add in all the pure squares 33 + 44 + 55
+
+                mul     t1, a3, a3
+                adds    r0, r0, t1
+                mul     t2, a4, a4
+                mul     t3, a5, a5
+                umulh   t1, a3, a3
+                adcs    r1, r1, t1
+                umulh   t1, a4, a4
+                adcs    r2, r2, t2
+                adcs    r3, r3, t1
+                umulh   t1, a5, a5
+                adcs    r4, r4, t3
+                adcs    r5, r5, t1
+                adc     r6, r6, xzr
+
+// Now compose the 34 + 35 + 45 terms, which need doubling
+
+                mul     t1, a3, a4
+                mul     t2, a3, a5
+                mul     t3, a4, a5
+                umulh   t4, a3, a4
+                adds    t2, t2, t4
+                umulh   t4, a3, a5
+                adcs    t3, t3, t4
+                umulh   t4, a4, a5
+                adc     t4, t4, xzr
+
+// Double and add. Recycle one of the no-longer-needed inputs as a temp
+
+#define t5 x5
+
+                adds    t1, t1, t1
+                adcs    t2, t2, t2
+                adcs    t3, t3, t3
+                adcs    t4, t4, t4
+                adc     t5, xzr, xzr
+
+                adds    r1, r1, t1
+                adcs    r2, r2, t2
+                adcs    r3, r3, t3
+                adcs    r4, r4, t4
+                adcs    r5, r5, t5
+                adc     r6, r6, xzr
+
+// We know, writing B = 2^{6*64} that the full implicit result is
+// B^2 c <= z + (B - 1) * p < B * p + (B - 1) * p < 2 * B * p,
+// so the top half is certainly < 2 * p. If c = 1 already, we know
+// subtracting p will give the reduced modulus. But now we do a
+// comparison to catch cases where the residue is >= p.
+// First set [0;0;0;t3;t2;t1] = 2^384 - p_384
+
+                mov     t1, 0xffffffff00000001
+                mov     t2, 0x00000000ffffffff
+                mov     t3, 0x0000000000000001
+
+// Let dd = [] be the 6-word intermediate result.
+// Set CF if the addition dd + (2^384 - p_384) >= 2^384, hence iff dd >= p_384.
+
+                adds    xzr, r0, t1
+                adcs    xzr, r1, t2
+                adcs    xzr, r2, t3
+                adcs    xzr, r3, xzr
+                adcs    xzr, r4, xzr
+                adcs    xzr, r5, xzr
+
+// Now just add this new carry into the existing r6. It's easy to see they
+// can't both be 1 by our range assumptions, so this gives us a {0,1} flag
+
+                adc     r6, r6, xzr
+
+// Now convert it into a bitmask
+
+                sub     r6, xzr, r6
+
+// Masked addition of 2^384 - p_384, hence subtraction of p_384
+
+                and     t1, t1, r6
+                adds    r0, r0, t1
+                and     t2, t2, r6
+                adcs    r1, r1, t2
+                and     t3, t3, r6
+                adcs    r2, r2, t3
+                adcs    r3, r3, xzr
+                adcs    r4, r4, xzr
+                adc     r5, r5, xzr
+
+// Store it back
+
+                stp     r0, r1, [x0]
+                stp     r2, r3, [x0, 16]
+                stp     r4, r5, [x0, 32]
+
+                ret
diff --git a/arm/p384/bignum_neg_p384.S b/arm/p384/bignum_neg_p384.S
new file mode 100644
index 000000000..6a5d56b23
--- /dev/null
+++ b/arm/p384/bignum_neg_p384.S
@@ -0,0 +1,87 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Negate modulo p_384, z := (-x) mod p_384, assuming x reduced
+// Input x[6]; output z[6]
+//
+//    extern void bignum_neg_p384 (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define x x1
+
+#define p x2
+#define t x3
+
+#define d0 x4
+#define d1 x5
+#define d2 x6
+#define d3 x7
+#define d4 x8
+#define d5 x9
+
+.text
+.globl bignum_neg_p384
+
+bignum_neg_p384:
+
+// Load the 6 digits of x
+
+                ldp     d0, d1, [x]
+                ldp     d2, d3, [x, 16]
+                ldp     d4, d5, [x, 32]
+
+// Set a bitmask p for the input being nonzero, so that we avoid doing
+// -0 = p_384 and hence maintain strict modular reduction
+
+                orr     p, d0, d1
+                orr     t, d2, d3
+                orr     p, p, t
+                orr     t, d4, d5
+                orr     p, p, t
+                cmp     p, 0
+                csetm   p, ne
+
+// Load and mask the complicated lower three words of
+// p_384 = [-1;-1;-1;n2;n1;n0] and subtract, using mask itself for upper digits
+
+                mov     t, 0x00000000ffffffff
+                and     t, t, p
+                subs    d0, t, d0
+
+                mov     t, 0xffffffff00000000
+                and     t, t, p
+                sbcs    d1, t, d1
+
+                mov     t, 0xfffffffffffffffe
+                and     t, t, p
+                sbcs    d2, t, d2
+
+                sbcs    d3, p, d3
+                sbcs    d4, p, d4
+                sbc     d5, p, d5
+
+// Write back the result
+
+                stp     d0, d1, [z]
+                stp     d2, d3, [z, 16]
+                stp     d4, d5, [z, 32]
+
+// Return
+
+                ret
diff --git a/arm/p384/bignum_optneg_p384.S b/arm/p384/bignum_optneg_p384.S
new file mode 100644
index 000000000..aea904229
--- /dev/null
+++ b/arm/p384/bignum_optneg_p384.S
@@ -0,0 +1,101 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Optionally negate modulo p_384, z := (-x) mod p_384 (if p nonzero) or
+// z := x (if p zero), assuming x reduced
+// Inputs p, x[6]; output z[6]
+//
+//    extern void bignum_optneg_p384
+//      (uint64_t z[static 6], uint64_t p, uint64_t x[static 6]);
+//
+// Standard ARM ABI: X0 = z, X1 = p, X2 = x
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define p x1
+#define x x2
+
+#define d0 x3
+#define d1 x4
+#define d2 x5
+#define d3 x6
+#define d4 x7
+#define d5 x8
+#define n0 x9
+#define n1 x10
+#define n2 x11
+#define n3 x12
+#define n4 x13
+#define n5 x14
+
+.text
+.globl bignum_optneg_p384
+
+bignum_optneg_p384:
+
+// Load the 6 digits of x
+
+                ldp     d0, d1, [x]
+                ldp     d2, d3, [x, 16]
+                ldp     d4, d5, [x, 32]
+
+// Adjust p by zeroing it if the input is zero (to avoid giving -0 = p, which
+// is not strictly reduced even though it's correct modulo p)
+
+                orr     n0, d0, d1
+                orr     n1, d2, d3
+                orr     n2, d4, d5
+                orr     n3, n0, n1
+                orr     n4, n2, n3
+                cmp     n4, 0
+                csel    p, xzr, p, eq
+
+// Load the complicated lower three words of p_384 = [-1;-1;-1;n2;n1;n0] and -1
+
+                mov     n0, 0x00000000ffffffff
+                mov     n1, 0xffffffff00000000
+                mov     n2, 0xfffffffffffffffe
+                mov     n5, 0xffffffffffffffff
+
+// Do the subtraction, which by hypothesis does not underflow
+
+                subs    n0, n0, d0
+                sbcs    n1, n1, d1
+                sbcs    n2, n2, d2
+                sbcs    n3, n5, d3
+                sbcs    n4, n5, d4
+                sbcs    n5, n5, d5
+
+// Set condition code if original x is nonzero and p was nonzero
+
+                cmp     p, 0
+
+// Hence multiplex and write back
+
+                csel    n0, n0, d0, ne
+                csel    n1, n1, d1, ne
+                csel    n2, n2, d2, ne
+                csel    n3, n3, d3, ne
+                csel    n4, n4, d4, ne
+                csel    n5, n5, d5, ne
+
+                stp     n0, n1, [z]
+                stp     n2, n3, [z, 16]
+                stp     n4, n5, [z, 32]
+
+// Return
+
+                ret
diff --git a/arm/p384/bignum_sub_p384.S b/arm/p384/bignum_sub_p384.S
new file mode 100644
index 000000000..2ef51ef2b
--- /dev/null
+++ b/arm/p384/bignum_sub_p384.S
@@ -0,0 +1,84 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Subtract modulo p_384, z := (x - y) mod p_384
+// Inputs x[6], y[6]; output z[6]
+//
+//    extern void bignum_sub_p384
+//     (uint64_t z[static 6], uint64_t x[static 6], uint64_t y[static 6]);
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = y
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define x x1
+#define y x2
+
+#define c x3
+#define l x4
+#define d0 x5
+#define d1 x6
+#define d2 x7
+#define d3 x8
+#define d4 x9
+#define d5 x10
+
+.text
+.globl bignum_sub_p384
+
+bignum_sub_p384:
+
+// First just subtract the numbers as [d5; d4; d3; d2; d1; d0]
+// Set a mask based on (inverted) carry indicating x < y = correction is needed
+
+                ldp     d0, d1, [x]
+                ldp     l, c, [y]
+                subs    d0, d0, l
+                sbcs    d1, d1, c
+                ldp     d2, d3, [x, #16]
+                ldp     l, c, [y, #16]
+                sbcs    d2, d2, l
+                sbcs    d3, d3, c
+                ldp     d4, d5, [x, #32]
+                ldp     l, c, [y, #32]
+                sbcs    d4, d4, l
+                sbcs    d5, d5, c
+
+// Create a mask for the condition x < y, when we need to correct
+
+                csetm   c, cc
+
+// Now correct by adding masked p_384
+
+                mov     l, 0x00000000ffffffff
+                and     l, l, c
+                adds    d0, d0, l
+                eor     l, l, c
+                adcs    d1, d1, l
+                mov     l, 0xfffffffffffffffe
+                and     l, l, c
+                adcs    d2, d2, l
+                adcs    d3, d3, c
+                adcs    d4, d4, c
+                adc     d5, d5, c
+
+// Store the result
+
+                stp     d0, d1, [z]
+                stp     d2, d3, [z, #16]
+                stp     d4, d5, [z, #32]
+
+                ret
diff --git a/arm/p384/bignum_tomont_p384.S b/arm/p384/bignum_tomont_p384.S
new file mode 100644
index 000000000..c3483a56c
--- /dev/null
+++ b/arm/p384/bignum_tomont_p384.S
@@ -0,0 +1,133 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Convert to Montgomery form z := (2^256 * x) mod p_256
+// Input x[6]; output z[6]
+//
+//    extern void bignum_tomont_p384
+//     (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+.globl   bignum_tomont_p384
+
+// ----------------------------------------------------------------------------
+// Core "x |-> (2^64 * x) mod p_384" macro, with x assumed to be < p_384.
+// Input is in [d6;d5;d4;d3;d2;d1] and output in [d5;d4;d3;d2;d1;d0]
+// using d6 as well as t1, t2, t3 as temporaries.
+// ----------------------------------------------------------------------------
+
+.macro modstep_p384 d6,d5,d4,d3,d2,d1,d0, t1,t2,t3
+// Initial quotient approximation q = min (h + 1) (2^64 - 1)
+                adds    \d6, \d6, 1
+                csetm   \t3, cs
+                add     \d6, \d6, \t3
+                orn     \t3, xzr, \t3
+                sub     \t2, \d6, 1
+                sub     \t1, xzr, \d6
+// Correction term [d6;t2;t1;d0] = q * (2^384 - p_384)
+                lsl     \d0, \t1, 32
+                extr    \t1, \t2, \t1, 32
+                lsr     \t2, \t2, 32
+                adds    \d0, \d0, \d6
+                adcs    \t1, \t1, xzr
+                adcs    \t2, \t2, \d6
+                adc     \d6, xzr, xzr
+// Addition to the initial value
+                adds    \d1, \d1, \t1
+                adcs    \d2, \d2, \t2
+                adcs    \d3, \d3, \d6
+                adcs    \d4, \d4, xzr
+                adcs    \d5, \d5, xzr
+                adc     \t3, \t3, xzr
+// Use net top of the 7-word answer in t3 for masked correction
+                mov     \t1, 0x00000000ffffffff
+                and     \t1, \t1, \t3
+                adds    \d0, \d0, \t1
+                eor     \t1, \t1, \t3
+                adcs    \d1, \d1, \t1
+                mov     \t1, 0xfffffffffffffffe
+                and     \t1, \t1, \t3
+                adcs    \d2, \d2, \t1
+                adcs    \d3, \d3, \t3
+                adcs    \d4, \d4, \t3
+                adc     \d5, \d5, \t3
+.endm
+
+bignum_tomont_p384:
+
+#define d0 x2
+#define d1 x3
+#define d2 x4
+#define d3 x5
+#define d4 x6
+#define d5 x7
+#define d6 x8
+
+#define t1 x9
+#define t2 x10
+#define t3 x11
+
+#define n0 x8
+#define n1 x9
+#define n2 x10
+#define n3 x11
+#define n4 x12
+#define n5 x1
+
+// Load the inputs
+
+                ldp     d0, d1, [x1]
+                ldp     d2, d3, [x1, 16]
+                ldp     d4, d5, [x1, 32]
+
+// Do an initial reduction to make sure this is < p_384, using just
+// a copy of the bignum_mod_p384 code. This is needed to set up the
+// invariant "input < p_384" for the main modular reduction steps.
+
+                mov     n0, 0x00000000ffffffff
+                mov     n1, 0xffffffff00000000
+                mov     n2, 0xfffffffffffffffe
+                subs    n0, d0, n0
+                sbcs    n1, d1, n1
+                sbcs    n2, d2, n2
+                adcs    n3, d3, xzr
+                adcs    n4, d4, xzr
+                adcs    n5, d5, xzr
+                csel    d0, d0, n0, cc
+                csel    d1, d1, n1, cc
+                csel    d2, d2, n2, cc
+                csel    d3, d3, n3, cc
+                csel    d4, d4, n4, cc
+                csel    d5, d5, n5, cc
+
+// Successively multiply by 2^64 and reduce
+
+                modstep_p384 d5,d4,d3,d2,d1,d0,d6, t1,t2,t3
+                modstep_p384 d4,d3,d2,d1,d0,d6,d5, t1,t2,t3
+                modstep_p384 d3,d2,d1,d0,d6,d5,d4, t1,t2,t3
+                modstep_p384 d2,d1,d0,d6,d5,d4,d3, t1,t2,t3
+                modstep_p384 d1,d0,d6,d5,d4,d3,d2, t1,t2,t3
+                modstep_p384 d0,d6,d5,d4,d3,d2,d1, t1,t2,t3
+
+// Store the result and return
+
+                stp     d1, d2, [x0]
+                stp     d3, d4, [x0, #16]
+                stp     d5, d6, [x0, #32]
+
+                ret
diff --git a/arm/p384/bignum_triple_p384.S b/arm/p384/bignum_triple_p384.S
new file mode 100644
index 000000000..e13ce79c2
--- /dev/null
+++ b/arm/p384/bignum_triple_p384.S
@@ -0,0 +1,130 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Triple modulo p_384, z := (3 * x) mod p_384
+// Input x[6]; output z[6]
+//
+//    extern void bignum_triple_p384
+//     (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// The input x can be any 6-digit bignum, not necessarily reduced modulo p_384,
+// and the result is always fully reduced, i.e. z = (3 * x) mod p_384.
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+#define z x0
+#define x x1
+
+#define d0 x2
+#define d1 x3
+#define d2 x4
+#define d3 x5
+#define d4 x6
+#define d5 x7
+#define h x8
+
+// Slightly offset aliases for the d_i for readability.
+
+#define a0 x3
+#define a1 x4
+#define a2 x5
+#define a3 x6
+#define a4 x7
+#define a5 x8
+
+// More aliases for the same thing at different stages
+
+#define q x8
+#define c x8
+
+// Other temporary variables
+
+#define t0 x9
+#define t1 x10
+
+.text
+.globl bignum_triple_p384
+
+bignum_triple_p384:
+
+// Load the inputs
+
+                ldp     a0, a1, [x]
+                ldp     a2, a3, [x, #16]
+                ldp     a4, a5, [x, #32]
+
+// First do the multiplication by 3, getting z = [h; d5; ...; d0]
+
+                lsl     d0, a0, 1
+                adds    d0, d0, a0
+                extr    d1, a1, a0, 63
+                adcs    d1, d1, a1
+                extr    d2, a2, a1, 63
+                adcs    d2, d2, a2
+                extr    d3, a3, a2, 63
+                adcs    d3, d3, a3
+                extr    d4, a4, a3, 63
+                adcs    d4, d4, a4
+                extr    d5, a5, a4, 63
+                adcs    d5, d5, a5
+                lsr     h, a5, 63
+                adc     h, h, xzr
+
+// For this limited range a simple quotient estimate of q = h + 1 works, where
+// h = floor(z / 2^384). Then -p_384 <= z - q * p_384 < p_384, so we just need
+// to subtract q * p_384 and then if that's negative, add back p_384.
+
+                add     q, h, 1
+
+// Initial subtraction of z - q * p_384, with bitmask c for the carry
+// Actually done as an addition of (z - 2^384 * h) + q * (2^384 - p_384)
+// which, because q = h + 1, is exactly 2^384 + (z - q * p_384), and
+// therefore CF <=> 2^384 + (z - q * p_384) >= 2^384 <=> z >= q * p_384.
+
+                lsl     t1, q, 32
+                subs    t0, q, t1
+                sbc     t1, t1, xzr
+
+                adds    d0, d0, t0
+                adcs    d1, d1, t1
+                adcs    d2, d2, q
+                adcs    d3, d3, xzr
+                adcs    d4, d4, xzr
+                adcs    d5, d5, xzr
+                csetm   c, cc
+
+// Use the bitmask c for final masked addition of p_384.
+
+                mov     t0, 0x00000000ffffffff
+                and     t0, t0, c
+                adds    d0, d0, t0
+                eor     t0, t0, c
+                adcs    d1, d1, t0
+                mov     t0, 0xfffffffffffffffe
+                and     t0, t0, c
+                adcs    d2, d2, t0
+                adcs    d3, d3, c
+                adcs    d4, d4, c
+                adc     d5, d5, c
+
+// Store the result
+
+                stp     d0, d1, [z]
+                stp     d2, d3, [z, #16]
+                stp     d4, d5, [z, #32]
+
+                ret
