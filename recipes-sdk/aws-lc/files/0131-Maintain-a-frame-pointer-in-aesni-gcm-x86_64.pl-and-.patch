From 4263903bd1d15d56c47cbd6440bea657df2c142e Mon Sep 17 00:00:00 2001
From: David Benjamin <davidben@google.com>
Date: Wed, 18 Jan 2023 18:55:39 -0500
Subject: [PATCH] Maintain a frame pointer in aesni-gcm-x86_64.pl and add SEH
 unwind codes

Some profiling systems cannot unwind with CFI and benefit from having a
frame pointer. Since this code doesn't have enough register pressure to
actually need to use rbp as a general register, this change tweaks
things so that a frame pointer is preserved.

As this would invalidate the SEH handler, just replace it with proper
unwind codes, which are more profiler-friendly and supportable by our
unwind tests. Some notes on this:

- We don't currently support the automatic calling convention conversion
  with unwind codes, but this file already puts all arguments in
  registers, so I just renamed the arguments and put the last two
  arguments in RDI and RSI. Those I stashed into the parameter stack
  area because it's free storage.

- It is tedious to write the same directives in both CFI and SEH. We
  really could do with an abstraction. Although since most of our
  functions need a Windows variation anyway.

- I restored the original file's use of PUSH to save the registers.
  This matches what Clang likes to output anyway, and push is probably
  smaller than the corresponding move with offset. (And it reduces how
  much thinking about offsets I need to do.)

- Although it's an extra instruction, I restored the original file's
  separate fixed stack allocation and alloca for the sake of clarity.

- The epilog is constrained by Windows being extremely picky about
  epilogs. (Windows doesn't annotate epilogs and instead simulates
  forward.) I think other options are possible, but using LEA with an
  offset to realign the stack for the POPs both matches the examples in
  Windows and what Clang seems to like to output. The original file used
  MOV with offset, but it seems to be related to the funny SEH handler.

- The offsets in SEH directives may be surprising to someone used to CFI
  directives or a SysV RBP frame pointer. All three use slightly
  different baselines:

  CFI's canonical frame address (CFA) is RSP just before a CALL (so
  before the saved RIP in stack order). It is 16-byte aligned by ABI.

  A SysV RBP frame pointer is 16 bytes after that, after a saved RIP and
  saved RBP. It is also 16-byte aligned.

  Windows' baseline is the top of the fixed stack allocation, so
  potentially some bytes after that (all pushreg and allocstack
  directives). This too is required to be 16-byte aligned.

  Windows, however, doesn't require the frame register actually contain
  the fixed stack allocation. You can specify an offset from the value
  in the register to the actual top. But all the offsets in savereg,
  etc., directives use this baseline.

Performance difference is within measurement noise.

This does not create a stack frame for internal functions so
frame-pointer unwinding may miss a function or two, but the broad
attribution will be correct.

Change originally by Clemens Fruhwirth. Then reworked from Adam
Langley's https://boringssl-review.googlesource.com/c/boringssl/+/55945
by me to work on Windows and fix up some issues with the RBP setup.

Bug: b/33072965, 259
Change-Id: I52302635a8ad3d9272404feac125e2a4a4a5d14c
Reviewed-on: https://boringssl-review.googlesource.com/c/boringssl/+/56128
Reviewed-by: Adam Langley <agl@google.com>
Commit-Queue: David Benjamin <davidben@google.com>
(cherry picked from commit 0d5b6086143d19f86cc5d01b8944a1c13f99be24)
---
 .../fipsmodule/modes/asm/aesni-gcm-x86_64.pl  | 381 ++++------
 crypto/fipsmodule/modes/gcm_test.cc           |  16 +-
 .../crypto/fipsmodule/aesni-gcm-x86_64.S      | 112 +--
 .../crypto/fipsmodule/aesni-gcm-x86_64.S      |  76 +-
 .../crypto/fipsmodule/aesni-gcm-x86_64.asm    | 706 ++++++++++--------
 5 files changed, 665 insertions(+), 626 deletions(-)

diff --git a/crypto/fipsmodule/modes/asm/aesni-gcm-x86_64.pl b/crypto/fipsmodule/modes/asm/aesni-gcm-x86_64.pl
index 508cb3bbb..e247491e1 100644
--- a/crypto/fipsmodule/modes/asm/aesni-gcm-x86_64.pl
+++ b/crypto/fipsmodule/modes/asm/aesni-gcm-x86_64.pl
@@ -75,14 +75,18 @@ open OUT,"| \"$^X\" \"$xlate\" $flavour \"$output\"";
 # no AVX2 instructions being used.
 if ($avx>1) {{{
 
-($inp,$out,$len,$key,$ivp,$Xip)=("%rdi","%rsi","%rdx","%rcx","%r8","%r9");
+# On Windows, only four parameters are passed in registers. The last two
+# parameters will be manually loaded into %rdi and %rsi.
+my ($inp, $out, $len, $key, $ivp, $Xip) =
+    $win64 ? ("%rcx", "%rdx", "%r8", "%r9", "%rdi", "%rsi") :
+             ("%rdi", "%rsi", "%rdx", "%rcx", "%r8", "%r9");
 
 ($Ii,$T1,$T2,$Hkey,
  $Z0,$Z1,$Z2,$Z3,$Xi) = map("%xmm$_",(0..8));
 
 ($inout0,$inout1,$inout2,$inout3,$inout4,$inout5,$rndkey) = map("%xmm$_",(9..15));
 
-($counter,$rounds,$ret,$const,$in0,$end0)=("%ebx","%ebp","%r10","%r11","%r14","%r15");
+($counter,$rounds,$const,$in0,$end0)=("%ebx","%r10d","%r11","%r14","%r15");
 
 $code=<<___;
 .text
@@ -391,7 +395,7 @@ _aesni_ctr32_ghash_6x:
 	  vaesenclast	$Hkey,$inout5,$inout5
 	 vpaddb		$T2,$Z3,$Hkey
 
-	add		\$0x60,$ret
+	add		\$0x60,%rax
 	sub		\$0x6,$len
 	jc		.L6x_done
 
@@ -425,46 +429,76 @@ ___
 #		struct { u128 Xi,H,Htbl[9]; } *Xip);
 $code.=<<___;
 .globl	aesni_gcm_decrypt
-.type	aesni_gcm_decrypt,\@function,6
+.type	aesni_gcm_decrypt,\@abi-omnipotent
 .align	32
 aesni_gcm_decrypt:
 .cfi_startproc
-	xor	$ret,$ret
+.seh_startproc
+	xor	%rax,%rax
 
 	# We call |_aesni_ctr32_ghash_6x|, which requires at least 96 (0x60)
 	# bytes of input.
 	cmp	\$0x60,$len			# minimal accepted length
 	jb	.Lgcm_dec_abort
 
-	lea	(%rsp),%rax			# save stack pointer
-.cfi_def_cfa_register	%rax
-	push	%rbx
-.cfi_push	%rbx
 	push	%rbp
 .cfi_push	%rbp
+.seh_pushreg	%rbp
+	mov	%rsp, %rbp			# save stack pointer
+.cfi_def_cfa_register	%rbp
+	push	%rbx
+.cfi_push	%rbx
+.seh_pushreg	%rbx
 	push	%r12
 .cfi_push	%r12
+.seh_pushreg	%r12
 	push	%r13
 .cfi_push	%r13
+.seh_pushreg	%r13
 	push	%r14
 .cfi_push	%r14
+.seh_pushreg	%r14
 	push	%r15
 .cfi_push	%r15
+.seh_pushreg	%r15
 ___
-$code.=<<___ if ($win64);
-	lea	-0xa8(%rsp),%rsp
-	movaps	%xmm6,-0xd8(%rax)
-	movaps	%xmm7,-0xc8(%rax)
-	movaps	%xmm8,-0xb8(%rax)
-	movaps	%xmm9,-0xa8(%rax)
-	movaps	%xmm10,-0x98(%rax)
-	movaps	%xmm11,-0x88(%rax)
-	movaps	%xmm12,-0x78(%rax)
-	movaps	%xmm13,-0x68(%rax)
-	movaps	%xmm14,-0x58(%rax)
-	movaps	%xmm15,-0x48(%rax)
-.Lgcm_dec_body:
+if ($win64) {
+$code.=<<___
+	lea	-0xa8(%rsp),%rsp		# 8 extra bytes to align the stack
+.seh_allocstack	0xa8
+.seh_setframe	%rbp, 0xa8+5*8
+	# Load the last two parameters. These go into %rdi and %rsi, which are
+	# non-volatile on Windows, so stash them in the parameter stack area
+	# first.
+	mov	%rdi, 0x10(%rbp)
+.seh_savereg	%rdi, 0xa8+5*8+0x10
+	mov	%rsi, 0x18(%rbp)
+.seh_savereg	%rsi, 0xa8+5*8+0x18
+	mov	0x30(%rbp), $ivp
+	mov	0x38(%rbp), $Xip
+	# Save non-volatile XMM registers.
+	movaps	%xmm6,-0xd0(%rbp)
+.seh_savexmm128	%xmm6, 0xa8+5*8-0xd0
+	movaps	%xmm7,-0xc0(%rbp)
+.seh_savexmm128	%xmm7, 0xa8+5*8-0xc0
+	movaps	%xmm8,-0xb0(%rbp)
+.seh_savexmm128	%xmm8, 0xa8+5*8-0xb0
+	movaps	%xmm9,-0xa0(%rbp)
+.seh_savexmm128	%xmm9, 0xa8+5*8-0xa0
+	movaps	%xmm10,-0x90(%rbp)
+.seh_savexmm128	%xmm10, 0xa8+5*8-0x90
+	movaps	%xmm11,-0x80(%rbp)
+.seh_savexmm128	%xmm11, 0xa8+5*8-0x80
+	movaps	%xmm12,-0x70(%rbp)
+.seh_savexmm128	%xmm12, 0xa8+5*8-0x70
+	movaps	%xmm13,-0x60(%rbp)
+.seh_savexmm128	%xmm13, 0xa8+5*8-0x60
+	movaps	%xmm14,-0x50(%rbp)
+.seh_savexmm128	%xmm14, 0xa8+5*8-0x50
+	movaps	%xmm15,-0x40(%rbp)
+.seh_savexmm128	%xmm15, 0xa8+5*8-0x40
 ___
+}
 $code.=<<___;
 	vzeroupper
 
@@ -492,7 +526,7 @@ $code.=<<___;
 .Ldec_no_key_aliasing:
 
 	vmovdqu		0x50($inp),$Z3		# I[5]
-	lea		($inp),$in0
+	mov		$inp,$in0
 	vmovdqu		0x40($inp),$Z0
 
 	# |_aesni_ctr32_ghash_6x| requires |$end0| to point to 2*96 (0xc0)
@@ -505,7 +539,7 @@ $code.=<<___;
 
 	vmovdqu		0x30($inp),$Z1
 	shr		\$4,$len
-	xor		$ret,$ret
+	xor		%rax,%rax
 	vmovdqu		0x20($inp),$Z2
 	 vpshufb	$Ii,$Z3,$Z3		# passed to _aesni_ctr32_ghash_6x
 	vmovdqu		0x10($inp),$T2
@@ -536,35 +570,37 @@ $code.=<<___;
 	vzeroupper
 ___
 $code.=<<___ if ($win64);
-	movaps	-0xd8(%rax),%xmm6
-	movaps	-0xc8(%rax),%xmm7
-	movaps	-0xb8(%rax),%xmm8
-	movaps	-0xa8(%rax),%xmm9
-	movaps	-0x98(%rax),%xmm10
-	movaps	-0x88(%rax),%xmm11
-	movaps	-0x78(%rax),%xmm12
-	movaps	-0x68(%rax),%xmm13
-	movaps	-0x58(%rax),%xmm14
-	movaps	-0x48(%rax),%xmm15
+	movaps	-0xd0(%rbp),%xmm6
+	movaps	-0xc0(%rbp),%xmm7
+	movaps	-0xb0(%rbp),%xmm8
+	movaps	-0xa0(%rbp),%xmm9
+	movaps	-0x90(%rbp),%xmm10
+	movaps	-0x80(%rbp),%xmm11
+	movaps	-0x70(%rbp),%xmm12
+	movaps	-0x60(%rbp),%xmm13
+	movaps	-0x50(%rbp),%xmm14
+	movaps	-0x40(%rbp),%xmm15
+	mov	0x10(%rbp),%rdi
+	mov	0x18(%rbp),%rsi
 ___
 $code.=<<___;
-	mov	-48(%rax),%r15
-.cfi_restore	%r15
-	mov	-40(%rax),%r14
-.cfi_restore	%r14
-	mov	-32(%rax),%r13
-.cfi_restore	%r13
-	mov	-24(%rax),%r12
-.cfi_restore	%r12
-	mov	-16(%rax),%rbp
-.cfi_restore	%rbp
-	mov	-8(%rax),%rbx
-.cfi_restore	%rbx
-	lea	(%rax),%rsp		# restore %rsp
-.cfi_def_cfa_register	%rsp
+	lea	-0x28(%rbp), %rsp	# restore %rsp to fixed allocation
+.cfi_def_cfa	%rsp, 0x38
+	pop	%r15
+.cfi_pop	%r15
+	pop	%r14
+.cfi_pop	%r14
+	pop	%r13
+.cfi_pop	%r13
+	pop	%r12
+.cfi_pop	%r12
+	pop	%rbx
+.cfi_pop	%rbx
+	pop	%rbp
+.cfi_pop	%rbp
 .Lgcm_dec_abort:
-	mov	$ret,%rax		# return value
 	ret
+.seh_endproc
 .cfi_endproc
 .size	aesni_gcm_decrypt,.-aesni_gcm_decrypt
 ___
@@ -664,15 +700,16 @@ _aesni_ctr32_6x:
 .size	_aesni_ctr32_6x,.-_aesni_ctr32_6x
 
 .globl	aesni_gcm_encrypt
-.type	aesni_gcm_encrypt,\@function,6
+.type	aesni_gcm_encrypt,\@abi-omnipotent
 .align	32
 aesni_gcm_encrypt:
 .cfi_startproc
+.seh_startproc
 #ifdef BORINGSSL_DISPATCH_TEST
 .extern	BORINGSSL_function_hit
 	movb \$1,BORINGSSL_function_hit+2(%rip)
 #endif
-	xor	$ret,$ret
+	xor	%rax,%rax
 
 	# We call |_aesni_ctr32_6x| twice, each call consuming 96 bytes of
 	# input. Then we call |_aesni_ctr32_ghash_6x|, which requires at
@@ -680,35 +717,64 @@ aesni_gcm_encrypt:
 	cmp	\$0x60*3,$len			# minimal accepted length
 	jb	.Lgcm_enc_abort
 
-	lea	(%rsp),%rax			# save stack pointer
-.cfi_def_cfa_register	%rax
-	push	%rbx
-.cfi_push	%rbx
 	push	%rbp
 .cfi_push	%rbp
+.seh_pushreg	%rbp
+	mov	%rsp, %rbp			# save stack pointer
+.cfi_def_cfa_register	%rbp
+	push	%rbx
+.cfi_push	%rbx
+.seh_pushreg	%rbx
 	push	%r12
 .cfi_push	%r12
+.seh_pushreg	%r12
 	push	%r13
 .cfi_push	%r13
+.seh_pushreg	%r13
 	push	%r14
 .cfi_push	%r14
+.seh_pushreg	%r14
 	push	%r15
 .cfi_push	%r15
+.seh_pushreg	%r15
 ___
-$code.=<<___ if ($win64);
-	lea	-0xa8(%rsp),%rsp
-	movaps	%xmm6,-0xd8(%rax)
-	movaps	%xmm7,-0xc8(%rax)
-	movaps	%xmm8,-0xb8(%rax)
-	movaps	%xmm9,-0xa8(%rax)
-	movaps	%xmm10,-0x98(%rax)
-	movaps	%xmm11,-0x88(%rax)
-	movaps	%xmm12,-0x78(%rax)
-	movaps	%xmm13,-0x68(%rax)
-	movaps	%xmm14,-0x58(%rax)
-	movaps	%xmm15,-0x48(%rax)
-.Lgcm_enc_body:
+if ($win64) {
+$code.=<<___
+	lea	-0xa8(%rsp),%rsp		# 8 extra bytes to align the stack
+.seh_allocstack	0xa8
+.seh_setframe	%rbp, 0xa8+5*8
+	# Load the last two parameters. These go into %rdi and %rsi, which are
+	# non-volatile on Windows, so stash them in the parameter stack area
+	# first.
+	mov	%rdi, 0x10(%rbp)
+.seh_savereg	%rdi, 0xa8+5*8+0x10
+	mov	%rsi, 0x18(%rbp)
+.seh_savereg	%rsi, 0xa8+5*8+0x18
+	mov	0x30(%rbp), $ivp
+	mov	0x38(%rbp), $Xip
+	# Save non-volatile XMM registers.
+	movaps	%xmm6,-0xd0(%rbp)
+.seh_savexmm128	%xmm6, 0xa8+5*8-0xd0
+	movaps	%xmm7,-0xc0(%rbp)
+.seh_savexmm128	%xmm7, 0xa8+5*8-0xc0
+	movaps	%xmm8,-0xb0(%rbp)
+.seh_savexmm128	%xmm8, 0xa8+5*8-0xb0
+	movaps	%xmm9,-0xa0(%rbp)
+.seh_savexmm128	%xmm9, 0xa8+5*8-0xa0
+	movaps	%xmm10,-0x90(%rbp)
+.seh_savexmm128	%xmm10, 0xa8+5*8-0x90
+	movaps	%xmm11,-0x80(%rbp)
+.seh_savexmm128	%xmm11, 0xa8+5*8-0x80
+	movaps	%xmm12,-0x70(%rbp)
+.seh_savexmm128	%xmm12, 0xa8+5*8-0x70
+	movaps	%xmm13,-0x60(%rbp)
+.seh_savexmm128	%xmm13, 0xa8+5*8-0x60
+	movaps	%xmm14,-0x50(%rbp)
+.seh_savexmm128	%xmm14, 0xa8+5*8-0x50
+	movaps	%xmm15,-0x40(%rbp)
+.seh_savexmm128	%xmm15, 0xa8+5*8-0x40
 ___
+}
 $code.=<<___;
 	vzeroupper
 
@@ -732,7 +798,7 @@ $code.=<<___;
 	sub		$end0,%rsp		# avoid aliasing with key
 .Lenc_no_key_aliasing:
 
-	lea		($out),$in0
+	mov		$out,$in0
 
 	# |_aesni_ctr32_ghash_6x| requires |$end0| to point to 2*96 (0xc0)
 	# bytes before the end of the input. Note, in particular, that this is
@@ -763,7 +829,7 @@ $code.=<<___;
 	vmovdqu		($Xip),$Xi		# load Xi
 	lea		0x20+0x20($Xip),$Xip	# size optimization
 	sub		\$12,$len
-	mov		\$0x60*2,$ret
+	mov		\$0x60*2,%rax
 	vpshufb		$Ii,$Xi,$Xi
 
 	call		_aesni_ctr32_ghash_6x
@@ -952,37 +1018,39 @@ $code.=<<___;
 	vzeroupper
 ___
 $code.=<<___ if ($win64);
-	movaps	-0xd8(%rax),%xmm6
-	movaps	-0xc8(%rax),%xmm7
-	movaps	-0xb8(%rax),%xmm8
-	movaps	-0xa8(%rax),%xmm9
-	movaps	-0x98(%rax),%xmm10
-	movaps	-0x88(%rax),%xmm11
-	movaps	-0x78(%rax),%xmm12
-	movaps	-0x68(%rax),%xmm13
-	movaps	-0x58(%rax),%xmm14
-	movaps	-0x48(%rax),%xmm15
+	movaps	-0xd0(%rbp),%xmm6
+	movaps	-0xc0(%rbp),%xmm7
+	movaps	-0xb0(%rbp),%xmm8
+	movaps	-0xa0(%rbp),%xmm9
+	movaps	-0x90(%rbp),%xmm10
+	movaps	-0x80(%rbp),%xmm11
+	movaps	-0x70(%rbp),%xmm12
+	movaps	-0x60(%rbp),%xmm13
+	movaps	-0x50(%rbp),%xmm14
+	movaps	-0x40(%rbp),%xmm15
+	mov	0x10(%rbp),%rdi
+	mov	0x18(%rbp),%rsi
 ___
 $code.=<<___;
-	mov	-48(%rax),%r15
-.cfi_restore	%r15
-	mov	-40(%rax),%r14
-.cfi_restore	%r14
-	mov	-32(%rax),%r13
-.cfi_restore	%r13
-	mov	-24(%rax),%r12
-.cfi_restore	%r12
-	mov	-16(%rax),%rbp
-.cfi_restore	%rbp
-	mov	-8(%rax),%rbx
-.cfi_restore	%rbx
-	lea	(%rax),%rsp		# restore %rsp
-.cfi_def_cfa_register	%rsp
+	lea	-0x28(%rbp), %rsp	# restore %rsp to fixed allocation
+.cfi_def_cfa	%rsp, 0x38
+	pop	%r15
+.cfi_pop	%r15
+	pop	%r14
+.cfi_pop	%r14
+	pop	%r13
+.cfi_pop	%r13
+	pop	%r12
+.cfi_pop	%r12
+	pop	%rbx
+.cfi_pop	%rbx
+	pop	%rbp
+.cfi_pop	%rbp
 .Lgcm_enc_abort:
-	mov	$ret,%rax		# return value
 	ret
+.seh_endproc
 .cfi_endproc
-.size	aesni_gcm_encrypt,.-aesni_gcm_encrypt
+.size	aesni_gcm_decrypt,.-aesni_gcm_decrypt
 ___
 
 $code.=<<___;
@@ -1000,127 +1068,6 @@ $code.=<<___;
 .asciz	"AES-NI GCM module for x86_64, CRYPTOGAMS by <appro\@openssl.org>"
 .align	64
 ___
-if ($win64) {
-$rec="%rcx";
-$frame="%rdx";
-$context="%r8";
-$disp="%r9";
-
-$code.=<<___
-.extern	__imp_RtlVirtualUnwind
-.type	gcm_se_handler,\@abi-omnipotent
-.align	16
-gcm_se_handler:
-	push	%rsi
-	push	%rdi
-	push	%rbx
-	push	%rbp
-	push	%r12
-	push	%r13
-	push	%r14
-	push	%r15
-	pushfq
-	sub	\$64,%rsp
-
-	mov	120($context),%rax	# pull context->Rax
-	mov	248($context),%rbx	# pull context->Rip
-
-	mov	8($disp),%rsi		# disp->ImageBase
-	mov	56($disp),%r11		# disp->HandlerData
-
-	mov	0(%r11),%r10d		# HandlerData[0]
-	lea	(%rsi,%r10),%r10	# prologue label
-	cmp	%r10,%rbx		# context->Rip<prologue label
-	jb	.Lcommon_seh_tail
-
-	mov	152($context),%rax	# pull context->Rsp
-
-	mov	4(%r11),%r10d		# HandlerData[1]
-	lea	(%rsi,%r10),%r10	# epilogue label
-	cmp	%r10,%rbx		# context->Rip>=epilogue label
-	jae	.Lcommon_seh_tail
-
-	mov	120($context),%rax	# pull context->Rax
-
-	mov	-48(%rax),%r15
-	mov	-40(%rax),%r14
-	mov	-32(%rax),%r13
-	mov	-24(%rax),%r12
-	mov	-16(%rax),%rbp
-	mov	-8(%rax),%rbx
-	mov	%r15,240($context)
-	mov	%r14,232($context)
-	mov	%r13,224($context)
-	mov	%r12,216($context)
-	mov	%rbp,160($context)
-	mov	%rbx,144($context)
-
-	lea	-0xd8(%rax),%rsi	# %xmm save area
-	lea	512($context),%rdi	# & context.Xmm6
-	mov	\$20,%ecx		# 10*sizeof(%xmm0)/sizeof(%rax)
-	.long	0xa548f3fc		# cld; rep movsq
-
-.Lcommon_seh_tail:
-	mov	8(%rax),%rdi
-	mov	16(%rax),%rsi
-	mov	%rax,152($context)	# restore context->Rsp
-	mov	%rsi,168($context)	# restore context->Rsi
-	mov	%rdi,176($context)	# restore context->Rdi
-
-	mov	40($disp),%rdi		# disp->ContextRecord
-	mov	$context,%rsi		# context
-	mov	\$154,%ecx		# sizeof(CONTEXT)
-	.long	0xa548f3fc		# cld; rep movsq
-
-	mov	$disp,%rsi
-	xor	%rcx,%rcx		# arg1, UNW_FLAG_NHANDLER
-	mov	8(%rsi),%rdx		# arg2, disp->ImageBase
-	mov	0(%rsi),%r8		# arg3, disp->ControlPc
-	mov	16(%rsi),%r9		# arg4, disp->FunctionEntry
-	mov	40(%rsi),%r10		# disp->ContextRecord
-	lea	56(%rsi),%r11		# &disp->HandlerData
-	lea	24(%rsi),%r12		# &disp->EstablisherFrame
-	mov	%r10,32(%rsp)		# arg5
-	mov	%r11,40(%rsp)		# arg6
-	mov	%r12,48(%rsp)		# arg7
-	mov	%rcx,56(%rsp)		# arg8, (NULL)
-	call	*__imp_RtlVirtualUnwind(%rip)
-
-	mov	\$1,%eax		# ExceptionContinueSearch
-	add	\$64,%rsp
-	popfq
-	pop	%r15
-	pop	%r14
-	pop	%r13
-	pop	%r12
-	pop	%rbp
-	pop	%rbx
-	pop	%rdi
-	pop	%rsi
-	ret
-.size	gcm_se_handler,.-gcm_se_handler
-
-.section	.pdata
-.align	4
-	.rva	.LSEH_begin_aesni_gcm_decrypt
-	.rva	.LSEH_end_aesni_gcm_decrypt
-	.rva	.LSEH_gcm_dec_info
-
-	.rva	.LSEH_begin_aesni_gcm_encrypt
-	.rva	.LSEH_end_aesni_gcm_encrypt
-	.rva	.LSEH_gcm_enc_info
-.section	.xdata
-.align	8
-.LSEH_gcm_dec_info:
-	.byte	9,0,0,0
-	.rva	gcm_se_handler
-	.rva	.Lgcm_dec_body,.Lgcm_dec_abort
-.LSEH_gcm_enc_info:
-	.byte	9,0,0,0
-	.rva	gcm_se_handler
-	.rva	.Lgcm_enc_body,.Lgcm_enc_abort
-___
-}
 }}} else {{{
 $code=<<___;	# assembler is too old
 .text
diff --git a/crypto/fipsmodule/modes/gcm_test.cc b/crypto/fipsmodule/modes/gcm_test.cc
index 965fd8aba..a4574ca50 100644
--- a/crypto/fipsmodule/modes/gcm_test.cc
+++ b/crypto/fipsmodule/modes/gcm_test.cc
@@ -175,17 +175,17 @@ TEST(GCMTest, ABI) {
 
         aes_hw_set_encrypt_key(kKey, 128, &aes_key);
         for (size_t blocks : kBlockCounts) {
-          CHECK_ABI(aesni_gcm_encrypt, buf, buf, blocks * 16, &aes_key, iv,
-                    gcm.Xi.u);
-          CHECK_ABI(aesni_gcm_encrypt, buf, buf, blocks * 16 + 7, &aes_key, iv,
-                    gcm.Xi.u);
+          CHECK_ABI_SEH(aesni_gcm_encrypt, buf, buf, blocks * 16, &aes_key, iv,
+                        gcm.Xi.u);
+          CHECK_ABI_SEH(aesni_gcm_encrypt, buf, buf, blocks * 16 + 7, &aes_key,
+                        iv, gcm.Xi.u);
         }
         aes_hw_set_decrypt_key(kKey, 128, &aes_key);
         for (size_t blocks : kBlockCounts) {
-          CHECK_ABI(aesni_gcm_decrypt, buf, buf, blocks * 16, &aes_key, iv,
-                    gcm.Xi.u);
-          CHECK_ABI(aesni_gcm_decrypt, buf, buf, blocks * 16 + 7, &aes_key, iv,
-                    gcm.Xi.u);
+          CHECK_ABI_SEH(aesni_gcm_decrypt, buf, buf, blocks * 16, &aes_key, iv,
+                        gcm.Xi.u);
+          CHECK_ABI_SEH(aesni_gcm_decrypt, buf, buf, blocks * 16 + 7, &aes_key,
+                        iv, gcm.Xi.u);
         }
       }
     }
diff --git a/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S b/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
index c98528cf0..20b61d9f8 100644
--- a/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
+++ b/generated-src/linux-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
@@ -221,7 +221,7 @@ _aesni_ctr32_ghash_6x:
 	movbeq	0(%r14),%r12
 	vaesenc	%xmm1,%xmm14,%xmm14
 	vmovups	160-128(%rcx),%xmm1
-	cmpl	$11,%ebp
+	cmpl	$11,%r10d
 	jb	.Lenc_tail
 
 	vaesenc	%xmm15,%xmm9,%xmm9
@@ -317,7 +317,7 @@ _aesni_ctr32_ghash_6x:
 	vaesenclast	%xmm3,%xmm14,%xmm14
 	vpaddb	%xmm2,%xmm7,%xmm3
 
-	addq	$0x60,%r10
+	addq	$0x60,%rax
 	subq	$0x6,%rdx
 	jc	.L6x_done
 
@@ -349,27 +349,35 @@ _aesni_ctr32_ghash_6x:
 .align	32
 aesni_gcm_decrypt:
 .cfi_startproc	
-	xorq	%r10,%r10
+
+	xorq	%rax,%rax
 
 
 
 	cmpq	$0x60,%rdx
 	jb	.Lgcm_dec_abort
 
-	leaq	(%rsp),%rax
-.cfi_def_cfa_register	%rax
-	pushq	%rbx
-.cfi_offset	%rbx,-16
 	pushq	%rbp
-.cfi_offset	%rbp,-24
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%rbp,-16
+
+	movq	%rsp,%rbp
+.cfi_def_cfa_register	%rbp
+	pushq	%rbx
+.cfi_offset	%rbx,-24
+
 	pushq	%r12
 .cfi_offset	%r12,-32
+
 	pushq	%r13
 .cfi_offset	%r13,-40
+
 	pushq	%r14
 .cfi_offset	%r14,-48
+
 	pushq	%r15
 .cfi_offset	%r15,-56
+
 	vzeroupper
 
 	vmovdqu	(%r8),%xmm1
@@ -383,7 +391,7 @@ aesni_gcm_decrypt:
 	vmovdqu	(%r11),%xmm0
 	leaq	128(%rcx),%rcx
 	leaq	32+32(%r9),%r9
-	movl	240-128(%rcx),%ebp
+	movl	240-128(%rcx),%r10d
 	vpshufb	%xmm0,%xmm8,%xmm8
 
 	andq	%r15,%r14
@@ -396,7 +404,7 @@ aesni_gcm_decrypt:
 .Ldec_no_key_aliasing:
 
 	vmovdqu	80(%rdi),%xmm7
-	leaq	(%rdi),%r14
+	movq	%rdi,%r14
 	vmovdqu	64(%rdi),%xmm4
 
 
@@ -409,7 +417,7 @@ aesni_gcm_decrypt:
 
 	vmovdqu	48(%rdi),%xmm5
 	shrq	$4,%rdx
-	xorq	%r10,%r10
+	xorq	%rax,%rax
 	vmovdqu	32(%rdi),%xmm6
 	vpshufb	%xmm0,%xmm7,%xmm7
 	vmovdqu	16(%rdi),%xmm2
@@ -438,23 +446,29 @@ aesni_gcm_decrypt:
 	vmovdqu	%xmm8,-64(%r9)
 
 	vzeroupper
-	movq	-48(%rax),%r15
+	leaq	-40(%rbp),%rsp
+.cfi_def_cfa	%rsp, 0x38
+	popq	%r15
+.cfi_adjust_cfa_offset	-8
 .cfi_restore	%r15
-	movq	-40(%rax),%r14
+	popq	%r14
+.cfi_adjust_cfa_offset	-8
 .cfi_restore	%r14
-	movq	-32(%rax),%r13
+	popq	%r13
+.cfi_adjust_cfa_offset	-8
 .cfi_restore	%r13
-	movq	-24(%rax),%r12
+	popq	%r12
+.cfi_adjust_cfa_offset	-8
 .cfi_restore	%r12
-	movq	-16(%rax),%rbp
-.cfi_restore	%rbp
-	movq	-8(%rax),%rbx
+	popq	%rbx
+.cfi_adjust_cfa_offset	-8
 .cfi_restore	%rbx
-	leaq	(%rax),%rsp
-.cfi_def_cfa_register	%rsp
+	popq	%rbp
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%rbp
 .Lgcm_dec_abort:
-	movq	%r10,%rax
 	.byte	0xf3,0xc3
+
 .cfi_endproc	
 .size	aesni_gcm_decrypt,.-aesni_gcm_decrypt
 .type	_aesni_ctr32_6x,@function
@@ -463,7 +477,7 @@ _aesni_ctr32_6x:
 .cfi_startproc	
 	vmovdqu	0-128(%rcx),%xmm4
 	vmovdqu	32(%r11),%xmm2
-	leaq	-1(%rbp),%r13
+	leaq	-1(%r10),%r13
 	vmovups	16-128(%rcx),%xmm15
 	leaq	32-128(%rcx),%r12
 	vpxor	%xmm4,%xmm1,%xmm9
@@ -556,12 +570,13 @@ _aesni_ctr32_6x:
 .align	32
 aesni_gcm_encrypt:
 .cfi_startproc	
+
 #ifdef BORINGSSL_DISPATCH_TEST
 .extern	BORINGSSL_function_hit
 .hidden BORINGSSL_function_hit
 	movb	$1,BORINGSSL_function_hit+2(%rip)
 #endif
-	xorq	%r10,%r10
+	xorq	%rax,%rax
 
 
 
@@ -569,20 +584,27 @@ aesni_gcm_encrypt:
 	cmpq	$288,%rdx
 	jb	.Lgcm_enc_abort
 
-	leaq	(%rsp),%rax
-.cfi_def_cfa_register	%rax
-	pushq	%rbx
-.cfi_offset	%rbx,-16
 	pushq	%rbp
-.cfi_offset	%rbp,-24
+.cfi_adjust_cfa_offset	8
+.cfi_offset	%rbp,-16
+
+	movq	%rsp,%rbp
+.cfi_def_cfa_register	%rbp
+	pushq	%rbx
+.cfi_offset	%rbx,-24
+
 	pushq	%r12
 .cfi_offset	%r12,-32
+
 	pushq	%r13
 .cfi_offset	%r13,-40
+
 	pushq	%r14
 .cfi_offset	%r14,-48
+
 	pushq	%r15
 .cfi_offset	%r15,-56
+
 	vzeroupper
 
 	vmovdqu	(%r8),%xmm1
@@ -594,7 +616,7 @@ aesni_gcm_encrypt:
 	leaq	128(%rcx),%rcx
 	vmovdqu	(%r11),%xmm0
 	andq	$-128,%rsp
-	movl	240-128(%rcx),%ebp
+	movl	240-128(%rcx),%r10d
 
 	andq	%r15,%r14
 	andq	%rsp,%r15
@@ -605,7 +627,7 @@ aesni_gcm_encrypt:
 	subq	%r15,%rsp
 .Lenc_no_key_aliasing:
 
-	leaq	(%rsi),%r14
+	movq	%rsi,%r14
 
 
 
@@ -636,7 +658,7 @@ aesni_gcm_encrypt:
 	vmovdqu	(%r9),%xmm8
 	leaq	32+32(%r9),%r9
 	subq	$12,%rdx
-	movq	$192,%r10
+	movq	$192,%rax
 	vpshufb	%xmm0,%xmm8,%xmm8
 
 	call	_aesni_ctr32_ghash_6x
@@ -816,25 +838,31 @@ aesni_gcm_encrypt:
 	vmovdqu	%xmm8,-64(%r9)
 
 	vzeroupper
-	movq	-48(%rax),%r15
+	leaq	-40(%rbp),%rsp
+.cfi_def_cfa	%rsp, 0x38
+	popq	%r15
+.cfi_adjust_cfa_offset	-8
 .cfi_restore	%r15
-	movq	-40(%rax),%r14
+	popq	%r14
+.cfi_adjust_cfa_offset	-8
 .cfi_restore	%r14
-	movq	-32(%rax),%r13
+	popq	%r13
+.cfi_adjust_cfa_offset	-8
 .cfi_restore	%r13
-	movq	-24(%rax),%r12
+	popq	%r12
+.cfi_adjust_cfa_offset	-8
 .cfi_restore	%r12
-	movq	-16(%rax),%rbp
-.cfi_restore	%rbp
-	movq	-8(%rax),%rbx
+	popq	%rbx
+.cfi_adjust_cfa_offset	-8
 .cfi_restore	%rbx
-	leaq	(%rax),%rsp
-.cfi_def_cfa_register	%rsp
+	popq	%rbp
+.cfi_adjust_cfa_offset	-8
+.cfi_restore	%rbp
 .Lgcm_enc_abort:
-	movq	%r10,%rax
 	.byte	0xf3,0xc3
+
 .cfi_endproc	
-.size	aesni_gcm_encrypt,.-aesni_gcm_encrypt
+.size	aesni_gcm_decrypt,.-aesni_gcm_decrypt
 .align	64
 .Lbswap_mask:
 .byte	15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0
diff --git a/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S b/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
index b4c01dab9..a6720a791 100644
--- a/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
+++ b/generated-src/mac-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.S
@@ -221,7 +221,7 @@ L$resume_ctr32:
 	movbeq	0(%r14),%r12
 	vaesenc	%xmm1,%xmm14,%xmm14
 	vmovups	160-128(%rcx),%xmm1
-	cmpl	$11,%ebp
+	cmpl	$11,%r10d
 	jb	L$enc_tail
 
 	vaesenc	%xmm15,%xmm9,%xmm9
@@ -317,7 +317,7 @@ L$enc_tail:
 	vaesenclast	%xmm3,%xmm14,%xmm14
 	vpaddb	%xmm2,%xmm7,%xmm3
 
-	addq	$0x60,%r10
+	addq	$0x60,%rax
 	subq	$0x6,%rdx
 	jc	L$6x_done
 
@@ -349,27 +349,34 @@ L$6x_done:
 .p2align	5
 _aesni_gcm_decrypt:
 
-	xorq	%r10,%r10
+
+	xorq	%rax,%rax
 
 
 
 	cmpq	$0x60,%rdx
 	jb	L$gcm_dec_abort
 
-	leaq	(%rsp),%rax
+	pushq	%rbp
+
+
+	movq	%rsp,%rbp
 
 	pushq	%rbx
 
-	pushq	%rbp
 
 	pushq	%r12
 
+
 	pushq	%r13
 
+
 	pushq	%r14
 
+
 	pushq	%r15
 
+
 	vzeroupper
 
 	vmovdqu	(%r8),%xmm1
@@ -383,7 +390,7 @@ _aesni_gcm_decrypt:
 	vmovdqu	(%r11),%xmm0
 	leaq	128(%rcx),%rcx
 	leaq	32+32(%r9),%r9
-	movl	240-128(%rcx),%ebp
+	movl	240-128(%rcx),%r10d
 	vpshufb	%xmm0,%xmm8,%xmm8
 
 	andq	%r15,%r14
@@ -396,7 +403,7 @@ _aesni_gcm_decrypt:
 L$dec_no_key_aliasing:
 
 	vmovdqu	80(%rdi),%xmm7
-	leaq	(%rdi),%r14
+	movq	%rdi,%r14
 	vmovdqu	64(%rdi),%xmm4
 
 
@@ -409,7 +416,7 @@ L$dec_no_key_aliasing:
 
 	vmovdqu	48(%rdi),%xmm5
 	shrq	$4,%rdx
-	xorq	%r10,%r10
+	xorq	%rax,%rax
 	vmovdqu	32(%rdi),%xmm6
 	vpshufb	%xmm0,%xmm7,%xmm7
 	vmovdqu	16(%rdi),%xmm2
@@ -438,32 +445,32 @@ L$dec_no_key_aliasing:
 	vmovdqu	%xmm8,-64(%r9)
 
 	vzeroupper
-	movq	-48(%rax),%r15
+	leaq	-40(%rbp),%rsp
 
-	movq	-40(%rax),%r14
+	popq	%r15
 
-	movq	-32(%rax),%r13
+	popq	%r14
 
-	movq	-24(%rax),%r12
+	popq	%r13
 
-	movq	-16(%rax),%rbp
+	popq	%r12
 
-	movq	-8(%rax),%rbx
+	popq	%rbx
 
-	leaq	(%rax),%rsp
+	popq	%rbp
 
 L$gcm_dec_abort:
-	movq	%r10,%rax
 	.byte	0xf3,0xc3
 
 
 
+
 .p2align	5
 _aesni_ctr32_6x:
 
 	vmovdqu	0-128(%rcx),%xmm4
 	vmovdqu	32(%r11),%xmm2
-	leaq	-1(%rbp),%r13
+	leaq	-1(%r10),%r13
 	vmovups	16-128(%rcx),%xmm15
 	leaq	32-128(%rcx),%r12
 	vpxor	%xmm4,%xmm1,%xmm9
@@ -556,11 +563,12 @@ L$handle_ctr32_2:
 .p2align	5
 _aesni_gcm_encrypt:
 
+
 #ifdef BORINGSSL_DISPATCH_TEST
 
 	movb	$1,_BORINGSSL_function_hit+2(%rip)
 #endif
-	xorq	%r10,%r10
+	xorq	%rax,%rax
 
 
 
@@ -568,20 +576,26 @@ _aesni_gcm_encrypt:
 	cmpq	$288,%rdx
 	jb	L$gcm_enc_abort
 
-	leaq	(%rsp),%rax
+	pushq	%rbp
+
+
+	movq	%rsp,%rbp
 
 	pushq	%rbx
 
-	pushq	%rbp
 
 	pushq	%r12
 
+
 	pushq	%r13
 
+
 	pushq	%r14
 
+
 	pushq	%r15
 
+
 	vzeroupper
 
 	vmovdqu	(%r8),%xmm1
@@ -593,7 +607,7 @@ _aesni_gcm_encrypt:
 	leaq	128(%rcx),%rcx
 	vmovdqu	(%r11),%xmm0
 	andq	$-128,%rsp
-	movl	240-128(%rcx),%ebp
+	movl	240-128(%rcx),%r10d
 
 	andq	%r15,%r14
 	andq	%rsp,%r15
@@ -604,7 +618,7 @@ _aesni_gcm_encrypt:
 	subq	%r15,%rsp
 L$enc_no_key_aliasing:
 
-	leaq	(%rsi),%r14
+	movq	%rsi,%r14
 
 
 
@@ -635,7 +649,7 @@ L$enc_no_key_aliasing:
 	vmovdqu	(%r9),%xmm8
 	leaq	32+32(%r9),%r9
 	subq	$12,%rdx
-	movq	$192,%r10
+	movq	$192,%rax
 	vpshufb	%xmm0,%xmm8,%xmm8
 
 	call	_aesni_ctr32_ghash_6x
@@ -815,25 +829,25 @@ L$enc_no_key_aliasing:
 	vmovdqu	%xmm8,-64(%r9)
 
 	vzeroupper
-	movq	-48(%rax),%r15
+	leaq	-40(%rbp),%rsp
 
-	movq	-40(%rax),%r14
+	popq	%r15
 
-	movq	-32(%rax),%r13
+	popq	%r14
 
-	movq	-24(%rax),%r12
+	popq	%r13
 
-	movq	-16(%rax),%rbp
+	popq	%r12
 
-	movq	-8(%rax),%rbx
+	popq	%rbx
 
-	leaq	(%rax),%rsp
+	popq	%rbp
 
 L$gcm_enc_abort:
-	movq	%r10,%rax
 	.byte	0xf3,0xc3
 
 
+
 .p2align	6
 L$bswap_mask:
 .byte	15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0
diff --git a/generated-src/win-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.asm b/generated-src/win-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.asm
index 6ab58129f..38d363bd6 100644
--- a/generated-src/win-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.asm
+++ b/generated-src/win-x86_64/crypto/fipsmodule/aesni-gcm-x86_64.asm
@@ -18,9 +18,9 @@ ALIGN	32
 _aesni_ctr32_ghash_6x:
 
 	vmovdqu	xmm2,XMMWORD[32+r11]
-	sub	rdx,6
+	sub	r8,6
 	vpxor	xmm4,xmm4,xmm4
-	vmovdqu	xmm15,XMMWORD[((0-128))+rcx]
+	vmovdqu	xmm15,XMMWORD[((0-128))+r9]
 	vpaddb	xmm10,xmm1,xmm2
 	vpaddb	xmm11,xmm10,xmm2
 	vpaddb	xmm12,xmm11,xmm2
@@ -34,16 +34,16 @@ ALIGN	32
 $L$oop6x:
 	add	ebx,100663296
 	jc	NEAR $L$handle_ctr32
-	vmovdqu	xmm3,XMMWORD[((0-32))+r9]
+	vmovdqu	xmm3,XMMWORD[((0-32))+rsi]
 	vpaddb	xmm1,xmm14,xmm2
 	vpxor	xmm10,xmm10,xmm15
 	vpxor	xmm11,xmm11,xmm15
 
 $L$resume_ctr32:
-	vmovdqu	XMMWORD[r8],xmm1
+	vmovdqu	XMMWORD[rdi],xmm1
 	vpclmulqdq	xmm5,xmm7,xmm3,0x10
 	vpxor	xmm12,xmm12,xmm15
-	vmovups	xmm2,XMMWORD[((16-128))+rcx]
+	vmovups	xmm2,XMMWORD[((16-128))+r9]
 	vpclmulqdq	xmm6,xmm7,xmm3,0x01
 
 
@@ -74,7 +74,7 @@ $L$resume_ctr32:
 	setnc	r12b
 	vpclmulqdq	xmm7,xmm7,xmm3,0x11
 	vaesenc	xmm11,xmm11,xmm2
-	vmovdqu	xmm3,XMMWORD[((16-32))+r9]
+	vmovdqu	xmm3,XMMWORD[((16-32))+rsi]
 	neg	r12
 	vaesenc	xmm12,xmm12,xmm2
 	vpxor	xmm6,xmm6,xmm5
@@ -83,7 +83,7 @@ $L$resume_ctr32:
 	vaesenc	xmm13,xmm13,xmm2
 	vpxor	xmm4,xmm1,xmm5
 	and	r12,0x60
-	vmovups	xmm15,XMMWORD[((32-128))+rcx]
+	vmovups	xmm15,XMMWORD[((32-128))+r9]
 	vpclmulqdq	xmm1,xmm0,xmm3,0x10
 	vaesenc	xmm14,xmm14,xmm2
 
@@ -101,10 +101,10 @@ $L$resume_ctr32:
 	mov	QWORD[((32+8))+rsp],r13
 	vaesenc	xmm13,xmm13,xmm15
 	mov	QWORD[((40+8))+rsp],r12
-	vmovdqu	xmm5,XMMWORD[((48-32))+r9]
+	vmovdqu	xmm5,XMMWORD[((48-32))+rsi]
 	vaesenc	xmm14,xmm14,xmm15
 
-	vmovups	xmm15,XMMWORD[((48-128))+rcx]
+	vmovups	xmm15,XMMWORD[((48-128))+r9]
 	vpxor	xmm6,xmm6,xmm1
 	vpclmulqdq	xmm1,xmm0,xmm5,0x00
 	vaesenc	xmm9,xmm9,xmm15
@@ -119,10 +119,10 @@ $L$resume_ctr32:
 	vaesenc	xmm12,xmm12,xmm15
 	vaesenc	xmm13,xmm13,xmm15
 	vpxor	xmm4,xmm4,xmm1
-	vmovdqu	xmm1,XMMWORD[((64-32))+r9]
+	vmovdqu	xmm1,XMMWORD[((64-32))+rsi]
 	vaesenc	xmm14,xmm14,xmm15
 
-	vmovups	xmm15,XMMWORD[((64-128))+rcx]
+	vmovups	xmm15,XMMWORD[((64-128))+r9]
 	vpxor	xmm6,xmm6,xmm2
 	vpclmulqdq	xmm2,xmm0,xmm1,0x00
 	vaesenc	xmm9,xmm9,xmm15
@@ -141,10 +141,10 @@ $L$resume_ctr32:
 	vaesenc	xmm13,xmm13,xmm15
 	mov	QWORD[((56+8))+rsp],r12
 	vpxor	xmm4,xmm4,xmm2
-	vmovdqu	xmm2,XMMWORD[((96-32))+r9]
+	vmovdqu	xmm2,XMMWORD[((96-32))+rsi]
 	vaesenc	xmm14,xmm14,xmm15
 
-	vmovups	xmm15,XMMWORD[((80-128))+rcx]
+	vmovups	xmm15,XMMWORD[((80-128))+r9]
 	vpxor	xmm6,xmm6,xmm3
 	vpclmulqdq	xmm3,xmm0,xmm2,0x00
 	vaesenc	xmm9,xmm9,xmm15
@@ -163,10 +163,10 @@ $L$resume_ctr32:
 	vaesenc	xmm13,xmm13,xmm15
 	mov	QWORD[((72+8))+rsp],r12
 	vpxor	xmm4,xmm4,xmm3
-	vmovdqu	xmm3,XMMWORD[((112-32))+r9]
+	vmovdqu	xmm3,XMMWORD[((112-32))+rsi]
 	vaesenc	xmm14,xmm14,xmm15
 
-	vmovups	xmm15,XMMWORD[((96-128))+rcx]
+	vmovups	xmm15,XMMWORD[((96-128))+r9]
 	vpxor	xmm6,xmm6,xmm5
 	vpclmulqdq	xmm5,xmm8,xmm3,0x10
 	vaesenc	xmm9,xmm9,xmm15
@@ -187,7 +187,7 @@ $L$resume_ctr32:
 	vaesenc	xmm14,xmm14,xmm15
 	vpxor	xmm6,xmm6,xmm1
 
-	vmovups	xmm15,XMMWORD[((112-128))+rcx]
+	vmovups	xmm15,XMMWORD[((112-128))+r9]
 	vpslldq	xmm5,xmm6,8
 	vpxor	xmm4,xmm4,xmm2
 	vmovdqu	xmm3,XMMWORD[16+r11]
@@ -205,11 +205,11 @@ $L$resume_ctr32:
 	vaesenc	xmm12,xmm12,xmm15
 	mov	QWORD[((104+8))+rsp],r12
 	vaesenc	xmm13,xmm13,xmm15
-	vmovups	xmm1,XMMWORD[((128-128))+rcx]
+	vmovups	xmm1,XMMWORD[((128-128))+r9]
 	vaesenc	xmm14,xmm14,xmm15
 
 	vaesenc	xmm9,xmm9,xmm1
-	vmovups	xmm15,XMMWORD[((144-128))+rcx]
+	vmovups	xmm15,XMMWORD[((144-128))+r9]
 	vaesenc	xmm10,xmm10,xmm1
 	vpsrldq	xmm6,xmm6,8
 	vaesenc	xmm11,xmm11,xmm1
@@ -220,8 +220,8 @@ $L$resume_ctr32:
 	vaesenc	xmm13,xmm13,xmm1
 	movbe	r12,QWORD[r14]
 	vaesenc	xmm14,xmm14,xmm1
-	vmovups	xmm1,XMMWORD[((160-128))+rcx]
-	cmp	ebp,11
+	vmovups	xmm1,XMMWORD[((160-128))+r9]
+	cmp	r10d,11
 	jb	NEAR $L$enc_tail
 
 	vaesenc	xmm9,xmm9,xmm15
@@ -236,9 +236,9 @@ $L$resume_ctr32:
 	vaesenc	xmm11,xmm11,xmm1
 	vaesenc	xmm12,xmm12,xmm1
 	vaesenc	xmm13,xmm13,xmm1
-	vmovups	xmm15,XMMWORD[((176-128))+rcx]
+	vmovups	xmm15,XMMWORD[((176-128))+r9]
 	vaesenc	xmm14,xmm14,xmm1
-	vmovups	xmm1,XMMWORD[((192-128))+rcx]
+	vmovups	xmm1,XMMWORD[((192-128))+r9]
 	je	NEAR $L$enc_tail
 
 	vaesenc	xmm9,xmm9,xmm15
@@ -253,9 +253,9 @@ $L$resume_ctr32:
 	vaesenc	xmm11,xmm11,xmm1
 	vaesenc	xmm12,xmm12,xmm1
 	vaesenc	xmm13,xmm13,xmm1
-	vmovups	xmm15,XMMWORD[((208-128))+rcx]
+	vmovups	xmm15,XMMWORD[((208-128))+r9]
 	vaesenc	xmm14,xmm14,xmm1
-	vmovups	xmm1,XMMWORD[((224-128))+rcx]
+	vmovups	xmm1,XMMWORD[((224-128))+r9]
 	jmp	NEAR $L$enc_tail
 
 ALIGN	32
@@ -265,7 +265,7 @@ $L$handle_ctr32:
 	vmovdqu	xmm5,XMMWORD[48+r11]
 	vpaddd	xmm10,xmm6,XMMWORD[64+r11]
 	vpaddd	xmm11,xmm6,xmm5
-	vmovdqu	xmm3,XMMWORD[((0-32))+r9]
+	vmovdqu	xmm3,XMMWORD[((0-32))+rsi]
 	vpaddd	xmm12,xmm10,xmm5
 	vpshufb	xmm10,xmm10,xmm0
 	vpaddd	xmm13,xmm11,xmm5
@@ -287,29 +287,29 @@ $L$enc_tail:
 	vpalignr	xmm8,xmm4,xmm4,8
 	vaesenc	xmm10,xmm10,xmm15
 	vpclmulqdq	xmm4,xmm4,xmm3,0x10
-	vpxor	xmm2,xmm1,XMMWORD[rdi]
+	vpxor	xmm2,xmm1,XMMWORD[rcx]
 	vaesenc	xmm11,xmm11,xmm15
-	vpxor	xmm0,xmm1,XMMWORD[16+rdi]
+	vpxor	xmm0,xmm1,XMMWORD[16+rcx]
 	vaesenc	xmm12,xmm12,xmm15
-	vpxor	xmm5,xmm1,XMMWORD[32+rdi]
+	vpxor	xmm5,xmm1,XMMWORD[32+rcx]
 	vaesenc	xmm13,xmm13,xmm15
-	vpxor	xmm6,xmm1,XMMWORD[48+rdi]
+	vpxor	xmm6,xmm1,XMMWORD[48+rcx]
 	vaesenc	xmm14,xmm14,xmm15
-	vpxor	xmm7,xmm1,XMMWORD[64+rdi]
-	vpxor	xmm3,xmm1,XMMWORD[80+rdi]
-	vmovdqu	xmm1,XMMWORD[r8]
+	vpxor	xmm7,xmm1,XMMWORD[64+rcx]
+	vpxor	xmm3,xmm1,XMMWORD[80+rcx]
+	vmovdqu	xmm1,XMMWORD[rdi]
 
 	vaesenclast	xmm9,xmm9,xmm2
 	vmovdqu	xmm2,XMMWORD[32+r11]
 	vaesenclast	xmm10,xmm10,xmm0
 	vpaddb	xmm0,xmm1,xmm2
 	mov	QWORD[((112+8))+rsp],r13
-	lea	rdi,[96+rdi]
+	lea	rcx,[96+rcx]
 	vaesenclast	xmm11,xmm11,xmm5
 	vpaddb	xmm5,xmm0,xmm2
 	mov	QWORD[((120+8))+rsp],r12
-	lea	rsi,[96+rsi]
-	vmovdqu	xmm15,XMMWORD[((0-128))+rcx]
+	lea	rdx,[96+rdx]
+	vmovdqu	xmm15,XMMWORD[((0-128))+r9]
 	vaesenclast	xmm12,xmm12,xmm6
 	vpaddb	xmm6,xmm5,xmm2
 	vaesenclast	xmm13,xmm13,xmm7
@@ -317,21 +317,21 @@ $L$enc_tail:
 	vaesenclast	xmm14,xmm14,xmm3
 	vpaddb	xmm3,xmm7,xmm2
 
-	add	r10,0x60
-	sub	rdx,0x6
+	add	rax,0x60
+	sub	r8,0x6
 	jc	NEAR $L$6x_done
 
-	vmovups	XMMWORD[(-96)+rsi],xmm9
+	vmovups	XMMWORD[(-96)+rdx],xmm9
 	vpxor	xmm9,xmm1,xmm15
-	vmovups	XMMWORD[(-80)+rsi],xmm10
+	vmovups	XMMWORD[(-80)+rdx],xmm10
 	vmovdqa	xmm10,xmm0
-	vmovups	XMMWORD[(-64)+rsi],xmm11
+	vmovups	XMMWORD[(-64)+rdx],xmm11
 	vmovdqa	xmm11,xmm5
-	vmovups	XMMWORD[(-48)+rsi],xmm12
+	vmovups	XMMWORD[(-48)+rdx],xmm12
 	vmovdqa	xmm12,xmm6
-	vmovups	XMMWORD[(-32)+rsi],xmm13
+	vmovups	XMMWORD[(-32)+rdx],xmm13
 	vmovdqa	xmm13,xmm7
-	vmovups	XMMWORD[(-16)+rsi],xmm14
+	vmovups	XMMWORD[(-16)+rdx],xmm14
 	vmovdqa	xmm14,xmm3
 	vmovdqu	xmm7,XMMWORD[((32+8))+rsp]
 	jmp	NEAR $L$oop6x
@@ -347,66 +347,82 @@ global	aesni_gcm_decrypt
 
 ALIGN	32
 aesni_gcm_decrypt:
-	mov	QWORD[8+rsp],rdi	;WIN64 prologue
-	mov	QWORD[16+rsp],rsi
-	mov	rax,rsp
-$L$SEH_begin_aesni_gcm_decrypt:
-	mov	rdi,rcx
-	mov	rsi,rdx
-	mov	rdx,r8
-	mov	rcx,r9
-	mov	r8,QWORD[40+rsp]
-	mov	r9,QWORD[48+rsp]
 
+$L$SEH_begin_aesni_gcm_decrypt_1:
+	xor	rax,rax
 
 
-	xor	r10,r10
 
-
-
-	cmp	rdx,0x60
+	cmp	r8,0x60
 	jb	NEAR $L$gcm_dec_abort
 
-	lea	rax,[rsp]
+	push	rbp
+
+$L$SEH_prolog_aesni_gcm_decrypt_2:
+	mov	rbp,rsp
 
 	push	rbx
 
-	push	rbp
-
+$L$SEH_prolog_aesni_gcm_decrypt_3:
 	push	r12
 
+$L$SEH_prolog_aesni_gcm_decrypt_4:
 	push	r13
 
+$L$SEH_prolog_aesni_gcm_decrypt_5:
 	push	r14
 
+$L$SEH_prolog_aesni_gcm_decrypt_6:
 	push	r15
 
+$L$SEH_prolog_aesni_gcm_decrypt_7:
 	lea	rsp,[((-168))+rsp]
-	movaps	XMMWORD[(-216)+rax],xmm6
-	movaps	XMMWORD[(-200)+rax],xmm7
-	movaps	XMMWORD[(-184)+rax],xmm8
-	movaps	XMMWORD[(-168)+rax],xmm9
-	movaps	XMMWORD[(-152)+rax],xmm10
-	movaps	XMMWORD[(-136)+rax],xmm11
-	movaps	XMMWORD[(-120)+rax],xmm12
-	movaps	XMMWORD[(-104)+rax],xmm13
-	movaps	XMMWORD[(-88)+rax],xmm14
-	movaps	XMMWORD[(-72)+rax],xmm15
-$L$gcm_dec_body:
+$L$SEH_prolog_aesni_gcm_decrypt_8:
+$L$SEH_prolog_aesni_gcm_decrypt_9:
+
+
+
+	mov	QWORD[16+rbp],rdi
+$L$SEH_prolog_aesni_gcm_decrypt_10:
+	mov	QWORD[24+rbp],rsi
+$L$SEH_prolog_aesni_gcm_decrypt_11:
+	mov	rdi,QWORD[48+rbp]
+	mov	rsi,QWORD[56+rbp]
+
+	movaps	XMMWORD[(-208)+rbp],xmm6
+$L$SEH_prolog_aesni_gcm_decrypt_12:
+	movaps	XMMWORD[(-192)+rbp],xmm7
+$L$SEH_prolog_aesni_gcm_decrypt_13:
+	movaps	XMMWORD[(-176)+rbp],xmm8
+$L$SEH_prolog_aesni_gcm_decrypt_14:
+	movaps	XMMWORD[(-160)+rbp],xmm9
+$L$SEH_prolog_aesni_gcm_decrypt_15:
+	movaps	XMMWORD[(-144)+rbp],xmm10
+$L$SEH_prolog_aesni_gcm_decrypt_16:
+	movaps	XMMWORD[(-128)+rbp],xmm11
+$L$SEH_prolog_aesni_gcm_decrypt_17:
+	movaps	XMMWORD[(-112)+rbp],xmm12
+$L$SEH_prolog_aesni_gcm_decrypt_18:
+	movaps	XMMWORD[(-96)+rbp],xmm13
+$L$SEH_prolog_aesni_gcm_decrypt_19:
+	movaps	XMMWORD[(-80)+rbp],xmm14
+$L$SEH_prolog_aesni_gcm_decrypt_20:
+	movaps	XMMWORD[(-64)+rbp],xmm15
+$L$SEH_prolog_aesni_gcm_decrypt_21:
 	vzeroupper
 
-	vmovdqu	xmm1,XMMWORD[r8]
+	vmovdqu	xmm1,XMMWORD[rdi]
 	add	rsp,-128
-	mov	ebx,DWORD[12+r8]
+	mov	ebx,DWORD[12+rdi]
 	lea	r11,[$L$bswap_mask]
-	lea	r14,[((-128))+rcx]
+	lea	r14,[((-128))+r9]
 	mov	r15,0xf80
-	vmovdqu	xmm8,XMMWORD[r9]
+	vmovdqu	xmm8,XMMWORD[rsi]
 	and	rsp,-128
 	vmovdqu	xmm0,XMMWORD[r11]
-	lea	rcx,[128+rcx]
-	lea	r9,[((32+32))+r9]
-	mov	ebp,DWORD[((240-128))+rcx]
+	lea	r9,[128+r9]
+	lea	rsi,[((32+32))+rsi]
+	mov	r10d,DWORD[((240-128))+r9]
 	vpshufb	xmm8,xmm8,xmm0
 
 	and	r14,r15
@@ -418,9 +434,9 @@ $L$gcm_dec_body:
 	sub	rsp,r15
 $L$dec_no_key_aliasing:
 
-	vmovdqu	xmm7,XMMWORD[80+rdi]
-	lea	r14,[rdi]
-	vmovdqu	xmm4,XMMWORD[64+rdi]
+	vmovdqu	xmm7,XMMWORD[80+rcx]
+	mov	r14,rcx
+	vmovdqu	xmm4,XMMWORD[64+rcx]
 
 
 
@@ -428,16 +444,16 @@ $L$dec_no_key_aliasing:
 
 
 
-	lea	r15,[((-192))+rdx*1+rdi]
+	lea	r15,[((-192))+r8*1+rcx]
 
-	vmovdqu	xmm5,XMMWORD[48+rdi]
-	shr	rdx,4
-	xor	r10,r10
-	vmovdqu	xmm6,XMMWORD[32+rdi]
+	vmovdqu	xmm5,XMMWORD[48+rcx]
+	shr	r8,4
+	xor	rax,rax
+	vmovdqu	xmm6,XMMWORD[32+rcx]
 	vpshufb	xmm7,xmm7,xmm0
-	vmovdqu	xmm2,XMMWORD[16+rdi]
+	vmovdqu	xmm2,XMMWORD[16+rcx]
 	vpshufb	xmm4,xmm4,xmm0
-	vmovdqu	xmm3,XMMWORD[rdi]
+	vmovdqu	xmm3,XMMWORD[rcx]
 	vpshufb	xmm5,xmm5,xmm0
 	vmovdqu	XMMWORD[48+rsp],xmm4
 	vpshufb	xmm6,xmm6,xmm0
@@ -450,57 +466,57 @@ $L$dec_no_key_aliasing:
 
 	call	_aesni_ctr32_ghash_6x
 
-	vmovups	XMMWORD[(-96)+rsi],xmm9
-	vmovups	XMMWORD[(-80)+rsi],xmm10
-	vmovups	XMMWORD[(-64)+rsi],xmm11
-	vmovups	XMMWORD[(-48)+rsi],xmm12
-	vmovups	XMMWORD[(-32)+rsi],xmm13
-	vmovups	XMMWORD[(-16)+rsi],xmm14
+	vmovups	XMMWORD[(-96)+rdx],xmm9
+	vmovups	XMMWORD[(-80)+rdx],xmm10
+	vmovups	XMMWORD[(-64)+rdx],xmm11
+	vmovups	XMMWORD[(-48)+rdx],xmm12
+	vmovups	XMMWORD[(-32)+rdx],xmm13
+	vmovups	XMMWORD[(-16)+rdx],xmm14
 
 	vpshufb	xmm8,xmm8,XMMWORD[r11]
-	vmovdqu	XMMWORD[(-64)+r9],xmm8
+	vmovdqu	XMMWORD[(-64)+rsi],xmm8
 
 	vzeroupper
-	movaps	xmm6,XMMWORD[((-216))+rax]
-	movaps	xmm7,XMMWORD[((-200))+rax]
-	movaps	xmm8,XMMWORD[((-184))+rax]
-	movaps	xmm9,XMMWORD[((-168))+rax]
-	movaps	xmm10,XMMWORD[((-152))+rax]
-	movaps	xmm11,XMMWORD[((-136))+rax]
-	movaps	xmm12,XMMWORD[((-120))+rax]
-	movaps	xmm13,XMMWORD[((-104))+rax]
-	movaps	xmm14,XMMWORD[((-88))+rax]
-	movaps	xmm15,XMMWORD[((-72))+rax]
-	mov	r15,QWORD[((-48))+rax]
+	movaps	xmm6,XMMWORD[((-208))+rbp]
+	movaps	xmm7,XMMWORD[((-192))+rbp]
+	movaps	xmm8,XMMWORD[((-176))+rbp]
+	movaps	xmm9,XMMWORD[((-160))+rbp]
+	movaps	xmm10,XMMWORD[((-144))+rbp]
+	movaps	xmm11,XMMWORD[((-128))+rbp]
+	movaps	xmm12,XMMWORD[((-112))+rbp]
+	movaps	xmm13,XMMWORD[((-96))+rbp]
+	movaps	xmm14,XMMWORD[((-80))+rbp]
+	movaps	xmm15,XMMWORD[((-64))+rbp]
+	mov	rdi,QWORD[16+rbp]
+	mov	rsi,QWORD[24+rbp]
+	lea	rsp,[((-40))+rbp]
 
-	mov	r14,QWORD[((-40))+rax]
+	pop	r15
 
-	mov	r13,QWORD[((-32))+rax]
+	pop	r14
 
-	mov	r12,QWORD[((-24))+rax]
+	pop	r13
 
-	mov	rbp,QWORD[((-16))+rax]
+	pop	r12
 
-	mov	rbx,QWORD[((-8))+rax]
+	pop	rbx
 
-	lea	rsp,[rax]
+	pop	rbp
 
 $L$gcm_dec_abort:
-	mov	rax,r10
-	mov	rdi,QWORD[8+rsp]	;WIN64 epilogue
-	mov	rsi,QWORD[16+rsp]
 	DB	0F3h,0C3h		;repret
+$L$SEH_end_aesni_gcm_decrypt_22:
+
 
-$L$SEH_end_aesni_gcm_decrypt:
 
 ALIGN	32
 _aesni_ctr32_6x:
 
-	vmovdqu	xmm4,XMMWORD[((0-128))+rcx]
+	vmovdqu	xmm4,XMMWORD[((0-128))+r9]
 	vmovdqu	xmm2,XMMWORD[32+r11]
-	lea	r13,[((-1))+rbp]
-	vmovups	xmm15,XMMWORD[((16-128))+rcx]
-	lea	r12,[((32-128))+rcx]
+	lea	r13,[((-1))+r10]
+	vmovups	xmm15,XMMWORD[((16-128))+r9]
+	lea	r12,[((32-128))+r9]
 	vpxor	xmm9,xmm1,xmm4
 	add	ebx,100663296
 	jc	NEAR $L$handle_ctr32_2
@@ -532,18 +548,18 @@ $L$oop_ctr32:
 
 	vmovdqu	xmm3,XMMWORD[r12]
 	vaesenc	xmm9,xmm9,xmm15
-	vpxor	xmm4,xmm3,XMMWORD[rdi]
+	vpxor	xmm4,xmm3,XMMWORD[rcx]
 	vaesenc	xmm10,xmm10,xmm15
-	vpxor	xmm5,xmm3,XMMWORD[16+rdi]
+	vpxor	xmm5,xmm3,XMMWORD[16+rcx]
 	vaesenc	xmm11,xmm11,xmm15
-	vpxor	xmm6,xmm3,XMMWORD[32+rdi]
+	vpxor	xmm6,xmm3,XMMWORD[32+rcx]
 	vaesenc	xmm12,xmm12,xmm15
-	vpxor	xmm8,xmm3,XMMWORD[48+rdi]
+	vpxor	xmm8,xmm3,XMMWORD[48+rcx]
 	vaesenc	xmm13,xmm13,xmm15
-	vpxor	xmm2,xmm3,XMMWORD[64+rdi]
+	vpxor	xmm2,xmm3,XMMWORD[64+rcx]
 	vaesenc	xmm14,xmm14,xmm15
-	vpxor	xmm3,xmm3,XMMWORD[80+rdi]
-	lea	rdi,[96+rdi]
+	vpxor	xmm3,xmm3,XMMWORD[80+rcx]
+	lea	rcx,[96+rcx]
 
 	vaesenclast	xmm9,xmm9,xmm4
 	vaesenclast	xmm10,xmm10,xmm5
@@ -551,13 +567,13 @@ $L$oop_ctr32:
 	vaesenclast	xmm12,xmm12,xmm8
 	vaesenclast	xmm13,xmm13,xmm2
 	vaesenclast	xmm14,xmm14,xmm3
-	vmovups	XMMWORD[rsi],xmm9
-	vmovups	XMMWORD[16+rsi],xmm10
-	vmovups	XMMWORD[32+rsi],xmm11
-	vmovups	XMMWORD[48+rsi],xmm12
-	vmovups	XMMWORD[64+rsi],xmm13
-	vmovups	XMMWORD[80+rsi],xmm14
-	lea	rsi,[96+rsi]
+	vmovups	XMMWORD[rdx],xmm9
+	vmovups	XMMWORD[16+rdx],xmm10
+	vmovups	XMMWORD[32+rdx],xmm11
+	vmovups	XMMWORD[48+rdx],xmm12
+	vmovups	XMMWORD[64+rdx],xmm13
+	vmovups	XMMWORD[80+rdx],xmm14
+	lea	rdx,[96+rdx]
 
 	DB	0F3h,0C3h		;repret
 ALIGN	32
@@ -589,69 +605,85 @@ global	aesni_gcm_encrypt
 
 ALIGN	32
 aesni_gcm_encrypt:
-	mov	QWORD[8+rsp],rdi	;WIN64 prologue
-	mov	QWORD[16+rsp],rsi
-	mov	rax,rsp
-$L$SEH_begin_aesni_gcm_encrypt:
-	mov	rdi,rcx
-	mov	rsi,rdx
-	mov	rdx,r8
-	mov	rcx,r9
-	mov	r8,QWORD[40+rsp]
-	mov	r9,QWORD[48+rsp]
-
-
 
+$L$SEH_begin_aesni_gcm_encrypt_1:
 %ifdef BORINGSSL_DISPATCH_TEST
 EXTERN	BORINGSSL_function_hit
 	mov	BYTE[((BORINGSSL_function_hit+2))],1
 %endif
-	xor	r10,r10
+	xor	rax,rax
 
 
 
 
-	cmp	rdx,0x60*3
+	cmp	r8,0x60*3
 	jb	NEAR $L$gcm_enc_abort
 
-	lea	rax,[rsp]
+	push	rbp
 
-	push	rbx
+$L$SEH_prolog_aesni_gcm_encrypt_2:
+	mov	rbp,rsp
 
-	push	rbp
+	push	rbx
 
+$L$SEH_prolog_aesni_gcm_encrypt_3:
 	push	r12
 
+$L$SEH_prolog_aesni_gcm_encrypt_4:
 	push	r13
 
+$L$SEH_prolog_aesni_gcm_encrypt_5:
 	push	r14
 
+$L$SEH_prolog_aesni_gcm_encrypt_6:
 	push	r15
 
+$L$SEH_prolog_aesni_gcm_encrypt_7:
 	lea	rsp,[((-168))+rsp]
-	movaps	XMMWORD[(-216)+rax],xmm6
-	movaps	XMMWORD[(-200)+rax],xmm7
-	movaps	XMMWORD[(-184)+rax],xmm8
-	movaps	XMMWORD[(-168)+rax],xmm9
-	movaps	XMMWORD[(-152)+rax],xmm10
-	movaps	XMMWORD[(-136)+rax],xmm11
-	movaps	XMMWORD[(-120)+rax],xmm12
-	movaps	XMMWORD[(-104)+rax],xmm13
-	movaps	XMMWORD[(-88)+rax],xmm14
-	movaps	XMMWORD[(-72)+rax],xmm15
-$L$gcm_enc_body:
+$L$SEH_prolog_aesni_gcm_encrypt_8:
+$L$SEH_prolog_aesni_gcm_encrypt_9:
+
+
+
+	mov	QWORD[16+rbp],rdi
+$L$SEH_prolog_aesni_gcm_encrypt_10:
+	mov	QWORD[24+rbp],rsi
+$L$SEH_prolog_aesni_gcm_encrypt_11:
+	mov	rdi,QWORD[48+rbp]
+	mov	rsi,QWORD[56+rbp]
+
+	movaps	XMMWORD[(-208)+rbp],xmm6
+$L$SEH_prolog_aesni_gcm_encrypt_12:
+	movaps	XMMWORD[(-192)+rbp],xmm7
+$L$SEH_prolog_aesni_gcm_encrypt_13:
+	movaps	XMMWORD[(-176)+rbp],xmm8
+$L$SEH_prolog_aesni_gcm_encrypt_14:
+	movaps	XMMWORD[(-160)+rbp],xmm9
+$L$SEH_prolog_aesni_gcm_encrypt_15:
+	movaps	XMMWORD[(-144)+rbp],xmm10
+$L$SEH_prolog_aesni_gcm_encrypt_16:
+	movaps	XMMWORD[(-128)+rbp],xmm11
+$L$SEH_prolog_aesni_gcm_encrypt_17:
+	movaps	XMMWORD[(-112)+rbp],xmm12
+$L$SEH_prolog_aesni_gcm_encrypt_18:
+	movaps	XMMWORD[(-96)+rbp],xmm13
+$L$SEH_prolog_aesni_gcm_encrypt_19:
+	movaps	XMMWORD[(-80)+rbp],xmm14
+$L$SEH_prolog_aesni_gcm_encrypt_20:
+	movaps	XMMWORD[(-64)+rbp],xmm15
+$L$SEH_prolog_aesni_gcm_encrypt_21:
 	vzeroupper
 
-	vmovdqu	xmm1,XMMWORD[r8]
+	vmovdqu	xmm1,XMMWORD[rdi]
 	add	rsp,-128
-	mov	ebx,DWORD[12+r8]
+	mov	ebx,DWORD[12+rdi]
 	lea	r11,[$L$bswap_mask]
-	lea	r14,[((-128))+rcx]
+	lea	r14,[((-128))+r9]
 	mov	r15,0xf80
-	lea	rcx,[128+rcx]
+	lea	r9,[128+r9]
 	vmovdqu	xmm0,XMMWORD[r11]
 	and	rsp,-128
-	mov	ebp,DWORD[((240-128))+rcx]
+	mov	r10d,DWORD[((240-128))+r9]
 
 	and	r14,r15
 	and	r15,rsp
@@ -662,7 +694,7 @@ $L$gcm_enc_body:
 	sub	rsp,r15
 $L$enc_no_key_aliasing:
 
-	lea	r14,[rsi]
+	mov	r14,rdx
 
 
 
@@ -671,9 +703,9 @@ $L$enc_no_key_aliasing:
 
 
 
-	lea	r15,[((-192))+rdx*1+rsi]
+	lea	r15,[((-192))+r8*1+rdx]
 
-	shr	rdx,4
+	shr	r8,4
 
 	call	_aesni_ctr32_6x
 	vpshufb	xmm8,xmm9,xmm0
@@ -690,34 +722,34 @@ $L$enc_no_key_aliasing:
 
 	call	_aesni_ctr32_6x
 
-	vmovdqu	xmm8,XMMWORD[r9]
-	lea	r9,[((32+32))+r9]
-	sub	rdx,12
-	mov	r10,0x60*2
+	vmovdqu	xmm8,XMMWORD[rsi]
+	lea	rsi,[((32+32))+rsi]
+	sub	r8,12
+	mov	rax,0x60*2
 	vpshufb	xmm8,xmm8,xmm0
 
 	call	_aesni_ctr32_ghash_6x
 	vmovdqu	xmm7,XMMWORD[32+rsp]
 	vmovdqu	xmm0,XMMWORD[r11]
-	vmovdqu	xmm3,XMMWORD[((0-32))+r9]
+	vmovdqu	xmm3,XMMWORD[((0-32))+rsi]
 	vpunpckhqdq	xmm1,xmm7,xmm7
-	vmovdqu	xmm15,XMMWORD[((32-32))+r9]
-	vmovups	XMMWORD[(-96)+rsi],xmm9
+	vmovdqu	xmm15,XMMWORD[((32-32))+rsi]
+	vmovups	XMMWORD[(-96)+rdx],xmm9
 	vpshufb	xmm9,xmm9,xmm0
 	vpxor	xmm1,xmm1,xmm7
-	vmovups	XMMWORD[(-80)+rsi],xmm10
+	vmovups	XMMWORD[(-80)+rdx],xmm10
 	vpshufb	xmm10,xmm10,xmm0
-	vmovups	XMMWORD[(-64)+rsi],xmm11
+	vmovups	XMMWORD[(-64)+rdx],xmm11
 	vpshufb	xmm11,xmm11,xmm0
-	vmovups	XMMWORD[(-48)+rsi],xmm12
+	vmovups	XMMWORD[(-48)+rdx],xmm12
 	vpshufb	xmm12,xmm12,xmm0
-	vmovups	XMMWORD[(-32)+rsi],xmm13
+	vmovups	XMMWORD[(-32)+rdx],xmm13
 	vpshufb	xmm13,xmm13,xmm0
-	vmovups	XMMWORD[(-16)+rsi],xmm14
+	vmovups	XMMWORD[(-16)+rdx],xmm14
 	vpshufb	xmm14,xmm14,xmm0
 	vmovdqu	XMMWORD[16+rsp],xmm9
 	vmovdqu	xmm6,XMMWORD[48+rsp]
-	vmovdqu	xmm0,XMMWORD[((16-32))+r9]
+	vmovdqu	xmm0,XMMWORD[((16-32))+rsi]
 	vpunpckhqdq	xmm2,xmm6,xmm6
 	vpclmulqdq	xmm5,xmm7,xmm3,0x00
 	vpxor	xmm2,xmm2,xmm6
@@ -726,19 +758,19 @@ $L$enc_no_key_aliasing:
 
 	vmovdqu	xmm9,XMMWORD[64+rsp]
 	vpclmulqdq	xmm4,xmm6,xmm0,0x00
-	vmovdqu	xmm3,XMMWORD[((48-32))+r9]
+	vmovdqu	xmm3,XMMWORD[((48-32))+rsi]
 	vpxor	xmm4,xmm4,xmm5
 	vpunpckhqdq	xmm5,xmm9,xmm9
 	vpclmulqdq	xmm6,xmm6,xmm0,0x11
 	vpxor	xmm5,xmm5,xmm9
 	vpxor	xmm6,xmm6,xmm7
 	vpclmulqdq	xmm2,xmm2,xmm15,0x10
-	vmovdqu	xmm15,XMMWORD[((80-32))+r9]
+	vmovdqu	xmm15,XMMWORD[((80-32))+rsi]
 	vpxor	xmm2,xmm2,xmm1
 
 	vmovdqu	xmm1,XMMWORD[80+rsp]
 	vpclmulqdq	xmm7,xmm9,xmm3,0x00
-	vmovdqu	xmm0,XMMWORD[((64-32))+r9]
+	vmovdqu	xmm0,XMMWORD[((64-32))+rsi]
 	vpxor	xmm7,xmm7,xmm4
 	vpunpckhqdq	xmm4,xmm1,xmm1
 	vpclmulqdq	xmm9,xmm9,xmm3,0x11
@@ -749,19 +781,19 @@ $L$enc_no_key_aliasing:
 
 	vmovdqu	xmm2,XMMWORD[96+rsp]
 	vpclmulqdq	xmm6,xmm1,xmm0,0x00
-	vmovdqu	xmm3,XMMWORD[((96-32))+r9]
+	vmovdqu	xmm3,XMMWORD[((96-32))+rsi]
 	vpxor	xmm6,xmm6,xmm7
 	vpunpckhqdq	xmm7,xmm2,xmm2
 	vpclmulqdq	xmm1,xmm1,xmm0,0x11
 	vpxor	xmm7,xmm7,xmm2
 	vpxor	xmm1,xmm1,xmm9
 	vpclmulqdq	xmm4,xmm4,xmm15,0x10
-	vmovdqu	xmm15,XMMWORD[((128-32))+r9]
+	vmovdqu	xmm15,XMMWORD[((128-32))+rsi]
 	vpxor	xmm4,xmm4,xmm5
 
 	vpxor	xmm8,xmm8,XMMWORD[112+rsp]
 	vpclmulqdq	xmm5,xmm2,xmm3,0x00
-	vmovdqu	xmm0,XMMWORD[((112-32))+r9]
+	vmovdqu	xmm0,XMMWORD[((112-32))+rsi]
 	vpunpckhqdq	xmm9,xmm8,xmm8
 	vpxor	xmm5,xmm5,xmm6
 	vpclmulqdq	xmm2,xmm2,xmm3,0x11
@@ -771,17 +803,17 @@ $L$enc_no_key_aliasing:
 	vpxor	xmm4,xmm7,xmm4
 
 	vpclmulqdq	xmm6,xmm8,xmm0,0x00
-	vmovdqu	xmm3,XMMWORD[((0-32))+r9]
+	vmovdqu	xmm3,XMMWORD[((0-32))+rsi]
 	vpunpckhqdq	xmm1,xmm14,xmm14
 	vpclmulqdq	xmm8,xmm8,xmm0,0x11
 	vpxor	xmm1,xmm1,xmm14
 	vpxor	xmm5,xmm6,xmm5
 	vpclmulqdq	xmm9,xmm9,xmm15,0x10
-	vmovdqu	xmm15,XMMWORD[((32-32))+r9]
+	vmovdqu	xmm15,XMMWORD[((32-32))+rsi]
 	vpxor	xmm7,xmm8,xmm2
 	vpxor	xmm6,xmm9,xmm4
 
-	vmovdqu	xmm0,XMMWORD[((16-32))+r9]
+	vmovdqu	xmm0,XMMWORD[((16-32))+rsi]
 	vpxor	xmm9,xmm7,xmm5
 	vpclmulqdq	xmm4,xmm14,xmm3,0x00
 	vpxor	xmm6,xmm6,xmm9
@@ -795,7 +827,7 @@ $L$enc_no_key_aliasing:
 	vpxor	xmm7,xmm7,xmm6
 
 	vpclmulqdq	xmm5,xmm13,xmm0,0x00
-	vmovdqu	xmm3,XMMWORD[((48-32))+r9]
+	vmovdqu	xmm3,XMMWORD[((48-32))+rsi]
 	vpxor	xmm5,xmm5,xmm4
 	vpunpckhqdq	xmm9,xmm12,xmm12
 	vpclmulqdq	xmm13,xmm13,xmm0,0x11
@@ -803,11 +835,11 @@ $L$enc_no_key_aliasing:
 	vpxor	xmm13,xmm13,xmm14
 	vpalignr	xmm14,xmm8,xmm8,8
 	vpclmulqdq	xmm2,xmm2,xmm15,0x10
-	vmovdqu	xmm15,XMMWORD[((80-32))+r9]
+	vmovdqu	xmm15,XMMWORD[((80-32))+rsi]
 	vpxor	xmm2,xmm2,xmm1
 
 	vpclmulqdq	xmm4,xmm12,xmm3,0x00
-	vmovdqu	xmm0,XMMWORD[((64-32))+r9]
+	vmovdqu	xmm0,XMMWORD[((64-32))+rsi]
 	vpxor	xmm4,xmm4,xmm5
 	vpunpckhqdq	xmm1,xmm11,xmm11
 	vpclmulqdq	xmm12,xmm12,xmm3,0x11
@@ -821,7 +853,7 @@ $L$enc_no_key_aliasing:
 	vxorps	xmm8,xmm8,xmm14
 
 	vpclmulqdq	xmm5,xmm11,xmm0,0x00
-	vmovdqu	xmm3,XMMWORD[((96-32))+r9]
+	vmovdqu	xmm3,XMMWORD[((96-32))+rsi]
 	vpxor	xmm5,xmm5,xmm4
 	vpunpckhqdq	xmm2,xmm10,xmm10
 	vpclmulqdq	xmm11,xmm11,xmm0,0x11
@@ -829,7 +861,7 @@ $L$enc_no_key_aliasing:
 	vpalignr	xmm14,xmm8,xmm8,8
 	vpxor	xmm11,xmm11,xmm12
 	vpclmulqdq	xmm1,xmm1,xmm15,0x10
-	vmovdqu	xmm15,XMMWORD[((128-32))+r9]
+	vmovdqu	xmm15,XMMWORD[((128-32))+rsi]
 	vpxor	xmm1,xmm1,xmm9
 
 	vxorps	xmm14,xmm14,xmm7
@@ -837,7 +869,7 @@ $L$enc_no_key_aliasing:
 	vxorps	xmm8,xmm8,xmm14
 
 	vpclmulqdq	xmm4,xmm10,xmm3,0x00
-	vmovdqu	xmm0,XMMWORD[((112-32))+r9]
+	vmovdqu	xmm0,XMMWORD[((112-32))+rsi]
 	vpxor	xmm4,xmm4,xmm5
 	vpunpckhqdq	xmm9,xmm8,xmm8
 	vpclmulqdq	xmm10,xmm10,xmm3,0x11
@@ -870,40 +902,40 @@ $L$enc_no_key_aliasing:
 	vpxor	xmm2,xmm2,xmm7
 	vpxor	xmm8,xmm8,xmm2
 	vpshufb	xmm8,xmm8,XMMWORD[r11]
-	vmovdqu	XMMWORD[(-64)+r9],xmm8
+	vmovdqu	XMMWORD[(-64)+rsi],xmm8
 
 	vzeroupper
-	movaps	xmm6,XMMWORD[((-216))+rax]
-	movaps	xmm7,XMMWORD[((-200))+rax]
-	movaps	xmm8,XMMWORD[((-184))+rax]
-	movaps	xmm9,XMMWORD[((-168))+rax]
-	movaps	xmm10,XMMWORD[((-152))+rax]
-	movaps	xmm11,XMMWORD[((-136))+rax]
-	movaps	xmm12,XMMWORD[((-120))+rax]
-	movaps	xmm13,XMMWORD[((-104))+rax]
-	movaps	xmm14,XMMWORD[((-88))+rax]
-	movaps	xmm15,XMMWORD[((-72))+rax]
-	mov	r15,QWORD[((-48))+rax]
+	movaps	xmm6,XMMWORD[((-208))+rbp]
+	movaps	xmm7,XMMWORD[((-192))+rbp]
+	movaps	xmm8,XMMWORD[((-176))+rbp]
+	movaps	xmm9,XMMWORD[((-160))+rbp]
+	movaps	xmm10,XMMWORD[((-144))+rbp]
+	movaps	xmm11,XMMWORD[((-128))+rbp]
+	movaps	xmm12,XMMWORD[((-112))+rbp]
+	movaps	xmm13,XMMWORD[((-96))+rbp]
+	movaps	xmm14,XMMWORD[((-80))+rbp]
+	movaps	xmm15,XMMWORD[((-64))+rbp]
+	mov	rdi,QWORD[16+rbp]
+	mov	rsi,QWORD[24+rbp]
+	lea	rsp,[((-40))+rbp]
 
-	mov	r14,QWORD[((-40))+rax]
+	pop	r15
 
-	mov	r13,QWORD[((-32))+rax]
+	pop	r14
 
-	mov	r12,QWORD[((-24))+rax]
+	pop	r13
 
-	mov	rbp,QWORD[((-16))+rax]
+	pop	r12
 
-	mov	rbx,QWORD[((-8))+rax]
+	pop	rbx
 
-	lea	rsp,[rax]
+	pop	rbp
 
 $L$gcm_enc_abort:
-	mov	rax,r10
-	mov	rdi,QWORD[8+rsp]	;WIN64 epilogue
-	mov	rsi,QWORD[16+rsp]
 	DB	0F3h,0C3h		;repret
+$L$SEH_end_aesni_gcm_encrypt_22:
+
 
-$L$SEH_end_aesni_gcm_encrypt:
 ALIGN	64
 $L$bswap_mask:
 	DB	15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0
@@ -920,116 +952,134 @@ $L$one_lsb:
 	DB	89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112
 	DB	114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
 ALIGN	64
-EXTERN	__imp_RtlVirtualUnwind
-
-ALIGN	16
-gcm_se_handler:
-	push	rsi
-	push	rdi
-	push	rbx
-	push	rbp
-	push	r12
-	push	r13
-	push	r14
-	push	r15
-	pushfq
-	sub	rsp,64
-
-	mov	rax,QWORD[120+r8]
-	mov	rbx,QWORD[248+r8]
-
-	mov	rsi,QWORD[8+r9]
-	mov	r11,QWORD[56+r9]
-
-	mov	r10d,DWORD[r11]
-	lea	r10,[r10*1+rsi]
-	cmp	rbx,r10
-	jb	NEAR $L$common_seh_tail
-
-	mov	rax,QWORD[152+r8]
-
-	mov	r10d,DWORD[4+r11]
-	lea	r10,[r10*1+rsi]
-	cmp	rbx,r10
-	jae	NEAR $L$common_seh_tail
-
-	mov	rax,QWORD[120+r8]
-
-	mov	r15,QWORD[((-48))+rax]
-	mov	r14,QWORD[((-40))+rax]
-	mov	r13,QWORD[((-32))+rax]
-	mov	r12,QWORD[((-24))+rax]
-	mov	rbp,QWORD[((-16))+rax]
-	mov	rbx,QWORD[((-8))+rax]
-	mov	QWORD[240+r8],r15
-	mov	QWORD[232+r8],r14
-	mov	QWORD[224+r8],r13
-	mov	QWORD[216+r8],r12
-	mov	QWORD[160+r8],rbp
-	mov	QWORD[144+r8],rbx
-
-	lea	rsi,[((-216))+rax]
-	lea	rdi,[512+r8]
-	mov	ecx,20
-	DD	0xa548f3fc
-
-$L$common_seh_tail:
-	mov	rdi,QWORD[8+rax]
-	mov	rsi,QWORD[16+rax]
-	mov	QWORD[152+r8],rax
-	mov	QWORD[168+r8],rsi
-	mov	QWORD[176+r8],rdi
-
-	mov	rdi,QWORD[40+r9]
-	mov	rsi,r8
-	mov	ecx,154
-	DD	0xa548f3fc
-
-	mov	rsi,r9
-	xor	rcx,rcx
-	mov	rdx,QWORD[8+rsi]
-	mov	r8,QWORD[rsi]
-	mov	r9,QWORD[16+rsi]
-	mov	r10,QWORD[40+rsi]
-	lea	r11,[56+rsi]
-	lea	r12,[24+rsi]
-	mov	QWORD[32+rsp],r10
-	mov	QWORD[40+rsp],r11
-	mov	QWORD[48+rsp],r12
-	mov	QWORD[56+rsp],rcx
-	call	QWORD[__imp_RtlVirtualUnwind]
-
-	mov	eax,1
-	add	rsp,64
-	popfq
-	pop	r15
-	pop	r14
-	pop	r13
-	pop	r12
-	pop	rbp
-	pop	rbx
-	pop	rdi
-	pop	rsi
-	DB	0F3h,0C3h		;repret
-
-
 section	.pdata rdata align=4
 ALIGN	4
-	DD	$L$SEH_begin_aesni_gcm_decrypt wrt ..imagebase
-	DD	$L$SEH_end_aesni_gcm_decrypt wrt ..imagebase
-	DD	$L$SEH_gcm_dec_info wrt ..imagebase
+	DD	$L$SEH_begin_aesni_gcm_decrypt_1 wrt ..imagebase
+	DD	$L$SEH_end_aesni_gcm_decrypt_22 wrt ..imagebase
+	DD	$L$SEH_info_aesni_gcm_decrypt_0 wrt ..imagebase
+
+	DD	$L$SEH_begin_aesni_gcm_encrypt_1 wrt ..imagebase
+	DD	$L$SEH_end_aesni_gcm_encrypt_22 wrt ..imagebase
+	DD	$L$SEH_info_aesni_gcm_encrypt_0 wrt ..imagebase
+
 
-	DD	$L$SEH_begin_aesni_gcm_encrypt wrt ..imagebase
-	DD	$L$SEH_end_aesni_gcm_encrypt wrt ..imagebase
-	DD	$L$SEH_gcm_enc_info wrt ..imagebase
 section	.xdata rdata align=8
-ALIGN	8
-$L$SEH_gcm_dec_info:
-	DB	9,0,0,0
-	DD	gcm_se_handler wrt ..imagebase
-	DD	$L$gcm_dec_body wrt ..imagebase,$L$gcm_dec_abort wrt ..imagebase
-$L$SEH_gcm_enc_info:
-	DB	9,0,0,0
-	DD	gcm_se_handler wrt ..imagebase
-	DD	$L$gcm_enc_body wrt ..imagebase,$L$gcm_enc_abort wrt ..imagebase
+ALIGN	4
+$L$SEH_info_aesni_gcm_decrypt_0:
+	DB	1
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_21-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	33
+	DB	213
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_21-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	248
+	DW	9
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_20-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	232
+	DW	8
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_19-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	216
+	DW	7
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_18-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	200
+	DW	6
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_17-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	184
+	DW	5
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_16-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	168
+	DW	4
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_15-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	152
+	DW	3
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_14-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	136
+	DW	2
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_13-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	120
+	DW	1
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_12-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	104
+	DW	0
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_11-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	100
+	DW	29
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_10-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	116
+	DW	28
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_9-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	3
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_8-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	1
+	DW	21
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_7-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	240
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_6-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	224
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_5-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	208
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_4-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	192
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_3-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	48
+	DB	$L$SEH_prolog_aesni_gcm_decrypt_2-$L$SEH_begin_aesni_gcm_decrypt_1
+	DB	80
+
+$L$SEH_info_aesni_gcm_encrypt_0:
+	DB	1
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_21-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	33
+	DB	213
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_21-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	248
+	DW	9
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_20-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	232
+	DW	8
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_19-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	216
+	DW	7
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_18-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	200
+	DW	6
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_17-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	184
+	DW	5
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_16-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	168
+	DW	4
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_15-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	152
+	DW	3
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_14-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	136
+	DW	2
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_13-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	120
+	DW	1
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_12-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	104
+	DW	0
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_11-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	100
+	DW	29
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_10-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	116
+	DW	28
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_9-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	3
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_8-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	1
+	DW	21
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_7-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	240
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_6-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	224
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_5-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	208
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_4-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	192
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_3-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	48
+	DB	$L$SEH_prolog_aesni_gcm_encrypt_2-$L$SEH_begin_aesni_gcm_encrypt_1
+	DB	80
 %endif
