From 513d2cf2ec3e15311b9f6e711dde01d1d64ecedf Mon Sep 17 00:00:00 2001
From: John Harrison <jargh@amazon.com>
Date: Tue, 28 Sep 2021 17:28:49 -0700
Subject: [PATCH] Add P-521 Montgomery multiplication

This is essentially the same as plain modular multiplication but
with different rotation at the end to divide by 2^576 mod p_521,
where 9 * 64 = 576 is the base derived from the "native size". As
already noted, since P-521 is a Mersenne number, ordinary modular
multiplication can be considered Montgomery to base 2^521.

s2n-bignum original commit: https://github.com/awslabs/s2n-bignum/commit/83054c2bc99ed91719afbc20426898d7ef23e0e0
---
 arm/p521/Makefile                  |   1 +
 arm/p521/bignum_montmul_p521.S     | 601 +++++++++++++++++++++++++++++
 x86_att/p521/bignum_montmul_p521.S | 406 +++++++++++++++++++
 3 files changed, 1008 insertions(+)
 create mode 100644 arm/p521/bignum_montmul_p521.S
 create mode 100644 x86_att/p521/bignum_montmul_p521.S

diff --git a/arm/p521/Makefile b/arm/p521/Makefile
index 26ba8b927..f62e5576a 100644
--- a/arm/p521/Makefile
+++ b/arm/p521/Makefile
@@ -38,6 +38,7 @@ OBJ = bignum_add_p521.o \
       bignum_double_p521.o \
       bignum_half_p521.o \
       bignum_mod_p521_9.o \
+      bignum_montmul_p521.o \
       bignum_montsqr_p521.o \
       bignum_mul_p521.o \
       bignum_neg_p521.o \
diff --git a/arm/p521/bignum_montmul_p521.S b/arm/p521/bignum_montmul_p521.S
new file mode 100644
index 000000000..ddeb10c2c
--- /dev/null
+++ b/arm/p521/bignum_montmul_p521.S
@@ -0,0 +1,601 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Montgomery multiply, z := (x * y / 2^576) mod p_521
+// Inputs x[9], y[9]; output z[9]
+//
+//    extern void bignum_montmul_p521
+//     (uint64_t z[static 9], uint64_t x[static 9], uint64_t y[static 9]);
+//
+// Does z := (x * y / 2^576) mod p_521, assuming x < p_521, y < p_521. This
+// means the Montgomery base is the "native size" 2^{9*64} = 2^576; since
+// p_521 is a Mersenne prime the basic modular multiplication bignum_mul_p521
+// can be considered a Montgomery operation to base 2^521.
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = y
+// ----------------------------------------------------------------------------
+
+        .globl  bignum_montmul_p521
+        .text
+        .balign 4
+
+// ---------------------------------------------------------------------------
+// Macro computing [c,b,a] := [b,a] + (x - y) * (w - z), adding with carry
+// to the [b,a] components but leaving CF aligned with the c term, which is
+// a sign bitmask for (x - y) * (w - z). Continued add-with-carry operations
+// with [c,...,c] will continue the carry chain correctly starting from
+// the c position if desired to add to a longer term of the form [...,b,a].
+//
+// c,h,l,t should all be different and t,h should not overlap w,z.
+// ---------------------------------------------------------------------------
+
+#define muldiffnadd(b,a,x,y,w,z)        \
+        subs    t, x, y;                \
+        cneg    t, t, cc;               \
+        csetm   c, cc;                  \
+        subs    h, w, z;                \
+        cneg    h, h, cc;               \
+        mul     l, t, h;                \
+        umulh   h, t, h;                \
+        cinv    c, c, cc;               \
+        adds    xzr, c, #1;             \
+        eor     l, l, c;                \
+        adcs    a, a, l;                \
+        eor     h, h, c;                \
+        adcs    b, b, h
+
+#define z x0
+#define x x1
+#define y x2
+
+#define a0 x3
+#define a1 x4
+#define a2 x5
+#define a3 x6
+#define b0 x7
+#define b1 x8
+#define b2 x9
+#define b3 x10
+
+#define s0 x11
+#define s1 x12
+#define s2 x13
+#define s3 x14
+#define s4 x15
+#define s5 x16
+#define s6 x17
+#define s7 x19
+#define s8 x20
+
+#define c  x21
+#define h  x22
+#define l  x23
+#define t  x24
+#define s  x25
+#define u  x26
+
+// ---------------------------------------------------------------------------
+// Core 4x4->8 ADK multiplication macro
+// Does [s7,s6,s5,s4,s3,s2,s1,s0] = [a3,a2,a1,a0] * [b3,b2,b1,b0]
+// ---------------------------------------------------------------------------
+
+#define mul4                                                            \
+/*  First accumulate all the "simple" products as [s7,s6,s5,s4,s0] */   \
+                                                                        \
+                mul     s0, a0, b0;                                     \
+                mul     s4, a1, b1;                                     \
+                mul     s5, a2, b2;                                     \
+                mul     s6, a3, b3;                                     \
+                                                                        \
+                umulh   s7, a0, b0;                                     \
+                adds    s4, s4, s7;                                     \
+                umulh   s7, a1, b1;                                     \
+                adcs    s5, s5, s7;                                     \
+                umulh   s7, a2, b2;                                     \
+                adcs    s6, s6, s7;                                     \
+                umulh   s7, a3, b3;                                     \
+                adc     s7, s7, xzr;                                    \
+                                                                        \
+/*  Multiply by B + 1 to get [s7;s6;s5;s4;s1;s0] */                     \
+                                                                        \
+                adds    s1, s4, s0;                                     \
+                adcs    s4, s5, s4;                                     \
+                adcs    s5, s6, s5;                                     \
+                adcs    s6, s7, s6;                                     \
+                adc     s7, xzr, s7;                                    \
+                                                                        \
+/*  Multiply by B^2 + 1 to get [s7;s6;s5;s4;s3;s2;s1;s0] */             \
+                                                                        \
+                adds    s2, s4, s0;                                     \
+                adcs    s3, s5, s1;                                     \
+                adcs    s4, s6, s4;                                     \
+                adcs    s5, s7, s5;                                     \
+                adcs    s6, xzr, s6;                                    \
+                adc     s7, xzr, s7;                                    \
+                                                                        \
+/*  Now add in all the "complicated" terms. */                          \
+                                                                        \
+                muldiffnadd(s6,s5, a2,a3, b3,b2);                       \
+                adc     s7, s7, c;                                      \
+                                                                        \
+                muldiffnadd(s2,s1, a0,a1, b1,b0);                       \
+                adcs    s3, s3, c;                                      \
+                adcs    s4, s4, c;                                      \
+                adcs    s5, s5, c;                                      \
+                adcs    s6, s6, c;                                      \
+                adc     s7, s7, c;                                      \
+                                                                        \
+                muldiffnadd(s5,s4, a1,a3, b3,b1);                       \
+                adcs    s6, s6, c;                                      \
+                adc     s7, s7, c;                                      \
+                                                                        \
+                muldiffnadd(s3,s2, a0,a2, b2,b0);                       \
+                adcs    s4, s4, c;                                      \
+                adcs    s5, s5, c;                                      \
+                adcs    s6, s6, c;                                      \
+                adc     s7, s7, c;                                      \
+                                                                        \
+                muldiffnadd(s4,s3, a0,a3, b3,b0);                       \
+                adcs    s5, s5, c;                                      \
+                adcs    s6, s6, c;                                      \
+                adc     s7, s7, c;                                      \
+                muldiffnadd(s4,s3, a1,a2, b2,b1);                       \
+                adcs    s5, s5, c;                                      \
+                adcs    s6, s6, c;                                      \
+                adc     s7, s7, c                                       \
+
+bignum_montmul_p521:
+
+// Save registers
+
+                stp     x19, x20, [sp, #-16]!
+                stp     x21, x22, [sp, #-16]!
+                stp     x23, x24, [sp, #-16]!
+                stp     x25, x26, [sp, #-16]!
+
+// Load 4-digit low parts and multiply them to get L
+
+                ldp     a0, a1, [x]
+                ldp     a2, a3, [x, #16]
+                ldp     b0, b1, [y]
+                ldp     b2, b3, [y, #16]
+                mul4
+
+// Shift right 256 bits modulo p_521 and stash in output buffer
+
+                lsl     c, s0, #9
+                extr    s0, s1, s0, #55
+                extr    s1, s2, s1, #55
+                extr    s2, s3, s2, #55
+                lsr     s3, s3, #55
+                stp     s4, s5, [z]
+                stp     s6, s7, [z, #16]
+                stp     c, s0, [z, #32]
+                stp     s1, s2, [z, #48]
+                str     s3, [z, #64]
+
+// Load 4-digit low parts and multiply them to get H
+
+                ldp     a0, a1, [x, #32]
+                ldp     a2, a3, [x, #48]
+                ldp     b0, b1, [y, #32]
+                ldp     b2, b3, [y, #48]
+                mul4
+
+// Add to the existing output buffer and re-stash.
+// This gives a result HL congruent to (2^256 * H + L) / 2^256 modulo p_521
+
+                ldp     l, h, [z]
+                adds    s0, s0, l
+                adcs    s1, s1, h
+                stp     s0, s1, [z]
+                ldp     l, h, [z, #16]
+                adcs    s2, s2, l
+                adcs    s3, s3, h
+                stp     s2, s3, [z, #16]
+                ldp     l, h, [z, #32]
+                adcs    s4, s4, l
+                adcs    s5, s5, h
+                stp     s4, s5, [z, #32]
+                ldp     l, h, [z, #48]
+                adcs    s6, s6, l
+                adcs    s7, s7, h
+                stp     s6, s7, [z, #48]
+                ldr     c, [z, #64]
+                adc     c, c, xzr
+                str     c, [z, #64]
+
+// Compute t,[a3,a2,a1,a0] = x_hi - x_lo
+// and     s,[b3,b2,b1,b0] = y_lo - y_hi
+// sign-magnitude differences, then XOR overall sign bitmask into s
+
+                ldp     l, h, [x]
+                subs    a0, a0, l
+                sbcs    a1, a1, h
+                ldp     l, h, [x, #16]
+                sbcs    a2, a2, l
+                sbcs    a3, a3, h
+                csetm   t, cc
+                ldp     l, h, [y]
+                subs    b0, l, b0
+                sbcs    b1, h, b1
+                ldp     l, h, [y, #16]
+                sbcs    b2, l, b2
+                sbcs    b3, h, b3
+                csetm   s, cc
+
+                eor     a0, a0, t
+                subs    a0, a0, t
+                eor     a1, a1, t
+                sbcs    a1, a1, t
+                eor     a2, a2, t
+                sbcs    a2, a2, t
+                eor     a3, a3, t
+                sbc     a3, a3, t
+
+                eor     b0, b0, s
+                subs    b0, b0, s
+                eor     b1, b1, s
+                sbcs    b1, b1, s
+                eor     b2, b2, s
+                sbcs    b2, b2, s
+                eor     b3, b3, s
+                sbc     b3, b3, s
+
+                eor     s, s, t
+
+// Now do yet a third 4x4 multiply to get mid-term product M
+
+                mul4
+
+// We now want, at the 256 position, 2^256 * HL + HL + (-1)^s * M
+// To keep things positive we use M' = p_521 - M in place of -M,
+// and this notion of negation just amounts to complementation in 521 bits.
+// Fold in the re-addition of the appropriately scaled lowest 4 words
+// The initial result is [s8; b3;b2;b1;b0; s7;s6;s5;s4;s3;s2;s1;s0]
+// Rebase it as a 9-word value at the 512 bit position using
+// [s8; b3;b2;b1;b0; s7;s6;s5;s4;s3;s2;s1;s0] ==
+// [s8; b3;b2;b1;b0; s7;s6;s5;s4] + 2^265 * [s3;s2;s1;s0] =
+// ([s8; b3;b2;b1;b0] + 2^9 * [s3;s2;s1;s0]); s7;s6;s5;s4]
+//
+// Accumulate as [s8; b3;b2;b1;b0; s7;s6;s5;s4] but leave out an additional
+// small c (s8 + suspended carry) to add at the 256 position here (512
+// overall). This can be added in the next block (to b0 = sum4).
+
+                ldp     a0, a1, [z]
+                ldp     a2, a3, [z, #16]
+
+                eor     s0, s0, s
+                adds    s0, s0, a0
+                eor     s1, s1, s
+                adcs    s1, s1, a1
+                eor     s2, s2, s
+                adcs    s2, s2, a2
+                eor     s3, s3, s
+                adcs    s3, s3, a3
+                eor     s4, s4, s
+
+                ldp     b0, b1, [z, #32]
+                ldp     b2, b3, [z, #48]
+                ldr     s8, [z, #64]
+
+                adcs    s4, s4, b0
+                eor     s5, s5, s
+                adcs    s5, s5, b1
+                eor     s6, s6, s
+                adcs    s6, s6, b2
+                eor     s7, s7, s
+                adcs    s7, s7, b3
+                adc     c, s8, xzr
+
+                adds    s4, s4, a0
+                adcs    s5, s5, a1
+                adcs    s6, s6, a2
+                adcs    s7, s7, a3
+                and     s, s, #0x1FF
+                lsl     t, s0, #9
+                orr     t, t, s
+                adcs    b0, b0, t
+                extr    t, s1, s0, #55
+                adcs    b1, b1, t
+                extr    t, s2, s1, #55
+                adcs    b2, b2, t
+                extr    t, s3, s2, #55
+                adcs    b3, b3, t
+                lsr     t, s3, #55
+                adc     s8, t, s8
+
+// Augment the total with the contribution from the top little words
+// w and v. If we write the inputs as 2^512 * w + x and 2^512 * v + y
+// then we are otherwise just doing x * y so we actually need to add
+// 2^512 * (2^512 * w * v + w * y + v * x). We do this is an involved
+// way chopping x and y into 52-bit chunks so we can do most of the core
+// arithmetic using only basic muls, no umulh (since w, v are only 9 bits).
+// This does however involve some intricate bit-splicing plus arithmetic.
+// To make things marginally less confusing we introduce some new names
+// at the human level: x = [c7;...;c0] and y = [d7;...d0], which are
+// not all distinct, and [sum8;sum7;...;sum0] for the running sum.
+// Also accumulate u = sum1 AND ... AND sum7 for the later comparison
+
+#define sum0 s4
+#define sum1 s5
+#define sum2 s6
+#define sum3 s7
+#define sum4 b0
+#define sum5 b1
+#define sum6 b2
+#define sum7 b3
+#define sum8 s8
+
+#define c0 a0
+#define c1 a1
+#define c2 a2
+#define c3 a0
+#define c4 a1
+#define c5 a2
+#define c6 a0
+#define c7 a1
+
+#define d0 s0
+#define d1 s1
+#define d2 s2
+#define d3 s0
+#define d4 s1
+#define d5 s2
+#define d6 s0
+#define d7 s1
+
+#define v a3
+#define w s3
+
+// 0 * 52 = 64 * 0 + 0
+
+                ldr     v, [y, #64]
+                ldp     c0, c1, [x]
+                and     l, c0, #0x000fffffffffffff
+                mul     l, v, l
+                ldr     w, [x, #64]
+                ldp     d0, d1, [y]
+                and     t, d0, #0x000fffffffffffff
+                mul     t, w, t
+                add     l, l, t
+
+// 1 * 52 = 64 * 0 + 52
+
+                extr    t, c1, c0, #52
+                and     t, t, #0x000fffffffffffff
+                mul     h, v, t
+                extr    t, d1, d0, #52
+                and     t, t, #0x000fffffffffffff
+                mul     t, w, t
+                add     h, h, t
+                lsr     t, l, #52
+                add     h, h, t
+
+                lsl     l, l, #12
+                extr    t, h, l, #12
+                adds    sum0, sum0, t
+
+// 2 * 52 = 64 * 1 + 40
+
+                ldp     c2, c3, [x, #16]
+                ldp     d2, d3, [y, #16]
+                extr    t, c2, c1, #40
+                and     t, t, #0x000fffffffffffff
+                mul     l, v, t
+                extr    t, d2, d1, #40
+                and     t, t, #0x000fffffffffffff
+                mul     t, w, t
+                add     l, l, t
+                lsr     t, h, #52
+                add     l, l, t
+
+                lsl     h, h, #12
+                extr    t, l, h, #24
+                adcs    sum1, sum1, t
+
+// 3 * 52 = 64 * 2 + 28
+
+                extr    t, c3, c2, #28
+                and     t, t, #0x000fffffffffffff
+                mul     h, v, t
+                extr    t, d3, d2, #28
+                and     t, t, #0x000fffffffffffff
+                mul     t, w, t
+                add     h, h, t
+                lsr     t, l, #52
+                add     h, h, t
+
+                lsl     l, l, #12
+                extr    t, h, l, #36
+                adcs    sum2, sum2, t
+                and     u, sum1, sum2
+
+// 4 * 52 = 64 * 3 + 16
+// At this point we also fold in the addition of c at the right place.
+// Note that 4 * 64 = 4 * 52 + 48 so we shift c left 48 places to align.
+
+                ldp     c4, c5, [x, #32]
+                ldp     d4, d5, [y, #32]
+                extr    t, c4, c3, #16
+                and     t, t, #0x000fffffffffffff
+                mul     l, v, t
+                extr    t, d4, d3, #16
+                and     t, t, #0x000fffffffffffff
+                mul     t, w, t
+                add     l, l, t
+
+                lsl     c, c, #48
+                add     l, l, c
+
+                lsr     t, h, #52
+                add     l, l, t
+
+                lsl     h, h, #12
+                extr    t, l, h, #48
+                adcs    sum3, sum3, t
+                and     u, u, sum3
+
+// 5 * 52 = 64 * 4 + 4
+
+                lsr     t, c4, #4
+                and     t, t, #0x000fffffffffffff
+                mul     h, v, t
+                lsr     t, d4, #4
+                and     t, t, #0x000fffffffffffff
+                mul     t, w, t
+                add     h, h, t
+
+                lsr     t, l, #52
+                add     h, h, t
+
+                lsl     l, l, #12
+                extr    s, h, l, #60
+
+// 6 * 52 = 64 * 4 + 56
+
+                extr    t, c5, c4, #56
+                and     t, t, #0x000fffffffffffff
+                mul     l, v, t
+                extr    t, d5, d4, #56
+                and     t, t, #0x000fffffffffffff
+                mul     t, w, t
+                add     l, l, t
+
+                lsr     t, h, #52
+                add     l, l, t
+
+                lsl     s, s, #8
+                extr    t, l, s, #8
+                adcs    sum4, sum4, t
+                and     u, u, sum4
+
+// 7 * 52 = 64 * 5 + 44
+
+                ldp     c6, c7, [x, #48]
+                ldp     d6, d7, [y, #48]
+                extr    t, c6, c5, #44
+                and     t, t, #0x000fffffffffffff
+                mul     h, v, t
+                extr    t, d6, d5, #44
+                and     t, t, #0x000fffffffffffff
+                mul     t, w, t
+                add     h, h, t
+
+                lsr     t, l, #52
+                add     h, h, t
+
+                lsl     l, l, #12
+                extr    t, h, l, #20
+                adcs    sum5, sum5, t
+                and     u, u, sum5
+
+// 8 * 52 = 64 * 6 + 32
+
+                extr    t, c7, c6, #32
+                and     t, t, #0x000fffffffffffff
+                mul     l, v, t
+                extr    t, d7, d6, #32
+                and     t, t, #0x000fffffffffffff
+                mul     t, w, t
+                add     l, l, t
+
+                lsr     t, h, #52
+                add     l, l, t
+
+                lsl     h, h, #12
+                extr    t, l, h, #32
+                adcs    sum6, sum6, t
+                and     u, u, sum6
+
+// 9 * 52 = 64 * 7 + 20
+
+                lsr     t, c7, #20
+                mul     h, v, t
+                lsr     t, d7, #20
+                mul     t, w, t
+                add     h, h, t
+
+                lsr     t, l, #52
+                add     h, h, t
+
+                lsl     l, l, #12
+                extr    t, h, l, #44
+                adcs    sum7, sum7, t
+                and     u, u, sum7
+
+// Top word
+
+                mul     t, v, w
+                lsr     h, h, #44
+                add     t, t, h
+                adc     sum8, sum8, t
+
+// Extract the high part h and mask off the low part l = [sum8;sum7;...;sum0]
+// but stuff sum8 with 1 bits at the left to ease a comparison below
+
+                lsr     h, sum8, #9
+                orr     sum8, sum8, #~0x1FF
+
+// Decide whether h + l >= p_521 <=> h + l + 1 >= 2^521. Since this can only
+// happen if digits sum7,...sum1 are all 1s, we use the AND of them "u" to
+// condense the carry chain, and since we stuffed 1 bits into sum8 we get
+// the result in CF without an additional comparison.
+
+                subs    xzr, xzr, xzr
+                adcs    xzr, sum0, h
+                adcs    xzr, u, xzr
+                adcs    xzr, sum8, xzr
+
+// Now if CF is set we want (h + l) - p_521 = (h + l + 1) - 2^521
+// while otherwise we want just h + l. So mask h + l + CF to 521 bits.
+// The masking is combined with the writeback in the next block.
+
+                adcs    sum0, sum0, h
+                adcs    sum1, sum1, xzr
+                adcs    sum2, sum2, xzr
+                adcs    sum3, sum3, xzr
+                adcs    sum4, sum4, xzr
+                adcs    sum5, sum5, xzr
+                adcs    sum6, sum6, xzr
+                adcs    sum7, sum7, xzr
+                adc     sum8, sum8, xzr
+
+// The result is actually [sum8;...;sum0] == product / 2^512, since we are
+// in the 512 position. For Montgomery we want product / 2^576, so write
+// back [sum8;...;sum0] rotated right by 64 bits, as a 521-bit quantity.
+
+                stp     sum1, sum2, [z]
+                stp     sum3, sum4, [z, #16]
+                stp     sum5, sum6, [z, #32]
+                lsl     h, sum0, #9
+                and     sum8, sum8, #0x1FF
+                orr     sum8, sum8, h
+                stp     sum7, sum8, [z, #48]
+                lsr     sum0, sum0, #55
+                str     sum0, [z, #64]
+
+// Restore regs and return
+
+                ldp     x25, x26, [sp], #16
+                ldp     x23, x24, [sp], #16
+                ldp     x21, x22, [sp], #16
+                ldp     x19, x20, [sp], #16
+                ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack,"",%progbits
+#endif
diff --git a/x86_att/p521/bignum_montmul_p521.S b/x86_att/p521/bignum_montmul_p521.S
new file mode 100644
index 000000000..05728824e
--- /dev/null
+++ b/x86_att/p521/bignum_montmul_p521.S
@@ -0,0 +1,406 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Montgomery multiply, z := (x * y / 2^576) mod p_521
+// Inputs x[9], y[9]; output z[9]
+//
+//    extern void bignum_montmul_p521
+//     (uint64_t z[static 9], uint64_t x[static 9], uint64_t y[static 9]);
+//
+// Does z := (x * y / 2^576) mod p_521, assuming x < p_521, y < p_521. This
+// means the Montgomery base is the "native size" 2^{9*64} = 2^576; since
+// p_521 is a Mersenne prime the basic modular multiplication bignum_mul_p521
+// can be considered a Montgomery operation to base 2^521.
+//
+// Standard x86-64 ABI: RDI = z, RSI = x, RDX = y
+// ----------------------------------------------------------------------------
+
+
+        .globl  bignum_montmul_p521
+        .text
+
+#define z %rdi
+#define x %rsi
+
+// Copied in
+
+#define y %rcx
+
+// mulpadd (high,low,x) adds rdx * x to a register-pair (high,low)
+// maintaining consistent double-carrying with adcx and adox,
+// using %rax and %rbx as temporaries.
+
+#define mulpadd(high,low,x)             \
+        mulxq   x, %rax, %rbx ;            \
+        adcxq   %rax, low ;               \
+        adoxq   %rbx, high
+
+bignum_montmul_p521:
+
+// Save more registers to play with
+
+        pushq   %rbp
+        pushq   %rbx
+        pushq   %r12
+        pushq   %r13
+        pushq   %r14
+        pushq   %r15
+
+// Copy y into a safe register to start with
+
+        movq    %rdx, y
+
+// Clone of the main body of bignum_8_16, writing back the low 8 words
+// to the output buffer z and keeping the top half in %r15,...,%r8
+
+        xorl   %ebp, %ebp
+        movq   (y), %rdx
+        mulxq  (x), %r8, %r9
+        movq   %r8, (z)
+        mulxq  0x8(x), %rbx, %r10
+        adcq   %rbx, %r9
+        mulxq  0x10(x), %rbx, %r11
+        adcq   %rbx, %r10
+        mulxq  0x18(x), %rbx, %r12
+        adcq   %rbx, %r11
+        mulxq  0x20(x), %rbx, %r13
+        adcq   %rbx, %r12
+        mulxq  0x28(x), %rbx, %r14
+        adcq   %rbx, %r13
+        mulxq  0x30(x), %rbx, %r15
+        adcq   %rbx, %r14
+        mulxq  0x38(x), %rbx, %r8
+        adcq   %rbx, %r15
+        adcq   %rbp, %r8
+        movq   0x8(y), %rdx
+        xorl   %ebp, %ebp
+        mulxq  (x), %rax, %rbx
+        adcxq  %rax, %r9
+        adoxq  %rbx, %r10
+        movq   %r9, 0x8(z)
+        mulxq  0x8(x), %rax, %rbx
+        adcxq  %rax, %r10
+        adoxq  %rbx, %r11
+        mulxq  0x10(x), %rax, %rbx
+        adcxq  %rax, %r11
+        adoxq  %rbx, %r12
+        mulxq  0x18(x), %rax, %rbx
+        adcxq  %rax, %r12
+        adoxq  %rbx, %r13
+        mulxq  0x20(x), %rax, %rbx
+        adcxq  %rax, %r13
+        adoxq  %rbx, %r14
+        mulxq  0x28(x), %rax, %rbx
+        adcxq  %rax, %r14
+        adoxq  %rbx, %r15
+        mulxq  0x30(x), %rax, %rbx
+        adcxq  %rax, %r15
+        adoxq  %rbx, %r8
+        mulxq  0x38(x), %rax, %r9
+        adcxq  %rax, %r8
+        adoxq  %rbp, %r9
+        adcq   %rbp, %r9
+        movq   0x10(y), %rdx
+        xorl   %ebp, %ebp
+        mulxq  (x), %rax, %rbx
+        adcxq  %rax, %r10
+        adoxq  %rbx, %r11
+        movq   %r10, 0x10(z)
+        mulxq  0x8(x), %rax, %rbx
+        adcxq  %rax, %r11
+        adoxq  %rbx, %r12
+        mulxq  0x10(x), %rax, %rbx
+        adcxq  %rax, %r12
+        adoxq  %rbx, %r13
+        mulxq  0x18(x), %rax, %rbx
+        adcxq  %rax, %r13
+        adoxq  %rbx, %r14
+        mulxq  0x20(x), %rax, %rbx
+        adcxq  %rax, %r14
+        adoxq  %rbx, %r15
+        mulxq  0x28(x), %rax, %rbx
+        adcxq  %rax, %r15
+        adoxq  %rbx, %r8
+        mulxq  0x30(x), %rax, %rbx
+        adcxq  %rax, %r8
+        adoxq  %rbx, %r9
+        mulxq  0x38(x), %rax, %r10
+        adcxq  %rax, %r9
+        adoxq  %rbp, %r10
+        adcq   %rbp, %r10
+        movq   0x18(y), %rdx
+        xorl   %ebp, %ebp
+        mulxq  (x), %rax, %rbx
+        adcxq  %rax, %r11
+        adoxq  %rbx, %r12
+        movq   %r11, 0x18(z)
+        mulxq  0x8(x), %rax, %rbx
+        adcxq  %rax, %r12
+        adoxq  %rbx, %r13
+        mulxq  0x10(x), %rax, %rbx
+        adcxq  %rax, %r13
+        adoxq  %rbx, %r14
+        mulxq  0x18(x), %rax, %rbx
+        adcxq  %rax, %r14
+        adoxq  %rbx, %r15
+        mulxq  0x20(x), %rax, %rbx
+        adcxq  %rax, %r15
+        adoxq  %rbx, %r8
+        mulxq  0x28(x), %rax, %rbx
+        adcxq  %rax, %r8
+        adoxq  %rbx, %r9
+        mulxq  0x30(x), %rax, %rbx
+        adcxq  %rax, %r9
+        adoxq  %rbx, %r10
+        mulxq  0x38(x), %rax, %r11
+        adcxq  %rax, %r10
+        adoxq  %rbp, %r11
+        adcq   %rbp, %r11
+        movq   0x20(y), %rdx
+        xorl   %ebp, %ebp
+        mulxq  (x), %rax, %rbx
+        adcxq  %rax, %r12
+        adoxq  %rbx, %r13
+        movq   %r12, 0x20(z)
+        mulxq  0x8(x), %rax, %rbx
+        adcxq  %rax, %r13
+        adoxq  %rbx, %r14
+        mulxq  0x10(x), %rax, %rbx
+        adcxq  %rax, %r14
+        adoxq  %rbx, %r15
+        mulxq  0x18(x), %rax, %rbx
+        adcxq  %rax, %r15
+        adoxq  %rbx, %r8
+        mulxq  0x20(x), %rax, %rbx
+        adcxq  %rax, %r8
+        adoxq  %rbx, %r9
+        mulxq  0x28(x), %rax, %rbx
+        adcxq  %rax, %r9
+        adoxq  %rbx, %r10
+        mulxq  0x30(x), %rax, %rbx
+        adcxq  %rax, %r10
+        adoxq  %rbx, %r11
+        mulxq  0x38(x), %rax, %r12
+        adcxq  %rax, %r11
+        adoxq  %rbp, %r12
+        adcq   %rbp, %r12
+        movq   0x28(y), %rdx
+        xorl   %ebp, %ebp
+        mulxq  (x), %rax, %rbx
+        adcxq  %rax, %r13
+        adoxq  %rbx, %r14
+        movq   %r13, 0x28(z)
+        mulxq  0x8(x), %rax, %rbx
+        adcxq  %rax, %r14
+        adoxq  %rbx, %r15
+        mulxq  0x10(x), %rax, %rbx
+        adcxq  %rax, %r15
+        adoxq  %rbx, %r8
+        mulxq  0x18(x), %rax, %rbx
+        adcxq  %rax, %r8
+        adoxq  %rbx, %r9
+        mulxq  0x20(x), %rax, %rbx
+        adcxq  %rax, %r9
+        adoxq  %rbx, %r10
+        mulxq  0x28(x), %rax, %rbx
+        adcxq  %rax, %r10
+        adoxq  %rbx, %r11
+        mulxq  0x30(x), %rax, %rbx
+        adcxq  %rax, %r11
+        adoxq  %rbx, %r12
+        mulxq  0x38(x), %rax, %r13
+        adcxq  %rax, %r12
+        adoxq  %rbp, %r13
+        adcq   %rbp, %r13
+        movq   0x30(y), %rdx
+        xorl   %ebp, %ebp
+        mulxq  (x), %rax, %rbx
+        adcxq  %rax, %r14
+        adoxq  %rbx, %r15
+        movq   %r14, 0x30(z)
+        mulxq  0x8(x), %rax, %rbx
+        adcxq  %rax, %r15
+        adoxq  %rbx, %r8
+        mulxq  0x10(x), %rax, %rbx
+        adcxq  %rax, %r8
+        adoxq  %rbx, %r9
+        mulxq  0x18(x), %rax, %rbx
+        adcxq  %rax, %r9
+        adoxq  %rbx, %r10
+        mulxq  0x20(x), %rax, %rbx
+        adcxq  %rax, %r10
+        adoxq  %rbx, %r11
+        mulxq  0x28(x), %rax, %rbx
+        adcxq  %rax, %r11
+        adoxq  %rbx, %r12
+        mulxq  0x30(x), %rax, %rbx
+        adcxq  %rax, %r12
+        adoxq  %rbx, %r13
+        mulxq  0x38(x), %rax, %r14
+        adcxq  %rax, %r13
+        adoxq  %rbp, %r14
+        adcq   %rbp, %r14
+        movq   0x38(y), %rdx
+        xorl   %ebp, %ebp
+        mulxq  (x), %rax, %rbx
+        adcxq  %rax, %r15
+        adoxq  %rbx, %r8
+        movq   %r15, 0x38(z)
+        mulxq  0x8(x), %rax, %rbx
+        adcxq  %rax, %r8
+        adoxq  %rbx, %r9
+        mulxq  0x10(x), %rax, %rbx
+        adcxq  %rax, %r9
+        adoxq  %rbx, %r10
+        mulxq  0x18(x), %rax, %rbx
+        adcxq  %rax, %r10
+        adoxq  %rbx, %r11
+        mulxq  0x20(x), %rax, %rbx
+        adcxq  %rax, %r11
+        adoxq  %rbx, %r12
+        mulxq  0x28(x), %rax, %rbx
+        adcxq  %rax, %r12
+        adoxq  %rbx, %r13
+        mulxq  0x30(x), %rax, %rbx
+        adcxq  %rax, %r13
+        adoxq  %rbx, %r14
+        mulxq  0x38(x), %rax, %r15
+        adcxq  %rax, %r14
+        adoxq  %rbp, %r15
+        adcq   %rbp, %r15
+
+// Accumulate x[8] * y[0..7], extending the window to %rbp,%r15,...,%r8
+
+        movq    64(x), %rdx
+        xorl    %ebp, %ebp
+        mulpadd(%r9,%r8,(y))
+        mulpadd(%r10,%r9,8(y))
+        mulpadd(%r11,%r10,16(y))
+        mulpadd(%r12,%r11,24(y))
+        mulpadd(%r13,%r12,32(y))
+        mulpadd(%r14,%r13,40(y))
+        mulpadd(%r15,%r14,48(y))
+        mulxq   56(y), %rax, %rbx
+        adcxq   %rax, %r15
+        adoxq   %rbp, %rbx
+        adcq    %rbx, %rbp
+
+// Accumulate y[8] * x[0..8] within this extended window %rbp,%r15,...,%r8
+
+        movq    64(y), %rdx
+        xorl    %eax, %eax
+        mulpadd(%r9,%r8,(x))
+        mulpadd(%r10,%r9,8(x))
+        mulpadd(%r11,%r10,16(x))
+        mulpadd(%r12,%r11,24(x))
+        mulpadd(%r13,%r12,32(x))
+        mulpadd(%r14,%r13,40(x))
+        mulpadd(%r15,%r14,48(x))
+        mulxq   56(x), %rax, %rbx
+        adcxq   %rax, %r15
+        adoxq   %rbx, %rbp
+        mulxq   64(x), %rax, %rbx
+        adcq    %rax, %rbp
+
+// Rotate the upper portion right 9 bits since 2^512 == 2^-9 (mod p_521)
+// Let rotated result %rbp,%r15,%r14,...,%r8 be h (high) and z[0..7] be l (low)
+
+        movq    %r8, %rax
+        andq    $0x1FF, %rax
+        shrdq   $9, %r9, %r8
+        shrdq   $9, %r10, %r9
+        shrdq   $9, %r11, %r10
+        shrdq   $9, %r12, %r11
+        shrdq   $9, %r13, %r12
+        shrdq   $9, %r14, %r13
+        shrdq   $9, %r15, %r14
+        shrdq   $9, %rbp, %r15
+        shrq    $9, %rbp
+        addq    %rax, %rbp
+
+// Force carry-in then add to get s = h + l + 1
+// but actually add all 1s in the top 53 bits to get simple carry out
+
+        stc
+        adcq    (z), %r8
+        adcq    8(z), %r9
+        adcq    16(z), %r10
+        adcq    24(z), %r11
+        adcq    32(z), %r12
+        adcq    40(z), %r13
+        adcq    48(z), %r14
+        adcq    56(z), %r15
+        adcq    $~0x1FF, %rbp
+
+// Now CF is set <=> h + l + 1 >= 2^521 <=> h + l >= p_521,
+// in which case the lower 521 bits are already right. Otherwise if
+// CF is clear, we want to subtract 1. Hence suntract the complement
+// of the carry flag then mask the top word, which scrubs the
+// padding in either case.
+
+        cmc
+        sbbq    $0, %r8
+        sbbq    $0, %r9
+        sbbq    $0, %r10
+        sbbq    $0, %r11
+        sbbq    $0, %r12
+        sbbq    $0, %r13
+        sbbq    $0, %r14
+        sbbq    $0, %r15
+        sbbq    $0, %rbp
+        andq    $0x1FF, %rbp
+
+// So far, this has been the same as a pure modular multiply.
+// Now finally the Montgomery ingredient, which is just a 521-bit
+// rotation by 9*64 - 521 = 55 bits right. Write digits back as
+// they are created.
+
+        movq    %r8, %rax
+        shrdq   $55, %r9, %r8
+        movq    %r8, (z)
+        shrdq   $55, %r10, %r9
+        movq    %r9, 8(z)
+        shrdq   $55, %r11, %r10
+        shlq    $9, %rax
+        movq    %r10, 16(z)
+        shrdq   $55, %r12, %r11
+        movq    %r11, 24(z)
+        shrdq   $55, %r13, %r12
+        movq    %r12, 32(z)
+        orq     %rax, %rbp
+        shrdq   $55, %r14, %r13
+        movq    %r13, 40(z)
+        shrdq   $55, %r15, %r14
+        movq    %r14, 48(z)
+        shrdq   $55, %rbp, %r15
+        movq    %r15, 56(z)
+        shrq    $55, %rbp
+        movq    %rbp, 64(z)
+
+// Restore registers and return
+
+        popq    %r15
+        popq    %r14
+        popq    %r13
+        popq    %r12
+        popq    %rbx
+        popq    %rbp
+
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack,"",%progbits
+#endif
