From 3b895dce68f180f4452e913edf659cfc6c97364e Mon Sep 17 00:00:00 2001
From: John Harrison <jargh@amazon.com>
Date: Mon, 24 Jan 2022 20:14:56 -0800
Subject: [PATCH] Add alternative variants of P-384 Montgomery operations

s2n-bignum original commit: https://github.com/awslabs/s2n-bignum/commit/0060c2ab5684333bc5bd2a85bc4afcde19ad805d
---
 arm/p384/Makefile                      |   2 +
 arm/p384/bignum_montmul_p384_alt.S     | 349 +++++++++++++++++++++++++
 arm/p384/bignum_montsqr_p384_alt.S     | 278 ++++++++++++++++++++
 x86_att/p384/bignum_montmul_p384_alt.S | 306 ++++++++++++++++++++++
 x86_att/p384/bignum_montsqr_p384_alt.S | 331 +++++++++++++++++++++++
 5 files changed, 1266 insertions(+)
 create mode 100644 arm/p384/bignum_montmul_p384_alt.S
 create mode 100644 arm/p384/bignum_montsqr_p384_alt.S
 create mode 100644 x86_att/p384/bignum_montmul_p384_alt.S
 create mode 100644 x86_att/p384/bignum_montsqr_p384_alt.S

diff --git a/arm/p384/Makefile b/arm/p384/Makefile
index f6e947c89..354898b28 100644
--- a/arm/p384/Makefile
+++ b/arm/p384/Makefile
@@ -44,7 +44,9 @@ OBJ = bignum_add_p384.o \
       bignum_mod_p384.o \
       bignum_mod_p384_6.o \
       bignum_montmul_p384.o \
+      bignum_montmul_p384_alt.o \
       bignum_montsqr_p384.o \
+      bignum_montsqr_p384_alt.o \
       bignum_mux_6.o \
       bignum_neg_p384.o \
       bignum_nonzero_6.o \
diff --git a/arm/p384/bignum_montmul_p384_alt.S b/arm/p384/bignum_montmul_p384_alt.S
new file mode 100644
index 000000000..78a3b7615
--- /dev/null
+++ b/arm/p384/bignum_montmul_p384_alt.S
@@ -0,0 +1,349 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Montgomery multiply, z := (x * y / 2^384) mod p_384
+// Inputs x[6], y[6]; output z[6]
+//
+//    extern void bignum_montmul_p384_alt
+//     (uint64_t z[static 6], uint64_t x[static 6], uint64_t y[static 6]);
+//
+// Does z := (2^{-384} * x * y) mod p_384, assuming that the inputs x and y
+// satisfy x * y <= 2^384 * p_384 (in particular this is true if we are in
+// the "usual" case x < p_384 and y < p_384).
+//
+// Standard ARM ABI: X0 = z, X1 = x, X2 = y
+// ----------------------------------------------------------------------------
+
+        .globl  bignum_montmul_p384_alt
+        .text
+        .balign 4
+
+// ---------------------------------------------------------------------------
+// Core one-step "short" Montgomery reduction macro. Takes input in
+// [d5;d4;d3;d2;d1;d0] and returns result in [d6;d5;d4;d3;d2;d1],
+// adding to the existing contents of [d5;d4;d3;d2;d1]. It is fine
+// for d6 to be the same register as d0.
+//
+// We want to add (2^384 - 2^128 - 2^96 + 2^32 - 1) * w
+// where w = [d0 + (d0<<32)] mod 2^64
+// ---------------------------------------------------------------------------
+
+#define montreds(d6,d5,d4,d3,d2,d1,d0, t3,t2,t1)                            \
+/* Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64            */  \
+/* Store it in d6 to make the 2^384 * w contribution already            */  \
+                lsl     t1, d0, #32;                                        \
+                add     d6, t1, d0;                                         \
+/* Now let [t3;t2;t1;-] = (2^384 - p_384) * w                    */         \
+/* We know the lowest word will cancel d0 so we don't need it    */         \
+                mov     t1, #0xffffffff00000001;                            \
+                umulh   t1, t1, d6;                                         \
+                mov     t2, #0x00000000ffffffff;                            \
+                mul     t3, t2, d6;                                         \
+                umulh   t2, t2, d6;                                         \
+                adds    t1, t1, t3;                                         \
+                adcs    t2, t2, d6;                                         \
+                adc     t3, xzr, xzr;                                       \
+/* Now add it, by subtracting from 2^384 * w + x */                         \
+                subs    d1, d1, t1;                                         \
+                sbcs    d2, d2, t2;                                         \
+                sbcs    d3, d3, t3;                                         \
+                sbcs    d4, d4, xzr;                                        \
+                sbcs    d5, d5, xzr;                                        \
+                sbc     d6, d6, xzr
+
+
+#define z x0
+#define x x1
+#define y x2
+
+// These are repeated mod 2 as we load pairs of inputs
+
+#define a0 x3
+#define a1 x4
+#define a2 x3
+#define a3 x4
+#define a4 x3
+#define a5 x4
+
+#define b0 x5
+#define b1 x6
+#define b2 x7
+#define b3 x8
+#define b4 x9
+#define b5 x10
+
+#define l x11
+
+#define u0 x12
+#define u1 x13
+#define u2 x14
+#define u3 x15
+#define u4 x16
+#define u5 x17
+#define u6 x19
+#define u7 x20
+#define u8 x21
+#define u9 x22
+#define u10 x2 // same as y
+#define u11 x1 // same as x
+#define h b5 // same as b5
+
+bignum_montmul_p384_alt:
+
+// Save more registers
+
+                stp     x19, x20, [sp, #-16]!
+                stp     x21, x22, [sp, #-16]!
+
+// Load operands and set up row 0 = [u6;...;u0] = a0 * [b5;...;b0]
+
+                ldp     a0, a1, [x]
+                ldp     b0, b1, [y]
+
+                mul     u0, a0, b0
+                umulh   u1, a0, b0
+                mul     l, a0, b1
+                umulh   u2, a0, b1
+                adds    u1, u1, l
+
+                ldp     b2, b3, [y, #16]
+
+                mul     l, a0, b2
+                umulh   u3, a0, b2
+                adcs    u2, u2, l
+
+                mul     l, a0, b3
+                umulh   u4, a0, b3
+                adcs    u3, u3, l
+
+                ldp     b4, b5, [y, #32]
+
+                mul     l, a0, b4
+                umulh   u5, a0, b4
+                adcs    u4, u4, l
+
+                mul     l, a0, b5
+                umulh   u6, a0, b5
+                adcs    u5, u5, l
+
+                adc     u6, u6, xzr
+
+// Row 1 = [u7;...;u0] = [a1;a0] * [b5;...;b0]
+
+                mul     l, a1, b0
+                adds    u1, u1, l
+                mul     l, a1, b1
+                adcs    u2, u2, l
+                mul     l, a1, b2
+                adcs    u3, u3, l
+                mul     l, a1, b3
+                adcs    u4, u4, l
+                mul     l, a1, b4
+                adcs    u5, u5, l
+                mul     l, a1, b5
+                adcs    u6, u6, l
+                cset    u7, cs
+
+                umulh   l, a1, b0
+                adds    u2, u2, l
+                umulh   l, a1, b1
+                adcs    u3, u3, l
+                umulh   l, a1, b2
+                adcs    u4, u4, l
+                umulh   l, a1, b3
+                adcs    u5, u5, l
+                umulh   l, a1, b4
+                adcs    u6, u6, l
+                umulh   l, a1, b5
+                adc     u7, u7, l
+
+// Row 2 = [u8;...;u0] = [a2;a1;a0] * [b5;...;b0]
+
+                ldp     a2, a3, [x, #16]
+
+                mul     l, a2, b0
+                adds    u2, u2, l
+                mul     l, a2, b1
+                adcs    u3, u3, l
+                mul     l, a2, b2
+                adcs    u4, u4, l
+                mul     l, a2, b3
+                adcs    u5, u5, l
+                mul     l, a2, b4
+                adcs    u6, u6, l
+                mul     l, a2, b5
+                adcs    u7, u7, l
+                cset    u8, cs
+
+                umulh   l, a2, b0
+                adds    u3, u3, l
+                umulh   l, a2, b1
+                adcs    u4, u4, l
+                umulh   l, a2, b2
+                adcs    u5, u5, l
+                umulh   l, a2, b3
+                adcs    u6, u6, l
+                umulh   l, a2, b4
+                adcs    u7, u7, l
+                umulh   l, a2, b5
+                adc     u8, u8, l
+
+// Row 3 = [u9;...;u0] = [a3;a2;a1;a0] * [b5;...;b0]
+
+                mul     l, a3, b0
+                adds    u3, u3, l
+                mul     l, a3, b1
+                adcs    u4, u4, l
+                mul     l, a3, b2
+                adcs    u5, u5, l
+                mul     l, a3, b3
+                adcs    u6, u6, l
+                mul     l, a3, b4
+                adcs    u7, u7, l
+                mul     l, a3, b5
+                adcs    u8, u8, l
+                cset    u9, cs
+
+                umulh   l, a3, b0
+                adds    u4, u4, l
+                umulh   l, a3, b1
+                adcs    u5, u5, l
+                umulh   l, a3, b2
+                adcs    u6, u6, l
+                umulh   l, a3, b3
+                adcs    u7, u7, l
+                umulh   l, a3, b4
+                adcs    u8, u8, l
+                umulh   l, a3, b5
+                adc     u9, u9, l
+
+// Row 4 = [u10;...;u0] = [a4;a3;a2;a1;a0] * [b5;...;b0]
+
+                ldp     a4, a5, [x, #32]
+
+                mul     l, a4, b0
+                adds    u4, u4, l
+                mul     l, a4, b1
+                adcs    u5, u5, l
+                mul     l, a4, b2
+                adcs    u6, u6, l
+                mul     l, a4, b3
+                adcs    u7, u7, l
+                mul     l, a4, b4
+                adcs    u8, u8, l
+                mul     l, a4, b5
+                adcs    u9, u9, l
+                cset    u10, cs
+
+                umulh   l, a4, b0
+                adds    u5, u5, l
+                umulh   l, a4, b1
+                adcs    u6, u6, l
+                umulh   l, a4, b2
+                adcs    u7, u7, l
+                umulh   l, a4, b3
+                adcs    u8, u8, l
+                umulh   l, a4, b4
+                adcs    u9, u9, l
+                umulh   l, a4, b5
+                adc     u10, u10, l
+
+// Row 5 = [u11;...;u0] = [a5;a4;a3;a2;a1;a0] * [b5;...;b0]
+
+                mul     l, a5, b0
+                adds    u5, u5, l
+                mul     l, a5, b1
+                adcs    u6, u6, l
+                mul     l, a5, b2
+                adcs    u7, u7, l
+                mul     l, a5, b3
+                adcs    u8, u8, l
+                mul     l, a5, b4
+                adcs    u9, u9, l
+                mul     l, a5, b5
+                adcs    u10, u10, l
+                cset    u11, cs
+
+                umulh   l, a5, b0
+                adds    u6, u6, l
+                umulh   l, a5, b1
+                adcs    u7, u7, l
+                umulh   l, a5, b2
+                adcs    u8, u8, l
+                umulh   l, a5, b3
+                adcs    u9, u9, l
+                umulh   l, a5, b4
+                adcs    u10, u10, l
+                umulh   l, a5, b5
+                adc     u11, u11, l
+
+// Montgomery rotate the low half
+
+                montreds(u0,u5,u4,u3,u2,u1,u0, b0,b1,b2)
+                montreds(u1,u0,u5,u4,u3,u2,u1, b0,b1,b2)
+                montreds(u2,u1,u0,u5,u4,u3,u2, b0,b1,b2)
+                montreds(u3,u2,u1,u0,u5,u4,u3, b0,b1,b2)
+                montreds(u4,u3,u2,u1,u0,u5,u4, b0,b1,b2)
+                montreds(u5,u4,u3,u2,u1,u0,u5, b0,b1,b2)
+
+// Add up the high and low parts as [h; u5;u4;u3;u2;u1;u0] = z
+
+                adds    u0, u0, u6
+                adcs    u1, u1, u7
+                adcs    u2, u2, u8
+                adcs    u3, u3, u9
+                adcs    u4, u4, u10
+                adcs    u5, u5, u11
+                adc     h, xzr, xzr
+
+// Now add [h; u11;u10;u9;u8;u7;u6] = z + (2^384 - p_384)
+
+                mov     l, #0xffffffff00000001
+                adds    u6, u0, l
+                mov     l, #0x00000000ffffffff
+                adcs    u7, u1, l
+                mov     l, #0x0000000000000001
+                adcs    u8, u2, l
+                adcs    u9, u3, xzr
+                adcs    u10, u4, xzr
+                adcs    u11, u5, xzr
+                adcs    h, h, xzr
+
+// Now z >= p_384 iff h is nonzero, so select accordingly
+
+                csel    u0, u0, u6, eq
+                csel    u1, u1, u7, eq
+                csel    u2, u2, u8, eq
+                csel    u3, u3, u9, eq
+                csel    u4, u4, u10, eq
+                csel    u5, u5, u11, eq
+
+// Store back final result
+
+                stp     u0, u1, [z]
+                stp     u2, u3, [z, #16]
+                stp     u4, u5, [z, #32]
+
+// Restore registers
+
+                ldp     x21, x22, [sp], #16
+                ldp     x19, x20, [sp], #16
+
+                ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack,"",%progbits
+#endif
diff --git a/arm/p384/bignum_montsqr_p384_alt.S b/arm/p384/bignum_montsqr_p384_alt.S
new file mode 100644
index 000000000..10aa8b1cb
--- /dev/null
+++ b/arm/p384/bignum_montsqr_p384_alt.S
@@ -0,0 +1,278 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Montgomery square, z := (x^2 / 2^384) mod p_384
+// Input x[6]; output z[6]
+//
+//    extern void bignum_montsqr_p384_alt
+//     (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// Does z := (x^2 / 2^384) mod p_384, assuming x^2 <= 2^384 * p_384, which is
+// guaranteed in particular if x < p_384 initially (the "intended" case).
+//
+// Standard ARM ABI: X0 = z, X1 = x
+// ----------------------------------------------------------------------------
+
+        .globl  bignum_montsqr_p384_alt
+        .text
+        .balign 4
+
+// ---------------------------------------------------------------------------
+// Core one-step "short" Montgomery reduction macro. Takes input in
+// [d5;d4;d3;d2;d1;d0] and returns result in [d6;d5;d4;d3;d2;d1],
+// adding to the existing contents of [d5;d4;d3;d2;d1]. It is fine
+// for d6 to be the same register as d0.
+//
+// We want to add (2^384 - 2^128 - 2^96 + 2^32 - 1) * w
+// where w = [d0 + (d0<<32)] mod 2^64
+// ---------------------------------------------------------------------------
+
+#define montreds(d6,d5,d4,d3,d2,d1,d0, t3,t2,t1)                            \
+/* Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64            */  \
+/* Store it in d6 to make the 2^384 * w contribution already            */  \
+                lsl     t1, d0, #32;                                        \
+                add     d6, t1, d0;                                         \
+/* Now let [t3;t2;t1;-] = (2^384 - p_384) * w                    */         \
+/* We know the lowest word will cancel d0 so we don't need it    */         \
+                mov     t1, #0xffffffff00000001;                            \
+                umulh   t1, t1, d6;                                         \
+                mov     t2, #0x00000000ffffffff;                            \
+                mul     t3, t2, d6;                                         \
+                umulh   t2, t2, d6;                                         \
+                adds    t1, t1, t3;                                         \
+                adcs    t2, t2, d6;                                         \
+                adc     t3, xzr, xzr;                                       \
+/* Now add it, by subtracting from 2^384 * w + x */                         \
+                subs    d1, d1, t1;                                         \
+                sbcs    d2, d2, t2;                                         \
+                sbcs    d3, d3, t3;                                         \
+                sbcs    d4, d4, xzr;                                        \
+                sbcs    d5, d5, xzr;                                        \
+                sbc     d6, d6, xzr
+
+#define z x0
+#define x x1
+
+#define a0 x2
+#define a1 x3
+#define a2 x4
+#define a3 x5
+#define a4 x6
+#define a5 x7
+
+#define l x8
+
+#define u0 x2 // The same as a0, which is safe
+#define u1 x9
+#define u2 x10
+#define u3 x11
+#define u4 x12
+#define u5 x13
+#define u6 x14
+#define u7 x15
+#define u8 x16
+#define u9 x17
+#define u10 x19
+#define u11 x20
+#define h x6 // same as a4
+
+bignum_montsqr_p384_alt:
+
+// It's convenient to have two more registers to play with
+
+                stp     x19, x20, [sp, #-16]!
+
+// Load all the elements as [a5;a4;a3;a2;a1;a0], set up an initial
+// window [u8;u7; u6;u5; u4;u3; u2;u1] = [34;05;03;01], and then
+// chain in the addition of 02 + 12 + 13 + 14 + 15 to that window
+// (no carry-out possible since we add it to the top of a product).
+
+                ldp     a0, a1, [x]
+
+                mul     u1, a0, a1
+                umulh   u2, a0, a1
+
+                ldp     a2, a3, [x, #16]
+
+                mul     l, a0, a2
+                adds    u2, u2, l
+
+                mul     u3, a0, a3
+                mul     l, a1, a2
+                adcs    u3, u3, l
+
+                umulh   u4, a0, a3
+                mul     l, a1, a3
+                adcs    u4, u4, l
+
+                ldp     a4, a5, [x, #32]
+
+                mul     u5, a0, a5
+                mul     l, a1, a4
+                adcs    u5, u5, l
+
+                umulh   u6, a0, a5
+                mul     l, a1, a5
+                adcs    u6, u6, l
+
+                mul     u7, a3, a4
+                adcs    u7, u7, xzr
+
+                umulh   u8, a3, a4
+                adc     u8, u8, xzr
+
+                umulh   l, a0, a2
+                adds    u3, u3, l
+                umulh   l, a1, a2
+                adcs    u4, u4, l
+                umulh   l, a1, a3
+                adcs    u5, u5, l
+                umulh   l, a1, a4
+                adcs    u6, u6, l
+                umulh   l, a1, a5
+                adcs    u7, u7, l
+                adc     u8, u8, xzr
+
+// Now chain in the 04 + 23 + 24 + 25 + 35 + 45 terms
+
+                mul     l, a0, a4
+                adds    u4, u4, l
+                mul     l, a2, a3
+                adcs    u5, u5, l
+                mul     l, a2, a4
+                adcs    u6, u6, l
+                mul     l, a2, a5
+                adcs    u7, u7, l
+                mul     l, a3, a5
+                adcs    u8, u8, l
+                mul     u9, a4, a5
+                adcs    u9, u9, xzr
+                umulh   u10, a4, a5
+                adc     u10, u10, xzr
+
+                umulh   l, a0, a4
+                adds    u5, u5, l
+                umulh   l, a2, a3
+                adcs    u6, u6, l
+                umulh   l, a2, a4
+                adcs    u7, u7, l
+                umulh   l, a2, a5
+                adcs    u8, u8, l
+                umulh   l, a3, a5
+                adcs    u9, u9, l
+                adc     u10, u10, xzr
+
+// Double that, with u11 holding the top carry
+
+                adds    u1, u1, u1
+                adcs    u2, u2, u2
+                adcs    u3, u3, u3
+                adcs    u4, u4, u4
+                adcs    u5, u5, u5
+                adcs    u6, u6, u6
+                adcs    u7, u7, u7
+                adcs    u8, u8, u8
+                adcs    u9, u9, u9
+                adcs    u10, u10, u10
+                cset    u11, cs
+
+// Add the homogeneous terms 00 + 11 + 22 + 33 + 44 + 55
+
+                umulh   l, a0, a0
+                mul     u0, a0, a0
+                adds    u1, u1, l
+
+                mul     l, a1, a1
+                adcs    u2, u2, l
+                umulh   l, a1, a1
+                adcs    u3, u3, l
+
+                mul     l, a2, a2
+                adcs    u4, u4, l
+                umulh   l, a2, a2
+                adcs    u5, u5, l
+
+                mul     l, a3, a3
+                adcs    u6, u6, l
+                umulh   l, a3, a3
+                adcs    u7, u7, l
+
+                mul     l, a4, a4
+                adcs    u8, u8, l
+                umulh   l, a4, a4
+                adcs    u9, u9, l
+
+                mul     l, a5, a5
+                adcs    u10, u10, l
+                umulh   l, a5, a5
+                adc     u11, u11, l
+
+// Montgomery rotate the low half
+
+                montreds(u0,u5,u4,u3,u2,u1,u0, a1,a2,a3)
+                montreds(u1,u0,u5,u4,u3,u2,u1, a1,a2,a3)
+                montreds(u2,u1,u0,u5,u4,u3,u2, a1,a2,a3)
+                montreds(u3,u2,u1,u0,u5,u4,u3, a1,a2,a3)
+                montreds(u4,u3,u2,u1,u0,u5,u4, a1,a2,a3)
+                montreds(u5,u4,u3,u2,u1,u0,u5, a1,a2,a3)
+
+// Add up the high and low parts as [h; u5;u4;u3;u2;u1;u0] = z
+
+                adds    u0, u0, u6
+                adcs    u1, u1, u7
+                adcs    u2, u2, u8
+                adcs    u3, u3, u9
+                adcs    u4, u4, u10
+                adcs    u5, u5, u11
+                adc     h, xzr, xzr
+
+// Now add [h; u11;u10;u9;u8;u7;u6] = z + (2^384 - p_384)
+
+                mov     l, #0xffffffff00000001
+                adds    u6, u0, l
+                mov     l, #0x00000000ffffffff
+                adcs    u7, u1, l
+                mov     l, #0x0000000000000001
+                adcs    u8, u2, l
+                adcs    u9, u3, xzr
+                adcs    u10, u4, xzr
+                adcs    u11, u5, xzr
+                adcs    h, h, xzr
+
+// Now z >= p_384 iff h is nonzero, so select accordingly
+
+                csel    u0, u0, u6, eq
+                csel    u1, u1, u7, eq
+                csel    u2, u2, u8, eq
+                csel    u3, u3, u9, eq
+                csel    u4, u4, u10, eq
+                csel    u5, u5, u11, eq
+
+// Store back final result
+
+                stp     u0, u1, [z]
+                stp     u2, u3, [z, #16]
+                stp     u4, u5, [z, #32]
+
+// Restore registers
+
+                ldp     x19, x20, [sp], #16
+
+                ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack,"",%progbits
+#endif
diff --git a/x86_att/p384/bignum_montmul_p384_alt.S b/x86_att/p384/bignum_montmul_p384_alt.S
new file mode 100644
index 000000000..7c8ae1053
--- /dev/null
+++ b/x86_att/p384/bignum_montmul_p384_alt.S
@@ -0,0 +1,306 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Montgomery multiply, z := (x * y / 2^384) mod p_384
+// Inputs x[6], y[6]; output z[6]
+//
+//    extern void bignum_montmul_p384_alt
+//     (uint64_t z[static 6], uint64_t x[static 6], uint64_t y[static 6]);
+//
+// Does z := (2^{-384} * x * y) mod p_384, assuming that the inputs x and y
+// satisfy x * y <= 2^384 * p_384 (in particular this is true if we are in
+// the "usual" case x < p_384 and y < p_384).
+//
+// Standard x86-64 ABI: RDI = z, RSI = x, RDX = y
+// -----------------------------------------------------------------------------
+
+
+        .globl  bignum_montmul_p384_alt
+        .text
+
+#define z %rdi
+#define x %rsi
+
+// We move the y argument here so we can use %rdx for multipliers
+
+#define y %rcx
+
+// Some temp registers for the last correction stage
+
+#define d %rax
+#define u %rdx
+#define v %rcx
+#define w %rbx
+
+// Add %rbx * m into a register-pair (high,low) maintaining consistent
+// carry-catching with carry (negated, as bitmask) and using %rax and %rdx
+// as temporaries
+
+#define mulpadd(carry,high,low,m)       \
+        movq    m, %rax ;                 \
+        mulq    %rbx;                    \
+        subq    carry, %rdx ;             \
+        addq    %rax, low ;               \
+        adcq    %rdx, high ;              \
+        sbbq    carry, carry
+
+// Initial version assuming no carry-in
+
+#define mulpadi(carry,high,low,m)       \
+        movq    m, %rax ;                 \
+        mulq    %rbx;                    \
+        addq    %rax, low ;               \
+        adcq    %rdx, high ;              \
+        sbbq    carry, carry
+
+// End version not catching the top carry-out
+
+#define mulpade(carry,high,low,m)       \
+        movq    m, %rax ;                 \
+        mulq    %rbx;                    \
+        subq    carry, %rdx ;             \
+        addq    %rax, low ;               \
+        adcq    %rdx, high
+
+// Core one-step Montgomery reduction macro. Takes input in
+// [d7;d6;d5;d4;d3;d2;d1;d0] and returns result in [d7;d6;d5;d4;d3;d2;d1],
+// adding to the existing contents, re-using d0 as a temporary internally
+//
+// We want to add (2^384 - 2^128 - 2^96 + 2^32 - 1) * w
+// where w = [d0 + (d0<<32)] mod 2^64
+//
+//       montredc(d7,d6,d5,d4,d3,d2,d1,d0)
+//
+// This particular variant, with its mix of addition and subtraction
+// at the top, is not intended to maintain a coherent carry or borrow out.
+// It is assumed the final result would fit in [d7;d6;d5;d4;d3;d2;d1].
+// which is always the case here as the top word is even always in {0,1}
+
+#define montredc(d7,d6,d5,d4,d3,d2,d1,d0)                               \
+/* Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64 */         \
+                movq    d0, %rbx ;                                        \
+                shlq    $32, %rbx ;                                        \
+                addq    d0, %rbx ;                                        \
+/* Construct [%rbp;%rdx;%rax;-] = (2^384 - p_384) * w */                   \
+/* We know the lowest word will cancel so we can re-use d0 as a temp */ \
+                xorl    %ebp, %ebp ;                                       \
+                movq    $0xffffffff00000001, %rax ;                        \
+                mulq    %rbx;                                            \
+                movq    %rdx, d0 ;                                        \
+                movq    $0x00000000ffffffff, %rax ;                        \
+                mulq    %rbx;                                            \
+                addq    d0, %rax ;                                        \
+                adcq    %rbx, %rdx ;                                       \
+                adcl    %ebp, %ebp ;                                       \
+/*  Now subtract that and add 2^384 * w */                              \
+                subq    %rax, d1 ;                                        \
+                sbbq    %rdx, d2 ;                                        \
+                sbbq    %rbp, d3 ;                                        \
+                sbbq    $0, d4 ;                                          \
+                sbbq    $0, d5 ;                                          \
+                sbbq    $0, %rbx ;                                         \
+                addq    %rbx, d6 ;                                        \
+                adcq    $0, d7
+
+bignum_montmul_p384_alt:
+
+// Save more registers to play with
+
+        pushq   %rbx
+        pushq   %rbp
+        pushq   %r12
+        pushq   %r13
+        pushq   %r14
+        pushq   %r15
+
+// Copy y into a safe register to start with
+
+        movq    %rdx, y
+
+// Do row 0 computation, which is a bit different:
+// set up initial window [%r14,%r13,%r12,%r11,%r10,%r9,%r8] = y[0] * x
+// Unlike later, we only need a single carry chain
+
+        movq    (y), %rbx
+        movq    (x), %rax
+        mulq    %rbx
+        movq    %rax, %r8
+        movq    %rdx, %r9
+
+        movq    8(x), %rax
+        mulq    %rbx
+        xorl    %r10d, %r10d
+        addq    %rax, %r9
+        adcq    %rdx, %r10
+
+        movq    16(x), %rax
+        mulq    %rbx
+        xorl    %r11d, %r11d
+        addq    %rax, %r10
+        adcq    %rdx, %r11
+
+        movq    24(x), %rax
+        mulq    %rbx
+        xorl    %r12d, %r12d
+        addq    %rax, %r11
+        adcq    %rdx, %r12
+
+        movq    32(x), %rax
+        mulq    %rbx
+        xorl    %r13d, %r13d
+        addq    %rax, %r12
+        adcq    %rdx, %r13
+
+        movq    40(x), %rax
+        mulq    %rbx
+        xorl    %r14d, %r14d
+        addq    %rax, %r13
+        adcq    %rdx, %r14
+
+        xorl    %r15d, %r15d
+
+// Montgomery reduce the zeroth window
+
+        montredc(%r15, %r14,%r13,%r12,%r11,%r10,%r9,%r8)
+
+// Add row 1
+
+        movq    8(y), %rbx
+        mulpadi(%r8,%r10,%r9,(x))
+        mulpadd(%r8,%r11,%r10,8(x))
+        mulpadd(%r8,%r12,%r11,16(x))
+        mulpadd(%r8,%r13,%r12,24(x))
+        mulpadd(%r8,%r14,%r13,32(x))
+        mulpadd(%r8,%r15,%r14,40(x))
+        negq    %r8
+
+// Montgomery reduce window 1
+
+        montredc(%r8, %r15,%r14,%r13,%r12,%r11,%r10,%r9)
+
+// Add row 2
+
+        movq    16(y), %rbx
+        mulpadi(%r9,%r11,%r10,(x))
+        mulpadd(%r9,%r12,%r11,8(x))
+        mulpadd(%r9,%r13,%r12,16(x))
+        mulpadd(%r9,%r14,%r13,24(x))
+        mulpadd(%r9,%r15,%r14,32(x))
+        mulpadd(%r9,%r8,%r15,40(x))
+        negq    %r9
+
+// Montgomery reduce window 2
+
+        montredc(%r9, %r8,%r15,%r14,%r13,%r12,%r11,%r10)
+
+// Add row 3
+
+        movq    24(y), %rbx
+        mulpadi(%r10,%r12,%r11,(x))
+        mulpadd(%r10,%r13,%r12,8(x))
+        mulpadd(%r10,%r14,%r13,16(x))
+        mulpadd(%r10,%r15,%r14,24(x))
+        mulpadd(%r10,%r8,%r15,32(x))
+        mulpadd(%r10,%r9,%r8,40(x))
+        negq    %r10
+
+// Montgomery reduce window 3
+
+        montredc(%r10, %r9,%r8,%r15,%r14,%r13,%r12,%r11)
+
+// Add row 4
+
+        movq    32(y), %rbx
+        mulpadi(%r11,%r13,%r12,(x))
+        mulpadd(%r11,%r14,%r13,8(x))
+        mulpadd(%r11,%r15,%r14,16(x))
+        mulpadd(%r11,%r8,%r15,24(x))
+        mulpadd(%r11,%r9,%r8,32(x))
+        mulpadd(%r11,%r10,%r9,40(x))
+        negq    %r11
+
+// Montgomery reduce window 4
+
+        montredc(%r11, %r10,%r9,%r8,%r15,%r14,%r13,%r12)
+
+// Add row 5
+
+        movq    40(y), %rbx
+        mulpadi(%r12,%r14,%r13,(x))
+        mulpadd(%r12,%r15,%r14,8(x))
+        mulpadd(%r12,%r8,%r15,16(x))
+        mulpadd(%r12,%r9,%r8,24(x))
+        mulpadd(%r12,%r10,%r9,32(x))
+        mulpadd(%r12,%r11,%r10,40(x))
+        negq    %r12
+
+// Montgomery reduce window 5
+
+        montredc(%r12, %r11,%r10,%r9,%r8,%r15,%r14,%r13)
+
+// We now have a pre-reduced 7-word form z = [%r12; %r11;%r10;%r9;%r8;%r15;%r14]
+// Next, accumulate in different registers z - p_384, or more precisely
+//
+//   [%r12; %r13;%rbp;%rdx;%rcx;%rbx;%rax] = z + (2^384 - p_384)
+
+        xorl    %edx, %edx
+        xorl    %ebp, %ebp
+        xorl    %r13d, %r13d
+
+        movq    $0xffffffff00000001, %rax
+        addq    %r14, %rax
+        movl    $0x00000000ffffffff, %ebx
+        adcq    %r15, %rbx
+        movl    $0x0000000000000001, %ecx
+        adcq    %r8, %rcx
+        adcq    %r9, %rdx
+        adcq    %r10, %rbp
+        adcq    %r11, %r13
+        adcq    $0, %r12
+
+// ~ZF <=> %r12 >= 1 <=> z + (2^384 - p_384) >= 2^384 <=> z >= p_384, which
+// determines whether to use the further reduced argument or the original z.
+
+        cmovnzq %rax, %r14
+        cmovnzq %rbx, %r15
+        cmovnzq %rcx, %r8
+        cmovnzq %rdx, %r9
+        cmovnzq %rbp, %r10
+        cmovnzq %r13, %r11
+
+// Write back the result
+
+        movq    %r14, (z)
+        movq    %r15, 8(z)
+        movq    %r8, 16(z)
+        movq    %r9, 24(z)
+        movq    %r10, 32(z)
+        movq    %r11, 40(z)
+
+// Restore registers and return
+
+        popq    %r15
+        popq    %r14
+        popq    %r13
+        popq    %r12
+        popq    %rbp
+        popq    %rbx
+
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack,"",%progbits
+#endif
diff --git a/x86_att/p384/bignum_montsqr_p384_alt.S b/x86_att/p384/bignum_montsqr_p384_alt.S
new file mode 100644
index 000000000..06d59f70e
--- /dev/null
+++ b/x86_att/p384/bignum_montsqr_p384_alt.S
@@ -0,0 +1,331 @@
+/*
+ * Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License").
+ * You may not use this file except in compliance with the License.
+ * A copy of the License is located at
+ *
+ *  http://aws.amazon.com/apache2.0
+ *
+ * or in the "LICENSE" file accompanying this file. This file is distributed
+ * on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either
+ * express or implied. See the License for the specific language governing
+ * permissions and limitations under the License.
+ */
+
+// ----------------------------------------------------------------------------
+// Montgomery square, z := (x^2 / 2^384) mod p_384
+// Input x[6]; output z[6]
+//
+//    extern void bignum_montsqr_p384_alt
+//     (uint64_t z[static 6], uint64_t x[static 6]);
+//
+// Does z := (x^2 / 2^384) mod p_384, assuming x^2 <= 2^384 * p_384, which is
+// guaranteed in particular if x < p_384 initially (the "intended" case).
+//
+// Standard x86-64 ABI: RDI = z, RSI = x
+// ----------------------------------------------------------------------------
+
+
+        .globl  bignum_montsqr_p384_alt
+        .text
+
+#define z %rdi
+#define x %rsi
+
+// Some temp registers for the last correction stage
+
+#define d %rax
+#define u %rdx
+#define v %r10
+#define w %r11
+
+// A zero register, very often
+
+#define zero %rbp
+#define zeroe %ebp
+
+// Add %rbx * m into a register-pair (high,low) maintaining consistent
+// carry-catching with carry (negated, as bitmask) and using %rax and %rdx
+// as temporaries
+
+#define mulpadd(carry,high,low,m)       \
+        movq    m, %rax ;                 \
+        mulq    %rbx;                    \
+        subq    carry, %rdx ;             \
+        addq    %rax, low ;               \
+        adcq    %rdx, high ;              \
+        sbbq    carry, carry
+
+// Initial version assuming no carry-in
+
+#define mulpadi(carry,high,low,m)       \
+        movq    m, %rax ;                 \
+        mulq    %rbx;                    \
+        addq    %rax, low ;               \
+        adcq    %rdx, high ;              \
+        sbbq    carry, carry
+
+// End version not catching the top carry-out
+
+#define mulpade(carry,high,low,m)       \
+        movq    m, %rax ;                 \
+        mulq    %rbx;                    \
+        subq    carry, %rdx ;             \
+        addq    %rax, low ;               \
+        adcq    %rdx, high
+
+// Core one-step "short" Montgomery reduction macro. Takes input in
+// [d5;d4;d3;d2;d1;d0] and returns result in [d6;d5;d4;d3;d2;d1],
+// adding to the existing [d5;d4;d3;d2;d1] and re-using d0 as a
+// temporary internally, as well as %rax, %rbx and %rdx.
+// It is OK for d6 and d0 to be the same register (they often are)
+//
+// We want to add (2^384 - 2^128 - 2^96 + 2^32 - 1) * w
+// where w = [d0 + (d0<<32)] mod 2^64
+//
+//       montreds(d6,d5,d4,d3,d2,d1,d0)
+
+#define montreds(d6,d5,d4,d3,d2,d1,d0)                                  \
+/* Our correction multiplier is w = [d0 + (d0<<32)] mod 2^64 */         \
+                movq    d0, %rbx ;                                        \
+                shlq    $32, %rbx ;                                        \
+                addq    d0, %rbx ;                                        \
+/* Construct [%rax;%rdx;d0;-] = (2^384 - p_384) * w            */         \
+/* We know the lowest word will cancel so we can re-use d0   */         \
+/* and %rbx as temps.                                         */         \
+                movq    $0xffffffff00000001, %rax ;                        \
+                mulq    %rbx;                                            \
+                movq    %rdx, d0 ;                                        \
+                movq    $0x00000000ffffffff, %rax ;                        \
+                mulq    %rbx;                                            \
+                addq    %rax, d0 ;                                        \
+                movl    $0, %eax ;                                         \
+                adcq    %rbx, %rdx ;                                       \
+                adcl    %eax, %eax ;                                       \
+/* Now subtract that and add 2^384 * w                       */         \
+                subq    d0, d1 ;                                         \
+                sbbq    %rdx, d2 ;                                        \
+                sbbq    %rax, d3 ;                                        \
+                sbbq    $0, d4 ;                                          \
+                sbbq    $0, d5 ;                                          \
+                movq    %rbx, d6 ;                                        \
+                sbbq    $0, d6
+
+bignum_montsqr_p384_alt:
+
+// Save more registers to play with
+
+        pushq   %rbx
+        pushq   %rbp
+        pushq   %r12
+        pushq   %r13
+        pushq   %r14
+        pushq   %r15
+
+// Set up an initial window [%rcx;%r15;...%r9] = [34;05;03;01]
+// Note that we are using %rcx as the first step past the rotating window
+
+        movq    (x), %rbx
+        movq    8(x), %rax
+        mulq    %rbx
+        movq    %rax, %r9
+        movq    %rdx, %r10
+
+        movq    24(x), %rax
+        mulq    %rbx
+        movq    %rax, %r11
+        movq    %rdx, %r12
+
+        movq    40(x), %rax
+        mulq    %rbx
+        movq    %rax, %r13
+        movq    %rdx, %r14
+
+        movq    24(x), %rax
+        mulq     32(x)
+        movq    %rax, %r15
+        movq    %rdx, %rcx
+
+// Chain in the addition of 02 + 12 + 13 + 14 + 15 to that window
+// (no carry-out possible)
+
+        movq    16(x), %rbx
+        mulpadi(%rbp,%r11,%r10,(x))
+        mulpadd(%rbp,%r12,%r11,8(x))
+        movq    8(x), %rbx
+        mulpadd(%rbp,%r13,%r12,24(x))
+        mulpadd(%rbp,%r14,%r13,32(x))
+        mulpade(%rbp,%r15,%r14,40(x))
+        adcq    $0, %rcx
+
+// Now chain in the 04 + 23 + 24 + 25 + 35 + 45 terms
+// We are running out of registers in our rotating window, so we start
+// using %rbx (and hence need care with using mulpadd after this). Thus
+// our result so far is in [%rbp;%rbx;%rcx;%r15;...%r9]
+
+        movq    32(x), %rbx
+        mulpadi(%rbp,%r13,%r12,(x))
+        movq    16(x), %rbx
+        mulpadd(%rbp,%r14,%r13,24(x))
+        mulpadd(%rbp,%r15,%r14,32(x))
+        mulpadd(%rbp,%rcx,%r15,40(x))
+
+        xorl    %ebx, %ebx
+        movq    24(x), %rax
+        mulq     40(x)
+        subq    %rbp, %rdx
+        xorl    %ebp, %ebp
+        addq    %rax, %rcx
+        adcq    %rdx, %rbx
+        adcl    %ebp, %ebp
+        movq    32(x), %rax
+        mulq     40(x)
+        addq    %rax, %rbx
+        adcq    %rdx, %rbp
+
+// Double the window as [%r8;%rbp;%rbx;%rcx;%r15;...%r9]
+
+        xorl    %r8d, %r8d
+        addq    %r9, %r9
+        adcq    %r10, %r10
+        adcq    %r11, %r11
+        adcq    %r12, %r12
+        adcq    %r13, %r13
+        adcq    %r14, %r14
+        adcq    %r15, %r15
+        adcq    %rcx, %rcx
+        adcq    %rbx, %rbx
+        adcq    %rbp, %rbp
+        adcl    %r8d, %r8d
+
+// Add the doubled window to the 00 + 11 + 22 + 33 + 44 + 55 terms
+// For one glorious moment the entire squaring result is all in the
+// register file as [%rsi;%rbp;%rbx;%rcx;%r15;...;%r8]
+// (since we've now finished with x we can re-use %rsi). But since
+// we are so close to running out of registers, we do a bit of
+// reshuffling and temporary storage in the output buffer.
+
+        movq    (x), %rax
+        mulq    %rax
+        movq    %r8, (z)
+        movq    %rax, %r8
+        movq    8(x), %rax
+        movq    %rbp, 8(z)
+        addq    %rdx, %r9
+        sbbq    %rbp, %rbp
+
+        mulq    %rax
+        negq    %rbp
+        adcq    %rax, %r10
+        adcq    %rdx, %r11
+        sbbq    %rbp, %rbp
+
+        movq    16(x), %rax
+        mulq    %rax
+        negq    %rbp
+        adcq    %rax, %r12
+        adcq    %rdx, %r13
+        sbbq    %rbp, %rbp
+
+        movq    24(x), %rax
+        mulq    %rax
+        negq    %rbp
+        adcq    %rax, %r14
+        adcq    %rdx, %r15
+        sbbq    %rbp, %rbp
+
+        movq    32(x), %rax
+        mulq    %rax
+        negq    %rbp
+        adcq    %rax, %rcx
+        adcq    %rdx, %rbx
+        sbbq    %rbp, %rbp
+
+        movq    40(x), %rax
+        mulq    %rax
+        negq    %rbp
+        adcq    8(z), %rax
+        adcq    (z), %rdx
+        movq    %rax, %rbp
+        movq    %rdx, %rsi
+
+// We need just *one* more register as a temp for the Montgomery steps.
+// Since we are writing to the z buffer anyway, make use of that again
+// to stash %rbx.
+
+        movq    %rbx, (z)
+
+// Montgomery reduce the %r13,...,%r8 window 6 times
+
+        montreds(%r8,%r13,%r12,%r11,%r10,%r9,%r8)
+        montreds(%r9,%r8,%r13,%r12,%r11,%r10,%r9)
+        montreds(%r10,%r9,%r8,%r13,%r12,%r11,%r10)
+        montreds(%r11,%r10,%r9,%r8,%r13,%r12,%r11)
+        montreds(%r12,%r11,%r10,%r9,%r8,%r13,%r12)
+        montreds(%r13,%r12,%r11,%r10,%r9,%r8,%r13)
+
+// Now we can safely restore %rbx before accumulating
+
+        movq    (z), %rbx
+
+        addq    %r8, %r14
+        adcq    %r9, %r15
+        adcq    %r10, %rcx
+        adcq    %r11, %rbx
+        adcq    %r12, %rbp
+        adcq    %r13, %rsi
+        movl    $0, %r8d
+        adcq    %r8, %r8
+
+// We now have a pre-reduced 7-word form z = [%r8; %rsi;%rbp;%rbx;%rcx;%r15;%r14]
+// Next, accumulate in different registers z - p_384, or more precisely
+//
+//   [%r8; %r13;%r12;%r11;%r10;%r9;%rax] = z + (2^384 - p_384)
+
+        xorq    %r11, %r11
+        xorq    %r12, %r12
+        xorq    %r13, %r13
+        movq    $0xffffffff00000001, %rax
+        addq    %r14, %rax
+        movl    $0x00000000ffffffff, %r9d
+        adcq    %r15, %r9
+        movl    $0x0000000000000001, %r10d
+        adcq    %rcx, %r10
+        adcq    %rbx, %r11
+        adcq    %rbp, %r12
+        adcq    %rsi, %r13
+        adcq    $0, %r8
+
+// ~ZF <=> %r12 >= 1 <=> z + (2^384 - p_384) >= 2^384 <=> z >= p_384, which
+// determines whether to use the further reduced argument or the original z.
+
+        cmovnzq %rax, %r14
+        cmovnzq %r9, %r15
+        cmovnzq %r10, %rcx
+        cmovnzq %r11, %rbx
+        cmovnzq %r12, %rbp
+        cmovnzq %r13, %rsi
+
+// Write back the result
+
+        movq    %r14, (z)
+        movq    %r15, 8(z)
+        movq    %rcx, 16(z)
+        movq    %rbx, 24(z)
+        movq    %rbp, 32(z)
+        movq    %rsi, 40(z)
+
+// Restore registers and return
+
+        popq    %r15
+        popq    %r14
+        popq    %r13
+        popq    %r12
+        popq    %rbp
+        popq    %rbx
+        ret
+
+#if defined(__linux__) && defined(__ELF__)
+.section .note.GNU-stack,"",%progbits
+#endif
