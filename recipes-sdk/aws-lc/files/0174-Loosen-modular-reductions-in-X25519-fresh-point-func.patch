From ab27daea58895396bd99239bb582247c4f524046 Mon Sep 17 00:00:00 2001
From: John Harrison <jargh@amazon.com>
Date: Mon, 6 Mar 2023 19:40:53 -0800
Subject: [PATCH] Loosen modular reductions in X25519 fresh-point functions

The main loop now just maintains coordinates modulo 2^256-38,
only fully reducing modulo 2^255-19 right at the end, and
also special-cases four of the iterations to pure doublings
because of the initial mangling of the scalar.

s2n-bignum original commit: https://github.com/awslabs/s2n-bignum/commit/be3a935063b45cf0450bf00bc4bf18a78c3dc728
---
 arm/curve25519/curve25519_x25519.S         | 487 ++++++++++++---------
 arm/curve25519/curve25519_x25519_alt.S     | 398 +++++++++--------
 x86_att/curve25519/curve25519_x25519.S     | 351 ++++++++-------
 x86_att/curve25519/curve25519_x25519_alt.S | 327 +++++++-------
 4 files changed, 852 insertions(+), 711 deletions(-)

diff --git a/arm/curve25519/curve25519_x25519.S b/arm/curve25519/curve25519_x25519.S
index 02b5b5128..27e41644d 100644
--- a/arm/curve25519/curve25519_x25519.S
+++ b/arm/curve25519/curve25519_x25519.S
@@ -75,10 +75,8 @@
 
 #define NSPACE (12*NUMSIZE)
 
-// Macros wrapping up the basic field operation calls
-// bignum_mul_p25519 and bignum_sqr_p25519.
-// These two are only trivially different from pure
-// function calls to those subroutines.
+// Macro wrapping up the basic field operation bignum_mul_p25519, only
+// trivially different from a pure function call to that subroutine.
 
 #define mul_p25519(p0,p1,p2)                    \
         ldp     x3, x4, [p1];                   \
@@ -243,121 +241,165 @@
         stp     x7, x8, [p0];                   \
         stp     x9, x10, [p0+16]
 
-#define sqr_p25519(p0,p1)                       \
-        ldp     x6, x7, [p1];                   \
-        ldp     x10, x11, [p1+16];              \
-        mul     x4, x6, x10;                    \
-        mul     x9, x7, x11;                    \
-        umulh   x12, x6, x10;                   \
-        subs    x13, x6, x7;                    \
-        cneg    x13, x13, cc;                   \
-        csetm   x3, cc;                         \
-        subs    x2, x11, x10;                   \
-        cneg    x2, x2, cc;                     \
-        mul     x8, x13, x2;                    \
-        umulh   x2, x13, x2;                    \
-        cinv    x3, x3, cc;                     \
-        eor     x8, x8, x3;                     \
-        eor     x2, x2, x3;                     \
-        adds    x5, x4, x12;                    \
-        adc     x12, x12, xzr;                  \
-        umulh   x13, x7, x11;                   \
-        adds    x5, x5, x9;                     \
-        adcs    x12, x12, x13;                  \
-        adc     x13, x13, xzr;                  \
-        adds    x12, x12, x9;                   \
-        adc     x13, x13, xzr;                  \
-        cmn     x3, #0x1;                       \
-        adcs    x5, x5, x8;                     \
-        adcs    x12, x12, x2;                   \
-        adc     x13, x13, x3;                   \
-        adds    x4, x4, x4;                     \
-        adcs    x5, x5, x5;                     \
-        adcs    x12, x12, x12;                  \
-        adcs    x13, x13, x13;                  \
-        adc     x14, xzr, xzr;                  \
-        mul     x2, x6, x6;                     \
-        mul     x8, x7, x7;                     \
-        mul     x15, x6, x7;                    \
-        umulh   x3, x6, x6;                     \
-        umulh   x9, x7, x7;                     \
-        umulh   x16, x6, x7;                    \
-        adds    x3, x3, x15;                    \
-        adcs    x8, x8, x16;                    \
-        adc     x9, x9, xzr;                    \
-        adds    x3, x3, x15;                    \
-        adcs    x8, x8, x16;                    \
-        adc     x9, x9, xzr;                    \
-        adds    x4, x4, x8;                     \
-        adcs    x5, x5, x9;                     \
-        adcs    x12, x12, xzr;                  \
+// A version of multiplication that only guarantees output < 2 * p_25519.
+// This basically skips the +1 and final correction in quotient estimation.
+
+#define mul_4(P0,P1,P2)                         \
+        ldp     x3, x4, [P1];                   \
+        ldp     x5, x6, [P2];                   \
+        mul     x7, x3, x5;                     \
+        umulh   x8, x3, x5;                     \
+        mul     x9, x4, x6;                     \
+        umulh   x10, x4, x6;                    \
+        subs    x4, x4, x3;                     \
+        cneg    x4, x4, cc;                     \
+        csetm   x16, cc;                        \
+        adds    x9, x9, x8;                     \
+        adc     x10, x10, xzr;                  \
+        subs    x3, x5, x6;                     \
+        cneg    x3, x3, cc;                     \
+        cinv    x16, x16, cc;                   \
+        mul     x15, x4, x3;                    \
+        umulh   x3, x4, x3;                     \
+        adds    x8, x7, x9;                     \
+        adcs    x9, x9, x10;                    \
+        adc     x10, x10, xzr;                  \
+        cmn     x16, #0x1;                      \
+        eor     x15, x15, x16;                  \
+        adcs    x8, x15, x8;                    \
+        eor     x3, x3, x16;                    \
+        adcs    x9, x3, x9;                     \
+        adc     x10, x10, x16;                  \
+        ldp     x3, x4, [P1+16];                \
+        ldp     x5, x6, [P2+16];                \
+        mul     x11, x3, x5;                    \
+        umulh   x12, x3, x5;                    \
+        mul     x13, x4, x6;                    \
+        umulh   x14, x4, x6;                    \
+        subs    x4, x4, x3;                     \
+        cneg    x4, x4, cc;                     \
+        csetm   x16, cc;                        \
+        adds    x13, x13, x12;                  \
+        adc     x14, x14, xzr;                  \
+        subs    x3, x5, x6;                     \
+        cneg    x3, x3, cc;                     \
+        cinv    x16, x16, cc;                   \
+        mul     x15, x4, x3;                    \
+        umulh   x3, x4, x3;                     \
+        adds    x12, x11, x13;                  \
+        adcs    x13, x13, x14;                  \
+        adc     x14, x14, xzr;                  \
+        cmn     x16, #0x1;                      \
+        eor     x15, x15, x16;                  \
+        adcs    x12, x15, x12;                  \
+        eor     x3, x3, x16;                    \
+        adcs    x13, x3, x13;                   \
+        adc     x14, x14, x16;                  \
+        ldp     x3, x4, [P1+16];                \
+        ldp     x15, x16, [P1];                 \
+        subs    x3, x3, x15;                    \
+        sbcs    x4, x4, x16;                    \
+        csetm   x16, cc;                        \
+        ldp     x15, x0, [P2];                  \
+        subs    x5, x15, x5;                    \
+        sbcs    x6, x0, x6;                     \
+        csetm   x0, cc;                         \
+        eor     x3, x3, x16;                    \
+        subs    x3, x3, x16;                    \
+        eor     x4, x4, x16;                    \
+        sbc     x4, x4, x16;                    \
+        eor     x5, x5, x0;                     \
+        subs    x5, x5, x0;                     \
+        eor     x6, x6, x0;                     \
+        sbc     x6, x6, x0;                     \
+        eor     x16, x0, x16;                   \
+        adds    x11, x11, x9;                   \
+        adcs    x12, x12, x10;                  \
         adcs    x13, x13, xzr;                  \
         adc     x14, x14, xzr;                  \
-        mul     x6, x10, x10;                   \
-        mul     x8, x11, x11;                   \
-        mul     x15, x10, x11;                  \
-        umulh   x7, x10, x10;                   \
-        umulh   x9, x11, x11;                   \
-        umulh   x16, x10, x11;                  \
-        adds    x7, x7, x15;                    \
-        adcs    x8, x8, x16;                    \
-        adc     x9, x9, xzr;                    \
-        adds    x7, x7, x15;                    \
-        adcs    x8, x8, x16;                    \
-        adc     x9, x9, xzr;                    \
-        adds    x6, x6, x12;                    \
-        adcs    x7, x7, x13;                    \
-        adcs    x8, x8, x14;                    \
-        adc     x9, x9, xzr;                    \
-        mov     x10, #0x26;                     \
-        and     x11, x6, #0xffffffff;           \
-        lsr     x12, x6, #32;                   \
-        mul     x11, x10, x11;                  \
-        mul     x12, x10, x12;                  \
-        adds    x2, x2, x11;                    \
-        and     x11, x7, #0xffffffff;           \
-        lsr     x7, x7, #32;                    \
-        mul     x11, x10, x11;                  \
-        mul     x7, x10, x7;                    \
-        adcs    x3, x3, x11;                    \
-        and     x11, x8, #0xffffffff;           \
-        lsr     x8, x8, #32;                    \
-        mul     x11, x10, x11;                  \
-        mul     x8, x10, x8;                    \
-        adcs    x4, x4, x11;                    \
-        and     x11, x9, #0xffffffff;           \
-        lsr     x9, x9, #32;                    \
-        mul     x11, x10, x11;                  \
-        mul     x9, x10, x9;                    \
-        adcs    x5, x5, x11;                    \
-        cset    x6, cs;                         \
-        lsl     x11, x12, #32;                  \
-        adds    x2, x2, x11;                    \
-        extr    x11, x7, x12, #32;              \
-        adcs    x3, x3, x11;                    \
-        extr    x11, x8, x7, #32;               \
-        adcs    x4, x4, x11;                    \
-        extr    x11, x9, x8, #32;               \
-        adcs    x5, x5, x11;                    \
-        lsr     x11, x9, #32;                   \
-        adc     x6, x6, x11;                    \
-        cmn     x5, x5;                         \
-        orr     x5, x5, #0x8000000000000000;    \
-        adc     x13, x6, x6;                    \
-        mov     x10, #0x13;                     \
-        madd    x11, x10, x13, x10;             \
-        adds    x2, x2, x11;                    \
-        adcs    x3, x3, xzr;                    \
-        adcs    x4, x4, xzr;                    \
-        adcs    x5, x5, xzr;                    \
-        csel    x10, x10, xzr, cc;              \
-        subs    x2, x2, x10;                    \
-        sbcs    x3, x3, xzr;                    \
-        sbcs    x4, x4, xzr;                    \
-        sbc     x5, x5, xzr;                    \
-        and     x5, x5, #0x7fffffffffffffff;    \
-        stp     x2, x3, [p0];                   \
-        stp     x4, x5, [p0+16]
+        mul     x2, x3, x5;                     \
+        umulh   x0, x3, x5;                     \
+        mul     x15, x4, x6;                    \
+        umulh   x1, x4, x6;                     \
+        subs    x4, x4, x3;                     \
+        cneg    x4, x4, cc;                     \
+        csetm   x9, cc;                         \
+        adds    x15, x15, x0;                   \
+        adc     x1, x1, xzr;                    \
+        subs    x6, x5, x6;                     \
+        cneg    x6, x6, cc;                     \
+        cinv    x9, x9, cc;                     \
+        mul     x5, x4, x6;                     \
+        umulh   x6, x4, x6;                     \
+        adds    x0, x2, x15;                    \
+        adcs    x15, x15, x1;                   \
+        adc     x1, x1, xzr;                    \
+        cmn     x9, #0x1;                       \
+        eor     x5, x5, x9;                     \
+        adcs    x0, x5, x0;                     \
+        eor     x6, x6, x9;                     \
+        adcs    x15, x6, x15;                   \
+        adc     x1, x1, x9;                     \
+        adds    x9, x11, x7;                    \
+        adcs    x10, x12, x8;                   \
+        adcs    x11, x13, x11;                  \
+        adcs    x12, x14, x12;                  \
+        adcs    x13, x13, xzr;                  \
+        adc     x14, x14, xzr;                  \
+        cmn     x16, #0x1;                      \
+        eor     x2, x2, x16;                    \
+        adcs    x9, x2, x9;                     \
+        eor     x0, x0, x16;                    \
+        adcs    x10, x0, x10;                   \
+        eor     x15, x15, x16;                  \
+        adcs    x11, x15, x11;                  \
+        eor     x1, x1, x16;                    \
+        adcs    x12, x1, x12;                   \
+        adcs    x13, x13, x16;                  \
+        adc     x14, x14, x16;                  \
+        mov     x3, #0x26;                      \
+        and     x5, x11, #0xffffffff;           \
+        lsr     x4, x11, #32;                   \
+        mul     x5, x3, x5;                     \
+        mul     x4, x3, x4;                     \
+        adds    x7, x7, x5;                     \
+        and     x5, x12, #0xffffffff;           \
+        lsr     x12, x12, #32;                  \
+        mul     x5, x3, x5;                     \
+        mul     x12, x3, x12;                   \
+        adcs    x8, x8, x5;                     \
+        and     x5, x13, #0xffffffff;           \
+        lsr     x13, x13, #32;                  \
+        mul     x5, x3, x5;                     \
+        mul     x13, x3, x13;                   \
+        adcs    x9, x9, x5;                     \
+        and     x5, x14, #0xffffffff;           \
+        lsr     x14, x14, #32;                  \
+        mul     x5, x3, x5;                     \
+        mul     x14, x3, x14;                   \
+        adcs    x10, x10, x5;                   \
+        cset    x11, cs;                        \
+        lsl     x5, x4, #32;                    \
+        adds    x7, x7, x5;                     \
+        extr    x5, x12, x4, #32;               \
+        adcs    x8, x8, x5;                     \
+        extr    x5, x13, x12, #32;              \
+        adcs    x9, x9, x5;                     \
+        extr    x5, x14, x13, #32;              \
+        adcs    x10, x10, x5;                   \
+        lsr     x5, x14, #32;                   \
+        adc     x11, x11, x5;                   \
+        cmn     x10, x10;                       \
+        bic     x10, x10, #0x8000000000000000;  \
+        adc     x0, x11, x11;                   \
+        mov     x3, #19;                        \
+        mul     x5, x3, x0;                     \
+        adds    x7, x7, x5;                     \
+        adcs    x8, x8, xzr;                    \
+        adcs    x9, x9, xzr;                    \
+        adc     x10, x10, xzr;                  \
+        stp     x7, x8, [P0];                   \
+        stp     x9, x10, [P0+16]
 
 // Multiplication just giving a 5-digit result (actually < 39 * 2^256)
 // by not doing anything beyond the first stage of reduction
@@ -625,21 +667,6 @@
         stp     x2, x3, [p0];                   \
         stp     x4, x5, [p0+16]
 
-// Plain 4-digit add without any normalization
-// With inputs < p_25519 (indeed < 2^255) it still gives a 4-digit result
-
-#define add_4(p0,p1,p2)                         \
-        ldp     x0, x1, [p1];                   \
-        ldp     x4, x5, [p2];                   \
-        adds    x0, x0, x4;                     \
-        adcs    x1, x1, x5;                     \
-        ldp     x2, x3, [p1+16];                \
-        ldp     x6, x7, [p2+16];                \
-        adcs    x2, x2, x6;                     \
-        adc     x3, x3, x7;                     \
-        stp     x0, x1, [p0];                   \
-        stp     x2, x3, [p0+16]
-
 // Add 5-digit inputs and normalize to 4 digits
 
 #define add5_4(p0,p1,p2)                        \
@@ -666,28 +693,29 @@
         stp     x0, x1, [p0];                   \
         stp     x2, x3, [p0+16]
 
-// Subtraction of a pair of numbers < p_25519 just sufficient
-// to give a 4-digit result. It actually always does (x - z) + (2^255-19)
-// which in turn is done by (x - z) - (2^255+19) discarding the 2^256
-// implicitly
+// Modular addition and doubling with double modulus 2 * p_25519 = 2^256 - 38.
+// This only ensures that the result fits in 4 digits, not that it is reduced
+// even w.r.t. double modulus. The result is always correct modulo provided
+// the sum of the inputs is < 2^256 + 2^256 - 38, so in particular provided
+// at least one of them is reduced double modulo.
 
-#define sub_4(p0,p1,p2)                         \
-        ldp     x5, x6, [p1];                   \
-        ldp     x4, x3, [p2];                   \
-        subs    x5, x5, x4;                     \
-        sbcs    x6, x6, x3;                     \
-        ldp     x7, x8, [p1+16];                \
-        ldp     x4, x3, [p2+16];                \
-        sbcs    x7, x7, x4;                     \
-        sbcs    x8, x8, x3;                     \
-        mov     x3, #19;                        \
-        subs    x5, x5, x3;                     \
-        sbcs    x6, x6, xzr;                    \
-        sbcs    x7, x7, xzr;                    \
-        mov     x4, #0x8000000000000000;        \
-        sbc     x8, x8, x4;                     \
-        stp     x5, x6, [p0];                   \
-        stp     x7, x8, [p0+16]
+#define add_twice4(P0,P1,P2)                    \
+        ldp     x3, x4, [P1];                   \
+        ldp     x7, x8, [P2];                   \
+        adds    x3, x3, x7;                     \
+        adcs    x4, x4, x8;                     \
+        ldp     x5, x6, [P1+16];                \
+        ldp     x7, x8, [P2+16];                \
+        adcs    x5, x5, x7;                     \
+        adcs    x6, x6, x8;                     \
+        mov     x9, #38;                        \
+        csel    x9, x9, xzr, cs;                \
+        adds    x3, x3, x9;                     \
+        adcs    x4, x4, xzr;                    \
+        adcs    x5, x5, xzr;                    \
+        adc     x6, x6, xzr;                    \
+        stp     x3, x4, [P0];                   \
+        stp     x5, x6, [P0+16]
 
 // Modular subtraction with double modulus 2 * p_25519 = 2^256 - 38
 
@@ -809,73 +837,68 @@ S2N_BN_SYMBOL(curve25519_x25519):
 
         mov     res, x0
 
-// Copy the inputs to the local variables while mangling them:
+// Copy the inputs to the local variables with minimal mangling:
 //
-//  - The scalar gets turned into 01xxx...xxx000 by tweaking bits.
-//    Actually the top zero doesn't matter since the loop below
-//    never looks at it, so we don't literally modify that.
+//  - The scalar is in principle turned into 01xxx...xxx000 but
+//    in the structure below the special handling of these bits is
+//    explicit in the main computation; the scalar is just copied.
 //
-//  - The point x coord is reduced mod 2^255 *then* mod 2^255-19
+//  - The point x coord is reduced mod 2^255 by masking off the
+//    top bit. In the main loop we only need reduction < 2 * p_25519.
 
         ldp     x10, x11, [x1]
-        bic     x10, x10, #7
         stp     x10, x11, [scalar]
         ldp     x12, x13, [x1, #16]
-        orr     x13, x13, #0x4000000000000000
         stp     x12, x13, [scalar+16]
 
         ldp     x10, x11, [x2]
-        subs    x6, x10, #-19
-        adcs    x7, x11, xzr
+        stp     x10, x11, [pointx]
         ldp     x12, x13, [x2, #16]
         and     x13, x13, #0x7fffffffffffffff
-        adcs    x8, x12, xzr
-        mov     x9, #0x7fffffffffffffff
-        sbcs    x9, x13, x9
-
-        csel    x10, x6, x10, cs
-        csel    x11, x7, x11, cs
-        csel    x12, x8, x12, cs
-        csel    x13, x9, x13, cs
-
-        stp     x10, x11, [pointx]
         stp     x12, x13, [pointx+16]
 
-// Initialize (xn,zn) = (1,0) and (xm,zm) = (x,1) with swap = 0
-// We use the fact that the point x coordinate is still in registers
+// Initialize with explicit doubling in order to handle set bit 254.
+// Set swap = 1 and (xm,zm) = (x,1) then double as (xn,zn) = 2 * (x,1).
+// We use the fact that the point x coordinate is still in registers.
+// Since zm = 1 we could do the doubling with an operation count of
+// 2 * S + M instead of 2 * S + 2 * M, but it doesn't seem worth
+// the slight complication arising from a different linear combination.
 
-        mov     x2, #1
-        stp     x2, xzr, [xn]
-        stp     xzr, xzr, [xn+16]
-        stp     xzr, xzr, [zn]
-        stp     xzr, xzr, [zn+16]
+        mov     swap, #1
         stp     x10, x11, [xm]
         stp     x12, x13, [xm+16]
-        stp     x2, xzr, [zm]
+        stp     swap, xzr, [zm]
         stp     xzr, xzr, [zm+16]
-        mov     swap, xzr
 
-// The outer loop over scalar bits from i = 254, ..., i = 0 (inclusive)
-// This starts at 254, and so implicitly masks bit 255 of the scalar.
+        sub_twice4(d,xm,zm)
+        add_twice4(s,xm,zm)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        mov     x1, 0xdb42
+        orr     x1, x1, 0x10000
+        cmadd_4(e,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
+
+// The main loop over unmodified bits from i = 253, ..., i = 3 (inclusive).
+// This is a classic Montgomery ladder, with the main coordinates only
+// reduced mod 2 * p_25519, some intermediate results even more loosely.
 
-        mov     i, #254
+        mov     i, #253
 
 scalarloop:
 
+
 // sm = xm + zm; sn = xn + zn; dm = xm - zm; dn = xn - zn
-// The adds don't need any normalization as they're fed to muls
-// Just make sure the subs fit in 4 digits
 
-        sub_4(dm, xm, zm)
-        add_4(sn, xn, zn)
-        sub_4(dn, xn, zn)
-        add_4(sm, xm, zm)
+        sub_twice4(dm,xm,zm)
+        add_twice4(sn,xn,zn)
+        sub_twice4(dn,xn,zn)
+        add_twice4(sm,xm,zm)
 
-// ADDING: dmsn = dm * sn; dnsm = sm * dn
 // DOUBLING: mux d = xt - zt and s = xt + zt for appropriate choice of (xt,zt)
 
-        mul_5(dmsn,sn,dm)
-
         lsr     x0, i, #6
         ldr     x2, [sp, x0, lsl #3]    // Exploiting scalar = sp exactly
         lsr     x2, x2, i
@@ -887,18 +910,22 @@ scalarloop:
         mux_4(d,dm,dn)
         mux_4(s,sm,sn)
 
+// ADDING: dmsn = dm * sn; dnsm = sm * dn
+
+        mul_5(dmsn,sn,dm)
+
         mul_5(dnsm,sm,dn)
 
-// DOUBLING: d = (xt - zt)^2 normalized only to 4 digits
+// DOUBLING: d = (xt - zt)^2
 
         sqr_4(d,d)
 
 // ADDING: dpro = (dmsn - dnsm)^2, spro = (dmsn + dnsm)^2
-// DOUBLING: s = (xt + zt)^2, normalized only to 4 digits
+// DOUBLING: s = (xt + zt)^2
 
         sub5_4(dpro,dmsn,dnsm)
-        sqr_4(s,s)
         add5_4(spro,dmsn,dnsm)
+        sqr_4(s,s)
         sqr_4(dpro,dpro)
 
 // DOUBLING: p = 4 * xt * zt = s - d
@@ -907,7 +934,7 @@ scalarloop:
 
 // ADDING: xm' = (dmsn + dnsm)^2
 
-        sqr_p25519(xm,spro)
+        sqr_4(xm,spro)
 
 // DOUBLING: e = 121666 * p + d
 
@@ -917,26 +944,66 @@ scalarloop:
 
 // DOUBLING: xn' = (xt + zt)^2 * (xt - zt)^2 = s * d
 
-        mul_p25519(xn,s,d)
-
-// ADDING: zm' = x * (dmsn - dnsm)^2
-
-        mul_p25519(zm,dpro,pointx)
+        mul_4(xn,s,d)
 
 // DOUBLING: zn' = (4 * xt * zt) * ((xt - zt)^2 + 121666 * (4 * xt * zt))
 //               = p * (d + 121666 * p)
 
-        mul_p25519(zn,p,e)
+        mul_4(zn,p,e)
+
+// ADDING: zm' = x * (dmsn - dnsm)^2
 
-// Loop down as far as 0 (inclusive)
+        mul_4(zm,dpro,pointx)
 
-        subs    i, i, #1
+// Loop down as far as 3 (inclusive)
+
+        sub     i, i, #1
+        cmp     i, #3
         bcs     scalarloop
 
-// Since the scalar was forced to be a multiple of 8, we know it's even.
-// Hence there is no need to multiplex: the projective answer is (xn,zn)
-// and we can ignore (xm,zm); indeed we could have avoided the last three
-// differential additions and just done the doublings.
+// Multiplex directly into (xn,zn) then do three pure doubling steps;
+// this accounts for the implicit zeroing of the three lowest bits
+// of the scalar. On the very last doubling we *fully* reduce zn mod
+// p_25519 to ease checking for degeneracy below.
+
+        cmp     swap, xzr
+        mux_4(xn,xm,xn)
+        mux_4(zn,zm,zn)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        mov     x1, 0xdb42
+        orr     x1, x1, 0x10000
+        cmadd_4(e,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        mov     x1, 0xdb42
+        orr     x1, x1, 0x10000
+        cmadd_4(e,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        mov     x1, 0xdb42
+        orr     x1, x1, 0x10000
+        cmadd_4(e,p,d)
+        mul_4(xn,s,d)
+        mul_p25519(zn,p,e)
+
+// The projective result of the scalar multiplication is now (xn,zn).
 // First set up the constant sn = 2^255 - 19 for the modular inverse.
 
         mov     x0, #-19
@@ -1307,7 +1374,7 @@ zfliploop:
         csel    x3, x3, xzr, ne
         stp     x2, x3, [xn+16]
 
-// Now the result is xn * (1/zn).
+// Now the result is xn * (1/zn), fully reduced modulo p.
 
         mul_p25519(resx,xn,zm)
 
diff --git a/arm/curve25519/curve25519_x25519_alt.S b/arm/curve25519/curve25519_x25519_alt.S
index 19b23f96c..5ae6c9fa1 100644
--- a/arm/curve25519/curve25519_x25519_alt.S
+++ b/arm/curve25519/curve25519_x25519_alt.S
@@ -75,10 +75,8 @@
 
 #define NSPACE (12*NUMSIZE)
 
-// Macros wrapping up the basic field operation calls
-// bignum_mul_p25519_alt and bignum_sqr_p25519_alt.
-// These two are only trivially different from pure
-// function calls to those subroutines.
+// Macro wrapping up the basic field operation bignum_mul_p25519_alt, only
+// trivially different from a pure function call to that subroutine.
 
 #define mul_p25519(p0,p1,p2)                    \
         ldp     x3, x4, [p1];                   \
@@ -184,87 +182,106 @@
         stp     x12, x13, [p0];                 \
         stp     x14, x15, [p0+16]
 
-#define sqr_p25519(p0,p1)                       \
-        ldp     x2, x3, [p1];                   \
-        mul     x9, x2, x3;                     \
-        umulh   x10, x2, x3;                    \
-        ldp     x4, x5, [p1+16];                \
-        mul     x11, x2, x5;                    \
-        umulh   x12, x2, x5;                    \
-        mul     x7, x2, x4;                     \
-        umulh   x6, x2, x4;                     \
-        adds    x10, x10, x7;                   \
-        adcs    x11, x11, x6;                   \
-        mul     x7, x3, x4;                     \
-        umulh   x6, x3, x4;                     \
-        adc     x6, x6, xzr;                    \
-        adds    x11, x11, x7;                   \
-        mul     x13, x4, x5;                    \
-        umulh   x14, x4, x5;                    \
-        adcs    x12, x12, x6;                   \
-        mul     x7, x3, x5;                     \
-        umulh   x6, x3, x5;                     \
-        adc     x6, x6, xzr;                    \
-        adds    x12, x12, x7;                   \
-        adcs    x13, x13, x6;                   \
-        adc     x14, x14, xzr;                  \
-        adds    x9, x9, x9;                     \
-        adcs    x10, x10, x10;                  \
-        adcs    x11, x11, x11;                  \
-        adcs    x12, x12, x12;                  \
-        adcs    x13, x13, x13;                  \
-        adcs    x14, x14, x14;                  \
-        cset    x6, hs;                         \
-        umulh   x7, x2, x2;                     \
-        mul     x8, x2, x2;                     \
-        adds    x9, x9, x7;                     \
-        mul     x7, x3, x3;                     \
-        adcs    x10, x10, x7;                   \
-        umulh   x7, x3, x3;                     \
-        adcs    x11, x11, x7;                   \
-        mul     x7, x4, x4;                     \
-        adcs    x12, x12, x7;                   \
-        umulh   x7, x4, x4;                     \
-        adcs    x13, x13, x7;                   \
-        mul     x7, x5, x5;                     \
-        adcs    x14, x14, x7;                   \
-        umulh   x7, x5, x5;                     \
-        adc     x6, x6, x7;                     \
-        mov     x3, #38;                        \
-        mul     x7, x3, x12;                    \
-        umulh   x4, x3, x12;                    \
-        adds    x8, x8, x7;                     \
-        mul     x7, x3, x13;                    \
-        umulh   x13, x3, x13;                   \
-        adcs    x9, x9, x7;                     \
-        mul     x7, x3, x14;                    \
-        umulh   x14, x3, x14;                   \
-        adcs    x10, x10, x7;                   \
-        mul     x7, x3, x6;                     \
-        umulh   x6, x3, x6;                     \
-        adcs    x11, x11, x7;                   \
-        cset    x12, hs;                        \
-        adds    x9, x9, x4;                     \
-        adcs    x10, x10, x13;                  \
-        adcs    x11, x11, x14;                  \
-        adc     x12, x12, x6;                   \
-        cmn     x11, x11;                       \
-        orr     x11, x11, #0x8000000000000000;  \
-        adc     x2, x12, x12;                   \
-        mov     x3, #19;                        \
-        madd    x7, x3, x2, x3;                 \
-        adds    x8, x8, x7;                     \
-        adcs    x9, x9, xzr;                    \
-        adcs    x10, x10, xzr;                  \
-        adcs    x11, x11, xzr;                  \
-        csel    x3, x3, xzr, lo;                \
-        subs    x8, x8, x3;                     \
-        sbcs    x9, x9, xzr;                    \
-        sbcs    x10, x10, xzr;                  \
-        sbc     x11, x11, xzr;                  \
-        and     x11, x11, #0x7fffffffffffffff;  \
-        stp     x8, x9, [p0];                   \
-        stp     x10, x11, [p0+16]
+// A version of multiplication that only guarantees output < 2 * p_25519.
+// This basically skips the +1 and final correction in quotient estimation.
+
+#define mul_4(P0,P1,P2)                         \
+        ldp     x3, x4, [P1];                   \
+        ldp     x7, x8, [P2];                   \
+        mul     x12, x3, x7;                    \
+        umulh   x13, x3, x7;                    \
+        mul     x11, x3, x8;                    \
+        umulh   x14, x3, x8;                    \
+        adds    x13, x13, x11;                  \
+        ldp     x9, x10, [P2+16];               \
+        mul     x11, x3, x9;                    \
+        umulh   x15, x3, x9;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x3, x10;                   \
+        umulh   x16, x3, x10;                   \
+        adcs    x15, x15, x11;                  \
+        adc     x16, x16, xzr;                  \
+        ldp     x5, x6, [P1+16];                \
+        mul     x11, x4, x7;                    \
+        adds    x13, x13, x11;                  \
+        mul     x11, x4, x8;                    \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x4, x9;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x4, x10;                   \
+        adcs    x16, x16, x11;                  \
+        umulh   x3, x4, x10;                    \
+        adc     x3, x3, xzr;                    \
+        umulh   x11, x4, x7;                    \
+        adds    x14, x14, x11;                  \
+        umulh   x11, x4, x8;                    \
+        adcs    x15, x15, x11;                  \
+        umulh   x11, x4, x9;                    \
+        adcs    x16, x16, x11;                  \
+        adc     x3, x3, xzr;                    \
+        mul     x11, x5, x7;                    \
+        adds    x14, x14, x11;                  \
+        mul     x11, x5, x8;                    \
+        adcs    x15, x15, x11;                  \
+        mul     x11, x5, x9;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x5, x10;                   \
+        adcs    x3, x3, x11;                    \
+        umulh   x4, x5, x10;                    \
+        adc     x4, x4, xzr;                    \
+        umulh   x11, x5, x7;                    \
+        adds    x15, x15, x11;                  \
+        umulh   x11, x5, x8;                    \
+        adcs    x16, x16, x11;                  \
+        umulh   x11, x5, x9;                    \
+        adcs    x3, x3, x11;                    \
+        adc     x4, x4, xzr;                    \
+        mul     x11, x6, x7;                    \
+        adds    x15, x15, x11;                  \
+        mul     x11, x6, x8;                    \
+        adcs    x16, x16, x11;                  \
+        mul     x11, x6, x9;                    \
+        adcs    x3, x3, x11;                    \
+        mul     x11, x6, x10;                   \
+        adcs    x4, x4, x11;                    \
+        umulh   x5, x6, x10;                    \
+        adc     x5, x5, xzr;                    \
+        umulh   x11, x6, x7;                    \
+        adds    x16, x16, x11;                  \
+        umulh   x11, x6, x8;                    \
+        adcs    x3, x3, x11;                    \
+        umulh   x11, x6, x9;                    \
+        adcs    x4, x4, x11;                    \
+        adc     x5, x5, xzr;                    \
+        mov     x7, #38;                        \
+        mul     x11, x7, x16;                   \
+        umulh   x9, x7, x16;                    \
+        adds    x12, x12, x11;                  \
+        mul     x11, x7, x3;                    \
+        umulh   x3, x7, x3;                     \
+        adcs    x13, x13, x11;                  \
+        mul     x11, x7, x4;                    \
+        umulh   x4, x7, x4;                     \
+        adcs    x14, x14, x11;                  \
+        mul     x11, x7, x5;                    \
+        umulh   x5, x7, x5;                     \
+        adcs    x15, x15, x11;                  \
+        cset    x16, hs;                        \
+        adds    x13, x13, x9;                   \
+        adcs    x14, x14, x3;                   \
+        adcs    x15, x15, x4;                   \
+        adc     x16, x16, x5;                   \
+        cmn     x15, x15;                       \
+        bic     x15, x15, #0x8000000000000000;  \
+        adc     x8, x16, x16;                   \
+        mov     x7, #19;                        \
+        mul     x11, x7, x8;                    \
+        adds    x12, x12, x11;                  \
+        adcs    x13, x13, xzr;                  \
+        adcs    x14, x14, xzr;                  \
+        adc     x15, x15, xzr;                  \
+        stp     x12, x13, [P0];                 \
+        stp     x14, x15, [P0+16]
 
 // Multiplication just giving a 5-digit result (actually < 39 * 2^256)
 // by not doing anything beyond the first stage of reduction
@@ -439,21 +456,6 @@
         stp     x8, x9, [p0];                   \
         stp     x10, x11, [p0+16]
 
-// Plain 4-digit add without any normalization
-// With inputs < p_25519 (indeed < 2^255) it still gives a 4-digit result
-
-#define add_4(p0,p1,p2)                         \
-        ldp     x0, x1, [p1];                   \
-        ldp     x4, x5, [p2];                   \
-        adds    x0, x0, x4;                     \
-        adcs    x1, x1, x5;                     \
-        ldp     x2, x3, [p1+16];                \
-        ldp     x6, x7, [p2+16];                \
-        adcs    x2, x2, x6;                     \
-        adc     x3, x3, x7;                     \
-        stp     x0, x1, [p0];                   \
-        stp     x2, x3, [p0+16]
-
 // Add 5-digit inputs and normalize to 4 digits
 
 #define add5_4(p0,p1,p2)                        \
@@ -480,28 +482,29 @@
         stp     x0, x1, [p0];                   \
         stp     x2, x3, [p0+16]
 
-// Subtraction of a pair of numbers < p_25519 just sufficient
-// to give a 4-digit result. It actually always does (x - z) + (2^255-19)
-// which in turn is done by (x - z) - (2^255+19) discarding the 2^256
-// implicitly
+// Modular addition and doubling with double modulus 2 * p_25519 = 2^256 - 38.
+// This only ensures that the result fits in 4 digits, not that it is reduced
+// even w.r.t. double modulus. The result is always correct modulo provided
+// the sum of the inputs is < 2^256 + 2^256 - 38, so in particular provided
+// at least one of them is reduced double modulo.
 
-#define sub_4(p0,p1,p2)                         \
-        ldp     x5, x6, [p1];                   \
-        ldp     x4, x3, [p2];                   \
-        subs    x5, x5, x4;                     \
-        sbcs    x6, x6, x3;                     \
-        ldp     x7, x8, [p1+16];                \
-        ldp     x4, x3, [p2+16];                \
-        sbcs    x7, x7, x4;                     \
-        sbcs    x8, x8, x3;                     \
-        mov     x3, #19;                        \
-        subs    x5, x5, x3;                     \
-        sbcs    x6, x6, xzr;                    \
-        sbcs    x7, x7, xzr;                    \
-        mov     x4, #0x8000000000000000;        \
-        sbc     x8, x8, x4;                     \
-        stp     x5, x6, [p0];                   \
-        stp     x7, x8, [p0+16]
+#define add_twice4(P0,P1,P2)                    \
+        ldp     x3, x4, [P1];                   \
+        ldp     x7, x8, [P2];                   \
+        adds    x3, x3, x7;                     \
+        adcs    x4, x4, x8;                     \
+        ldp     x5, x6, [P1+16];                \
+        ldp     x7, x8, [P2+16];                \
+        adcs    x5, x5, x7;                     \
+        adcs    x6, x6, x8;                     \
+        mov     x9, #38;                        \
+        csel    x9, x9, xzr, cs;                \
+        adds    x3, x3, x9;                     \
+        adcs    x4, x4, xzr;                    \
+        adcs    x5, x5, xzr;                    \
+        adc     x6, x6, xzr;                    \
+        stp     x3, x4, [P0];                   \
+        stp     x5, x6, [P0+16]
 
 // Modular subtraction with double modulus 2 * p_25519 = 2^256 - 38
 
@@ -623,73 +626,68 @@ S2N_BN_SYMBOL(curve25519_x25519_alt):
 
         mov     res, x0
 
-// Copy the inputs to the local variables while mangling them:
+// Copy the inputs to the local variables with minimal mangling:
 //
-//  - The scalar gets turned into 01xxx...xxx000 by tweaking bits.
-//    Actually the top zero doesn't matter since the loop below
-//    never looks at it, so we don't literally modify that.
+//  - The scalar is in principle turned into 01xxx...xxx000 but
+//    in the structure below the special handling of these bits is
+//    explicit in the main computation; the scalar is just copied.
 //
-//  - The point x coord is reduced mod 2^255 *then* mod 2^255-19
+//  - The point x coord is reduced mod 2^255 by masking off the
+//    top bit. In the main loop we only need reduction < 2 * p_25519.
 
         ldp     x10, x11, [x1]
-        bic     x10, x10, #7
         stp     x10, x11, [scalar]
         ldp     x12, x13, [x1, #16]
-        orr     x13, x13, #0x4000000000000000
         stp     x12, x13, [scalar+16]
 
         ldp     x10, x11, [x2]
-        subs    x6, x10, #-19
-        adcs    x7, x11, xzr
+        stp     x10, x11, [pointx]
         ldp     x12, x13, [x2, #16]
         and     x13, x13, #0x7fffffffffffffff
-        adcs    x8, x12, xzr
-        mov     x9, #0x7fffffffffffffff
-        sbcs    x9, x13, x9
-
-        csel    x10, x6, x10, cs
-        csel    x11, x7, x11, cs
-        csel    x12, x8, x12, cs
-        csel    x13, x9, x13, cs
-
-        stp     x10, x11, [pointx]
         stp     x12, x13, [pointx+16]
 
-// Initialize (xn,zn) = (1,0) and (xm,zm) = (x,1) with swap = 0
-// We use the fact that the point x coordinate is still in registers
+// Initialize with explicit doubling in order to handle set bit 254.
+// Set swap = 1 and (xm,zm) = (x,1) then double as (xn,zn) = 2 * (x,1).
+// We use the fact that the point x coordinate is still in registers.
+// Since zm = 1 we could do the doubling with an operation count of
+// 2 * S + M instead of 2 * S + 2 * M, but it doesn't seem worth
+// the slight complication arising from a different linear combination.
 
-        mov     x2, #1
-        stp     x2, xzr, [xn]
-        stp     xzr, xzr, [xn+16]
-        stp     xzr, xzr, [zn]
-        stp     xzr, xzr, [zn+16]
+        mov     swap, #1
         stp     x10, x11, [xm]
         stp     x12, x13, [xm+16]
-        stp     x2, xzr, [zm]
+        stp     swap, xzr, [zm]
         stp     xzr, xzr, [zm+16]
-        mov     swap, xzr
 
-// The outer loop over scalar bits from i = 254, ..., i = 0 (inclusive)
-// This starts at 254, and so implicitly masks bit 255 of the scalar.
+        sub_twice4(d,xm,zm)
+        add_twice4(s,xm,zm)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        mov     x1, 0xdb42
+        orr     x1, x1, 0x10000
+        cmadd_4(e,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
+
+// The main loop over unmodified bits from i = 253, ..., i = 3 (inclusive).
+// This is a classic Montgomery ladder, with the main coordinates only
+// reduced mod 2 * p_25519, some intermediate results even more loosely.
 
-        mov     i, #254
+        mov     i, #253
 
 scalarloop:
 
+
 // sm = xm + zm; sn = xn + zn; dm = xm - zm; dn = xn - zn
-// The adds don't need any normalization as they're fed to muls
-// Just make sure the subs fit in 4 digits
 
-        sub_4(dm, xm, zm)
-        add_4(sn, xn, zn)
-        sub_4(dn, xn, zn)
-        add_4(sm, xm, zm)
+        sub_twice4(dm,xm,zm)
+        add_twice4(sn,xn,zn)
+        sub_twice4(dn,xn,zn)
+        add_twice4(sm,xm,zm)
 
-// ADDING: dmsn = dm * sn; dnsm = sm * dn
 // DOUBLING: mux d = xt - zt and s = xt + zt for appropriate choice of (xt,zt)
 
-        mul_5(dmsn,sn,dm)
-
         lsr     x0, i, #6
         ldr     x2, [sp, x0, lsl #3]    // Exploiting scalar = sp exactly
         lsr     x2, x2, i
@@ -701,18 +699,22 @@ scalarloop:
         mux_4(d,dm,dn)
         mux_4(s,sm,sn)
 
+// ADDING: dmsn = dm * sn; dnsm = sm * dn
+
+        mul_5(dmsn,sn,dm)
+
         mul_5(dnsm,sm,dn)
 
-// DOUBLING: d = (xt - zt)^2 normalized only to 4 digits
+// DOUBLING: d = (xt - zt)^2
 
         sqr_4(d,d)
 
 // ADDING: dpro = (dmsn - dnsm)^2, spro = (dmsn + dnsm)^2
-// DOUBLING: s = (xt + zt)^2, normalized only to 4 digits
+// DOUBLING: s = (xt + zt)^2
 
         sub5_4(dpro,dmsn,dnsm)
-        sqr_4(s,s)
         add5_4(spro,dmsn,dnsm)
+        sqr_4(s,s)
         sqr_4(dpro,dpro)
 
 // DOUBLING: p = 4 * xt * zt = s - d
@@ -721,7 +723,7 @@ scalarloop:
 
 // ADDING: xm' = (dmsn + dnsm)^2
 
-        sqr_p25519(xm,spro)
+        sqr_4(xm,spro)
 
 // DOUBLING: e = 121666 * p + d
 
@@ -731,26 +733,66 @@ scalarloop:
 
 // DOUBLING: xn' = (xt + zt)^2 * (xt - zt)^2 = s * d
 
-        mul_p25519(xn,s,d)
-
-// ADDING: zm' = x * (dmsn - dnsm)^2
-
-        mul_p25519(zm,dpro,pointx)
+        mul_4(xn,s,d)
 
 // DOUBLING: zn' = (4 * xt * zt) * ((xt - zt)^2 + 121666 * (4 * xt * zt))
 //               = p * (d + 121666 * p)
 
-        mul_p25519(zn,p,e)
+        mul_4(zn,p,e)
+
+// ADDING: zm' = x * (dmsn - dnsm)^2
+
+        mul_4(zm,dpro,pointx)
 
-// Loop down as far as 0 (inclusive)
+// Loop down as far as 3 (inclusive)
 
-        subs    i, i, #1
+        sub     i, i, #1
+        cmp     i, #3
         bcs     scalarloop
 
-// Since the scalar was forced to be a multiple of 8, we know it's even.
-// Hence there is no need to multiplex: the projective answer is (xn,zn)
-// and we can ignore (xm,zm); indeed we could have avoided the last three
-// differential additions and just done the doublings.
+// Multiplex directly into (xn,zn) then do three pure doubling steps;
+// this accounts for the implicit zeroing of the three lowest bits
+// of the scalar. On the very last doubling we *fully* reduce zn mod
+// p_25519 to ease checking for degeneracy below.
+
+        cmp     swap, xzr
+        mux_4(xn,xm,xn)
+        mux_4(zn,zm,zn)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        mov     x1, 0xdb42
+        orr     x1, x1, 0x10000
+        cmadd_4(e,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        mov     x1, 0xdb42
+        orr     x1, x1, 0x10000
+        cmadd_4(e,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        mov     x1, 0xdb42
+        orr     x1, x1, 0x10000
+        cmadd_4(e,p,d)
+        mul_4(xn,s,d)
+        mul_p25519(zn,p,e)
+
+// The projective result of the scalar multiplication is now (xn,zn).
 // First set up the constant sn = 2^255 - 19 for the modular inverse.
 
         mov     x0, #-19
@@ -1121,7 +1163,7 @@ zfliploop:
         csel    x3, x3, xzr, ne
         stp     x2, x3, [xn+16]
 
-// Now the result is xn * (1/zn).
+// Now the result is xn * (1/zn), fully reduced modulo p.
 
         mul_p25519(resx,xn,zm)
 
diff --git a/x86_att/curve25519/curve25519_x25519.S b/x86_att/curve25519/curve25519_x25519.S
index 689429c4b..d83479a80 100644
--- a/x86_att/curve25519/curve25519_x25519.S
+++ b/x86_att/curve25519/curve25519_x25519.S
@@ -78,10 +78,8 @@
 
 #define NSPACE (13*NUMSIZE)
 
-// Macros wrapping up the basic field operation calls
-// bignum_mul_p25519 and bignum_sqr_p25519.
-// These two are only trivially different from pure
-// function calls to those subroutines.
+// Macro wrapping up the basic field operation bignum_mul_p25519, only
+// trivially different from a pure function call to that subroutine.
 
 #define mul_p25519(P0,P1,P2)                    \
         xorl   %edi, %edi ;                        \
@@ -176,78 +174,89 @@
         movq   %r10, 0x10+P0 ;                  \
         movq   %r11, 0x18+P0
 
-#define sqr_p25519(P0,P1)                       \
-        movq   P1, %rdx ;                       \
-        mulxq  %rdx, %r8, %r15 ;                    \
-        mulxq  0x8+P1, %r9, %r10 ;               \
-        mulxq  0x18+P1, %r11, %r12 ;             \
-        movq   0x10+P1, %rdx ;                  \
-        mulxq  0x18+P1, %r13, %r14 ;             \
-        xorl   %ebx, %ebx ;                        \
-        mulxq  P1, %rax, %rcx ;                  \
+
+// A version of multiplication that only guarantees output < 2 * p_25519.
+// This basically skips the +1 and final correction in quotient estimation.
+
+#define mul_4(P0,P1,P2)                         \
+        xorl   %ecx, %ecx ;                        \
+        movq   P2, %rdx ;                       \
+        mulxq  P1, %r8, %r9 ;                    \
+        mulxq  0x8+P1, %rax, %r10 ;              \
+        addq   %rax, %r9 ;                         \
+        mulxq  0x10+P1, %rax, %r11 ;             \
+        adcq   %rax, %r10 ;                        \
+        mulxq  0x18+P1, %rax, %r12 ;             \
+        adcq   %rax, %r11 ;                        \
+        adcq   %rcx, %r12 ;                        \
+        xorl   %ecx, %ecx ;                        \
+        movq   0x8+P2, %rdx ;                   \
+        mulxq  P1, %rax, %rbx ;                  \
+        adcxq  %rax, %r9 ;                         \
+        adoxq  %rbx, %r10 ;                        \
+        mulxq  0x8+P1, %rax, %rbx ;              \
         adcxq  %rax, %r10 ;                        \
-        adoxq  %rcx, %r11 ;                        \
-        mulxq  0x8+P1, %rax, %rcx ;              \
+        adoxq  %rbx, %r11 ;                        \
+        mulxq  0x10+P1, %rax, %rbx ;             \
         adcxq  %rax, %r11 ;                        \
-        adoxq  %rcx, %r12 ;                        \
-        movq   0x18+P1, %rdx ;                  \
-        mulxq  0x8+P1, %rax, %rcx ;              \
+        adoxq  %rbx, %r12 ;                        \
+        mulxq  0x18+P1, %rax, %r13 ;             \
         adcxq  %rax, %r12 ;                        \
         adoxq  %rcx, %r13 ;                        \
-        adcxq  %rbx, %r13 ;                        \
+        adcxq  %rcx, %r13 ;                        \
+        xorl   %ecx, %ecx ;                        \
+        movq   0x10+P2, %rdx ;                  \
+        mulxq  P1, %rax, %rbx ;                  \
+        adcxq  %rax, %r10 ;                        \
+        adoxq  %rbx, %r11 ;                        \
+        mulxq  0x8+P1, %rax, %rbx ;              \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rbx, %r12 ;                        \
+        mulxq  0x10+P1, %rax, %rbx ;             \
+        adcxq  %rax, %r12 ;                        \
+        adoxq  %rbx, %r13 ;                        \
+        mulxq  0x18+P1, %rax, %r14 ;             \
+        adcxq  %rax, %r13 ;                        \
+        adoxq  %rcx, %r14 ;                        \
+        adcxq  %rcx, %r14 ;                        \
+        xorl   %ecx, %ecx ;                        \
+        movq   0x18+P2, %rdx ;                  \
+        mulxq  P1, %rax, %rbx ;                  \
+        adcxq  %rax, %r11 ;                        \
+        adoxq  %rbx, %r12 ;                        \
+        mulxq  0x8+P1, %rax, %rbx ;              \
+        adcxq  %rax, %r12 ;                        \
+        adoxq  %rbx, %r13 ;                        \
+        mulxq  0x10+P1, %rax, %rbx ;             \
+        adcxq  %rax, %r13 ;                        \
         adoxq  %rbx, %r14 ;                        \
-        adcq   %rbx, %r14 ;                        \
-        xorl   %ebx, %ebx ;                        \
-        adcxq  %r9, %r9 ;                          \
-        adoxq  %r15, %r9 ;                         \
-        movq   0x8+P1, %rdx ;                   \
-        mulxq  %rdx, %rax, %rdx ;                   \
-        adcxq  %r10, %r10 ;                        \
-        adoxq  %rax, %r10 ;                        \
-        adcxq  %r11, %r11 ;                        \
-        adoxq  %rdx, %r11 ;                        \
-        movq   0x10+P1, %rdx ;                  \
-        mulxq  %rdx, %rax, %rdx ;                   \
-        adcxq  %r12, %r12 ;                        \
-        adoxq  %rax, %r12 ;                        \
-        adcxq  %r13, %r13 ;                        \
-        adoxq  %rdx, %r13 ;                        \
-        movq   0x18+P1, %rdx ;                  \
-        mulxq  %rdx, %rax, %r15 ;                   \
-        adcxq  %r14, %r14 ;                        \
-        adoxq  %rax, %r14 ;                        \
-        adcxq  %rbx, %r15 ;                        \
-        adoxq  %rbx, %r15 ;                        \
+        mulxq  0x18+P1, %rax, %r15 ;             \
+        adcxq  %rax, %r14 ;                        \
+        adoxq  %rcx, %r15 ;                        \
+        adcxq  %rcx, %r15 ;                        \
         movl   $0x26, %edx ;                       \
-        xorl   %ebx, %ebx ;                        \
-        mulxq  %r12, %rax, %rcx ;                   \
+        xorl   %ecx, %ecx ;                        \
+        mulxq  %r12, %rax, %rbx ;                   \
         adcxq  %rax, %r8 ;                         \
-        adoxq  %rcx, %r9 ;                         \
-        mulxq  %r13, %rax, %rcx ;                   \
+        adoxq  %rbx, %r9 ;                         \
+        mulxq  %r13, %rax, %rbx ;                   \
         adcxq  %rax, %r9 ;                         \
-        adoxq  %rcx, %r10 ;                        \
-        mulxq  %r14, %rax, %rcx ;                   \
+        adoxq  %rbx, %r10 ;                        \
+        mulxq  %r14, %rax, %rbx ;                   \
         adcxq  %rax, %r10 ;                        \
-        adoxq  %rcx, %r11 ;                        \
+        adoxq  %rbx, %r11 ;                        \
         mulxq  %r15, %rax, %r12 ;                   \
         adcxq  %rax, %r11 ;                        \
-        adoxq  %rbx, %r12 ;                        \
-        adcxq  %rbx, %r12 ;                        \
+        adoxq  %rcx, %r12 ;                        \
+        adcxq  %rcx, %r12 ;                        \
         shldq  $0x1, %r11, %r12 ;                   \
-        movl   $0x13, %edx ;                       \
-        leaq   0x1(%r12), %rax ;                  \
-        bts    $0x3f, %r11 ;                       \
-        imulq  %rdx, %rax ;                        \
-        addq   %rax, %r8 ;                         \
-        adcq   %rbx, %r9 ;                         \
-        adcq   %rbx, %r10 ;                        \
-        adcq   %rbx, %r11 ;                        \
-        cmovbq %rbx, %rdx ;                        \
-        subq   %rdx, %r8 ;                         \
-        sbbq   %rbx, %r9 ;                         \
-        sbbq   %rbx, %r10 ;                        \
-        sbbq   %rbx, %r11 ;                        \
         btr    $0x3f, %r11 ;                       \
+        movl   $0x13, %edx ;                       \
+        imulq  %r12, %rdx ;                        \
+        addq   %rdx, %r8 ;                         \
+        adcq   %rcx, %r9 ;                         \
+        adcq   %rcx, %r10 ;                        \
+        adcq   %rcx, %r11 ;                        \
         movq   %r8, P0 ;                        \
         movq   %r9, 0x8+P0 ;                    \
         movq   %r10, 0x10+P0 ;                  \
@@ -407,23 +416,6 @@
         movq   %r10, 0x10+P0 ;                  \
         movq   %r11, 0x18+P0
 
-// Plain 4-digit add without any normalization
-// With inputs < p_25519 (indeed < 2^255) it still gives a 4-digit result
-
-#define add_4(P0,P1,P2)                         \
-        movq    P1, %rax ;                      \
-        addq    P2, %rax ;                      \
-        movq    %rax, P0 ;                      \
-        movq    8+P1, %rax ;                    \
-        adcq    8+P2, %rax ;                    \
-        movq    %rax, 8+P0 ;                    \
-        movq    16+P1, %rax ;                   \
-        adcq    16+P2, %rax ;                   \
-        movq    %rax, 16+P0 ;                   \
-        movq    24+P1, %rax ;                   \
-        adcq    24+P2, %rax ;                   \
-        movq    %rax, 24+P0
-
 // Add 5-digit inputs and normalize to 4 digits
 
 #define add5_4(P0,P1,P2)                        \
@@ -451,29 +443,32 @@
         movq   %r10, 0x10+P0 ;                  \
         movq   %r11, 0x18+P0
 
-// Subtraction of a pair of numbers < p_25519 just sufficient
-// to give a 4-digit result. It actually always does (x - z) + (2^255-19)
-// which in turn is done by (x - z) - (2^255+19) discarding the 2^256
-// implicitly
+// Modular addition with double modulus 2 * p_25519 = 2^256 - 38.
+// This only ensures that the result fits in 4 digits, not that it is reduced
+// even w.r.t. double modulus. The result is always correct modulo provided
+// the sum of the inputs is < 2^256 + 2^256 - 38, so in particular provided
+// at least one of them is reduced double modulo.
 
-#define sub_4(P0,P1,P2)                         \
+#define add_twice4(P0,P1,P2)                    \
         movq    P1, %r8 ;                       \
-        subq    P2, %r8 ;                       \
-        movq    8+P1, %r9 ;                     \
-        sbbq    8+P2, %r9 ;                     \
-        movq    16+P1, %r10 ;                   \
-        sbbq    16+P2, %r10 ;                   \
-        movq    24+P1, %rax ;                   \
-        sbbq    24+P2, %rax ;                   \
-        subq    $19, %r8 ;                         \
+        xorl    %ecx, %ecx ;                       \
+        addq    P2, %r8 ;                       \
+        movq    0x8+P1, %r9 ;                   \
+        adcq    0x8+P2, %r9 ;                   \
+        movq    0x10+P1, %r10 ;                 \
+        adcq    0x10+P2, %r10 ;                 \
+        movq    0x18+P1, %r11 ;                 \
+        adcq    0x18+P2, %r11 ;                 \
+        movl    $38, %eax ;                        \
+        cmovncq %rcx, %rax ;                       \
+        addq    %rax, %r8 ;                        \
+        adcq    %rcx, %r9 ;                        \
+        adcq    %rcx, %r10 ;                       \
+        adcq    %rcx, %r11 ;                       \
         movq    %r8, P0 ;                       \
-        sbbq    $0, %r9 ;                          \
-        movq    %r9, 8+P0 ;                     \
-        sbbq    $0, %r10 ;                         \
-        movq    %r10, 16+P0 ;                   \
-        sbbq    $0, %rax ;                         \
-        btc     $63, %rax ;                        \
-        movq    %rax, 24+P0
+        movq    %r9, 0x8+P0 ;                   \
+        movq    %r10, 0x10+P0 ;                 \
+        movq    %r11, 0x18+P0
 
 // Modular subtraction with double modulus 2 * p_25519 = 2^256 - 38
 
@@ -613,23 +608,22 @@ S2N_BN_SYMBOL(curve25519_x25519):
 
         movq    %rdi, res
 
-// Copy the inputs to the local variables while mangling them:
+// Copy the inputs to the local variables with minimal mangling:
 //
-//  - The scalar gets turned into 01xxx...xxx000 by tweaking bits.
-//    Actually the top zero doesn't matter since the loop below
-//    never looks at it, so we don't literally modify that.
+//  - The scalar is in principle turned into 01xxx...xxx000 but
+//    in the structure below the special handling of these bits is
+//    explicit in the main computation; the scalar is just copied.
 //
-//  - The point x coord is reduced mod 2^255 *then* mod 2^255-19
+//  - The point x coord is reduced mod 2^255 by masking off the
+//    top bit. In the main loop we only need reduction < 2 * p_25519.
 
         movq    (%rsi), %rax
-        andq    $~7, %rax
         movq    %rax, (%rsp)
         movq    8(%rsi), %rax
         movq    %rax, 8(%rsp)
         movq    16(%rsi), %rax
         movq    %rax, 16(%rsp)
         movq    24(%rsi), %rax
-        bts     $62, %rax
         movq    %rax, 24(%rsp)
 
         movq    (%rdx), %r8
@@ -637,70 +631,57 @@ S2N_BN_SYMBOL(curve25519_x25519):
         movq    16(%rdx), %r10
         movq    24(%rdx), %r11
         btr     $63, %r11
-        movq    $19, %r12
-        xorq    %r13, %r13
-        xorq    %r14, %r14
-        xorq    %r15, %r15
-        addq    %r8, %r12
-        adcq    %r9, %r13
-        adcq    %r10, %r14
-        adcq    %r11, %r15
-        btr     $63, %r15 // x >= 2^255 - 19 <=> x + 19 >= 2^255
-        cmovcq  %r12, %r8
         movq    %r8, 32(%rsp)
-        cmovcq  %r13, %r9
         movq    %r9, 40(%rsp)
-        cmovcq  %r14, %r10
         movq    %r10, 48(%rsp)
-        cmovcq  %r15, %r11
         movq    %r11, 56(%rsp)
 
-// Initialize (xn,zn) = (1,0) and (xm,zm) = (x,1) with swap = 0
-// We use the fact that the point x coordinate is still in registers
+// Initialize with explicit doubling in order to handle set bit 254.
+// Set swap = 1 and (xm,zm) = (x,1) then double as (xn,zn) = 2 * (x,1).
+// We use the fact that the point x coordinate is still in registers.
+// Since zm = 1 we could do the doubling with an operation count of
+// 2 * S + M instead of 2 * S + 2 * M, but it doesn't seem worth
+// the slight complication arising from a different linear combination.
 
-        movq    $1, %rax
-        movq    %rax, 320(%rsp)
-        movq    %rax, 96(%rsp)
-        xorl    %eax, %eax
+        movl    $1, %eax
         movq    %rax, swap
-        movq    %rax, 160(%rsp)
-        movq    %rax, 328(%rsp)
-        movq    %rax, 104(%rsp)
-        movq    %rax, 168(%rsp)
-        movq    %rax, 336(%rsp)
-        movq    %rax, 112(%rsp)
-        movq    %rax, 176(%rsp)
-        movq    %rax, 344(%rsp)
-        movq    %rax, 120(%rsp)
-        movq    %rax, 184(%rsp)
-        movq    32(%rsp), %rax
         movq    %r8, 256(%rsp)
+        movq    %rax, 96(%rsp)
+        xorl    %eax, %eax
         movq    %r9, 264(%rsp)
+        movq    %rax, 104(%rsp)
         movq    %r10, 272(%rsp)
+        movq    %rax, 112(%rsp)
         movq    %r11, 280(%rsp)
+        movq    %rax, 120(%rsp)
 
-// The outer loop over scalar bits from i = 254, ..., i = 0 (inclusive)
-// This starts at 254, and so implicitly masks bit 255 of the scalar.
+        sub_twice4(d,xm,zm)
+        add_twice4(s,xm,zm)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        cmadd_4(e,0x1db42,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
+
+// The main loop over unmodified bits from i = 253, ..., i = 3 (inclusive).
+// This is a classic Montgomery ladder, with the main coordinates only
+// reduced mod 2 * p_25519, some intermediate results even more loosely.
 
-        movl    $254, %eax
+        movl    $253, %eax
         movq    %rax, i
 
 scalarloop:
 
 // sm = xm + zm; sn = xn + zn; dm = xm - zm; dn = xn - zn
-// The adds don't need any normalization as they're fed to muls
-// Just make sure the subs fit in 4 digits.
 
-        sub_4(dm,xm,zm)
-        add_4(sn,xn,zn)
-        sub_4(dn,xn,zn)
-        add_4(sm,xm,zm)
+        sub_twice4(dm,xm,zm)
+        add_twice4(sn,xn,zn)
+        sub_twice4(dn,xn,zn)
+        add_twice4(sm,xm,zm)
 
-// ADDING: dmsn = dm * sn; dnsm = sm * dn
 // DOUBLING: mux d = xt - zt and s = xt + zt for appropriate choice of (xt,zt)
 
-        mul_5(dmsn,sn,dm)
-
         movq    i, %rdx
         movq    %rdx, %rcx
         shrq    $6, %rdx
@@ -709,22 +690,25 @@ scalarloop:
         andq    $1, %rdx
         cmpq    swap, %rdx
         movq    %rdx, swap
-
         mux_4(d,dm,dn)
         mux_4(s,sm,sn)
 
+// ADDING: dmsn = dm * sn; dnsm = sm * dn
+
+        mul_5(dmsn,sn,dm)
+
         mul_5(dnsm,sm,dn)
 
-// DOUBLING: d = (xt - zt)^2 normalized only to 4 digits
+// DOUBLING: d = (xt - zt)^2
 
         sqr_4(d,d)
 
 // ADDING: dpro = (dmsn - dnsm)^2, spro = (dmsn + dnsm)^2
-// DOUBLING: s = (xt + zt)^2, normalized only to 4 digits
+// DOUBLING: s = (xt + zt)^2
 
         sub5_4(dpro,dmsn,dnsm)
-        sqr_4(s,s)
         add5_4(spro,dmsn,dnsm)
+        sqr_4(s,s)
         sqr_4(dpro,dpro)
 
 // DOUBLING: p = 4 * xt * zt = s - d
@@ -733,7 +717,7 @@ scalarloop:
 
 // ADDING: xm' = (dmsn + dnsm)^2
 
-        sqr_p25519(xm,spro)
+        sqr_4(xm,spro)
 
 // DOUBLING: e = 121666 * p + d
 
@@ -741,28 +725,63 @@ scalarloop:
 
 // DOUBLING: xn' = (xt + zt)^2 * (xt - zt)^2 = s * d
 
-        mul_p25519(xn,s,d)
-
-// ADDING: zm' = x * (dmsn - dnsm)^2
-
-        mul_p25519(zm,dpro,pointx)
+        mul_4(xn,s,d)
 
 // DOUBLING: zn' = (4 * xt * zt) * ((xt - zt)^2 + 121666 * (4 * xt * zt))
 //               = p * (d + 121666 * p)
 
-        mul_p25519(zn,p,e)
+        mul_4(zn,p,e)
+
+// ADDING: zm' = x * (dmsn - dnsm)^2
 
-// Loop down as far as 0 (inclusive)
+        mul_4(zm,dpro,pointx)
+
+// Loop down as far as 3 (inclusive)
 
         movq    i, %rax
         subq    $1, %rax
         movq    %rax, i
+        cmpq    $3, %rax
         jnc     scalarloop
 
-// Since the scalar was forced to be a multiple of 8, we know it's even.
-// Hence there is no need to multiplex: the projective answer is (xn,zn)
-// and we can ignore (xm,zm); indeed we could have avoided the last three
-// differential additions and just done the doublings.
+// Multiplex directly into (xn,zn) then do three pure doubling steps;
+// this accounts for the implicit zeroing of the three lowest bits
+// of the scalar. On the very last doubling we *fully* reduce zn mod
+// p_25519 to ease checking for degeneracy below.
+
+        movq    swap, %rdx
+        testq   %rdx, %rdx
+        mux_4(xn,xm,xn)
+        mux_4(zn,zm,zn)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        cmadd_4(e,0x1db42,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        cmadd_4(e,0x1db42,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        cmadd_4(e,0x1db42,p,d)
+        mul_4(xn,s,d)
+        mul_p25519(zn,p,e)
+
+// The projective result of the scalar multiplication is now (xn,zn).
 // First set up the constant sn = 2^255 - 19 for the modular inverse.
 
         movq    $-19, %rax
@@ -1200,7 +1219,7 @@ fliploop:
         cmovzq  %rax, %rcx
         movq    %rcx, 344(%rsp)
 
-// Now the result is xn * (1/zn).
+// Now the result is xn * (1/zn), fully reduced modulo p.
 
         movq    res, %rbp
         mul_p25519(resx,xn,zm)
diff --git a/x86_att/curve25519/curve25519_x25519_alt.S b/x86_att/curve25519/curve25519_x25519_alt.S
index 1d132ab05..5abfaf018 100644
--- a/x86_att/curve25519/curve25519_x25519_alt.S
+++ b/x86_att/curve25519/curve25519_x25519_alt.S
@@ -78,10 +78,8 @@
 
 #define NSPACE (13*NUMSIZE)
 
-// Macros wrapping up the basic field operation calls
-// bignum_mul_p25519_alt and bignum_sqr_p25519_alt.
-// These two are only trivially different from pure
-// function calls to those subroutines.
+// Macro wrapping up the basic field operation bignum_mul_p25519_alt, only
+// trivially different from a pure function call to that subroutine.
 
 #define mul_p25519(P0,P1,P2)                    \
         movq    P1, %rax ;                      \
@@ -214,77 +212,91 @@
         movq    %r10, 0x10+P0 ;                  \
         movq    %r11, 0x18+P0
 
-#define sqr_p25519(P0,P1)                       \
+// A version of multiplication that only guarantees output < 2 * p_25519.
+// This basically skips the +1 and final correction in quotient estimation.
+
+#define mul_4(P0,P1,P2)                         \
         movq    P1, %rax ;                      \
-        mulq    %rax;                            \
+        mulq     P2;                 \
         movq    %rax, %r8 ;                         \
         movq    %rdx, %r9 ;                         \
         xorq    %r10, %r10 ;                        \
         xorq    %r11, %r11 ;                        \
         movq    P1, %rax ;                      \
-        mulq     0x8+P1;             \
-        addq    %rax, %rax ;                        \
-        adcq    %rdx, %rdx ;                        \
-        adcq    $0x0, %r11 ;                        \
+        mulq     0x8+P2;             \
+        addq    %rax, %r9 ;                         \
+        adcq    %rdx, %r10 ;                        \
+        movq    0x8+P1, %rax ;                  \
+        mulq     P2;                 \
         addq    %rax, %r9 ;                         \
         adcq    %rdx, %r10 ;                        \
         adcq    $0x0, %r11 ;                        \
         xorq    %r12, %r12 ;                        \
+        movq    P1, %rax ;                      \
+        mulq     0x10+P2;            \
+        addq    %rax, %r10 ;                        \
+        adcq    %rdx, %r11 ;                        \
+        adcq    %r12, %r12 ;                        \
         movq    0x8+P1, %rax ;                  \
-        mulq    %rax;                            \
+        mulq     0x8+P2;             \
         addq    %rax, %r10 ;                        \
         adcq    %rdx, %r11 ;                        \
         adcq    $0x0, %r12 ;                        \
-        movq    P1, %rax ;                      \
-        mulq     0x10+P1;            \
-        addq    %rax, %rax ;                        \
-        adcq    %rdx, %rdx ;                        \
-        adcq    $0x0, %r12 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     P2;                 \
         addq    %rax, %r10 ;                        \
         adcq    %rdx, %r11 ;                        \
         adcq    $0x0, %r12 ;                        \
         xorq    %r13, %r13 ;                        \
         movq    P1, %rax ;                      \
-        mulq     0x18+P1;            \
-        addq    %rax, %rax ;                        \
-        adcq    %rdx, %rdx ;                        \
-        adcq    $0x0, %r13 ;                        \
+        mulq     0x18+P2;            \
         addq    %rax, %r11 ;                        \
         adcq    %rdx, %r12 ;                        \
-        adcq    $0x0, %r13 ;                        \
+        adcq    %r13, %r13 ;                        \
         movq    0x8+P1, %rax ;                  \
-        mulq     0x10+P1;            \
-        addq    %rax, %rax ;                        \
-        adcq    %rdx, %rdx ;                        \
+        mulq     0x10+P2;            \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
         adcq    $0x0, %r13 ;                        \
+        movq    0x10+P1, %rax ;                 \
+        mulq     0x8+P2;             \
+        addq    %rax, %r11 ;                        \
+        adcq    %rdx, %r12 ;                        \
+        adcq    $0x0, %r13 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq     P2;                 \
         addq    %rax, %r11 ;                        \
         adcq    %rdx, %r12 ;                        \
         adcq    $0x0, %r13 ;                        \
         xorq    %r14, %r14 ;                        \
         movq    0x8+P1, %rax ;                  \
-        mulq     0x18+P1;            \
-        addq    %rax, %rax ;                        \
-        adcq    %rdx, %rdx ;                        \
-        adcq    $0x0, %r14 ;                        \
+        mulq     0x18+P2;            \
         addq    %rax, %r12 ;                        \
         adcq    %rdx, %r13 ;                        \
-        adcq    $0x0, %r14 ;                        \
+        adcq    %r14, %r14 ;                        \
         movq    0x10+P1, %rax ;                 \
-        mulq    %rax;                            \
+        mulq     0x10+P2;            \
+        addq    %rax, %r12 ;                        \
+        adcq    %rdx, %r13 ;                        \
+        adcq    $0x0, %r14 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq     0x8+P2;             \
         addq    %rax, %r12 ;                        \
         adcq    %rdx, %r13 ;                        \
         adcq    $0x0, %r14 ;                        \
         xorq    %r15, %r15 ;                        \
         movq    0x10+P1, %rax ;                 \
-        mulq     0x18+P1;            \
-        addq    %rax, %rax ;                        \
-        adcq    %rdx, %rdx ;                        \
-        adcq    $0x0, %r15 ;                        \
+        mulq     0x18+P2;            \
+        addq    %rax, %r13 ;                        \
+        adcq    %rdx, %r14 ;                        \
+        adcq    %r15, %r15 ;                        \
+        movq    0x18+P1, %rax ;                 \
+        mulq     0x10+P2;            \
         addq    %rax, %r13 ;                        \
         adcq    %rdx, %r14 ;                        \
         adcq    $0x0, %r15 ;                        \
         movq    0x18+P1, %rax ;                 \
-        mulq    %rax;                            \
+        mulq     0x18+P2;            \
         addq    %rax, %r14 ;                        \
         adcq    %rdx, %r15 ;                        \
         movl    $0x26, %esi ;                       \
@@ -313,25 +325,16 @@
         movq    %rdx, %r12 ;                        \
         adcq    %rcx, %r12 ;                        \
         shldq   $0x1, %r11, %r12 ;                    \
-        leaq    0x1(%r12), %rax ;                  \
-        movl    $0x13, %esi ;                       \
-        bts     $63, %r11 ;                         \
-        imulq   %rsi, %rax ;                        \
-        addq    %rax, %r8 ;                         \
-        adcq    %rcx, %r9 ;                         \
-        adcq    %rcx, %r10 ;                        \
-        adcq    %rcx, %r11 ;                        \
-        sbbq    %rax, %rax ;                        \
-        notq    %rax;                            \
-        andq    %rsi, %rax ;                        \
-        subq    %rax, %r8 ;                         \
-        sbbq    %rcx, %r9 ;                         \
-        sbbq    %rcx, %r10 ;                        \
-        sbbq    %rcx, %r11 ;                        \
-        btr     $63, %r11 ;                         \
-        movq    %r8, P0 ;                        \
-        movq    %r9, 0x8+P0 ;                    \
-        movq    %r10, 0x10+P0 ;                  \
+        btr     $0x3f, %r11 ;                      \
+        movl    $0x13, %edx ;                      \
+        imulq   %r12, %rdx ;                       \
+        addq    %rdx, %r8 ;                        \
+        adcq    %rcx, %r9 ;                        \
+        adcq    %rcx, %r10 ;                       \
+        adcq    %rcx, %r11 ;                       \
+        movq    %r8, P0 ;                       \
+        movq    %r9, 0x8+P0 ;                   \
+        movq    %r10, 0x10+P0 ;                 \
         movq    %r11, 0x18+P0
 
 // Multiplication just giving a 5-digit result (actually < 39 * p_25519)
@@ -567,23 +570,6 @@
         movq    %r10, 0x10+P0 ;                 \
         movq    %r11, 0x18+P0
 
-// Plain 4-digit add without any normalization
-// With inputs < p_25519 (indeed < 2^255) it still gives a 4-digit result
-
-#define add_4(P0,P1,P2)                         \
-        movq    P1, %rax ;                      \
-        addq    P2, %rax ;                      \
-        movq    %rax, P0 ;                      \
-        movq    8+P1, %rax ;                    \
-        adcq    8+P2, %rax ;                    \
-        movq    %rax, 8+P0 ;                    \
-        movq    16+P1, %rax ;                   \
-        adcq    16+P2, %rax ;                   \
-        movq    %rax, 16+P0 ;                   \
-        movq    24+P1, %rax ;                   \
-        adcq    24+P2, %rax ;                   \
-        movq    %rax, 24+P0
-
 // Add 5-digit inputs and normalize to 4 digits
 
 #define add5_4(P0,P1,P2)                        \
@@ -611,29 +597,32 @@
         movq   %r10, 0x10+P0 ;                  \
         movq   %r11, 0x18+P0
 
-// Subtraction of a pair of numbers < p_25519 just sufficient
-// to give a 4-digit result. It actually always does (x - z) + (2^255-19)
-// which in turn is done by (x - z) - (2^255+19) discarding the 2^256
-// implicitly
+// Modular addition with double modulus 2 * p_25519 = 2^256 - 38.
+// This only ensures that the result fits in 4 digits, not that it is reduced
+// even w.r.t. double modulus. The result is always correct modulo provided
+// the sum of the inputs is < 2^256 + 2^256 - 38, so in particular provided
+// at least one of them is reduced double modulo.
 
-#define sub_4(P0,P1,P2)                         \
+#define add_twice4(P0,P1,P2)                    \
         movq    P1, %r8 ;                       \
-        subq    P2, %r8 ;                       \
-        movq    8+P1, %r9 ;                     \
-        sbbq    8+P2, %r9 ;                     \
-        movq    16+P1, %r10 ;                   \
-        sbbq    16+P2, %r10 ;                   \
-        movq    24+P1, %rax ;                   \
-        sbbq    24+P2, %rax ;                   \
-        subq    $19, %r8 ;                         \
+        xorl    %ecx, %ecx ;                       \
+        addq    P2, %r8 ;                       \
+        movq    0x8+P1, %r9 ;                   \
+        adcq    0x8+P2, %r9 ;                   \
+        movq    0x10+P1, %r10 ;                 \
+        adcq    0x10+P2, %r10 ;                 \
+        movq    0x18+P1, %r11 ;                 \
+        adcq    0x18+P2, %r11 ;                 \
+        movl    $38, %eax ;                        \
+        cmovncq %rcx, %rax ;                       \
+        addq    %rax, %r8 ;                        \
+        adcq    %rcx, %r9 ;                        \
+        adcq    %rcx, %r10 ;                       \
+        adcq    %rcx, %r11 ;                       \
         movq    %r8, P0 ;                       \
-        sbbq    $0, %r9 ;                          \
-        movq    %r9, 8+P0 ;                     \
-        sbbq    $0, %r10 ;                         \
-        movq    %r10, 16+P0 ;                   \
-        sbbq    $0, %rax ;                         \
-        btc     $63, %rax ;                        \
-        movq    %rax, 24+P0
+        movq    %r9, 0x8+P0 ;                   \
+        movq    %r10, 0x10+P0 ;                 \
+        movq    %r11, 0x18+P0
 
 // Modular subtraction with double modulus 2 * p_25519 = 2^256 - 38
 
@@ -779,23 +768,22 @@ S2N_BN_SYMBOL(curve25519_x25519_alt):
 
         movq    %rdi, res
 
-// Copy the inputs to the local variables while mangling them:
+// Copy the inputs to the local variables with minimal mangling:
 //
-//  - The scalar gets turned into 01xxx...xxx000 by tweaking bits.
-//    Actually the top zero doesn't matter since the loop below
-//    never looks at it, so we don't literally modify that.
+//  - The scalar is in principle turned into 01xxx...xxx000 but
+//    in the structure below the special handling of these bits is
+//    explicit in the main computation; the scalar is just copied.
 //
-//  - The point x coord is reduced mod 2^255 *then* mod 2^255-19
+//  - The point x coord is reduced mod 2^255 by masking off the
+//    top bit. In the main loop we only need reduction < 2 * p_25519.
 
         movq    (%rsi), %rax
-        andq    $~7, %rax
         movq    %rax, (%rsp)
         movq    8(%rsi), %rax
         movq    %rax, 8(%rsp)
         movq    16(%rsi), %rax
         movq    %rax, 16(%rsp)
         movq    24(%rsi), %rax
-        bts     $62, %rax
         movq    %rax, 24(%rsp)
 
         movq    (%rdx), %r8
@@ -803,70 +791,57 @@ S2N_BN_SYMBOL(curve25519_x25519_alt):
         movq    16(%rdx), %r10
         movq    24(%rdx), %r11
         btr     $63, %r11
-        movq    $19, %r12
-        xorq    %r13, %r13
-        xorq    %r14, %r14
-        xorq    %r15, %r15
-        addq    %r8, %r12
-        adcq    %r9, %r13
-        adcq    %r10, %r14
-        adcq    %r11, %r15
-        btr     $63, %r15 // x >= 2^255 - 19 <=> x + 19 >= 2^255
-        cmovcq  %r12, %r8
         movq    %r8, 32(%rsp)
-        cmovcq  %r13, %r9
         movq    %r9, 40(%rsp)
-        cmovcq  %r14, %r10
         movq    %r10, 48(%rsp)
-        cmovcq  %r15, %r11
         movq    %r11, 56(%rsp)
 
-// Initialize (xn,zn) = (1,0) and (xm,zm) = (x,1) with swap = 0
-// We use the fact that the point x coordinate is still in registers
+// Initialize with explicit doubling in order to handle set bit 254.
+// Set swap = 1 and (xm,zm) = (x,1) then double as (xn,zn) = 2 * (x,1).
+// We use the fact that the point x coordinate is still in registers.
+// Since zm = 1 we could do the doubling with an operation count of
+// 2 * S + M instead of 2 * S + 2 * M, but it doesn't seem worth
+// the slight complication arising from a different linear combination.
 
-        movq    $1, %rax
-        movq    %rax, 320(%rsp)
-        movq    %rax, 96(%rsp)
-        xorl    %eax, %eax
+        movl    $1, %eax
         movq    %rax, swap
-        movq    %rax, 160(%rsp)
-        movq    %rax, 328(%rsp)
-        movq    %rax, 104(%rsp)
-        movq    %rax, 168(%rsp)
-        movq    %rax, 336(%rsp)
-        movq    %rax, 112(%rsp)
-        movq    %rax, 176(%rsp)
-        movq    %rax, 344(%rsp)
-        movq    %rax, 120(%rsp)
-        movq    %rax, 184(%rsp)
-        movq    32(%rsp), %rax
         movq    %r8, 256(%rsp)
+        movq    %rax, 96(%rsp)
+        xorl    %eax, %eax
         movq    %r9, 264(%rsp)
+        movq    %rax, 104(%rsp)
         movq    %r10, 272(%rsp)
+        movq    %rax, 112(%rsp)
         movq    %r11, 280(%rsp)
+        movq    %rax, 120(%rsp)
+
+        sub_twice4(d,xm,zm)
+        add_twice4(s,xm,zm)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        cmadd_4(e,0x1db42,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
 
-// The outer loop over scalar bits from i = 254, ..., i = 0 (inclusive)
-// This starts at 254, and so implicitly masks bit 255 of the scalar.
+// The main loop over unmodified bits from i = 253, ..., i = 3 (inclusive).
+// This is a classic Montgomery ladder, with the main coordinates only
+// reduced mod 2 * p_25519, some intermediate results even more loosely.
 
-        movl    $254, %eax
+        movl    $253, %eax
         movq    %rax, i
 
 scalarloop:
 
 // sm = xm + zm; sn = xn + zn; dm = xm - zm; dn = xn - zn
-// The adds don't need any normalization as they're fed to muls
-// Just make sure the subs fit in 4 digits.
 
-        sub_4(dm,xm,zm)
-        add_4(sn,xn,zn)
-        sub_4(dn,xn,zn)
-        add_4(sm,xm,zm)
+        sub_twice4(dm,xm,zm)
+        add_twice4(sn,xn,zn)
+        sub_twice4(dn,xn,zn)
+        add_twice4(sm,xm,zm)
 
-// ADDING: dmsn = dm * sn; dnsm = sm * dn
 // DOUBLING: mux d = xt - zt and s = xt + zt for appropriate choice of (xt,zt)
 
-        mul_5(dmsn,sn,dm)
-
         movq    i, %rdx
         movq    %rdx, %rcx
         shrq    $6, %rdx
@@ -875,22 +850,25 @@ scalarloop:
         andq    $1, %rdx
         cmpq    swap, %rdx
         movq    %rdx, swap
-
         mux_4(d,dm,dn)
         mux_4(s,sm,sn)
 
+// ADDING: dmsn = dm * sn; dnsm = sm * dn
+
+        mul_5(dmsn,sn,dm)
+
         mul_5(dnsm,sm,dn)
 
-// DOUBLING: d = (xt - zt)^2 normalized only to 4 digits
+// DOUBLING: d = (xt - zt)^2
 
         sqr_4(d,d)
 
 // ADDING: dpro = (dmsn - dnsm)^2, spro = (dmsn + dnsm)^2
-// DOUBLING: s = (xt + zt)^2, normalized only to 4 digits
+// DOUBLING: s = (xt + zt)^2
 
         sub5_4(dpro,dmsn,dnsm)
-        sqr_4(s,s)
         add5_4(spro,dmsn,dnsm)
+        sqr_4(s,s)
         sqr_4(dpro,dpro)
 
 // DOUBLING: p = 4 * xt * zt = s - d
@@ -899,7 +877,7 @@ scalarloop:
 
 // ADDING: xm' = (dmsn + dnsm)^2
 
-        sqr_p25519(xm,spro)
+        sqr_4(xm,spro)
 
 // DOUBLING: e = 121666 * p + d
 
@@ -907,28 +885,63 @@ scalarloop:
 
 // DOUBLING: xn' = (xt + zt)^2 * (xt - zt)^2 = s * d
 
-        mul_p25519(xn,s,d)
-
-// ADDING: zm' = x * (dmsn - dnsm)^2
-
-        mul_p25519(zm,dpro,pointx)
+        mul_4(xn,s,d)
 
 // DOUBLING: zn' = (4 * xt * zt) * ((xt - zt)^2 + 121666 * (4 * xt * zt))
 //               = p * (d + 121666 * p)
 
-        mul_p25519(zn,p,e)
+        mul_4(zn,p,e)
+
+// ADDING: zm' = x * (dmsn - dnsm)^2
 
-// Loop down as far as 0 (inclusive)
+        mul_4(zm,dpro,pointx)
+
+// Loop down as far as 3 (inclusive)
 
         movq    i, %rax
         subq    $1, %rax
         movq    %rax, i
+        cmpq    $3, %rax
         jnc     scalarloop
 
-// Since the scalar was forced to be a multiple of 8, we know it's even.
-// Hence there is no need to multiplex: the projective answer is (xn,zn)
-// and we can ignore (xm,zm); indeed we could have avoided the last three
-// differential additions and just done the doublings.
+// Multiplex directly into (xn,zn) then do three pure doubling steps;
+// this accounts for the implicit zeroing of the three lowest bits
+// of the scalar. On the very last doubling we *fully* reduce zn mod
+// p_25519 to ease checking for degeneracy below.
+
+        movq    swap, %rdx
+        testq   %rdx, %rdx
+        mux_4(xn,xm,xn)
+        mux_4(zn,zm,zn)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        cmadd_4(e,0x1db42,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        cmadd_4(e,0x1db42,p,d)
+        mul_4(xn,s,d)
+        mul_4(zn,p,e)
+
+        sub_twice4(d,xn,zn)
+        add_twice4(s,xn,zn)
+        sqr_4(d,d)
+        sqr_4(s,s)
+        sub_twice4(p,s,d)
+        cmadd_4(e,0x1db42,p,d)
+        mul_4(xn,s,d)
+        mul_p25519(zn,p,e)
+
+// The projective result of the scalar multiplication is now (xn,zn).
 // First set up the constant sn = 2^255 - 19 for the modular inverse.
 
         movq    $-19, %rax
@@ -1366,7 +1379,7 @@ fliploop:
         cmovzq  %rax, %rcx
         movq    %rcx, 344(%rsp)
 
-// Now the result is xn * (1/zn).
+// Now the result is xn * (1/zn), fully reduced modulo p.
 
         movq    res, %rbp
         mul_p25519(resx,xn,zm)
